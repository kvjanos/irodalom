<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">



  <title>
    Ground Truth Dataset and Benchmarks for Mid-Level Vision
  </title>
</head>
<body style="margin: 20px; width: 500px;">

<img style="float: right;" src="labeledchair.png" width="150" border="0">
<h1>Ground Truth Dataset and Benchmarks for Mid-Level Vision</h1>

<h3>People</h3>

<div style="margin-left: 20px;">
  <a href="http://vision.bc.edu/%7Edmartin/"><img src="dmartin.jpg" border="0" height="100"></a>
  <a href="http://vision.bc.edu/%7Edmartin/">David Martin</a> (PI)
</div>

<h3>Sponsor</h3>

<div style="margin-left: 20px;">
  <a href="http://www.nsf.gov/">National Science Foundation</a>
  <br>
  <a href="http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=5262">NSF 05-579 CAREER Program</a> 
  <br>
  <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=0643887">Award IIS-0643887</a>
</div>

<h3>Abstract</h3>

<div style="margin-left: 20px;">
<p>Machine vision systems can now do amazing things: Reading irises and
faces, helping to drive autonomous cars in real environments, locating
and measuring anatomical structures in medical scans -- these are just
a few examples of capabilities that have emerged in recent years.
Special-purpose domains still mark the limit of our success, however.
The goal of human-level machine vision is still out of reach because
the solutions found to these problems do not require the machine to
understand the rich structure of visual information.

</p><p>It is essential to take an empirical approach to the problem of visual
perception.  The primary goal of this project is to build a dataset of
ground truth image annotations that provides the perception of scenes
at the level of surfaces, objects, and basic 3D scene geometry.  This
dataset will be unprecedentedly rich and detailed, providing precisely
the information and representations needed to bring general purpose
capabilities to machine vision systems.  A secondary goal of this
project is to create the associated benchmarks and methodologies for
evaluating machine systems with respect to the ground truth data.

</p><p><i>Intellectual Merit</i>: At the heart of this proposal is the design of a
dataset for mid-level vision.  The mid-level representation of visual
information proposed in this project is of fundamental importance,
because there is currently no viable mid-level representation in
machine vision.  A good mid-level representation is both computable
from images as well as useful for higher level tasks.  A generic,
concrete, and testable mid-level representation is perhaps the most
important deliverable of the proposed project.

</p><p><i>Broader Impact</i>: The proposed project will have broad impact on the
machine vision and human vision research communities.  Machine vision
models require complex ground truth data for training and benchmarks
for evaluation, while psychophysics modelers face the challenge of
data from natural images.  Additionally, this project will make all its
data and tools freely available to the research community.  In
general, this is an exciting time for machine vision.  We are at the
threshold of building machines that attain human-level visual
perception, which would dramatically alter the relationship between
people and machines.  With datasets targeting the strategic research
problems, significant progress is at hand.
</p></div>

<h3>Publications</h3>

<div style="margin-left: 20px;">
<font size="-1">TBA</font>
</div>

<h3>Downloads</h3>

<div style="margin-left: 20px;">
<font size="-1">TBA</font>
</div>

<h3>Related Links</h3>

<table border="0">
  <tbody><tr>
    <td align="right">
      <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/">
        <img src="163085-image.jpg" border="0" height="125"><br>
        <img src="163085-outlines.jpg" border="0" height="125">
      </a>
    </td>
    <td>
      <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/">
        Berkeley Segmentation Dataset and Benchmark
      </a>
    </td>
  </tr>
  <tr>
    <td align="right">
      <a href="http://labelme.csail.mit.edu/">
        LabelMe: The open image labeling tool
      </a>
    </td>
    <td>
      <a href="http://labelme.csail.mit.edu/">
        <img src="labelme-logo.gif" border="0"><br>
        <img src="labelme-car.gif" border="0">
        </a>
    </td>
  </tr>
  <tr>
    <td align="right">
      <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html">
        <img src="caltech101-mean.jpg" border="0">
      </a>
    </td>
    <td>
      <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/Caltech101.html">
        Caltech 101
      </a>
    </td>
  </tr>
  <tr>
    <td align="right">
      <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">
        Caltech 256
      </a>
    </td>
    <td>
      <a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">
        <img src="caltech256-logo.jpg" border="0" height="200">
      </a>
    </td>
  </tr>
  <tr>
    <td align="right">
      <a href="http://www.lotushill.org/LHIFrameEn.html">
        <img src="lotus.jpg" border="0" height="200">
      </a>
    </td>
    <td>
      <a href="http://www.lotushill.org/LHIFrameEn.html">
        Lotus Hill Research Institute
      </a>
    </td>
  </tr>
  <tr>
    <td align="right">
      <a href="http://www.pascal-network.org/challenges/VOC/">
        PASCAL Visual Object Classes
      </a>
    </td>
    <td>
      <a href="http://www.pascal-network.org/challenges/VOC/">
        <img src="pascal-voc-cat_06.jpg" border="0" height="200">
      </a>
    </td>
  </tr>
  <tr>
    <td align="right">
      <a href="http://yann.lecun.com/exdb/mnist/">
        <img src="mnist-digits.png" border="0">
      </a>
    </td>
    <td>
      <a href="http://yann.lecun.com/exdb/mnist/">
        MNHIST Dataset of Handwritten Digits
      </a>
    </td>
  </tr>
</tbody></table>

</body>
</html>
