Journal of Neuroscience Methods 170 (2008) 165–178

Associative image analysis: A method for automated quantiﬁcation of 3D multi-parameter images of brain tissue
Christopher S. Bjornsson a,c,1 , Gang Lin b,1 , Yousef Al-Kofahi b,1 , Arunachalam Narayanaswamy b , Karen L. Smith a , William Shain a , Badrinath Roysam b,∗
Center for Neural Communication Technology, Wadsworth Center, New York State Department of Health, PO Box 509, Albany, NY 12201-0509, United States b Department of Electrical, Computer and Systems Engineering & Center for Subsurface Sensing & Imaging Systems, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY 12180-3590, United States c Biotechnology Center, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY 12180-3590, United States Received 15 June 2007; received in revised form 6 December 2007; accepted 27 December 2007
a

Abstract Brain structural complexity has confounded prior efforts to extract quantitative image-based measurements. We present a systematic ‘divide and conquer’ methodology for analyzing three-dimensional (3D) multi-parameter images of brain tissue to delineate and classify key structures, and compute quantitative associations among them. To demonstrate the method, thick (∼100 m) slices of rat brain tissue were labeled using three to ﬁve ﬂuorescent signals, and imaged using spectral confocal microscopy and unmixing algorithms. Automated 3D segmentation and tracing algorithms were used to delineate cell nuclei, vasculature, and cell processes. From these segmentations, a set of 23 intrinsic and 8 associative image-based measurements was computed for each cell. These features were used to classify astrocytes, microglia, neurons, and endothelial cells. Associations among cells and between cells and vasculature were computed and represented as graphical networks to enable further analysis. The automated results were validated using a graphical interface that permits investigator inspection and corrective editing of each cell in 3D. Nuclear counting accuracy was >89%, and cell classiﬁcation accuracy ranged from 81 to 92% depending on cell type. We present a software system named FARSIGHT implementing our methodology. Its output is a detailed XML ﬁle containing measurements that may be used for diverse quantitative hypothesis-driven and exploratory studies of the central nervous system. © 2008 Elsevier B.V. All rights reserved.
Keywords: Multi-spectral confocal microscopy; Associative measurements; Automated image analysis; Nuclear segmentation; Glial process tracing; Cell classiﬁcation; Neurovascular mapping; Brain cell mapping

1. Introduction Brain tissue has a complex three-dimensional (3D) architecture containing multiple neuronal and glial cell types, immune system cells, and vascular elements (Bear, 2006). This complexity presents a fundamental challenge to studies that require quantitative image-based measurements at the tissue scale. Examples of such studies include quantifying the location and

Corresponding author at: Department of Electrical, Computer and Systems Engineering, NSF Center for Subsurface Sensing & Imaging Systems, Rensselaer Center for Open-Source Software, JEC 7010, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY 12180-3590, United States. Tel.: +1 518 276 8067; fax: +1 518 276 8715. E-mail address: roysam@ecse.rpi.edu (B. Roysam). 1 These authors contributed equally to this work. 0165-0270/$ – see front matter © 2008 Elsevier B.V. All rights reserved. doi:10.1016/j.jneumeth.2007.12.024

∗

distribution of cells in the neural stem-cell niche (Fuchs et al., 2004) and the cancer stem cell microenvironment (Calabrese et al., 2007), cell relationships associated with the blood–brain barrier (Iadecola, 2004), and changes in cell associations with injury or disease (Spataro et al., 2005). Our methodology for describing the cellular organization in the brain is based on 3D spectral confocal microscopy, and computational image analysis (Pawley, 2006). By careful choice of ﬂuorescent probes, it is now possible to image multiple molecular species simultaneously while preserving their spatial relationships. The advent of spectrally resolved confocal microscopes, such as the Zeiss LSM META, allows a 32-point emission spectrum to be recorded at each point in the image. These data can now be processed with linear unmixing software provided by the microscope manufacturer to compute a smaller number of essentially ‘pure’ channels containing negligible

166

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

Fig. 1. Maximum intensity projection of a ﬁve-channel 3D image collected from a 100 m-thick section of rat hippocampus. The microscope collects a 32-point emission spectrum at each point. After linear unmixing, separate channels were obtained computationally for (a) CyQuant-labeled cell nuclei (cyan; small arrow head points to a pyramidal cell nucleus; the large arrowhead points to an endothelial cell nucleus). (b) NeuroTrace-labeled Nissl substance (purple). (c) Iba1-labeled

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

167

crosstalk from other ﬂuorophores (Dickinson et al., 2001; Garini et al., 2006; Zimmerman, 2005). In summary, it is now possible to acquire (i) sufﬁciently large 3D images to permit extensive measurements of cell-to-cell relationships and (ii) sufﬁciently large number of imaging channels to provide independent labeling of all the components under study (Lin et al., 2005a; Seymour and Kipke, 2007; Spataro et al., 2005). In this report, we present 3D images with ﬁve channels corresponding to cell nuclei, neurons, microglia, astrocytes, and blood vessels (see Fig. 1). Once such large 3D multi-channel images of brain tissue are acquired, there is a need to extract meaningful quantitative measurements. Traditionally, this is performed by computerassisted manual analysis, relying on human pattern recognition and computerized data recording using software tools such as Metamorph (Molecular devices, Sunnyvale, CA), Volocity (Improvision Inc., Waltham, MA), ImageJ (NIH), and Neurolucida (MBF Biosciences). Among manual methods, two important approaches exist: object-based, and ﬂuorescence intensity-based. An example of the former is stereology, as implemented in tools like Stereo Investigator (MBF Biosciences, Williston, VT). In this method, the user counts and/or delineates a random subset of objects guided by unbiased stereological principles. Fluorescence intensity-based methods utilize the overall signal intensity for quantiﬁcation without explicitly delineating structures (e.g., Biran et al., 2007; Zhong and Bellamkonda, 2007). Stereological methods suffer from three major limitations: (i) extensive manual sampling of objects is required to reduce the variance, obviating the advantages of subsampling; (ii) they assume that the tissue is homogeneous, but the brain histology is not homogeneous; and (iii) they are inherently limited in their ability to cope with multi-dimensional data. Fluorescence intensity-based methods suffer from lack of structural information, and the fact that the ﬂuorescence signal is an unreliable indicator of molecular concentration since it is affected by specimen preparation, and imaging artifacts. Overall, manual methods suffer from slowness, cost, tedium, subjectivity, lack of steadiness in tracing structures, limited attention span, limitations of the human visual system that can only perform binocular stereo viewing rather than volumetric viewing, and that scoring methods are limited to the two-dimensional (2D) computer screen. Finally, manual methods are inherently limited in their ability to analyze associations among structures—their greatest limitation. Automated algorithms have been developed to delineate individual elements of neuroanatomy, e.g., counting of nuclei, tracing neuronal processes, branching pattern of dendrites for a single neuron, and vascular tracing (Al-Kofahi et al., 2003, 2008; Fernandez-Gonzalez et al., 2002; Lin et al., 2005a, 2007; Tyrrell et al., 2007; Weaver et al., 2004; Zhang et al., 2007). These tools are able to delineate a speciﬁc class of structures from single

channel images with error rates in the range of 5–10%, and have progressed greatly in their ability to handle morphological variability. Since 100% automation is beyond the current state of the art, we accept these tools to be automated, and introduce an efﬁcient inspection and editing method that can allow the investigator to further reduce the error as necessary. Next, considering that brain tissue has a diverse architecture with complex networks of relationships, two major needs exist. First, there is a need to apply multiple segmentation tools simultaneously to analyze multi-parameter datasets. Second, there is a need to develop comprehensive and broadly applicable methods to relate the measurements from multiple segmentations in a biologically meaningful manner. We propose a ‘divide and conquer’ strategy to tackle the complexity of multi-channel, 3D image data (see Fig. 2). This strategy uses commercially available linear unmixing software to divide the 32-channel spectral image data into a small number of non-overlapping channels (for a thorough discussion of this method see Dickinson et al., 2001; Garini et al., 2006; Zimmerman, 2005). Each channel has a much lower morphological diversity compared to the overall tissue, making it feasible to segment (delineate) the structures in each channel using separate previously developed algorithms, each specialized to one morphological category. Once the main structures in each channel are segmented, inspected, and edited to achieve acceptable accuracy, a rich set of measurements can be computed. We consider two classes of measurements: intrinsic and associative. Intrinsic measurements quantify aspects of objects within an individual channel, including the spatial locations, sizes, morphologies of cellular compartments, lengths and branching patterns of processes, width variations of vessels, and areas of membranous surfaces. Methods for computing intrinsic measurements are widely described (see Sonka et al., 2008). Associative measurements a central contribution of this work. They quantify relationships among two or more structures identiﬁed by segmentation. The importance of associative measurements cannot be overstated—they are essential to developing a system-level understanding of brain tissue. The objects being associated may arise from a single channel (e.g., two cell nuclei), or different channels (e.g., nuclei and vessels). Precursors to our concept of associative measurements include 3D-catFISH for quantifying immediate-early gene transcription (Guzowski et al., 2005), and 3D quantiﬁcation of the neurovascular unit (Lin et al., 2005a). Given the large number of constituents of brain tissue, a combinatorial number of associations are possible. However, a much smaller set of associations are biologically relevant, and immediately useful. In this paper, we focus on associations that are based on the broadly relevant notions of spatial proximity and adjacency. Other important associations, such as synaptic connectivity and neuronal networks, are not addressed here.

microglia (yellow). (d) GFAP-labeled astrocytes (red). (e) EBA-labeled blood vessels (green). (f) Combined projection illustrating a rich dataset describing the position and morphology of the various nuclei and cell types and vasculature. (g) Excitation spectra. The vertical lines indicate the laser lines used for excitation (stars). (h) Emission spectra. Careful optimization was required to achieve the full dynamic range for each of the ﬁve ﬂuorophores. Linear unmixing was able to separate each ﬂuorophore despite considerable overlap in emission spectra. Scale bar = 50 m. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of the article.)

168

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

Fig. 2. Flowchart summary of the main image processing steps. The Zeiss LSM META microscope acquires a 32-point emission spectrum at each voxel. These data are spectrally unmixed to yield a ﬁve-channel 3D image. Separate algorithms are used to segment nuclei and vessel surfaces, and to trace processes of astrocytes and microglia. Intrinsic and associative measurements are computed after the segmentations are validated. The initial unsupervised classiﬁcation produces a training set that is inspected and edited to compute a SVM classiﬁer that generates the ﬁnal classiﬁcations. The ﬁnal results are saved as an XML ﬁle for further analysis.

Once a set of associative measurements is computed, an important issue is how to represent them. Traditional twodimensional representations such as tables and charts are often inadequate to capture the information in the network of associative measurements describing the 3D multi-channel data. We have adopted attributed graphs as an effective representation due to their many advantages. First, they are supported by graph theory, a well-studied mathematical discipline that is speciﬁcally intended for abstract analysis of relationships (Gross and Yellen, 2005). Second, general-purpose software tools are available for visualizing and analyzing graphs (e.g., Graphviz, www.graphviz.org). An attributed graph is composed of nodes representing objects, and links representing associations among objects. Attached to each node is a list of intrinsic measurements (attributes) of the object, and attached to each link is a list of associative measurements quantifying the relationship. In our work, the graph data structure is represented as an extensible markup language (XML) ﬁle. Once an attributed graph is constructed, it can be queried to answer speciﬁc questions. The main advantage of our strategy is ﬂexibility, generality, and the potential for future expansion. We present examples illustrating the power of this strategy. This paper introduces a divide-and-conquer segmentation strategy, and the concepts of associative image analysis for 3D multi-parameter images. It presents FARSIGHT, a software system that implements these strategies. There are several features that make FARSIGHT a versatile tool for multi-parameter image analysis: (i) it permits efﬁcient investigator validation of the results; (ii) the code is able to handle morphological diversity; and (iii) its modular design will enable adaptation to other applications, and future extensions. Thus FARSIGHT provides a systematic and methodical strategy for making quantitative measurements of complex tissue-level image data. It is broadly applicable for quantifying the organization of brain tissue, and changes in organization during development, or following disease, injury, application of a pharmaceutical agent, or aging.

2. Materials and methods 2.1. Specimen preparation and imaging The Wadsworth Center Institutional Animal Care and Use Committee (IACUC) approved all animal procedures. Three adult male Sprague–Dawley rats were anesthetized with a ketamine/xylazine mixture, and transcardially perfused with 200 mL warm (37 ◦ C) phosphate buffered saline (PBS) followed by 200 mL 4% paraformaldehyde in PBS using a constantpressure system (Olsen, 1985). Brains were removed and immersion ﬁxed in 4% paraformaldehyde for an additional 24 h, then washed in HEPES-buffered Hanks’ saline (HBHS) containing azide (90 mg/L). Horizontal 100- m-thick tissue slices were cut using a vibratome (FHC, Inc.). Sections 1600–1800 m down from the dorsal surface of the brain were used for this study. Brieﬂy, histochemistry was performed using the following procedure. Tissue slices were incubated in 5 mg/mL NaBH4 , washed in HBHS, incubated in 5% BSA in HBHS, and then incubated overnight in primary antibodies identifying astrocytes (chicken anti-GFAP, Chemicon), microglia (rabbit anti-Iba1, Wako), and vasculature (mouse anti-EBA, Sternberger Monoclonals Inc., Lutherville, MA). After extensive washing, sections were incubated overnight in secondary antibodies (Alexa 405 anti-mouse, Alexa 568 anti-rabbit, Alexa 633 anti-chicken, Invitrogen, Eugene OR). CyQuant (Invitrogen) and 530/615 NeuroTrace (Invitrogen) were added to this cocktail to stain nuclei and Nissl substance. Following extensive washing, sections were mounted in ProLong Gold (Invitrogen) for confocal imaging. Generally, these immunohistochemical methods provide for uniform labeling through the entire slice (Lin et al., 2005a; Spataro et al., 2005; Szarowski et al., 2003; Turner et al., 1999). This uniformity depends in part on the antibodies used; for this study the Iba1 and EBA antibodies demonstrated uniform labeling throughout the slice, while the GFAP antibody only labeled structures up to 25 m from the section edges.

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178 Table 1 Results of manual validation studies over three datasets

169

Data were tabulated from the automated output (A) and after validation (V). The ﬁrst two rows of the table report automated vs. manual counts for the three data sets. The lower ﬁve rows report the actual number of cell classiﬁcation errors for each data set and for each cell type. The last row reports the classiﬁcation errors as a percentage of the total population of validated counts (V) for each data set.

Spectral imaging was performed on a Zeiss LSM 510 META system using emissions from four lasers (405 nm, 488 nm, 568 nm, and 633 nm) and a 25 × W/0.8 N.A. apochromat objective. Two images from hippocampus and one from cingulate cortex were used for this study. The ﬁrst hippocampus image was 1024 × 1024 pixels; the other two were cropped to 512 × 512 pixels (cropped regions are shown in Table 1b and c). z-Series image stacks were collected as 1.7 m optical sections as determined by the depth of focus of the objective. The stage was moved in 1.5 m steps to capture the entire thickness of each tissue slice. Thus the voxel resolution for all images was 0.36 m × 0.36 m × 1.5 m along the x, y, and z axes, respectively. Emission from each ﬂuorophore was balanced to achieve the full dynamic range for the ﬁve ﬂourophores. Using the spectral ﬁngerprinting software available on the Zeiss META system, 32-point spectral ﬁngerprints were captured for each ﬂuorophore, and the built-in linear unmixing software was used to isolate the signals into a set of ﬁve individual channels (see Fig. 1) labeling: (i) cell nuclei (CyQuant); (ii) astrocytes (GFAP); (iii) neurons (Nissl); (iv) blood vessels (EBA); and (v) microglia (Iba1). No background subtraction was used. One advantage of our choice of labels is that they were all separated spatially, making evaluation of the unmixing process straightforward. Through-focus projections were used for visual presentation. Additionally, visualization of the 3D data

is presented as three movies of rotating datasets (Electronic Supplement A). The image analysis algorithms work directly on these data as described in the following paragraphs. 2.2. Automated multi-channel segmentation Our divide and conquer segmentation strategy is illustrated in Fig. 2. Fist, the CyQuant-labeled cell nuclei were segmented using a previously reported algorithm (Lin et al., 2007), and the results are shown in Fig. 3a. Brieﬂy, the images were de-noised using a median ﬁlter (width = 5), followed by morphological opening and closing operations. The kernels for both morphological operations were ellipsoids with a major axis of width w1 = 5, and minor axis of width w2 = max{w1 × (dxy /dz ), 3}, where dxy and dz were the voxel sizes along the lateral and axial directions, respectively. The resulting image was adaptively binarized. A 3D distance map was computed on this binarized image. Combining this map with the image intensity gradient, a gradient-weighted distance map was computed (Lin et al., 2003). The 3D watershed algorithm was used on the gradientweighted distance map to break up the CyQuant channel image into nuclear fragments. A model-based merging algorithm assembled these fragments into image objects representing nuclei (Lin et al., 2005b). For each object, a conﬁdence metric was computed reﬂecting the quality of segmentation, repre-

170

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

Fig. 3. Automated segmentation results for individual channels, and computation of some associative features. All images are 2D projections of 3D datasets. The ﬁeld is ∼195 m wide for the lower-magniﬁcation representations (a–c and e). (a) Nuclear segmentation from CyQuant channel. Each segmented (but unclassiﬁed) nucleus is assigned a unique identiﬁcation number. (b) Automated tracing of microglial processes and segmentation of cell bodies from Iba1 channel, shown in yellow over the image data that is shown in grayscale. (c) Automated segmentation of the vasculature from EBA channel. (d) Enlarged view of a representative segment (red box in c). The triangulated 3D mesh representation of the vessel surface can be seen. (e) Automated traces of astrocyte processes from GFAP channel displayed in red over a grayscale representation of the image data. (f) Enlarged view of a representative region (blue box in e). The calculated convergence point of the GFAP cytoskeleton for one cell is indicated with a green cross. (g) Illustrates the zone around a segmented nucleus for summing the Iba1 signal for identifying microglial nuclei. (h) Illustrates the zone surrounding a different segmented nucleus for identifying NeutoTrace-positive nuclei. (i) Illustrates the computation of a segmented nucleus’ proximity to a vessel segment for identifying endothelial cells. (j) Illustrates the proximity of the convergence point of astrocyte processes (green cross) to a segmented nucleus for identifying astrocytes. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of the article.)

sented by the goodness-of-ﬁt of each object to pre-computed object models. The conﬁdence score computation described in Lin et al. (2003) was carried out as follows. After segmentation, a vector of m features X = (x1 , x2 , . . ., xm ) representing intensity

and morphological properties of each object are computed. An m-dimensional Gaussian model is ﬁtted to the set of feature vectors. The mean and covariance of that model are estimated from a training set C, which is a subset of the segmented objects

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

171

in the image. The conﬁdence score for any given object y with feature vector X equals the probability that the object feature vector X ﬁts the Gaussian model, as follows: Sy = 1 (2πm/2 |
X

the unit vectors as (Chikuse, 2003; Watson, 1983): σθ = 1 − 1 K
K

u(i) ,
i=1

0 ≤ σθ ≤ 1.

|1/2 )

exp −

1 2

¯ (X − X)

T

−1 X

¯ (X − X) .

Our datasets contain large numbers of nuclei (e.g., the dataset in Fig. 1 contains 1019 nuclei). Validating these large-scale automatic segmentations is a major task. To reduce the effort, a software tool was developed that permits manual inspection, and corrective editing of the results. The purpose of the conﬁdence score is to rank order all the segmented objects to prioritize inspection and validation. Thus, a validated segmentation and a set of measurements (features) are produced for each nucleus (detailed in Electronic Supplement B). The processes of microglia (Iba1) were traced using an automated algorithm described earlier (Abdul-Karim et al., 2003; Al-Kofahi et al., 2003), and are displayed in yellow in Fig. 3b. Brieﬂy, the algorithm searches the image over a coarse grid (12-voxel spacing) looking for intersections with processes with high sensitivity, but low speciﬁcity. This produces a set of seed locations starting from which processes were traced recursively ﬁtting low-pass differentiator templates (lengths 8–18 voxels, set adaptively). The tracing algorithm was designed to follow along points in the image that exhibit a pair of parallel edges in steps (2–10 voxels, set adaptively). This algorithm also generates a segmentation of the soma. Blood vessel walls (EBA-labeled objects) were segmented using a novel robust surface segmentation algorithm, and are shown in Fig. 3c. Additional details of this methodology are presented in Electronic Supplement C. Brieﬂy, we conducted a robust hypothesis test (Mahadevan et al., 2004; Wilcox, 1997) over small 7 × 7 × 3 voxel windows centered on a sparse grid of points (1:9 sampling to reduce computation) in the image data. The alternate hypothesis detects the presence of a membrane signal at the center of the window exceeding the background. We then linked the detected points into a triangulated 3D mesh data structure using a regularized isosurface extraction algorithm (Treece et al., 1999). This mesh is smoothed using a volumeconserving algorithm (Kuprat et al., 2001; Chan and Purisima, 1998). The end result is a triangulated mesh representation of the surface. An enlarged view of the mesh representation is shown in Fig. 3d. The processes of astrocytes (GFAP) were also traced using the same automated algorithm that was used for tracing microglial processes (Abdul-Karim et al., 2003; Al-Kofahi et al., 2003), and are displayed in Fig. 3e. The convergence points of the traced processes were then detected (for computation of associative features described in the next subsection), and the results are shown in an enlarged form in Fig. 3f. The image was divided into non-overlapping 25 × 25 × 15 boxes. For each box, the K locations at which the processes crossed the faces of the box were found and then boxes with K < 3 were removed. For the remaining boxes, we estimated K unit vectors, denoted {u(1) , u(2) , . . . , u(K) } representing the directions of the processes at the K crossing points. We calculated the angular variance of

Next, we selected those boxes with σ θ > 0.5 and used their centers as our initial estimates of the convergence points. Finally, the locations of the convergence points were reﬁned by applying a morphological opening operation, followed by a closing operation, and choosing the point with the maximum intensity in each box. 2.3. Computation of associative features We computed a 3D Euclidean distance map around each nucleus to provide a rapid readout of distances to segmented objects and points of closest proximity at each voxel of the 3D image (Maurer et al., 2003). Using this map, we computed distance-based associative features for each segmented nucleus to quantify relationships with other channels (see Fig. 3g–j). Speciﬁcally, each of the segmented nuclei deﬁnes a 3D compartment that delineates the nuclear interior, surface, and exterior. These compartments were used to compute the amount of intranuclear and cytoplasmic (approximated as a 5-voxel region surrounding the nuclear compartment) of Iba1 (g) and Nissl (see h) signals. The nuclei of microglia, as well as the surrounding cytoplasmic region in the Iba1 channel are labeled, as illustrated in g. Neuronal nuclei are spatially surrounded by Nissl signal. This relationship was quantiﬁed by summing the signals in the Nissl channel within a 5-voxels zone around the nuclear surface. Vascular endothelial cell nuclei have distinctive elongated morphologies and are located adjacent to the EBA signal, as illustrated in Fig. 3i. Nuclear morphology was captured by the nuclear shape features. Location was determined by recording the distance from the centroid of each nucleus to the nearest point on the surface of the triangulated surface mesh resulting from segmentation of the EBA channel. This operation took advantage of a distance map computed relative to the segmented vasculature (also shown in Fig. 5d). Astrocyte nuclei were characterized by a high level of GFAP signal in their neighborhood and, in addition, by their proximity to process convergence points (see Fig. 3j). These observations were quantiﬁed by summation of the GFAP signal within a 5voxel neighborhood and the distance from a nuclear centroid to the nearest astrocyte process convergence point. Because not all astrocytes express detectable levels of GFAP (Kimelberg, 2004; Nixdorf-Bergweiler et al., 1994), and since the anti-GFAP antibody did not label the tissue slice uniformly, convergence points strongly supported, but were not always necessary, for astrocyte classiﬁcation. Overall, a set of 23 channel-speciﬁc (intrinsic), and eight cross-channel (associative) measurements were computed for each nucleus for the purpose of cell classiﬁcation. Intrinsic measures included volume, intensity, surface area, texture, and shape measurements. A complete list of measurements and their explanations is provided in Electronic Supplement B.

172

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

Fig. 4. Illustrating the accuracy of cell classiﬁcation (upper row), and showing a 3D rendering of all segmentation and classiﬁcation results. (a) Automated classiﬁcation results for all segmented nuclei (neurons are magenta, astrocytes red, microglia yellow, and endothelial cells are green). (b) Manual classiﬁcation resulting from the nucleus-by-nucleus validation of the entire dataset. (c) Difference map illustrating speciﬁc nuclei that were misclassiﬁed in comparison to the validation results. The misclassiﬁed objects were colored according to the investigator indicated results. (d) Composite 3D rendering of validated cell segmentation and classiﬁcation results, astrocyte processes (red lines), microglial processes (yellow lines), and vessel segments (aqua surfaces). This rendering demonstrates the density of information extracted by FARSIGHT and enables qualitative assessment of cell distributions and their relationships with each other. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of the article.)

2.4. Cell classiﬁcation based on measurements Using the 31 measurements noted above, we classiﬁed each nucleus to identify cell type (Fig. 4a). Both supervised and unsupervised methods can be used for classiﬁcation. Supervised algorithms require user-indicated examples of cells of each type (the training set) to build mathematical models and classiﬁcation rules. The manual burden of this approach is excessive. The unsupervised approach is fully automated and identiﬁes natu-

ral groupings in the feature data, but lacks the speciﬁcity and predictability of supervised methods. We therefore developed a hybrid methodology. Assuming that the number of cell types (c) is known a priori, we used a fuzzy c-means unsupervised classiﬁer (Bezdek, 1981) on the list of measurements to cluster the cells into c classes. This classiﬁer assigns a class membership value in the range (0,1) with 1 representing highest conﬁdence to each cell. Next, the nuclei with membership values exceeding a set conﬁ-

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

173

dence threshold were used as automatically extracted examples (training set) to train a supervised classiﬁer based on the support vector machines (SVMs) principle (Fan et al., 2005). The conﬁdence threshold for each class was set to the 75th percentile. However, errors in the automatically extracted examples can result in SVM classiﬁcation errors. To overcome this problem, we used an optional step that allows the user to inspect and make corrections to the training set before sending them to the SVM classiﬁer. The SVM classiﬁer was chosen because it ﬁnds optimal separators between classes without the need to estimate class-conditional probability distributions. We ﬁrst ran our classiﬁer using all 31 features, and then using the three principal components computed by principal component analysis (PCA) on the feature vectors (Jolliffe, 2002). PCA is useful for dimensionality reduction by projecting the data into a low-dimensional coordinate system. It offers two advantages. First, the lower-dimensional (3D) feature space is computationally less expensive. Second, using a 3D feature space allows us to visualize the classiﬁcation results by plotting the 3D points

in different class speciﬁc colors. However, the classiﬁcation errors increased slightly when using three principal components since the low-dimensional space is an approximate representation of the actual 31-dimensional space. Furthermore, using a 31-dimensional space was not computationally prohibitive in our work—it took less than 5 s to complete all the classiﬁcation steps. So, the results presented in this paper are based on all the features but PCA analysis is still an option for future work with more features. Overall, this hybrid methodology offers a high degree of automation, an appropriate level of supervision, ability to achieve computational tradeoffs, and acceptable performance. 2.5. Secondary analysis based on cell–cell networks Once all cells were segmented and classiﬁed as described above, additional forms of associative analysis become possible using graph representations of relationships. Three examples are presented from our largest dataset to illustrate the methodology (Fig. 5).

Fig. 5. Associative analysis of FARSIGHT-generated cell segmentation and classiﬁcation results. (a) A graph describing a six nearest-neighbor analysis of neurons in the dataset. The lines represent the neuron-to-neuron associations. Differences in line length are partly due to foreshortening that occurs when the 3D maps are projected for a 2D illustration. (b) Frequency histogram of the average nearest neighbor distances showing two populations. The major population includes neurons in the denser CA1 pyramidal cell layer in the lower right. The minor population includes neurons in the extra-pyramidal cell region. (c) The neurons in the extra-pyramidal region are displayed in dark purple to distinguish them from the neurons in the pyramidal cell layer. (d) Map of distances from vascular elements. The mapped distances were the shorter distance to either a segmented EBA-labeled vascular element or a validated vascular nucleus. Distances to vascular elements of each voxel are indicated in grayscale, black being closest to a vascular element, white being at the greatest distance. The apparent differences in the distances depicted in the ﬁgure are the result of foreshortening due to the 2D projection of the 3D map. (e and f) Frequency histograms describing cell distributions in the pyramidal cell layer and extra-pyramidal region. Data were plotted as the distance from each cell to the nearest vascular unit. Measurements from the vascular distance map (d) were used to determine the shortest distance between neurons, astrocytes, and microglia and the nearest vascular element. The volumes of the dataset containing the pyramidal cell layer and extra-pyramidal region were estimated using the analysis of neuron distributions (c). These data indicate that, overall, cells in the pyramidal cell layer are closer to vascular elements (mode ∼30 m) than cells in the extra-pyramidal region (mode ∼45 m). Note: The data used for this analysis were selected to eliminate edge-sampling errors. See Table 2 for more complete analysis. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of the article.)

174

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

2.5.1. Nearest neighbor graph for neurons In order to determine the distribution of each classiﬁed neuronal nucleus, we constructed a 3D graph by selecting just the neuronal nuclei, and identifying links from each neuronal nucleus to its six nearest neighbors (Fig. 5a). Keep in mind that this ﬁgure is a 2D projection of a 3D graph, making some links appear shorter than they are. To eliminate the image boundary effect, any neuron that was closer to the image border than a nearest neighbor was ignored. A histogram of the average distance of each neuronal nucleus to its six neighbors was then computed (b). The histogram is appropriately bi-modal, reﬂecting the fact that the image consists of two populations—tightly packed neurons in the pyramidal region (lower right), and loosely packed, neurons in the extra-pyramidal region. Simply separating the two populations using a threshold permits objective discrimination of extra-pyramidal and pyramidal domains (c). 2.5.2. Cell-vascular distance In order to quantify the spatial relationship of cells with vasculature, we computed the geometric distance from the centroid of each nucleus to the nearest vascular element using a 3D dis-

tance map relative to the vessels, as shown in Fig. 5d. This panel shows the segmented vascular segments in light blue, classiﬁed endothelial cells in green, and the distance of every image point to the nearest vascular segment as a gray map. In this map, lighter points are farther from vessels than the darker points. To help the reader view this three-dimensionally, we have included a supplemental ﬁle showing the distance map as a rotating movie (Electronic Supplement D). The resulting associations provide a quantitative anatomical description of vascular domains. 2.5.3. Regional cell distribution After determining the pyramidal cell layer and extrapyramidal region as described above, we computed various cell distributions within each of these domains. The purpose of this analysis is to demonstrate how FARSIGHT enables novel ways to extract associative measurements from a complex 3D dataset. 2.6. Validation Expert human observers are the only available gold standard for validating the automated analysis. We did not consider

Fig. 6. Graphical interface for inspection and editing of segmentation and classiﬁcation results. (a) Screen view of the user interface illustrating through-focus projections in x − y (main panel) with segmented nuclei shown as white outlines, y − z (immediate lower panel) and x − z (immediate right panel) projections, drop down menus showing choices on how objects can be visited, validation type (lower most left panel), and measurements for a selected object (lower most right panel). (b) Screen capture of the validation window illustrating one selected nucleus from the main panel (yellow box and arrow). The slider on the immediate right side of the slice image indicates the optical section being viewed. The slider below the image indicates relative magniﬁcation. The buttons below the image allow editing of segmentation and classiﬁcation. Each object can be viewed alone or in relation to other objects through every optical section of the 3D image stack. (c) Illustrating observed segmentation errors, including (i) hyper-segmentation, (ii) under-segmentation, and (iii) one object encroaching on another. (d) Tabular summary of nuclear segmentation errors for three different datasets for each cell type. (e–g) Projections of three datasets that were validated. Boxes indicate regions that were randomly selected for intensive validation. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of the article.)

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

175

stereological analysis since it would undersample the data, and not provide an adequately detailed indication of algorithm performance on a cell-by-cell basis. A more comprehensive approach is to have a domain expert analyze the data using computer-assisted annotation. Commercial software packages such as MetaMorph (Molecular Devices Corp., Sunnyvale, CA) or Volocity (Improvision Inc., Waltham, MA) could have been used for this, but these tools lacked several important features. First, they do not contain prioritization features to minimize the manual effort for validation. This is important since the images contain large numbers of nuclei, most of which are segmented correctly. Second, they would also require complex translation of proprietary data formats that must be validated separately. Therefore, a custom software tool was developed within FARSIGHT to inspect and validate the segmentation and classiﬁcation of cell nuclei (Fig. 6a). This tool consists of a 3D image visualization window (b) that is centered on a selected object, devices to edit nuclear segmentation and cell classiﬁcation, and annotation recording. Segmented objects can be sequentially observed according to object number, position in the image, the calculated conﬁdence, or in random user-selected order. When an object is visited, all of its measurements are displayed in a window and all user annotations are recorded. For this study, all nuclei were inspected in order of segmentation conﬁdence from low to high. The accuracy of segmentation in every relevant z-slice was evaluated (c). In most cases, individual nuclei could be resolved; however, in a few cases the overlap of multiple objects required a group evaluation. Our criteria were strict. For instance, seven objects were automatically segmented to represent a cluster of ﬁve nuclei. This group was scored collectively as two cases of over-segmentation and ﬁve cases of encroachment. Classiﬁcation was evaluated based on morphology and association with cell-speciﬁc labels in other channels. Segmented objects that were at the section or image edge with partial proﬁles were considered ‘expected classiﬁcation errors’ and excluded from subsequent analysis. Validation notes were tabulated in spreadsheet form (Microsoft Excel) and the numbers of segmentation and classiﬁcation errors were calculated. 3. Results Three datasets were used to test our segmentation and classiﬁcation analyses. Two of these were from the hippocampus; the third was from cingulate cortex. One of the hippocampal datasets was used for our primary example because of its clear laminar organization. In addition, a large majority of hippocampal astrocytes express sufﬁcient amounts of glial ﬁbrillary acidic protein (GFAP) to enable immunohistochemical identiﬁcation of this cell population (Kimelberg, 2004; NixdorfBergweiler et al., 1994), thus were we able to more completely identify and measure these cells. The cingulate cortex dataset provides a test to demonstrate that FARSIGHT can be used to accurately analyze other brain regions. Other cell-type speciﬁc labeling included NeuroTrace (Nissl) to identify neuronal soma, antibodies to Iba1, a calcium binding protein, to identify microglia (Ito et al., 1998) and antibodies to endothelial barrier antigen (EBA) to identify vascular endothelia (Sternberger

and Sternberger, 1987). By careful selection and use of primary and secondary antibodies (and their concentrations) and implementing linear unmixing procedures with spectral confocal microscopy, it was possible to collect 3D datasets containing ﬁve separate data channels from 100- m-thick tissue slices (Fig. 1, and Electronic Supplement A for full 3D viewing). Excitation and signal detection conditions were optimized to produce high signal-to-noise outputs for each data channel (Fig. 1g and h). There is high variability in nuclear morphology (a); compare the large round pyramidal cell nuclei (large arrowhead; relatively light overall labeling and the presence of a variable number of small more brightly labeled nucleoli) with vascular cell nuclei (small arrowheads; small, compact, crescent-shaped, and very intensely labeled). NeuroTrace labeled the Nissl substance and nucleoli of neurons (Fig. 1b). Iba1-labeled cell bodies and processes of microglia (c). GFAP labeled the cytoskeleton of astrocytes, highlighting the processes of these cells (d). EBA labeled the surfaces of vascular elements including microvessels and larger arteries and veins (e). Both stains (CyQuant, NeuroTrace) and the Iba1 and EBA antibodies penetrated the tissue sections completely; these results demonstrate that our IHC protocol can be effectively applied to thick tissue sections. The GFAP antibody achieved partial penetration, reaching 25 m into the tissue from both sides of the slice, suggesting that incomplete labeling is antibody-dependent. Overall, these data demonstrate that our sample preparation and imaging conditions resulted in spectral channels with negligible overlap and sufﬁcient signal-to-noise resolution for automated image analysis. Using the 3D visualization and validation tool described earlier (Fig. 6), the results of automated segmentation and classiﬁcation were evaluated for accuracy compared to a human observer. The types of errors recorded included oversegmentation, under-segmentation, and encroachment (c). Cases of over- and under-segmentation were straightforward (nuclei were either correctly or incorrectly identiﬁed as a single object). Cases of encroachment ranged from minor infringement to signiﬁcant overlap—all of these were counted. For the ﬁrst dataset (hippocampus), automated analysis identiﬁed 1019 nuclei. The validation review identiﬁed 1014 nuclei. When nuclei that were truncated by the image or tissue section edges were excluded from analysis, a total of 913 and 911 cells were identiﬁed by automated and validated analysis, respectively. For the second dataset (hippocampus), automated segmentation identiﬁed 98 objects (79 that were clear of the edges), while validation identiﬁed 103 nuclei (82 that were clear of the edges). For the third dataset (cingulate cortex), automated analysis identiﬁed 452 objects (386 that were away from edges), while validation identiﬁed 437 nuclei (386 that were clear of the edges). Because there were nearly equal numbers of over- and under-segmentation errors, the total number of nuclei is nearly the same for the automated and validated results (see Table 1). These results demonstrate that consistently accurate counts can be obtained using FARSIGHT. The combined list of 23 intrinsic and 8 associative measurements was used for automatic classiﬁcation of nuclei using the three-step procedure described above. This includes an initial

176

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

Table 2 Quantitative measurements of numbers and distributions of various cell types in the pyramidal cell layer and extra-pyramidal region from the dataset shown in Fig. 1 Portion of dataset Pyramidal cell layer Extra-pyramidal region Pyramidal/extra-pyramidal Neurons 286T , 127.8D 62T , 6.5D 19.8 Astrocytes 76T , 34.0D 240T , 25.0D 1.4 Microglia 21T , 9.4D 76T , 7.9D 1.2 Neurons/astrocytes 3.76 0.26 Neurons/microglia 13.62 0.82 Microglia/astrocytes 0.28 0.32

For each cell type, the total counts (indicated by superscript T), and the density of cells per m3 (indicated by superscript D). To calculate the density values, the volumes of the pyramidal cell layer was estimated from the image as 2.24 × 106 m3 , and the extra-pyramidal region was estimated to be 9.59 × 106 m3 by partitioning the dataset according to the distribution of sub-categorized neurons (see Fig. 5c).

unsupervised classiﬁcation to extract training samples. These samples were inspected and edited by the investigator (for example, only ﬁve corrections were required for the ﬁrst dataset). The veriﬁed samples were used to complete the classiﬁcation automatically. Electronic Supplement E shows the 31 measurements as well as the cell classiﬁcation data as a XML ﬁle for further analysis. Electronic Supplement F shows three movie ﬁles displaying the segmentation and classiﬁcation results for the three example datasets. Validation of the automated classiﬁcation results included inspection of every segmented nucleus, evaluation of automatically assigned classiﬁcations, and annotation. For the ﬁrst dataset, the overall accuracy was 87.9%. For the second dataset, overall classiﬁcation accuracy was 78%. Overall accuracy for the third dataset was 83.7%. For all datasets, accuracy was cell type dependent (Table 1). Fig. 4a–c shows the automated results, manual results, and the difference for the ﬁrst dataset. In c, the colors indicate the correct results from manual validation for the cells that were classiﬁed incorrectly by the algorithm. The most common classiﬁcation error for the ﬁrst dataset was separation of astrocytes from endothelial cells. The second dataset showed a more even distribution of classiﬁcation errors, while the third dataset exhibited more errors among neurons and astrocytes. These results demonstrate that reasonably consistent classiﬁcation results can be obtained for different datasets. The differences between each set are especially interesting in light of the differences in segmentation errors. For example, segmentation of the third dataset resulted in a greater proportion of under-segmented cells, especially neurons and astrocytes. Since neurons and astrocytes were more commonly misclassiﬁed, it suggests the classiﬁcation has a strong dependence on the accuracy of the initial segmentation, as we would expect. Indeed, if we look at the proportion of misclassiﬁed cells among those that were segmented correctly, we achieve accuracies of 93.2%, 84.1%, and 90.7% for the ﬁrst, second, and third dataset, respectively. The validated results were used for further visualization and analyses described below. All of the segmentations and cell classiﬁcations can be visualized in context using 3D rendering (Fig. 4d). Such renderings provide a visual summary of the ﬁve-channel segmentation and classiﬁcation results in a manner that preserves the 3D context of all the structures, and in a manner that can be compared with the image data (compare Fig. 5d and 1f). Although these renderings are impressive in recapitulating the density and complexity of brain tissue, they must still be recognized as being incomplete, since they do not include axons and dendrites of neurons

and oligodendrocytes. Nevertheless, this composite of segmentation, tracing, and classiﬁcation results clearly demonstrates the power of the FARSIGHT methodology. Cell classiﬁcation sets the stage for a further set of tissue-scale associative analyses. Two illustrative examples are presented below. First, the six nearest neighbors of all neurons were identiﬁed (Fig. 5a). A frequency histogram of average distances to the nearest neighbors (b) indicates two sub-populations—a larger population of cells that are relatively close to each other with a modal neuron-to-neuron spacing of ∼10 m and a second, much smaller, population with a modal spacing of ∼30 m. Correlations of these data with graphs (e.g., Fig. 5a) indicated that neurons with the smaller spacing were in the pyramidal cell layer. Other neurons were in the extra-pyramidal region. By automatically separating the histogram in Fig. 5b into two classes, we were able to deﬁne the pyramidal cell layer and the extra-pyramidal region. This enabled us to compare cell densities and the ratio of different cell types in these two regions (Table 2). A nearest neighbor analysis of microglia demonstrated that these cells appear to be uniformly distributed throughout the entire ﬁeld (average shortest distance between neighbors was 32.2 ± 14.0 m, based on this one image). The distribution of cells around brain vasculature can also be quantiﬁed from the segmentation results by associative analysis. A 3D distance map from segmented vascular elements was generated for the ﬁrst dataset (Fig. 5d) using both segmented vessel walls and validated endothelial cell nuclei. This strategy was used because not all vascular elements exhibited uniform labeling intensity. Cell distances were measured from each classiﬁed nucleus to the nearest vascular element, either a segmented vessel wall or a validated endothelial cell nucleus, whichever was closest. These data demonstrated that the associations of cells to vascular elements were different in the pyramidal cell layer and the extra-pyramidal region (Fig. 5e and f). In the pyramidal cell layer the modal distances to vascular elements were shorter than in the extra-pyramidal region (∼30 m vs. ∼45 m). 4. Discussion This study demonstrates the power and utility of associative image analysis for computing biologically meaningful measurements of complex brain tissue from multi-channel 3D images. The measurement data generated from each image are very rich. They can be tapped for diverse hypothesis-driven and/or discovery-oriented studies. To enable widespread usage, we have chosen to cast all the intrinsic and associative measure-

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178

177

ments, as well as the attributed graphs in the widely used XML format. Since these ﬁles are both machine- and human-readable, investigators can directly access, edit, and interpret XML ﬁles using common tools. The XML representation is an accepted standard of the worldwide web consortium, and is supported by all major database tools and spreadsheets including Microsoft Excel. Therefore, we have set the stage for novel ways to query and summarize the measurements. We have also set the stage for linking our segmentation data with XML-based systems modeling software tools. The FARSIGHT approach to image analysis differs from other image-based quantitation approaches by placing a high emphasis on associative measurements, on analyzing every object in the ﬁeld rather than a randomly sampled subset as in stereology (Howard, 2005), and on pragmatic automation strategies. Our approach to automation recognizes that complete automation of visual image-based tasks (especially segmentation and classiﬁcation) to a level of accuracy that does not require human oversight is beyond the current state of the art in biological image analysis. To overcome this barrier, we developed efﬁcient methods for reviewing and editing the automated results. From the biological user’s standpoint, these methods provide direct visual conﬁrmation and assurance of correctness, resulting in a validated set of measurements to answer important biological questions. At the same time, from the software developer’s standpoint, these methods deliver detailed data on automated segmentation and classiﬁcation errors. This ability to assess automated outputs was used extensively during the development of FARSIGHT, and continues to drive future enhancements. For example, we incorporated convergence points of traced GFAP elements to improve astrocyte classiﬁcation when an earlier performance assessment indicated a high rate of errors for this cell type. In our validation study, we deliberately chose to evaluate each cell in our test datasets, rather than a random sub-sample, to thoroughly evaluate the automated analysis method. For large-scale usage, the FARSIGHT toolbox provides a mechanism for more streamlined validation. Speciﬁcally, the investigator can inspect and edit results in a prioritized order based on the conﬁdence scores that accompany segmentation and classiﬁcation results. This strategy prioritizes the effort on image objects that are most likely to have errors (requiring corrective edits). As inspection progresses to higher conﬁdence scores, the experienced user can choose to accept the remaining automated results exceeding a chosen conﬁdence threshold. At the completion of this process, the user is assured of a targeted level of accuracy. As a rule of thumb, the manual effort is roughly proportional to the error rate of the automated algorithms. The record of edit operations provides valuable clues about the nature of errors and provides the basis for reﬁning the algorithms. In the future, we expect to incorporate algorithms that analyze patterns of user editing, and use that information to make editing more efﬁcient. With the addition of electronic signatures and security features, the record of edits can also form the basis for implementing good automated laboratory practices (Weinberg, 1993). Three ideas were most important in the design of FARSIGHT: (i) complexity reduction by unmixing the ﬂuorescence

spectra into a small number of non-overlapping channels; (ii) a ‘divide and conquer’ automated segmentation strategy that overcomes the otherwise insurmountable problem of mixedmorphology segmentation; and (iii) the integrative concept of associative analysis that enables us to relate information across imaging channels, and also within individual channels. Even with the linear unmixing-based complexity reduction, sophisticated automated segmentation algorithms were required for this effort. As examples, the multiple-model nuclear segmentation algorithm (Lin et al., 2007), and the vessel segmentation algorithm (Electronic Supplement C) are recent developments. In addition, this work has leveraged recent advances in image processing. For example, the idea of distance-based associative features is very intuitive, but was computationally prohibitive until the recent development of fast and accurate algorithms for computing Euclidean distance maps (Maurer et al., 2003). The FARSIGHT system is designed for broad applicability. Structural units such as cell nuclei, cell bodies, processes, and vasculature are present in all types of tissue, and can be labeled with high speciﬁcity. In addition, techniques for labeling molecules that are speciﬁc to chosen cell types, and indicators of cellular activities are advancing rapidly. Complementary advances in multi-channel imaging instrumentation make it practical to record multiple structures and functional markers in their 3D context. The power of our method lies in being able to quantify such contextual associations in a meaningful and efﬁcient manner. Finally, our divide and conquer approach is inherently scalable—it is poised to exploit further advances in instrumentation and multiplexed labeling technologies. Currently, FARSIGHT is being shared with interested colleagues on a collaborative basis in several areas including stem-cell niches, stroke research, neurobiology, and tumor biology. Appendix A. Supplementary data Supplementary data associated with this article can be found, in the online version, at doi:10.1016/j.jneumeth.2007.12.024. References
Abdul-Karim MA, Al-Kofahi K, Brown EB, Jain RK, Roysam B. Automated tracing and change analysis of angiogenic vasculature from in vivo multiphoton confocal image time series. Microvasc Res 2003;66(2):113–25. Al-Kofahi K, Lasek S, Szarowski DH, Dowell-Mesﬁn N, Shain W, Turner JN, et al. Median-based robust algorithms for tracing neurons from noisy confocal microscope images. IEEE Trans Inf Technol Biomed 2003;7:302–17. Al-Kofahi Y, Dowell-Mesﬁn D, Pace C, Shain W, Turner JN, Roysam B. Improved detection of branching points in algorithms for automated neuron tracing from 3D confocal images. Cytometry A 2008;73(1):36–43. Bear M. Neuroscience: exploring the brain. 3rd ed. New York: Lippincott Williams & Wilkins; 2006. Bezdek J. Pattern recognition with fuzzy objective function algorithms. New York: Plenum Press; 1981. Biran R, Martin DC, Tresco PA. The brain tissue response to implanted silicon microelectrode arrays is increased when the device is tethered to the skull. J Biomed Mater Res A 2007;82(1):169–78. Calabrese C, Poppleton H, Kocak M, Hogg TL, Fuller C, Hamner B, et al. A perivascular niche for brain tumor stem cells. Cancer Cell 2007;11:69–82.

178

C.S. Bjornsson et al. / Journal of Neuroscience Methods 170 (2008) 165–178 Mahadevan V, Narasimha-Iyer H, Roysam B, Tanenbaum HL. Robust modelbased vasculature detection in noisy biomedical images. IEEE Trans Inf Technol Biomed 2004;8:360–76. Maurer Jr CR, Rensheng Q, Raghavan V. A linear time algorithm for computing exact Euclidean distance transforms of binary images in arbitrary dimensions. IEEE Trans Pattern Anal Mach Intell 2003;25:265– 70. Nixdorf-Bergweiler BE, Albrecht D, Heinemann U. Developmental changes on the number, size, and orientation of GFAP-positive cells in the CA1 region of rat hippocampus. Glia 1994;12:180–95. Olsen K. preparation of ﬁsh tissue for electron microscopy. J Electron Microsc Tech 1985;2:217–28. Pawley J. Handbook of biological confocal microscopy. 3rd ed. New York: Springer; 2006. p. 985. Seymour JP, Kipke DR. Neural probe design for reduced tissue encapsulation in CNS. Biomaterials 2007;28(25):3594–607. Sonka M, Hlavac V, Boyle R. Image processing, analysis, and machine vision. 3rd ed. Toronto, Ontario, Canada: Thomson; 2008. Spataro L, Dilgen J, Retterer S, Spence AJW, Isaacson M, Turner JN, et al. Dexamethasone treatment reduces astroglia responses to inserted neuroprosthetic devices in rat neocortex. Exp Neurol 2005;194:289–300. Sternberger NH, Sternberger LA. Blood–brain barrier protein recognized by monoclonal antibody. Proc Natl Acad Sci USA 1987;84:8169–73. Szarowski DH, Andersen MD, Retterer S, Spence AJ, Isaacson M, Craighead HG, et al. Brain responses to micro-machined silicon devices. Brain Res 2003;983(1-2):23–35. Treece GM, Prager RW, Gee AH. Regularized marching tetrahedra: improved isosurface extraction. Comput Graph 1999;23:583–98. Turner JN, Shain W, Szarowski DH, Andersen M, Martins S, Isaacson M, et al. Cerebral astrocyte response to micromachined silicon implants. Exp Neurol 1999;156(1):33–49. Tyrrell JA, diTomaso E, Fuja D, Tong R, Kozak K, Brown EB, et al. Robust 3D modeling of vasculature imagery using superellipsoids. IEEE Trans Med Imag 2007;26:223–37. Watson GS. Statistics on spheres. New York: Wiley; 1983. Weaver CM, Hof PR, Wearne SL, Lindquist WB. Automated algorithms for multiscale morphometry of neuronal dendrites. Neural Comput 2004;16:1283–353. Weinberg. Good automated laboratory practices. Qual Assur 1993;2:62–6. Wilcox R. Introduction to robust estimation and hypothesis testing. San Diego: Academic Press; 1997. Zhang Y, Zhou X, Degterev A, Lipinski M, Adjeroh D, Yuan J, et al. A novel tracing algorithm for high throughput imaging. Screening of neuron-based assays. J Neurosci Methods 2007;160:149–62. Zhong Y, Bellamkonda RV. Dexamethasone-coated neural probes elicit attenuated inﬂammatory response and neuronal loss compared to uncoated neural probes. Brain Res 2007;1148:15–27 (ePub). Zimmerman T. Spectral imaging and linear unmixing in light microscopy. Adv Biochem Eng Technol 2005;95:245–65.

Chan SL, Purisima EO. A new tetrahedral tessellation scheme for isosurface generation. Comput Graph 1998;22:83–90. Chikuse Y. Statistics on special manifolds. Lecture notes in statistics. New York: Springer; 2003. p. 174. Dickinson MEBG, Tille S, Lansford R, Fraser SE. Multi-spectral imaging and linear unmixing add a whole new dimension to laser scanning ﬂuorescence microscopy. Biotechniques 2001;31:1274–6. Fan RE, Chen PH, Lin CJ. Working set selection using the second order information for training SVM. J Mach Learn Res 2005;6:1889–918. Fernandez-Gonzalez R, Jones A, Garcia-Rodriguez E, Chen PY, Idica A, Lockett SJ, et al. System for combined three-dimensional morphological and molecular analysis of thick tissue specimens. Microsc Res Tech 2002;59:522–30. Fuchs E, Tudlow T, Guasch G. Socializing with the neighbors: stem cells and their niche. Cell 2004;116:769–78. Garini Y, Young IT, McNamara G. Spectral imaging: principles and applications. Cytometry A 2006;69:735–47. Gross JL, Yellen J. Graph theory and its applications. 2nd ed. New York: Chapman & Hall/CRC; 2005. Guzowski JF, Timlin J, Roysam B, McNaughton BC, Worley PF, Barnes CA. Mapping behaviorally relevant neural circuits with immediate-early gene expression. Curr Opin Neurbiol 2005;15:599–606. Howard V. Unbiased stereology: three-dimensional measurement in microscopy. 2nd ed. New York, NY: Garland Science/BIOS Scientiﬁc Publishers; 2005. Iadecola C. Neurovascular regulation in the normal brain and in Alzheimer’s disease. Nat Rev Neurosci 2004;5:347–60. Ito D, Imai Y, Ohsawa K, Nakajima K, Fukuuchi Y, Kahsaka S. Microgliaspeciﬁc localization of a novel calcium binding protein, Iba1. Brain Res Mol Brain Res 1998;57:1–9. Jolliffe IT. Principal component analysis. 2nd ed. Springer; 2002. Kimelberg HK. The problem of astrocyte identity. Neurochem Int 2004;45: 191–202. Kuprat A, Khamayseh A, George D, Larkey L. Volume conserving smoothing for piecewise linear curves, surfaces and triple lines. J Comput Phys 2001;172:99–118. Lin G, Adiga U, Olson K, Guzowski JF, Barnes CA, Roysam B. A hybrid 3D watershed algorithm incorporating gradient cues and object models for automatic segmentation of nuclei in confocal imaging stacks. Cytometry A 2003;56:23–36. Lin G, Bjornsson CS, Smith KL, Abdul-Karim MA, Turner JN, Shain W, et al. Automated image analysis methods for 3D quantiﬁcation of neurovascular unit from multichannel confocal microscope images. Cytometry A 2005a;66:9–23. Lin G, Chawla MK, Olson K, Guzowski JF, Barnes CA, Roysam B. Hierarchical, model-based merging of multiple fragments for improved three-dimensional segmentation of nuclei. Cytometry A 2005b;63:20–33. Lin G, Chawla MK, Olson K, Barnes CA, Guzowski JF, Roysam B. A multi-model approach to simultaneous segmentation and classiﬁcation of heterogeneous populations of cell nuclei in 3D confocal microscope images. Cytometry A 2007;71(9):724–36.

