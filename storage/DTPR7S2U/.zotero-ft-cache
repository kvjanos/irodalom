January 2003

Trevor Hastie, Stanford Statistics

1

Support Vector Machines, Kernel Logistic Regression, and Boosting
Trevor Hastie Statistics Department Stanford University Collaborators: Brad Efron, Jerome Friedman, Saharon Rosset, Rob Tibshirani, Ji Zhu
http://www-stat.stanford.edu/∼hastie/Papers/svmtalk.pdf

January 2003

Trevor Hastie, Stanford Statistics

2

Outline
✔ Optimal separating hyperplanes and relaxations ✔ SVMs: nonlinear generalizations of separating hyperplanes ✔ SVM as a function estimation problem ✔ Kernel logistic regression ✖ Reproducing kernel Hilbert spaces ✔ Connections between SVM, KLR and Boosting.
First part based on work by Vapnik (1996), Wahba (1990), Evgeniou, Pontil, and Poggio (1999); described in Hastie, Tibshirani and Friedman (2001) Elements of Statistical Learning, Springer, NY. Gunnar R¨tsch a and coworkers have also made coonection between SVMs and Boosting.

January 2003

Trevor Hastie, Stanford Statistics

3

Maximum Margin Classiﬁer
xT β + β0 = 0
• • • • • •• • •• • •• •

Vapnik(1995) xi ∈ IRp , yi ∈ {−1, 1}

• •• • ••

C

margin C

β,β0 , β =1

max

C

subject to

yi (xT β + β0 ) ≥ C, i = 1, . . . , N. i

January 2003

Trevor Hastie, Stanford Statistics

4

Overlapping Classes
xT β + β0 = 0
∗ • ξ4 ∗ • ∗• ξ3 ξ • 1 •∗ •• • ξ2 •• • •

• • • • ∗ ••

•

ξ5

• •• C

margin C

•
∗ ξi = Cξi

β,β0 , β =1

max

C
i ξi

subject to yi (xT β + β0 ) ≥ C(1 − ξi ), ξi ≥ 0, i

≤B

January 2003

Trevor Hastie, Stanford Statistics

5

Example
...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ............... ...................... ...................... ...................... ...................... ....... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... . . . . . . . . . . Error:. 0.270. Training . . . . . . . . . . ...................... ...................... ...................... ...................... ...................... ..... Test . . . . . . . . . . . . Error: . . . . 0.288. ...................... ...................... ...................... . . . . . . . . Error:. . . 0.210. ..... ..... Bayes . . . . o.... o.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... . . ............................................... . . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o ............................................... .. . .o. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . ............................................... . ... . o o . .o. . .o. . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . ..o........................................... ............................................. ............................................... ..... o . . .o.o. . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .................................... ............................................... o o ............ . . . . . .o. . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o ............................................... . .. . o o oo . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . o ... ... ... ...o ... ... ... ... ... ... ... ... ... ... ... ... ... ...o .... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... . . . o . . . o ... ... ... ... ..... ... ... ... ...o... ... ... ... ... ... ...o ...oo... ... ... ... ... ... ... ... ... ... ... ...o... ... ... ... ... ... ... ... ... ... ... ... ... . . o. . o. o. .o o.... .... . . . . . . . . . . . . .... . . . . . . . . . . . . . .o . . • . . o .. .. .. .. . . . . .o . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................... .o . . . .... . . . . . . o o o o..... ..... o..... ..... .... .... .... .... .... .... .... .... .... ..... ....o .... .... .... o.... .... .... .... .... .... ....o.... .... .... .... .... .... .... .... .... ....o .... .... .... .... .... .... . o o ....o.. ....oo..o o.. .. .. ..o .. .. .. ....o .. .. .... .. .. .. o.. .. ..o.. ..o.. .. .. .. .. ..o.... .. .. .. .. .. .. • ooooo.. ... .... .... ..o.... .... .. .. .. .. .... .. .. .. ... o.. .. ... .. .. .. .... .. .. .. ....... .. .. .... .. .. .. .. .. .. .... ... .. .. .. .. ..o.. . . . o o.... . . . . . . . . . . . . o. o. .... . . . . . . . .o . . . . . . . . . . . . o. . . . . . o . o.....o... o... o... ... ... ... ... ... ...o....o....o...o ... ... ...o... ... oo... ... ...o... ... ... ... ... ... ... o.... ... ... ... ... ... ... . . . . . . . . . . .. . .. . . . . . . . . . . . . . .o . o . . .o. . . . oo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..... . o o . . . . . o. .. . ... .. .... .. .. . . . .. . . ..o . . . .. . .. . . . . . .. . . . . . . . o . . . . . . . . . . .o . . .o .o. . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . o . . .. . o. . . . .o . . . .. . . . . . . .. . . . . . . . . . . . . . .. . . . . .. . o....oo.... o... ... ... o... .... ... .. .. .. .. ... .. .. .. .. .. o.. .. .. .. .. .. .. .. .. .. .. ..o .. .. ..o.. .. .. .. .. .. . .. . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . o. . . . . . . o . . . . . . . . . . . . . . . . . . . . . . . o. . . o. . . . . o . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . o. . . . ........ . . . . o ... .... . . .o . . . o. . o .... .... .... .... .... .... .... o.... .... .... .... .... .... ....o ....oo....o.... .... o.... .... ....o.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... . . .. . . .. . .o. . . . . o. . .o. o . . o . . . . . . . .• . . . . .o . . . . . . . . . . . . . .............................................. o . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . o . ... .. . . . . . . . . . . . . . . o . . . . o o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o . .. . .. . . . ............................................... . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .o .o . . o . . . . . . . . . . . . . . . . . . . . . . . . . o .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. o.. .. .. .. .. .. ..o.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . . . ............................................... . . . . . . . . . . o .... .... ....o.... ....o.... .... ....o .... .... o.... .... .... .... .... .... .... o.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... . . . . . ............................................ . . . . .o . . . . . . . . . . . . . o. . . . . . . . .o . . . . . . . . . . . . . . . . . .... .. . . . . . . . . . . . . . . . . .o . . . . . .o . . . . o. . . . . . . . . . . . . . . . . . . . . ............................................... o . ...............o.............................. .... . . . . . . . . . . . . . . . oo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . .o. . . . . . . . . . . . . . . . . . . . .................................. ................ . . . . . . . . . . . . . . . . . . o . .o. o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . ......................... ............................................... ....................... o . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . ............................................... ............ o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . ............................................... ............................................... ............................................... . ... . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . o . . . . . . . . . . . . . . ...............................................

o

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

.. .. .. .. .. ..

. . o.. .. .. .. .. .. .. .. .. .. .. .. .. .. ..

................ ................ ................ ................

ˆ ˆ ˆ Fitted function is f (x) = xT β + β0 ˆ ˆ Resulting classiﬁer is G(x) = sign[f (x)]

January 2003

Trevor Hastie, Stanford Statistics

6

Quadratic Programming Solution
After a lot of *stuﬀ* we arrive at a Lagrange dual LD 1 = αi − 2 i=1
N N N

αi αi yi yi xT xi i
i=1 i =1

which we maximize subject to constraints (involving B as well). The solution is expressed in terms of ﬁtted Lagrange multipliers αi : ˆ
N

ˆ β=
i=1

αi yi xi ˆ

Some fraction of αi are exactly zero (from KKT conditions); the xi ˆ for which αi > 0 are called support points S. ˆ ˆ ˆ ˆ f (x) = xT β + β 0 =
i∈S

ˆ αi yi xT xi + β 0 ˆ

January 2003

Trevor Hastie, Stanford Statistics

7

Flexible Classiﬁers
SVM - Degree-4 Polynomial in Feature Space
...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... . ...................... ...................... ..................... ...................... ...................... . ...................... ...................... .................. ... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ... ...................... ...................... ................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ..... ..... . . . . . . . . . . Error:. 0.180. Training ...................... ...................... ...................... ...................... . . . . . . Error: . . . . 0.245. Test. . . . . . . . . . . . . . . . . ..... ...... ..... ...................... ...................... ...................... . . . . . . . . Error:. . . 0.210. ..... Bayes . . . . . . . o.... o.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... •.. ...... .. .. ..o.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . ............................................... o . . .. . .o. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . . .o. . .o. . . . . . . . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . o o ............................................... . . . o . . . . . . . . . . . . . . . •. . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................. • ... ...o...o... ...o.. .. .. ..o.. .. .. .. .. .. .. .... .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . . . . . .•. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .............. .. . o o o . . . .o. . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................ . . . . . oo .o . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o ............................................... o ............................... ............... .. . . . . . o . . . . . . . . . . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . o ... ... ... ...o ... ... ... ... ... ... ... ... ... ... ... ... ... ...o ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... . . . . o . . . o ... ... ... ... .... ... ... ... ...o... ... ... ... ... ... ...o ...oo... ... ... ... ... ... ... ... ... ... ... ...o... ... ... ... ... ... ... ... ... ... ... ... ... o .... ... . . . o . . . . . . . . . . . . .o . o o. . . o . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................... ...o . ... . o . . .. .. . . . . . . . . . •.. . . . . . . . . .o . . . . . . . o o. . . . . . . . . . . . . . . . . . . . . . . . . o . . . . . . . . . . . . . . . . . . . . o o o.... .... •.... .... .... .... .... .... .... .... .... .... .... .... ...o ... ... ... o... ... ... ... ... ... ...o... ... ... ... ... ... ... ... ... ...• ... ... ... ... ... ... . . . . . . .o .o o . . . . o o ... .. .. ... .. o..o .. .. .. .. ..o .. .. .. ...o .. .. ... .. .. .. o.. .. ..o.. ..o.. .. .. .. .. ..o.... .. .. .. .. .. .. . . o . . oo oo.. .. .. .... ..o .. .. .. .. .. .... o.. o..• .. .. .. .. .. .. .... .. .. .. .... .. .. .... .. .. .. .. .. .. .... o.. .. .. .. ..o . •oooo.... .. .. .. .. .. .... .. .. .. .. .. .. ....o....o.... .. .. .. ..o.. .. ..o.. .. .. .. .. .. .. .. .. .. .. .. o.... .. .. .. .. .. .... . o . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . o . . ...... . . . . . . . . . . . . . . . . .• . . . . . . . . . . . .o. o. o . . . . oo . . oo. . . . . . . o. . . .o. . . . . . . . . . . . . . . .. . . . . .o . o . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ... .... . o o ........ . o. . . . . . . . . . . . . .o . . .o .o. . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . o . . ... . o. . . . .o . . ..o ... ... ... ... ... .... .... ... ... ... ... ... ... .. .. . . .. . ....... ..... ... . . ............................................... . . . o..oo.. o.. .. ..o. . . . . . . . . . . . . o. . . . . . . . . . . .o . . .o. . . . . . . .. . . .... .. . . . . . . . . • . . . o . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . •. . . . . . ............................................... . o . . . . o. . .• . . . . oo . . . . o. . . . . . . o o. . . . . . . . . . . . . . . . . . o . . . .. . .. .. . o .. ... . . . . . . .o . . . o. . . o ..... ..... ..... ..... ..... ..... ..... o..... ..... ..... ..... ..... ..... .....o .....oo.....o..... ..... o..... ..... .....o..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... . . .. . . .. . . .. .. .o..o. .. .. . .. .. .. . .. .. . .. .. .. . .. . ........ o . ... .. ..... . . . . . o. .o. . . . . o. . .o. . . . . . o. . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . .o . . . . o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..................... . . . . . . . . . . . . . . . .oo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o . . . .. ... .. .. . . .. .. . . . .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. ............................................... . . . . . . . . . . . . . . . . .o . . o . . . . . . . . . . . . . . . . . . . . . . . . . . o ... ... ... ... ... ... ... ... ... ... .. .. .. ..o .. o.. .. .. o.. .. .. .. .. .. ..o.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . . . . . . . . . o .... .... ....o.... ....o.... .... ....o .... ....o.... .... .... .... .... .... .... o.... .... .... .... .... .... ....•.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... • ... ... ... ....... ...o....... ... ... ... ....... ... ... ....... ... ... ... ... ... o... ....... ... ... ... ... ... ... ...o....... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .o . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . oo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................... .... . . . . . . . . . . . . . . . . . o . . . . o. . . .o. . . . . . . . . . . . . . . . . . . . ............................................... ......................................... .. . . . . . . . . . . . . . . o . .o. o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................... o .. . . . ............................................... ........................................ .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . .... . .o . . ............................................... ............................................... .................................. . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . o . . . . . . . . . . . . . . ......... . .. .. . ...............................................

o

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. . .. . .. ..

.. .. . .. .. .. .

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

.. .. .. .. .. ..

. . o.. .. .. .. .. .. .. .. .. .. .. .. .. .. ..

................ ................ ................ ................

Enlarge the feature space via basis expansions, e.g. polynomials of total degree 4. h(x) = (h1 (x), h2 (x), . . . , hM (x)) ˆ ˆ ˆ f (x) = h(x)T β + β0

January 2003

Trevor Hastie, Stanford Statistics

8

SVM
1 = αi − 2 i=1 f (x)
N N N

LD

αi αi yi yi h(xi ), h(xi )
i=1 i =1

= h(x)T β + β0
N

=
i=1

αi yi h(x), h(xi ) + β0 .

LD and constraints involve h(x) only through inner-products K(x, x ) = h(x), h(x ) Given a suitable positive kernel K(x, x ), don’t need h(x) at all! ˆ f (x) =
i∈S

ˆ αi yi K(x, xi ) + β0 ˆ

January 2003

Trevor Hastie, Stanford Statistics

9

Popular Kernels
K(x, x ) is a symmetric, positive (semi-)deﬁnite function. dth deg. poly.: K(x, x ) = (1 + x, x )d radial basis: K(x, x ) = exp(− x − x Example: 2nd degree polynomial in IR2 . K(x, x ) = (1 + x, x )2 = (1 + x1 x1 + x2 x2 )2 = 1 + 2x1 x1 + 2x2 x2 + (x1 x1 )2 + (x2 x2 )2 + 2x1 x1 x2 x2 Then M = 6, and if we choose √ √ h1 (x) = 1, h2 (x) = 2x1 , h3 (x) = 2x2 , h4 (x) = x2 , h5 (x) = x2 , 1 2 √ and h6 (x) = 2x1 x2 , then K(x, x ) = h(x), h(x ) .
2

/c)

January 2003

Trevor Hastie, Stanford Statistics

10

SVM - Radial Kernel in Feature Space
...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... . ...................... ................... .. . ...................... ...................... .................. ... . ...................... ............... ...... . ...................... ...................... .............. ....... ...................... ...................... . ...................... .......... ........... . ...................... ...................... ......... ............ ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ..... ..... . . . . . . . . . . Error:. 0.160. Training ...................... ...................... ...................... ...................... . . . . . . Error: . . . . 0.218. ................ Test. . . . . . . . . . . . . . . . . ..... ...... ..... ...................... ...... ...................... . . . . . . . . Error:. . . 0.210. ..... Bayes . . . . . . . o.... o.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... •.. ...... .. .. ..o.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . . . . . . .•. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o ............................................... . . ... . . .o. o . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . o o ............................................... .. . . . . . o . .o. . . . . . . . . . . . •. . . . . . . . . . . . . . . . . . . . . . . . . . . .......................... .................... . . . .o. . . . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o . . .•.o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................. ... . . o o o . . . .o. . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...................................... . . . . . . . o • o ......o...... ...... o...... ......o...... ...... ..... ..... ..... ..... ..... ..... ..... ..... .....o.....o..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... . . . . o oo • . . . . . . . . . . . . . . . . . .o . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................ o . . o ... ... ... ... ... ... ... ... ... o... ... ... ... ... ... ...o ... ... o.. .. .. .. .. .. .. .. .. .. .. ..o.. .. .. .. .. .. .. .. .. .. .. .. .. o .. . . o .. .. .. ... ... .. ... ... ...o .. ... •... .. .. .. .. .. .. .. .. .. .. .. .. .. ..... .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . . . . . . .o . o o. . . o . . . . . . . . . . . . .• . . . . . . . . . . . . . . . . o . . o o o..... ..... o..... ..... .... .... .... .... .... .... .... .... .... ..... ....o ..... .... .... o.... .... .... .... .... .... ....o.... .... .... .... .... .... .... .... .... ....o .... .... .... .... .... .... . • •o o .......o.. .......o.. .o. o. . . .o. . .. . .....o. . ..... . . . . . . . o. . . . . . . . .o..... . . . . . . . . . . . •. . . . . . . .• . . . . o . . . . . o. . . . . . o. . . . . . . . . . . . . . . oo . .. oo oo.. .. ... .. .. .. .. .. .. .. .. ... o.. o.. .... ... .. .. .. .. .. ... .. .. ..o.. .. ... .. .. .. .. .. .. ... o.. .. .. .. ..o oo o... . . . . . . . . . . . . ... . ... . . . . . . . . . .o . .... . . . . . . . . . . •. . . . . ... •o..... .. .. o.. o.. .. .. .. .. .. .. .. ..o..o..o.. .. ..o.. .. oo.. .. .. .. .. .. .. .. .. .. .. o..... .. .. .. .. .. .. o . • . . . . . . .. . ...........o.................................. . ..... ... ..... . . .. . . .o. .o . o . . .o.o . . oo. . . . . . . . . . . . o. . . . . . . . . . . . . . . o • . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . •. . . . . . . . . . . . . . o . . o . . . . . . . . . . .o . . .o .o. . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . o . . ... . o. . . . .o . . .o . . . . . ... .. . . . . . . . . . . . . . .. . . . .. . . . o...oo.... o.... .... .... o.... .... ... .. .. .. .. ... .. .. .. .. ..• .. .. .. ... .. .. .. .. .. .. .. ..o .. .. ..o.. .. .. .. .. .. . .. . . . . . . . • . . . . . . . . . . . o. . . . . . o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . .. . . . . . o. . . o. . . . oo . . . . o. . . . . . . o o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . •. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. o . . . . . . .o . . . o. . . . o .... .... .... .... .... .... .... o.... .... .... .... .... .... ....o ....oo....o.... .... o.... .... ....o.... •.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... . . .. . . ......o..o............................ . . .o. . . . . o. . .o. . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . o . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . .o . . . . o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .......... . ...... . . . . . . . . . . . . . . .oo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o . ... .. . .. . . .. . ............................................... . . . . . . . . . . . . . . . . .o . . o . . . . . . . . . . . . . . . . . . . . . . . . . . o • o ... ... ... ... ... ... ... ... ... ... ... ... ... ...o ...• ... ... ... o... ... ... ... ... ... ...o... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... . . . . . . . . . . . . o .... .... ....o.... ....o.... .... ....o .... .... o.... .... .... .... .... .... .... o.... .... .... .... .... .... ....•.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... • ... ... ... ....... ...o....... ... ... ... ....... ... ... ....... ... ... ... ... ... o... ....... ... ... ... ... ... ... ...o....... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .• . . . . . . .o . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . oo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................... . .... . . . . . . . . . . . . . . . . . o . . . . o. . . .o. . . . . . . . . . . . . . . . . . . . ............................................... .. . . . . . . . . . . . . . . . o . .o. o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . ............................................... . ............................................... o • ............................................... .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . ............................................... ............................................... . .. . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . o . . . . . . . . . . . . . . ............................................... . .. . . . . . . . . . . . . . . .• . . . . . . . . . . . . . . . • . . . . . . . . . . . . . .

o •

Dim h(x) inﬁnite

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

. . . . . .

.. .. .. .. .. ..

.. .. .. .. .. ..

.. .. .. .. .. ..

. . o.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. •

................ ................ ................ ................

• Fraction of support points depends on overlap; here 45%. • The smaller B, the smaller the overlap, and more wiggly the function. • B controls generalization error.

January 2003

Trevor Hastie, Stanford Statistics

11

Curse of Dimensionality
Support Vector Machines can suﬀer in high dimensions.
Test Error (SE) Method 1 2 3 4 5 6 SV Classiﬁer SVM/poly 2 SVM/poly 5 SVM/poly 10 BRUTO MARS Bayes No Noise Features 0.450 (0.003) 0.078 (0.003) 0.180 (0.004) 0.230 (0.003) 0.084 (0.003) 0.156 (0.004) 0.029 Six Noise Features 0.472 (0.003) 0.152 (0.004) 0.370 (0.004) 0.434 (0.002) 0.090 (0.003) 0.173 (0.005) 0.029

The addition of 6 noise features to the 4-dimensional feature space causes the performance of the SVM to degrade. The true decision boundary is the surface of a sphere, hence a quadratic monomial (additive) function is suﬃcient.

January 2003

Trevor Hastie, Stanford Statistics

12

SVM via Loss + Penalty
3.0 Binomial Log-likelihood Support Vector 2.5

Loss

2.0

With f (x) = h(x)T β +β0 and yi ∈ {−1, 1}, consider
N

1.5

1.0

β0 , β

min

[1−yi f (xi )]+ +λ β
i=1

2

0.0

Solution identical to SVM solution, with λ = λ(B).
-3 -2 -1 0 1 2 3

0.5

N

yf (x) (margin)

In general min

β0 , β

L[yi , f (xi )]+λ β
i=1

2

January 2003

Trevor Hastie, Stanford Statistics

13

Loss Functions
For Y ∈ {−1, 1} Log-likelihood: L[Y, f (X)] = log 1 + e−Y f (X) • (negative) binomial log-likelihood or deviance. • estimates the logit Pr(Y = 1|X) f (X) = log Pr(Y = −1|X) SVM: L[Y, f (X)] = (1 − Y f (X))+ . • Called “hinge loss” • Estimates the classiﬁer (threshold) C(x) = sign Pr(Y = 1|X) − 1 2

January 2003

Trevor Hastie, Stanford Statistics

14

SVM and Function Estimation
SVM with general kernel K minimizes:
N

(1 − yi f (xi ))+ + λ f
i=1

2 HK

with f = b + h, h ∈ HK , b ∈ R. HK is the reproducing kernel Hilbert space (RKHS) of functions generated by the kernel K. The norm f HK is generally interpreted as a roughness penalty. More generally we can optimize
N

L(yi , f (xi )) + λ f
i=1

2 HK

January 2003

Trevor Hastie, Stanford Statistics

15

The solutions have the form
N

ˆ f (x) = ˆ + b
i=1

αi K(x, xi ), ˆ

a ﬁnite expansion in the representers K(x, xi ).

January 2003

Trevor Hastie, Stanford Statistics

16

Aside: RKHS
Function space HK generated by a positive (semi-) deﬁnite function K(x, x ).
∞

Eigen expansion: K(x, y) =
i=1

γi φi (x)φi (y)

with γi ≥ 0,

∞ i=1

2 γi < ∞. f ∈ HK if ∞

f (x) ci ||f ||2 K H

=
i=1

ci φi (x) φi (t)f (t)dt
∞

=
def

=

c2 /γi < ∞ i
i=1

The squared norm J(f ) = ||f ||2 K is viewed as a roughness penalty. H

January 2003

Trevor Hastie, Stanford Statistics
LR - Radial Kernel in Feature Space
...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... . ...................... ..................... ...................... ...................... . ...................... ...................... .................. ... . ...................... ................ ..... ...................... ...................... ...................... . ...................... ............ ......... . ...................... ...................... ........... .......... . ...................... ......... ............ . ...................... ....... .............. . ...................... ..... ................ .... ...................... ...................... .................. ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... ...................... Training . . . . . . . . . . . . . . . . . . . . Error:. 0.150. ...................... ...................... ...................... ...................... ..... Test . . . . . . . . . . . . Error: . . . . 0.221. ...................... ...................... ...................... . . . . . . . . Error:. . . 0.210. ..... Bayes . . . . . . . . . o.... o.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... . . . . . . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................... o .......................... .................... ... . .. . .o. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . ......................... ..................... . . . . o o . .o. . .o. . . . . . . . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . ..o. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . ........................ ...................... .. . . . .o. . . . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....................... ....................... . . o . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o o o ............................................... . . . . . . .o. . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . oo .o . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..................... ........................ o .. . . o .. o . .. o ..... ..... ..... .....o ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... .....oo..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... ..... . .. .. o . . o ... ... ... ... ..... ... ... ... ...o... ... ... ... ... ... ...o ...oo... ... ... ... ... ... ... ... ... ... ... ...o... ... ... ... ... ... ... ... ... ... ... ... ... o .... .... . o. . o. o. . ... o. . . . . . . . . . . . . . .... . . . . . . . . . . . . . .o . . o .. .. .. .. . . . . .o . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................... .... .o . . . . . . . . . o o o o..... ..... o..... ..... .... .... .... .... .... .... .... .... .... ..... ....o .... .... .... o.... .... .... .... .... .... ....o.... .... .... .... .... .... .... .... .... ....o .... .... .... .... .... .... . . . . . . . . o o ....o.. ....oo..o o.. .. .. ..o .. .. .. ....o .. .. .... .. .. .. o.. .. ..o.. ..o.. .. .. .. .. ..o.... .. .. .. .. .. .. oo oo.. .. .... .... ..o .... .. .. .. .. .... o.. o.. o.. .. .. .. .. .. .... .. .. .. .... .. .. .... .. .. .. .. .. .. .... o.. .. .. .. ..o . oo o..... . . . . . ..... . . . . . . ... . ... . ..... . . . . . . . .o . . . . . . . . . . . . ... . . . . . ... o o.....o... o... o... ... ... ... ... ... ...o...o...o...o ... ... ...o... ... oo... ... ...o... ... ... ... ... ... ... o... ... ... ... ... ... ... . . . . . . . . . .. . . . . . . . . . . . .. . . . . . .o . o . . .o. . . . oo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..... o o . . . . . o. .. . ... .. .... .. .. . . . .. . . ..o . . . .. . .. . . . . . .. . . . . . . . o . . . . . . . . . . . .o . . .o .o. . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . o . . .. . o. . . . .o . . . ... . . . . . . .. . . . . . . . . . . . . . .. . . .. . . . o....oo.... o... ... ...o... .... ... .. .. .. .. ... .. .. .. .. .. o.. .. .. .. .. .. .. .. .. .. .. ..o .. .. ..o.. .. .. .. .. .. . .. . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .. . . . . . . . . . o . . o. . . . . . o. . . . . . . o . . . . . . . . . . . . . . . . . . . . . o. . . o. . . . . o . . . o . . . . . . . . . o. . . . . . . . . . . . . . . . . ............................................ o . ... . . . .. .. . ..o.. ... . . . .. . . . .. .. .. . . . . . . .. . . . .. . . . . . . .... . . . . . . . . o. . . . . . . . . . . .o. . . . . . . . . . . . . . . . . . . . . . . . . . . o . . o ... ... ... ... ... ... ... ... o.. .. .. .. .. ..o .. ... .. o... .. .. o.. .. ..o.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . ............................................... . .. . ... .... .. . . . oo . . . . .o. . . . . o. . .o. o . . o . . . . . . . . . . . . . .o . . . . . . . . . . . . . .................... . ............... ....... .. . . . . . . . . o . . . . o o. o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o ... ... ... ... .... ...o ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ........... . ........... . o . . o ... ... ... ... ... ... ... ... ... ... .. .. .. ..o .. o.. .. .. o.. .. .. .. .. .. ..o.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. . . . . . . . . . . ............................................... . . . . . . . . . . o .... .... ....o.... ....o.... .... ....o .... ....o.... .... .... .... .... .... .... o.... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... .... . . . . . . .. . . . . . . .o . . . . . . . . . . . . . o. . . . . . . . .o . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . ...................... . . ............................................... o . ...............o.............................. .... . . . . . . . . . . . . . oo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . .o. . . . . . . . . . . . . . . . . . . . ............................................... ... . . . . . . . . . . . . . . . . . o . .o. o. . . . . . . . . . . . . . . . . . . . . . . . . . . ............................................... . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . ............................................... .. o . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . . ............................................... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . . ............................................... . ............................................... . . . . . . . . . . . . . . .o . . . . . . . . . . . . . . . o . . . . . . . . . . . . . . ............................................... . .. ............................................... ...............................................

17

o

Kernel Logistic Regression

.. .. .. .. ..

. . . . .

.. .. .. .. ..

.. .. .. .. ..

. . . . .

.. .. .. .. ..

.. .. .. .. ..

.. .. .. .. ..

. . . . .

.. .. .. .. ..

.. .. .. .. ..

. . . . .

.. .. .. .. ..

.. .. .. .. ..

. . . . .

.. .. .. .. ..

.. .. .. .. ..

.. .. .. .. ..

. . o.. .. .. .. .. .. .. .. .. .. .. .. .. .. ..

................ ................ ................

• Replace (1 − yf )+ with ln(1 + e−yf ), the binomial deviance.
ˆ ˆ ˆ • Pr(Y = 1|x) = ef (x) /(1 + ef (x) ), so class probabilities directly available.

• We have graphed the 0.5 (solid), 0.25, and 0.75 (broken) ˆ contours of Pr(Y = 1|x).

January 2003

Trevor Hastie, Stanford Statistics

18

Comparison: KLR vs SVM
• The classiﬁcation performance is very similar. • Has limiting optimal margin properties (next slide). • Provides estimates of the class probabilities. Often these are more useful than the classiﬁcations (e.g. credit risk scoring). • Generalizes naturally to M-class classiﬁcation through kernel multi-logit regression: efj (x) Pr(Y = j|x) = f (x) e 1 + · · · efM (x) with m fm (x) = 0. Fit using multinomial log-likelihood and M penalty m=1 fm HK .

January 2003

Trevor Hastie, Stanford Statistics

19

KLR and Optimal Margins
Suppose h(x) is rich enough so that f (x) = h(x)T β + β0 can separate the training data. ˆ Consider β(λ), the solution to
N β0 , β

min

L[yi , f (xi )] + λ β
i=1

2

,

where L is the binomial deviance (negative log-likelihood). Theorem (Rosset, Zhu & Hastie 2002) ˆ limλ→0 β(λ) = β ∗ , the maximum margin solution.

January 2003

Trevor Hastie, Stanford Statistics

20

Multi-class IVM - with 32 import points
.................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. ............................................................................................... ............. ...... .......................................................................................... ...................... .. ....................................................................................... ........................... ..................................................................................... ............................. ................................................................................... ............................... ................................................................................. ................................. ................................................................................ .................................. .............................................................................. .................................... ............................................................................. ..................................... ............................................................................ ...................................... .......................................................................... ........................................ ......................................................................... ......................................... ........................................................................ .......................................... ....................................................................... ........................................... ..................................................................... ............................................. ............................................... ................................................................... ............................................ ..................................................................... . .......................................... .................................................................... .... ........................................ .................................................................. ........ ....................................... ............................................................. .............. ...................................... ..................................................... ....................... ..................................... ............................................ ................................. ..................................... ........................................ ..................................... .................................... ........................................ ...................................... .................................... ....................................... ....................................... ..................................... ...................................... ....................................... ..................................... ...................................... ....................................... ..................................... ...................................... ....................................... ...................................... ...................................... ...................................... ...................................... .................................... ...... ................... ............... ...................................... .................................. .......................................... .................................... ....... ............................ ........................................... ................................. .............. ....................... ............................................ .............................. ................... .................... ............................................. ........................... ....................... .................. .............................................. ........................ .......................... ................. ............................................... ..................... ............................. ................. ............................................... ................. ................................ ................. ................................................ ............ .................................... .................. ................................................ .... ........................................... .................. ................................................. .............................................. ................... ................................................. .............................................. .................. .................................................. .............................................. .................. .................................................. ............................................... ............... .................................................... ................................................. ........... ...................................................... .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. Training Error: 0.237 .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. Test Error: 0.259 .................................................................................................................. .................................................................................................................. .................................................................................................................. .................................................................................................................. Bayes Error: 0.251

o o •o o o o • • o ooo o oo o o o o o oo • o o o o o o • o o o o o o o oo oo o o o o o oo oo o o o ooo oo o o o o oo • o o o o • oo o oo o o oooo o o o o o o o oooo o • o o • o o o o oo o oo o o oo o o o o o o o • • o ooo ooooo o o o oo o o o o • o o • o o ooo o oo o • o o oooo o o o oo o oo o• o oo o o oo o o o o o o oo o o o o o o • oo o o • oo o o oo o oo oo o o oo oo o • o o o oo o • o o oo o oo oo o o o o • oo o o oo o o o oo o o o o o •o o o • oo • ooo o o o o o o o oo • • o o o ooo o o oo • o • oo o o •o oo o •o oo o o o oo oo o • oo oo o • o

o

o

o • o o

January 2003

Trevor Hastie, Stanford Statistics

21

Disadvantages: KLR vs SVM
• Computationally more expensive O(N 3 ) versus O(N 2 m), where m is the number of support points. In noisy problems, m can be large, approx N/2.
N ˆ • With KLR ﬁt f (x) = ˆ + i=1 αi K(x, xi ), all the αi are b ˆ ˆ typically nonzero. For the SVM, only the support points have nonzero αi . This allows for a useful data compression and ˆ quicker lookup.

• SVMs are hot right now, while logistic regression is a traditional statistical tool.

January 2003

Trevor Hastie, Stanford Statistics

22

Final Classifier G(x) = sign
Weighted Sample
M m=1

αm Gm (x)

GM (x)

Boosting
Weighted Sample

G3 (x) Classiﬁers are trained on weighted versions of the dataset, and then combined to produce a ﬁnal prediction.

Weighted Sample

G2 (x)

Training Sample

G1 (x)

January 2003

Trevor Hastie, Stanford Statistics

23

AdaBoost (Freund & Schapire, 1996)
• Start with weights wi = 1/N ∀i = 1, . . . , N . yi ∈ {−1, 1}. • Repeat for m = 1, 2, . . . , M : • Estimate the weak learner fm (x) ∈ {−1, 1} from the training data with weights wi . • Compute em = Ew [1(y = fm (x)], cm = log((1 − em )/em ). • Set wi ← wi exp[cm · 1(yi = fm (xi )], i = 1, 2, . . . N , and renormalize so that i wi = 1. • Output the majority weight classiﬁer M C(x) = sign[ m=1 cm fm (x)].

January 2003

Trevor Hastie, Stanford Statistics

24

3.0

Loss

0.0

0.5

1.0

1.5

2.0

SVM, KLR and Boosting?

Misclassification Exponential Binomial Deviance Support Vector

2.5 -2

-1

0

1

2

yf (margin) • Boosting builds a sequence of models fJ (x) = J gj (x), where j=1 each gj (x) is a “weak” classiﬁer ﬁt to weighted training data. • Even though at stage J, fJ (x) may have zero training errors, boosting increases the “margin”. • Actually, boosting is ﬁtting the model f (x) = log Pr(Y = 1|x)/Pr(Y = −1|x) by stagewise optimization of the loss function L[Y, f (X)] = exp[−Y f (X)] (FHT, 2000), Ann. Stat.

January 2003

Trevor Hastie, Stanford Statistics

25

Boosting and L1 Penalized Fitting
In a restricted setting where • the base learners are chosen from a ﬁxed set of basis functions; • the increments at each boosting step are shrunk towards zero; • + a few mild assumptions (yeah, right!), the boosting sequence corresponds to a sequence (as λ varies) of solutions to the L1 penalized optimization problem
N

min
β i=1

L[yi , f (xi )] + λ β

1

where L[Y, f (X)] = exp[−Y f (X)]. ˆ • As λ ↓ 0, β → β ∗ , the L1 optimal margin separator.

January 2003

Trevor Hastie, Stanford Statistics

26

Details
• forward stagewise:(idealized boosting with shrinkage). Given a family of basis functions h1 (x), . . . hM (x), and loss function L.
m k βm hm (x).

• Model at kth step is Fk (x) =

• At step k + 1, identify coordinate m with largest |∂L/∂βm |, k+1 k and update βm ← βm + . • Equivalent to the lasso: min L(β) + λk ||β||1 • As λk ↓ 0, β k → β ∗ , the L1 optimal margin separator.

January 2003

Trevor Hastie, Stanford Statistics

27

Example and Illustration
Lasso
lcavol

Forward Stagewise
lcavol

0.6

0.4

Coeﬃcients

0.2

0.0

gleason

0.0

0.2

svi lweight pgg45 lbph

Coeﬃcients

0.4

0.6

svi lweight pgg45 lbph

gleason

age

age

-0.2

lcp

-0.2

lcp

0.0

0.5

1.0

1.5
k

2.0

2.5

0

50

100

150

200

250

t=

|αk |

Iteration

January 2003

Trevor Hastie, Stanford Statistics

28

Summary
• SVM can be viewed as regularized ﬁtting with a particular loss function: hinge loss. • Regularized logistic regression gives very similar ﬁt, with added beneﬁts. Also approaches a separating hyperplane. Uses binomial deviance as loss. • Boosting can be viewed as L1 regularized ﬁtting (exponential or binomial loss); has optimal margin limiting behavior.

