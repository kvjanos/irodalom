Distributed Data Warehousing Using Web Technology ISBN: 0814405886 by R.A. Moeller AMACOM © 2001, 384 pages Turn to this book to learn to manage mountains of data through distributed data warehousing techniques.

Table of Contents Back Cover

Colleague Comments

Synopsis In this age of the Internet, corporations are turning increasingly to the power of the web to manage their vast stores of information. And rightly so, as the web gives users access anytime and anyplace. But managing the new data warehouse requires an understanding of new techniques. This book covers them all from creation to organization to management, and then down to tips on analysis and synthesis. Data warehousing is of vital importance to today's business strategies, and this book sets any corporation on the right track.

Table of Contents:
Distributed Data Warehousing Using Web Technology......................................................... - 7 How to Build a More Cost-Effective and Flexible Warehouse ...................................... - 7 Dedication...................................................................................................................................... - 7 Preface ...................................................................................................................................... - 7 Acknowledgments .................................................................................................................... - 8 Chapter 1: Introduction ................................................................................................................ - 8 Overview..................................................................................................................................... - 9 A Brief History of Data Warehousing ........................................................................................ - 9 What Is a Data Warehouse? ...................................................................................................... - 11 Pressure to Distribute Data ....................................................................................................... - 12 Data Mart Issues........................................................................................................................ - 17 The Web and Other Emerging Technologies............................................................................ - 20 In Brief ...................................................................................................................................... - 23 Chapter 2: Data Warehouse Architecture .............................................................................. - 23 Overview................................................................................................................................... - 23 Role of a Data Warehouse ........................................................................................................ - 25 Data Warehouse Definition....................................................................................................... - 25 Enterprise Data Model....................................................................................................... - 30 Metadata .............................................................................................................................. - 30 Data Integration/Upload .................................................................................................... - 30 Warehouse (Database) ..................................................................................................... - 31 OLAP .................................................................................................................................... - 31 Management ....................................................................................................................... - 31 Traditional Data Warehouse Architecture ................................................................................ - 31 Another Approach .............................................................................................................. - 35 Challenges of a Centralized Data Warehouse........................................................................... - 37 Data Model/Metadata ........................................................................................................ - 37 Parochialism........................................................................................................................ - 38 -2-

Implementation Challenges .............................................................................................. - 38 Distributed Data Warehouse Architecture ................................................................................ - 40 Challenges of a Distributed Data Warehouse ........................................................................... - 46 Distributed Database Management Tool/System ......................................................... - 46 Data Model/Metadata ........................................................................................................ - 47 Data/User Interface ............................................................................................................ - 47 Maintenance........................................................................................................................ - 47 What to Choose......................................................................................................................... - 48 Chapter 3: Data Placement in Distributed Warehouses—Distribution and Transformation...48 Overview................................................................................................................................... - 48 The Data Placement Problem.................................................................................................... - 49 A Practical Approach to the Placement Problem...................................................................... - 52 Maximizing User Access ................................................................................................... - 52 Minimizing Load Times...................................................................................................... - 55 Integration of Diverse Database Systems ................................................................................. - 57 Abstractions......................................................................................................................... - 57 Transformations.................................................................................................................. - 57 Mapping ............................................................................................................................... - 58 The Global Data Model............................................................................................................. - 59 Methodologies..................................................................................................................... - 59 What to Remember ................................................................................................................... - 62 Chapter 4: Concurrency Control in Distributed Warehouses .............................................. - 64 Overview................................................................................................................................... - 64 Transactions .............................................................................................................................. - 65 Interference between Concurrent Transactions......................................................................... - 68 Schedules and Serialization ...................................................................................................... - 70 Concurrency Control Techniques ............................................................................................. - 73 Locking Protocols ............................................................................................................... - 73 Timestamping ..................................................................................................................... - 78 Optimistic Methods............................................................................................................. - 79 Special Case Concurrency Control ........................................................................................... - 80 Data Warehousing and Concurrency Control ........................................................................... - 83 Chapter 5: The Web and the Data Warehouse ..................................................................... - 84 Overview................................................................................................................................... - 85 Emerging Trends....................................................................................................................... - 86 Intranet Basics........................................................................................................................... - 89 The Advantages of Intranets and Data Warehousing................................................................ - 92 The Challenges of Intranets and Data Warehousing................................................................. - 93 Analytic Layer ..................................................................................................................... - 94 File Management Layer..................................................................................................... - 94 Security Layer ..................................................................................................................... - 95 Common Gateway Interfaces.................................................................................................... - 95 The Future of Web-Enabled Data Warehousing....................................................................... - 97 Chapter 6: Data Marts ............................................................................................................... - 98 Overview................................................................................................................................... - 98 Data Mart Solutions .................................................................................................................. - 99 What Is a Data Mart? .............................................................................................................. - 100 Dependent Data Marts .................................................................................................... - 100 Multidimensional Data Marts .......................................................................................... - 101 ROLAP Data Marts .......................................................................................................... - 101 Independent Data Marts.................................................................................................. - 102 -3-

Point Solution Data Marts ............................................................................................... - 102 Integrated Data Marts...................................................................................................... - 103 Networked Independent Data Marts = Distributed Data Warehouse....................... - 103 Implementing a Data Mart ...................................................................................................... - 105 Source Data ...................................................................................................................... - 105 DBMS Type ....................................................................................................................... - 105 External Data Sources..................................................................................................... - 105 Metadata and Data Model............................................................................................... - 106 Performance...................................................................................................................... - 106 Scalability........................................................................................................................... - 107 Flexibility ............................................................................................................................ - 107 Security .............................................................................................................................. - 108 Data Mart Architecture ........................................................................................................... - 108 Hardware Architectures................................................................................................... - 110 DBMS Schema and Indices............................................................................................ - 110 The Future of Data Marts........................................................................................................ - 112 Chapter 7: Object Technologies ............................................................................................ - 113 Overview................................................................................................................................. - 113 Object-Oriented Technologies ................................................................................................ - 114 Distributed Object Computing ................................................................................................ - 117 ORB Architecture ................................................................................................................... - 119 Standards................................................................................................................................. - 120 Data Warehouses and Object Technologies............................................................................ - 121 Extended Relational Database Management Systems ............................................................ - 121 In Brief .................................................................................................................................... - 122 Chapter 8: Security Issues for the Distributed Data Warehouse ...................................... - 122 Overview................................................................................................................................. - 122 Key Concepts of Security Planning ........................................................................................ - 124 Malicious Programs ......................................................................................................... - 124 Snooping............................................................................................................................ - 125 Physical Theft of Data ..................................................................................................... - 125 Network Security Issues.......................................................................................................... - 127 Repudiation ....................................................................................................................... - 127 Integrity .............................................................................................................................. - 128 Confidentiality ................................................................................................................... - 128 Access Control.................................................................................................................. - 128 Overview of Network Architecture......................................................................................... - 129 TCP/IP "Stacks"................................................................................................................ - 130 TCP/IP Network Security Architecture.......................................................................... - 132 Common Gateway Interface (CGI)......................................................................................... - 133 Managing Security .................................................................................................................. - 134 Chapter 9: Tools for Distributed Warehouse and Web Security....................................... - 135 Overview................................................................................................................................. - 135 Firewalls.................................................................................................................................. - 135 Encryption............................................................................................................................... - 137 Symmetric Algorithms...................................................................................................... - 138 Public Key Algorithms...................................................................................................... - 138 Data Encryption Software ............................................................................................... - 140 Authentication Services .......................................................................................................... - 141 DCE/Kerberos................................................................................................................... - 142 PKI ...................................................................................................................................... - 143 Secure Sockets Layer/Transport Layer Security..................................................................... - 144 -4-

Internet Protocol Security ....................................................................................................... - 145 Virtual Private Networks ........................................................................................................ - 146 Browser Security Options ....................................................................................................... - 148 Host Security........................................................................................................................... - 149 Final Thoughts ........................................................................................................................ - 151 Chapter 10: Implementing a Distributed Data Warehouse................................................ - 151 Overview................................................................................................................................. - 151 Overview................................................................................................................................. - 152 Data Marts............................................................................................................................... - 153 To Be Considered.................................................................................................................... - 155 Pitfalls ..................................................................................................................................... - 156 Business Factors...................................................................................................................... - 158 Initial Tools Selection....................................................................................................... - 158 Customization ................................................................................................................... - 159 Prototyping ........................................................................................................................ - 159 Implementation Plan ............................................................................................................... - 160 Moving Data into the Warehouse through Publish/Subscribe................................... - 161 Final Considerations ............................................................................................................... - 162 Chapter 11: Future Developments ........................................................................................ - 163 Overview................................................................................................................................. - 163 Evolution................................................................................................................................. - 164 Intelligent Distributed Systems............................................................................................... - 166 Merging Expert Systems and Databases ..................................................................... - 167 Natural Language Processing ........................................................................................ - 169 The Economic Reality of the Web.......................................................................................... - 170 The New Paradigm.................................................................................................................. - 171 Conclusions............................................................................................................................. - 172 Glossary............................................................................................................................. - 174 A .......................................................................................................................................... - 174 B-C...................................................................................................................................... - 175 D.......................................................................................................................................... - 176 E-F ...................................................................................................................................... - 179 G-I ....................................................................................................................................... - 180 J-M ...................................................................................................................................... - 182 N-O ..................................................................................................................................... - 183 P .......................................................................................................................................... - 185 Q-R ..................................................................................................................................... - 186 S .......................................................................................................................................... - 187 T .......................................................................................................................................... - 189 U-W..................................................................................................................................... - 190 X-P ...................................................................................................................................... - 192 Chapter 1: Introduction ........................................................................................................... - 192 Chapter 2: Data Warehouse Architecture ............................................................................... - 192 Chapter 3: Data Placement in Distributed Warehouses—Distribution and Transformation .. - 192 Chapter 4: Concurrency Control in Distributed Warehouses ................................................. - 192 Chapter 5: The Web and the Data Warehouse........................................................................ - 193 Chapter 6: Data Marts ............................................................................................................. - 193 Chapter 7: Object Technologies.............................................................................................. - 193 Chapter 8: Security Issues for the Distributed Data Warehouse............................................. - 193 Chapter 9: Tools for Distributed Warehouse and Web Security ............................................ - 193 Chapter 10: Implementing a Distributed Data Warehouse ..................................................... - 193 Chapter 11: Future Developments .......................................................................................... - 193 -5-

Table of Contents Distributed Data Warehousing Using Web Technology Preface Acknowledgments Chapter 1 - Introduction Chapter 2 - Data Warehouse Architecture Chapter 3 Data Placement in Distributed Warehouses—Distribution and Transformation

Chapter 4 - Concurrency Control in Distributed Warehouses Chapter 5 - The Web and the Data Warehouse Chapter 6 - Data Marts Chapter 7 - Object Technologies Chapter 8 - Security Issues for the Distributed Data Warehouse Chapter 9 - Tools for Distributed Warehouse and Web Security Chapter 10 - Implementing a Distributed Data Warehouse Chapter 11 - Future Developments Glossary Index List of Figures

Back Cover How to transform raw data into knowledge—and empower businesses with a significant competitive edge! What is a “data warehouse?” It’s a database that’s structured to support business decision making. A data warehouse organizes, analyzes, and synthesizes huge amounts of raw data—so that it’s intelligible and useful to employees. Until recently, all that data would be stored in centralized, monolithic storage media. It was cumbersome to manage, slow to operate, and expensive to maintain. Internet technology has changed all that forever! In this insightful book, managers and information technology professionals get a clear-eyed overview of what’s required to set up and use distributed data warehouses—which utilize multiple computers (networked together via Internet technology) to store and access data as needed. The book takes a look at: • • • • Basic functions and benefits of a distributed data warehouse and what’s required to set one up Many of the complex technical issues involved, such as placement of functions and data, web-enabled computing technologies, object technologies, and more The full spectrum of what a data warehouse can deliver—and how to exploit it How to present and justify a business case for creating a data warehouse.

About the Author

-6-

R.A. Moeller (New York City) is the managing partner of C.V. Associates, a company which provides specialized technical and managerial services to the I.T. industry.

Distributed Data Warehousing Using Web Technology
How to Build a More Cost-Effective and Flexible Warehouse
R. A. Moeller AMACOM American Management Association New York • Atlanta • Boston • Chicago • Kansas City • San Francisco • Washington, D.C. • Brussels • Mexico City • Tokyo • Toronto

Special discounts on bulk quantities of AMACOM books are available to corporations, professional associations, and other organizations. For details, contact Special Sales Department, AMACOM, a division of American Management Association, 1601 Broadway, New York, NY 10019. Tel.: 212-903-8316. Fax: 212-903-8083. Web site:
www.amacombooks.org

This publication is designed to provide accurate and authoritative information in regard to the subject matter covered. It is sold with the understanding that the publisher is not engaged in rendering legal, accounting, or other professional service. If legal advice or other expert assistance is required, the services of a competent professional person should be sought. Library of Congress Cataloging-in-Publication Data Moeller, R. A. Distributed data warehousing using Web technology : how to build a more costeffective and flexible warehouse / R.A. Moeller. p. cm.
ISBN 0-8144-0588-6 1. Data warehousing. 2. Electronic data processing—Distributed processing 3. World Wide Web. I. Title.

QA76.9.D37 M64 2000 658.4'038'0285574—dc21 00-038984
Copyright © 2001 R. A. Moeller.

All rights reserved. Printed in the United States of America. This publication may not be reproduced, stored in a retrieval system, or transmitted in whole or in part, in any form or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of AMACOM, a division of American Management Association, 1601 Broadway, New York, NY 10019. Printing number 10 9 8 7 6 5 4 3 2 1
Dedication To Louis and Mary Anna, for a thousand reasons

Preface
Imagine a world without news magazines, cable companies, telephone service providers, daily newspapers, or television networks. Replace them with entertainment and information providers, high-speed data carriers, and knowledge stores all catering to us, the information consumers. Not

-7-

much of a stretch—rather, a realistic snap-shot of the near future. Today's businesses must forge ahead into this electronic future, responding the best they can to the evolutionary pressures of a fast-changing environment. And the greatest prize of all, survival, will go to the few businesses with a rapid response capability based on the best information. A big part of this paradigm shift is fueled by distributed computing systems, which are rapidly gaining importance in our economy. In their most notorious form, the World Wide Web, they link hundreds of thousands of people across five continents into a global community. In their less conspicuous incarnation, distributed systems function daily to process millions of tasks for mediumsize and large businesses worldwide. As these distributed systems grow, acquiring more computing power, better connectivity, and faster access tools, they will inevitably evolve to interactive and web-enabled data ware-houses. This marriage of communication facilities with database systems takes them from centralized, monolithic storage facilities to decentralized, almost organic entities in which shared data is the glue that binds together the dissimilar parts. Designing a distributed data warehouse system so that the data required by its individual components, applications, and processes will be available when needed is a daunting engineering task. Both networking and data storage issues are of great importance, and the classical tenets of communications engineering and centralized database design no longer work in a distributed environment. However, new tenets must be established to provide the rapid response businesses will need in the near future. Data warehousing has already proven it gives the best decision support for many companies. Sooner or later, every business will implement a data warehouse. The question in a business's mind cannot be if, but rather when and how, if it hopes to remain competitive. The overwhelming challenge for the developer is to design a framework that will allow the diverse parts of a distributed data warehouse to work together. Web technology is the answer. Data warehouses come in all shapes and sizes. Some are simplistic, others complex. Some encompass a single database while others span the entire mix of the enterprise's computing systems. Whatever they may look like and whatever their complexity, data warehouses transform raw data into knowledge, empowering businesses with a competitive edge. This book, too, is about sharing knowledge, from beginning concepts in data warehousing to building and implementing a distributed data warehouse supported by an Internet/intranet environment. It analyzes technical issues, such as the placement of functions and data in a distributed environment, web-enabled computing technologies, object technologies, and more. It also discusses such business-oriented concepts as when and where to build a data warehouse, what to expect from a data warehouse, and how to get the most for your data warehousing dollars.

Acknowledgments
Writing a book is one of the most time-consuming, stressful, and ultimately rewarding tasks I have ever undertaken. It would not have been possible without the support of my family, friends, and fellow consultants. Many people helped to make this book possible, and the following deserve special mention: Ted Codd and Chris Date, for laying the foundation for data warehousing and, on a personal note, for their generous encouragement of the author. Patricia Ferdinandi, for both her technical expertise and her constant support and enthusiasm. Michael Rothstein, for that "kick in the pants" needed to get the project on target and moving. Jacquie Flynn, for her guidance throughout the daunting publishing process. Mike Sivilli, for shepherding this book and its author through the editing and production process. Joanne O'Neill, for her valuable input concerning data models. Tony Vlamis, for his guidance, experience, and understanding of book-making. Special thanks to Robert Moore, valued business partner and associate, for his many contributions to this project including reviews, proofreading, and, more important, his patience.

Chapter 1: Introduction
-8-

Overview
At no time in the history of data processing have there been so many challenging innovations vying for the attention of computing professionals as there are right now. And at no other time in history has it been so important to the business community that their technologists make the right choices. The very survival of a corporation may depend on the architecture, computing systems, networks, and software chosen today. Computers are no longer just the "back-office operation," useful only for printing statements and addressing envelopes; corporations now depend on them for marketing and sales functions and, most important, as sources of mission-critical information. But how are data processing professionals to chose what is right for their companies? Technological advances are continually being made in hardware, software, and methodologies. The announcement of new software products, the publication of ingenious ideas, and the introduction of faster, more powerful hardware—all bring demands from the user community for inclusion in the users' systems, whether or not there is a match between innovation and the computing environment. There is a perpetual cry for more functionality, flexibility, and performance, even if users are unaware of any particular development likely to improve matters. So the technologist designing a new application or prolonging the life of an old one must always be looking for ways of linking recent developments and ideas to the needs of the users. One area in which new ideas and concepts are becoming increasingly viable and useful from a corporate point of view is distributed data warehousing. These systems manage data stores attached to several computers linked by communications networks and allow users to rapidly retrieve the information stored there. The basic concepts of distributed databases were first discussed in the mid1970s. Schemes for architectures started to appear in the 1980s. Advances in hardware, software, and networking capabilities have finally reached the point that now, at the start of the twenty-first century, distributed data warehouses are a reality.

A Brief History of Data Warehousing
The original conception of a data warehouse was elegant and simple—it would be an enterprisewide information repository (Figure 1-1). All the organization's data would be homogenized and fed into a unified structure under the control of a single database product and reside at one location. From this, all users could gather a view of the organization's activities and various departments could share information. Marketing could "borrow" the customer records from all the salespeople, and a bond trader might use the securities desk's call list to prospect for new clients.

Figure 1-1: Enterprisewide information repository.

-9-

To populate this early data warehouse, all the organization's production systems would feed it their pieces of the puzzle as well as having departments and even individuals contribute important data. All extractions and transformations came under the control of a single process, usually the services of the relational database management system (RDBMS) used for data storage. Reports were printed and some extracted data was transferred to users' personal computers for use in early analytic programs. Many businesses chose to try pilot warehouse projects, with initial deployment limited to a small subset of users in a functional department such as sales. But demand for data access always grows in direct proportion to the success of the warehouse. As the value of the warehouse came to be realized first by sales and later by other areas, demand increased dramatically, spreading from sales to marketing, marketing to research and development, and on throughout the organization. What began as an experiment grew from a few users to a corporatewide necessity. As the warehouse expanded to encompass more departments, the quantity and quality of information compounded, providing better decision support to the entire corporation. While traditional data warehouses successfully organized and directed the information of many companies in the 1980s and 1990s, the quantity of information organizations must manage is snowballing. Organized and readily accessed data seems to breed even more data, sometimes beyond the bounds of the regular data warehouse. Rather than employ an enormous central system, some organizations are moving toward workgroup servers. The workgroup server provides the basic data processing services for a department (e.g., transaction processing or intradepartment e-mail). These workgroup servers are then attached to enterprise servers, which in turn are linked together with communications facilities, yielding a truly distributed data processing environment. This networking together of computing equipment from different "levels" within an organization is often referred to as the creation of an n-tiered environment. The key component in this interconnected architecture is the distributed database that allows the warehouse's data to reside in multiple locations joined by a network. A distributed database environment creates a number of challenges: 1. Making the distributed database transparent to the users while simultaneously delivering the highest levels of performance, reliability, and availability. 2. Accommodating the heterogeneity of the supporting environment. Mix-and-match hardware, differing network protocols, and diverse operating systems are all part of the problem. 3. Locating (or building) systems management tools that will accommodate all the diverse software products and function over the network infrastructure. 4. Managing the communications network to ensure security, reliability, availability, and consistent high levels of performance. These and other problems will be addressed later in this book. As data warehouses continue to grow beyond the bounds of a single computing system, a way must be found to successfully connect all the participating hardware and all the users. A corporate intranet is the ideal vehicle for widespread connectivity within an organization. This access method takes advantage of Internet technologies to deliver data throughout the corporation from any connected internal source, regardless of platform or location. By leveraging Java and other web-enabled application-building tools, businesses can deploy their own custom data access applications easily, making them instantly available to everyone within the company. Many excellent, third-party data access tools are already web-enabled and waiting to be employed in an intranet environment. Since web-enabled applications reside on a server, neither the user's physical location (provided it has access to the corporate intranet) nor the configuration of the individual personal computer are obstacles to accessing and using the product. As a result, the barriers to application deployment disappear. This thin-client approach means that mobile users (i.e., laptop users with limited storage or telecommuters with home machines) have easy access to applications and data as well. Moving from a desktop-centric to a network-centric model is revolutionary for any organization. Installing and maintaining easy-to-use access tools on the desktop of a few users, who are all in one location, is certainly possible for a pilot project—this is a fat-client approach. However, installing software on every desktop in an organization quickly becomes overwhelming, and maintenance is a nightmare. Widespread installs and upkeep result in huge costs to a business.

- 10 -

By making browsers standard equipment for all enterprise desk-tops and installing only web-enabled software, the organization can move toward a thin-client model with a fraction of the systems management cost of the fat-client approach. The network-centric approach eliminates the need for service on every desktop computer. It saves the company time and money as well as simplifying application rollout and maintenance.

What Is a Data Warehouse?
A data warehouse is a tool used by many businesses to gain and maintain a competitive advantage. Gathered from both in-house and external data sources, the modern warehouse is a collection of information stored in a way that improves access to data. Once the information is assembled, some type of assisted query software is used on the desktop to retrieve data from the warehouse, where it is analyzed, manipulated, and reported. In short, owning a data warehouse means faster and better access to widespread information at all levels of the corporation. The idea behind a traditional, centralized data warehouse is to put a wide range of operational data from internal and external sources into one place so it can be better utilized by executives, line managers, and analysts. Keep in mind that while a data warehouse contains data gathered from many different sources and satisfies a diverse range of analytical needs, its smaller version, the data mart, can be equally useful. A data mart focuses on a subset of data, such as sales and marketing or financials only, and produces a select view for a particular purpose—the right tool for the right job. Large businesses often suffer from a common problem: Data is stored everywhere—some in locations known only to a few selected users—and it is associated with many different types of applications and kept in a variety of formats. In some cases, information is locked away in legacy systems to be accessed only when the data processing department is willing to write special programs to extract it. Data may also be stored for use with transactional systems (e.g., billing, inventory, and order entry) and is not organized in a manner that is useful to the user. As a result, it is impossible to consolidate data across applications or to compare data between systems; no one has a reliable picture of the business as a whole. A data warehouse resolves these issues by delivering improved information access and higher-quality decision support. A data warehouse must be viewed as an environment, not a single product or even a convenient package. It cannot be purchased off-the-shelf, ready to go. To acquire a data warehouse, a business must build its own, from the ground up. The component parts need to be chosen carefully for compatibility and usability and to provide the appropriate information. The data warehouse environment consists of: Source systems from which data is extracted Middleware used to extract data for loading into the data warehouse Data warehouse structure where the data is stored Desktop tools used for querying and reporting, to facilitate decision support The source systems for the data warehouse can include: Mainframe and minicomputer systems Workgroup and client/server data files and databases Spreadsheet data Data from external sources Data from personal computers Middleware, the suite of tools used to populate the data warehouse, is responsible for the cleansing and transformation of the data from the source systems. These tools can be designed and coded in-house or purchased from one or more third-party vendors. The cleaning process alone can be very timeconsuming and costly if the data in the source systems has not been well maintained. The data transformation process will vary in complexity from one system to the next and can include: Field translations (i.e., PC to mainframe formats)

- 11 -

Data formatting changes (i.e., Julian date to Gregorian date) Field reformatting (i.e., truncate or pad fields to accommodate the field characteristics specified in the warehouse) Reformatting data structures (i.e., reordering table columns) Substitution through table lookups (i.e., replacing transaction codes with reporting codes) Logical data transformations (based on user-defined business rules) Aggregation of low-level data into summary information Some third-party data transformation tools are also able to schedule extractions from source systems and loads of the data into the warehouse. Unlike the cleaning operations, which are generally only done once for the initial warehouse load, data transformation is an ongoing process. It is often the most difficult and expensive continuing task required to maintain a data warehouse. The heart of any data warehouse is its database, where all the information is stored. Most traditional data warehouses use one of the relational products for this purpose. Because they can manage extremely large amounts of data (hundreds of terabytes) mainframe relational databases, such as DB2, are used for some of the world's largest data warehouses. Universal data servers such as those from Oracle or Informix may be a good choice for medium-size warehouses because they manage a variety of data types extremely well. Multidimensional databases are also becoming more popular, but limit the size of the warehouse to less than 5 gigabytes for now. These multidimensional databases typically run on single- and multiprocessor hardware using either the Microsoft Windows NT or the UNIX operating system. Users have employed a variety of tools to query the data warehouse. With the development of online analytical processing (OLAP) and relational online analytical processing (ROLAP) tools, this task has become easier. OLAP tools work against data stored in multidimensional databases, whereas ROLAP tools work against data stored in relational databases. Some of these tools function as add-ons to popular spreadsheets, so data can be extracted from the warehouse and loaded into a worksheet for analysis. These third-party tools are feature-rich, providing such functions as: "Drill-down" navigation (i.e., moving from summary information to supporting detailed information) Multidimensional "views" Rotation of dimensions Charting and trend analysis Ranking Exception reporting Data mining Row/column matrix and cross-tabulation Most OLAP and ROLAP tools are web-enabled, so queries can be run from a standard Internet/intranet browser. The resulting data can be published as simple web pages accessed by any Internet/intranet user. There are a few vendors who claim to provide an end-to-end data warehouse solution, but most data warehouses involve cooperation between multivendor products.

Pressure to Distribute Data
In service for the last twenty-five years, large relational databases manage the information of many corporations such as banks, insurance companies, health maintenance organizations, and reservation networks. Some have even been organized into data warehouse structures. It was originally thought that collecting all the data into a single relational reservoir (Figure 1-2) would simplify data access.

- 12 -

Using the toolkit of relational services such as data definition languages (DDL), data manipulation languages (DML), referential integrity checkers, constraint monitors, and structured query language (SQL), technologists hoped to provide open access to everyone, including the user community. Even if this approach could not entirely remove programmers from the access path for all data, it was thought that it would at least speed up the query process.

Figure 1-2: Data warehouse as a data reservoir. However, many years of experience with this approach has taught that, while it was certainly acceptable for some users, few got anything like optimal service. There were even a certain percentage of users who had no service because SQL proved too difficult to learn or ad hoc SQL-based queries were too costly and time-consuming to process. Even when functioning optimally, data structures cannot be dynamically altered to fit individual needs without formal data processing intervention. As a side effect of centralized processing, owners lost control of their data stores and found data maintenance complicated by the centralized system. While the organization as a whole may have benefited from these data reservoirs, they are obviously not the ideal solution. As companies grow, so does user frustration if data remains centralized. With the advent of the more recent generations of powerful personal computers, users in many organizations "voted with their feet" and opted for de facto decentralization. They bypassed their own data processing departments and acquired and installed local departmental databases and local area networks (LANs) suited to their group's needs. Difficulties arose in maintaining consistency between the central and local systems, and there were constant problems with data transfer between locations. For example, many users acquired data for their local systems by keying in data from reports generated by the centralized computing system. Not only is this a waste of time and labor, it opens the local system to keypunch errors and virtually guarantees that the local system will be behind the central system because of the time lag between printing the report and keying in the data. So pressure started to grow for formal decentralization of databases and database functions (Figure 1-3) while maintaining integration and perhaps some centralized control.

- 13 -

Figure 1-3: The user in a decentralized environment. Often quoted in the early days of data processing to justify centralization, Grosch's Law stated that "the power of a computer is proportional to the square of its cost." But this principle was repealed in the late 1970s by the onward march of technology—notably, the advent of "cheap memory." New legislation— Moore's Law—took effect when Gordon Moore, the cofounder of Intel Corp., was able to say that "every eighteen months computing capacity doubles and price halves." At about the same time this shift in pricing structure was occurring, reliable and affordable communications network facilities opened the door to data distribution. While the "user pull" for distribution was occurring, these hardware advances opened the door to a "technology push" to distribute data. There are other very good technical reasons for distributing data, such as: Security concerns Capacity planning Recovery issues Distribution can, in some ways, make security simpler. The surest way to protect data is to deny access to it. If the data to be secured is stored on a departmental server, access to it can be limited by denying requests for information originating outside the department. Increasing the capacity of a centralized computing system is a difficult task, often requiring the replacement of the central processor and downtime for the entire computing system. In a distributed environment, a new server added to the network often solves capacity problems without any interruption in service. For disaster recovery and backup purposes it is clear that having data and processing at various locations is desirable; if one processor fails, another picks up its workload and continues on. Decentralization of data brings several benefits to a business: It simplifies systems evolution, allowing improved response to user requirements. It allows local autonomy, giving back control of data to the users. It provides a simple, fault-tolerant, and flexible system architecture, saving the company money. It ensures good performance.

- 14 -

A number of objections can be raised to counterbalance these advantages, some sufficiently troublesome that they might prevent the adoption of the distributed information systems idea. Technological problems currently occupying the attention of developers are: Ensuring that intersite accessing and processing are carried out in an efficient manner Distributing processing between nodes/sites Distributing data to best advantage around the various sites of a network Controlling access (i.e., security) to data linked by networks Supporting disaster recovery efficiently and safely Controlling transaction processing to avoid destructive interference Some design issues are: Estimating capacity for distributed systems Predicting the traffic in distributed systems Designing effectively to maximize data, object, and program placement Minimizing competition for resources between nodes/sites While it is important to recognize and address these difficulties, the basic advantages, both technical and business, of distributed processing give it a decided edge for large-scale projects such as data warehouses. At the same time this pressure for distribution was mounting, a quite different pressure was also building—to use the new technology to integrate, with as little disruption as possible for the users, data systems that were already distributed (and perhaps incompatible). Many of these systems were founded by users dissatisfied with the services provided by centralized computing and were designed and installed without the aid or approval of the enterprise's data processing department. Hence, they vary widely in platform chosen, communications protocols used, and software installed. Having been at the forefront of the distributed data trend, these users will not be satisfied if they are left behind when the rest of the organization is interconnected. The advent of decentralized systems is well justified. But arguments in favor of decentralization have much less force if the integration of already-distributed systems is not addressed (Figure 1-4). Developers are currently tackling the issue of stitching together heterogeneous systems as well as incorporating the more manageable homogeneous ones.

Figure 1-4: Isolation caused by failure to integrate.

- 15 -

The need to integrate disparate data can arise from many sources. Data may be stored at several locations within an organization for historical reasons, but managers at the parent company require summary data from local sites. Integrating the local sites into a global network is only practical. Also, there is often a driving need for integration to handle change. Departments and divisions within an enterprise reorganize, and companies merge or split apart. Any of these actions requires reconfiguration of the computing systems to best serve the resulting organization's needs. Applications are continually being developed in response to changing user needs, changes in local or global standards, new government regulations, or simply to tune their performance. Therefore, management of change is of central importance in database systems for both business and technological reasons. To accommodate change, a database may have to be converted from one logical data model to another. Several databases, each using a different logical data model and a different database management system, may have to be integrated for any number of technology-related reasons, such as: A desire to exploit new database-related technology A desire to get a better match from among the available database systems for a given application profile A desire for greater flexibility or better services These technological issues compound the user pull for change. With almost overwhelming business and technological reasons for integrated distributed systems, it is a foregone conclusion that they must happen. However, just installing communications networks to link computing systems is not by any means a complete answer to distributed systems. Programmers and others who want to make a query using two or more networked databases must: Know the location of the data to be retrieved. Split the query up into bits that can be addressed in single nodes. Resolve, where necessary, any inconsistencies between the data types, formats, or units as stored at various locations. Arrange for the results of each single-site query to be accumulated at the originating system. Merge the data acquired from all sites. Extract the details required. When it is considered that data must also be kept current and consistent and that failures in communications links must be detected and corrected, it is clear that system support services are badly needed. For several years, the issues raised while trying to provide this type of service have been the focus of many in the database community. It is important to avoid the mistake of treating the distribution of data under the umbrella of a data warehouse as providing the ultimate answer to all data handling problems. The systems introduced in this book must be seen as just one tool in the future array of information handling products (Figure 1-5). For now and in the near future, distributed data warehousing offers the best in decision-support services; there are, however, other business needs and new products that promise to fulfill them.

- 16 -

Figure 1-5: A variety of information handling systems.

Data Mart Issues
Business issues may influence the construction of a distributed data warehouse. All companies keep a careful eye on the bottom line, and the political environment in some organizations can be a formidable factor. The politics of data warehousing center on data ownership, user refusal to share data, and users' hidden agendas. Like old-fashioned librarians of legend, some users think that they own the data entrusted to their care. They view data not as a resource whose value increases when shared, but rather as a matter of personal property to be defended against outside encroachment. The basic tenets of data warehousing are anathema to these users, and they will resist warehousing with all their resources. Large data warehouse projects often involve millions of dollars and thousands of workeryears to produce anything useful, a serious expenditure for even the largest of corporations. The scope of a warehouse project is difficult to estimate, and deadlines are hard to set and even harder to meet, leading to cost overruns and missed deadlines. Only rigid control and excellent management can keep a warehousing project from becoming a financial black hole. There are also technology issues that may influence warehouse construction. Performance problems frequently arise in large warehouses because there are difficulties handling many simultaneous requests. With a distributed warehouse design, this problem is ameliorated, but does not completely disappear. Also, the length of time it takes to produce a warehouse often discourages potential users. They argue that, by the time the project is finished, their requirements will have either changed or disappeared altogether. For these and other reasons, data marts, which are smaller subsets of the data warehouse specifically designed to meet departmental needs, have come into fashion. There are two basic types of data marts: dependent and independent. For the dependent data mart, data is loaded from production systems into the centralized warehouse and then extracted, enhanced, and loaded into the data marts (Figure 1-6). This type of data mart eliminates many performance problems and even helps resolve a political issue by returning control of some data to the users. It does nothing, however, for the financial problems. If anything, it exacerbates them because the data warehouse must be built before the first data mart can be implemented.

- 17 -

Figure 1-6: Dependent data mart. With independent data marts, an organization can start small and move quickly, often realizing limited results in three to six months. The data marts are developed one at a time, as need dictates and finances allow, independent of any data warehouse project. The data necessary to populate the independent data mart is extracted directly from the corporate production systems. Issues of political parochialism are ignored by this approach, although they tend to come up later on, whenever someone suggests networking the independent data marts into a distributed data warehouse. Proponents of this approach argue that after starting with a single, small data mart, other marts can proliferate in departments that have the need (Figure 1-7). Eventually, by satisfying the various divisional requirements, an organization can build its way up to a full-scale distributed data warehouse. This approach, like any other, has consequences. The independently developed data marts may not be exact duplicates, in terms of hardware and software, of one another and could be difficult to network and jointly manage. Also, multiple data marts developed without reference to each other require multiple (and sometimes redundant) data extracts from production systems.

- 18 -

Figure 1-7: Independent data marts. And populating data marts or data warehouses even once with usable data is a serious challenge. To avoid the "garbage in, garbage out" syndrome, a number of issues surrounding the acquisition and utility of data must be resolved. The greatest problem is the number and diversity of the existing legacy systems. Many legacy systems are, by virtue of their age or design, relics from a bygone computing era. The antediluvian remains are usually constructed around flat files or hierarchical data structures. Some are EBCDIC and others are ASCII encoded. Even within the classification of hierarchical, there are numerous products and packages used to construct the data, including the ubiquitous COBOL homegrown files. Of course, newer generations of systems have implemented SQL-based relational structures, which prove no friendlier to data extraction than their older COBOL cousins. As if extracting data from numerous internal sources was not enough, external sources of data are being added at the request of business-line users who want to enhance the value of the information in a data warehouse. These sources come in different formats and must be matched, or converted to match, to internal sources of data. Each of these generations, products, and homegrown systems has its own unique quirks with regard to data storage and the best mechanism for wresting the data from them. Diversity is also a challenge in the data mart arena, especially when using independent data marts. If the system architect is not vigilant, each mart will have its own independent extract from each production system. The jumble of lines in Figure 1-7 indicates the duplication of uploads and extractions to the various data marts when this occurs. Each extraction and cleaning process is costly, and doing similar things multiple times wastes corporate resources. The extraction process also takes time and system resources, placing a drain on the production systems. Too many extractions will adversely affect production system availability. Eventually, the production system could be so busy feeding the data marts that it has insufficient time or resources to perform the processing for which it was designed. Alternatively, reducing production availability to the data marts creates obsolete data marts. Further adding to the challenge is the fact that the independent data marts are often created with no overall enterprise data model in mind. The result is that any given data element may be extracted and transformed into multiple different versions of itself, depending on the point of view of each specific data mart requesting the data element. Data consistency problems are not confined to the data marts alone. The legacy systems may each capture and process similar information (e.g., gender, marital status, or order information). However, their isolation from one another leads to the legacy systems utilizing dozens of different mechanisms for structuring that data syntactically and semantically. Data may be captured using a variety of different units of measure (e.g., pounds versus tons). One legacy system may encode gender using a singleletter designation, while another may use binary designators; a third system may have a full word, and yet the fourth system may have two data elements (one each for male and female). Marital status and

- 19 -

conventions for capturing names (e.g., Jr., Dr., Mr., Mrs., Ms.)—all are candidates for yet another form of diversity that presents problems in data consistency. Once consistent semantics and syntax are established, there is the not-so-minor problem of data content. Is the data in the legacy systems accurate? Are there misspellings of proper nouns that will cause multiple records when there should be only one? Have poor data-entry validations allowed such anomalies as dummy social security numbers or blank customer account identifiers, so that there are people with the same SSN or multiple customers with the same customer numbers? Are there ZIP codes missing in one system, or some that do not correlate with the city/state in the address fields? To avoid these kinds of issues, the data architect must develop mechanisms for ensuring that both consistent and correct data enters the data marts or warehouse. Incorrect data—bad data—is very costly to a company. Imagine paying the accrued yearly profits for a money market account to a customer based on an average deposit of $800,000.00 instead of $800.00. It can and has happened, when a programmer thought he knew that the data in the balance column of a table was kept in thousands of dollars (103 instead of 101), and there should not have been any balance less than one (1). Or, how many sales are missed because a query to locate all customers in a certain geographic area, as defined by ZIP code, fails to locate the 40 percent of potential clients whose ZIP codes are wrong. Unfortunately, the examples are almost endless, and corporations lose millions annually due to bad data.

The Web and Other Emerging Technologies
Today's businesses are operating in a very different environment from that of only a few years ago. Information is proliferating at dizzying rates. Data marts and data warehouses are springing up everywhere to accommodate the mounting stores of facts that are the lifeblood of business. More important, technologists and business-line users alike are under intense pressure to quickly transform the "straw" of raw data into the "spun gold" of meaningful information upon which key business decisions can be based. Partially responsible for this change, the Internet and World Wide Web make fast, easy, global communications a reality. The Web has opened the door for global commerce, engendering a community where everyone moves quickly and makes decisions fast. The advantage goes to the organization that can do it better and faster than the competition. In this environment, finding an effective way to place knowledge in the hands of the people who need it, when or before they want it, wherever they are, is a major business challenge. Data warehouse systems are the primary technological tool enabling a business to respond more quickly and in a more intelligent manner in this rapid-paced environment. Warehouses provide to analysts and all levels of management the information they need to understand customer preferences and buying habits, product performance, and the success or failure of marketing efforts. Many companies have made competitive gains in the marketplace as a result of using data warehousing technology. But any data warehouse is only as good as its data. For a ware-housing project to be successful, its designers must: Extract data from existing systems (i.e., legacy systems, transaction-based systems, relational databases, and external data sources). Cleanse and validate the data. Integrate the data into a logically and syntactically coherent whole, in accordance with the enterprise data model. The delivery of data warehouse information to decision makers throughout the enterprise has been an expensive and challenging task. Once data has been extracted and organized for user access, analytic software has traditionally been loaded on each user's work-station. Users are then trained, and ongoing user support staffs are recruited. User requirements and even the users themselves change constantly, resulting in "moving target requirements" and a significant support burden. The Internet, the World Wide Web (WWW), emerging object-oriented (OO) technologies, and webenabled decision-support tools offer at least partial solutions to some of these issues. In addition to simplifying the installation and upgrade of data warehouse access tools, an intranet can introduce a new level of collaborative analysis and information sharing among decision makers. Moreover, the OO

- 20 -

technologies can speed applications development and allow the by-products to be reused. The Internet/intranet and World Wide Web play four major roles in a distributed data warehouse with great advantage to the corporation. These network technologies permit: Data transfer, via a corporate intranet or other private network, from source systems within the company to the ware-house and data marts Access to data sources, via the public Internet and WWW, located outside the company and the transfer of that information to the warehouse Data transfer, via a corporate intranet or other private network, between the distributed components of the warehouse and/or between the warehouse and data marts End-user access, via a corporate intranet or other private network, to data stored in the warehouse and data marts Each of these functions places a different kind of stress on the corporate network, which must be explicitly managed to ensure acceptable service levels for the users. In the future, additional network considerations will come into play to support expanding cooperation among distributed warehouse components. Most data warehouses will be sourced, at least in part, from mainframe databases for years to come. Many legacy systems house corporate data, often in transaction-based or production systems that are best included in the data warehouse by means of data extraction and transfer. Other mainframe-based systems, often using relational technology, could ideally be included, as is, becoming a component part of a distributed data warehouse, if communications and cross-system management facilities are available. High-speed access to these mainframes is necessary for efficient transfer of data to the warehouse and/or data mart environments. While the mainframe can attach to multiple networking services, highspeed interfaces ensure a practical way to transfer data into the warehouse. Most local area networks do not have the bandwidth for large data transfers to a data mart without seriously impacting service levels. Many mainframe suppliers have embraced IBM's Enterprise Systems Connection (ESCON) standard. ESCON provides inherent advantages for the distributed environment, with data transfers rates up to 17 megabytes per second and extended distances up to 60 kilo-meters. For system success, new distributed warehouses must coexist with mainframes and maximize the speeds and feeds of data between these two critical components. The implementation of LAN technologies is also undergoing a radical change. With the focus on higherbandwidth connections internally and externally and on commodity components, the near future offers even faster networks and lower LAN startup or replacement costs. As LAN communication technologies strain to reach the "magic" 100 Mbps (megabits per second) mark, the traditional constraints of network bandwidths will fall, enormously simplifying support for small data marts within a LAN environment. These changes encourage corporate use of data marts as a cost-effective way to move to data warehousing. Businesses' growing reliance on network-centric computing environments, and the variety of different technologies and network protocols used, makes systems management critical and more complex than ever. In addition, performance monitoring, backup, and disaster recovery must all adapt to support the new environment. Adaptability to emerging technologies and changing concepts must be built into any systems management strategy. A network-centric computing environment is a corporate necessity if the potential of a data warehouse is to be realized. As with the advent of most new technologies, it not only provides better service, it is more economically efficient than traditional data access methods. The inclusion of an intranet should cause the enterprise to re-think its computing strategies—from the desktop to the local area network to the division server to the mainframe. Entire architectures can change when methods of networking, protocols, bandwidth considerations, and most important, software must be considered when making design and implementation decisions. An intranet allows a company to take advantage of web-enabled applications. They can play a critical role in ensuring the success of information access within an organization.

- 21 -

How does an organization choose its distributed warehouse tools? It must look for solutions that can handle all aspects of user computing from designing and building reports to deployment, administration, and management of decision-support applications. Depending on resources available and the skill level of the user community, a company may want to look for a solution that is truly user-friendly, requiring no programming or scripting. Web-enabled decision-support tools have come a long way in a relatively short time. However, the level of sophistication of the solutions can vary widely. Web-enabled decision-support tools can be roughly grouped into four categories: Web Client-Based Tools. Such tools deliver the ability to publish static information in HTML format so that it can be accessed and read by browsers. They provide basic file distribution services, saving or converting reports and documents into HTML files that can be stored on the web server so they are available to all. Web Client and Server Shared Functions. These tools deliver the ability to publish static reports on a scheduled basis, deliver on-demand reports, or initiate a report for dynamic execution from the desktop browser. They provide dynamic HTML publishing (i.e., HTML documents are created on the fly in response to user requests), with an HTML page or document as the output. These tools implement a four-tier architecture (web browsers, web servers, application servers, and databases) and most use the Common Gateway Interface (CGI) to link web servers to external programs. Interactive Web-Enabled Environments. These tools deliver the ability to execute applications in a browser environment, and they can have some application components written in Java or ActiveX. They provide Java-assisted publishing, adding Java applets or other client-side programs to enhance the interface between the user and the Web, and also provide support for localized processing. Dynamic Use of Java and OO Technologies. These technologies provide dynamic Java publishing. These programs are designed, maintained, and executed entirely on the Web using Java code. This method uses a classic three-tier architecture (Java applets, a Java application server, and a back-end database or resource manager) and is not constrained by HTML and HTTP (see Chapter 5). These definitions provide a useful starting point for evaluating the hundreds of web-enabled decisionsupport tools and web configurations available today and in the near future. To be most useful and effective, web-enabled decision-support tools should support Internet standards such as HTML output, hot-linking, and CGI scripts. The tools should also take advantage of JPEG and GIF formats for images (to extend HTML's output capabilities) and may have Java applets and ActiveX controls in place. Deployment of Java applets, in particular, can allow a product to provide interactive and content-rich decision-support environments. Whatever the tools chosen, be certain that the vendor has a strategy for the future, allowing an easy migration path to emerging technologies and standards as they mature. According to many industry experts and analysts, server-based processing is key to successful webenabled decision support. Web-enabled products that lack server-based processing, including many of the popular user-centric query and reporting tools, are going to be at a distinct disadvantage when it comes to delivering complete, web-enabled decision support. Without server-based processing capabilities, users will not be able to take advantage of the benefits of automated scheduling and ondemand reporting. It is also important that the product support server-based processing on both UNIX and Windows NT platforms. Being able to use the tool with both UNIX and NT network operating systems increases scalability and ensures that the medium-size to large organizations that use both platforms can deploy a single seamless solution to all their users. It is very important to understand what level of support for server-based processing a product is actually delivering. Some products have the ability to perform all of their processing on the server. Others offer only partial implementations, with only a portion of the processing (e.g., scheduling) actually being performed on the server. Since the Internet/intranet is a server-based environment, if a product requires any processing to be executed on the user's desktop (e.g., SQL generation, calculations, sorting, correlation of result-sets, or formatting), it may not be satisfactory either in terms of performance or functionality.

- 22 -

Whether organizations choose to deploy their web-enabled decision-support solution within UNIX or Windows NT environments or both, it is essential that they have complete flexibility in mapping to backend data sources. This includes access to data warehouses, data marts, and corporate, departmental, and personal databases.

In Brief
As we begin the twenty-first century, our world is experiencing the unprecedented growth of a technology that promises to bring people closer together in both business and society. Corporations recognize that information placed in the hands of decision makers is a powerful tool. To meet decision makers' nearly insatiable appetite for information, data is being extracted from operational systems and placed in warehouses. Eventually, every enterprise will implement some type of data warehousing solution, be it a full-fledged, comprehensive distributed data warehouse, a virtual data warehouse, or a simple departmental data mart. The options and potential are limitless. Most large corporations already have databases, workstations, and access to the Internet. Many of them have also developed private intranets and recognized the need to employ emerging technologies to improve decision support. This book explains how to leverage World Wide Web technologies in the construction of a successful distributed data warehouse. It analyzes key technical issues such as distribution of data and functions across systems, web-enabled decision-support applications, web-assisted information flow throughout the distributed environment, and object technologies and their interrelationships. This book also discusses how to store data to best meet an organization's decision-support needs, and how to provide users with common, simple, transparent access to information, regardless of where it resides. Finally, it contains suggestions for successful warehouse implementation, such as when a split between operational and informational databases is necessary and how to accomplish it, and how web and data warehousing technologies combine to fulfill decision-support needs.

Chapter 2: Data Warehouse Architecture
Overview
Corporate data represents a valuable strategic asset for any firm; its value becomes tangible, however, only when that data combines into actionable information. The information technology (IT) industry has been talking about delivering data to knowledge workers and providing them with decision-support systems for two decades. In the late 1970s, relational database management systems (RDBMS) were introduced and gained acceptance as production systems. One of RDBMS's biggest selling points was its relative ease of direct query, without the aid of a program. The RDBMS proved so easy, in fact, that many business users learned SQL (structured query language) to take advantage of the new systems. This opened a completely new market to vendors to provide easy-to-use, SQL-based query and reporting tools. The SQL query products simplified database access to the point where almost any knowledge worker could go directly to the RDBMS production system and retrieve the exact data the user wanted, when the user wanted it. The revolution had begun; users would never again be satisfied with their data processing departments deciding what data they would have and when they could have it. Of course, there was a big drawback to this situation, and it showed itself almost immediately. As soon as knowledge workers started to issue ad hoc queries against production data in significant numbers, the production system developed horrendous performance and availability problems. In the worst case, users who were not trained in using SQL could submit queries that would tie up the entire system for hours, locking out not only other users but also mission-critical production applications. Even when users were knowledgeable in SQL, their queries impeded production applications. The technologist's answer to this problem was to severely curtail, or in some cases forbid, ad hoc queries to production data. Once the jinn was out of the bottle, business users were not about to let the data processing department force him back in. Users would not quietly return to the old world of rigidly scheduled reports constituting their only access to corporate data. To answer the surging demand for information, businesses were

- 23 -

forced to invest resources in building databases, separate from the production systems, for housing and retrieving query data. IBM Corp. dubbed this concept of a separate database to support the knowledge worker the "information warehouse." Later, Bill Inmon would coin the phrase that stuck—the "data warehouse." Like all major technology trends, data warehousing should enjoy a long life cycle with three main stages: introduction, deployment, and maturity. Today, warehousing is just entering the deployment phase of its cycle. However, many leading practitioners see a roadblock along the way to its reaching the next stage, maturity, when a technology is universally adopted. In one way, the problem is that data warehousing has proven to be too successful for its own good. The more useful the data warehouse, the more the users want to enhance its functionality even further by adding more data. The size of the typical data warehouse is growing so rapidly that 100 gigabyte data warehouses are becoming commonplace and 100+ terabyte monsters are not unheard of. The building, maintenance, and administration of such large traditional data warehouses are very complicated and enormously expensive. If there is no change in either the current trend toward larger warehouses or in the way those larger data warehouses are structured, a substantial segment of the market will not be able to afford a data warehouse, and the technology may never fully reach the mature stage. If building a data warehouse is so time-consuming and expensive, why have corporations been willing to do so? It is certainly not a task any business will undertake unless it can expect great benefits. Information access for all, not just a handful of users, is the promised reward for the expenditure of thousands of worker-hours and a price tag upwards of $3 million. In a successful data warehouse implementation, thousands of users can access and analyze the warehouse's data, share information, and collaborate in the decision-making process. If the data warehouse only meets the needs of a small group, it is a failure, viewed as a technological "black hole" sucking resources from the company's coffers. Corporations today must respond to a rapidly changing marketplace, dealing with pressures such as a newly energized global economy, deregulatory changes, and occasional mergers, acquisitions, and megacorporate breakups. Just to stay current, companies must access external databases, creating a demand for web-enabled applications to navigate the Internet. To reduce costs and remain competitive, many corporations have had to downsize, placing pressure on the knowledge worker to become more productive. Increasingly, corporations worldwide are adopting the data warehouse as the architectural foundation for making better decisions and responding faster in a quicksilver business environment. If the end user is not included in the architectural process, their requirements will never be fully met. Lack of user input limits the effectiveness of the data warehouse project and can even undermine it completely. Employees from all business areas want enterprisewide information access to make more intelligent business decisions. Whether it is to identify business trends, analyze customer buying profiles, formulate marketing strategies, perform competitive cost analyses, or support strategic decision making, the properly architected data warehouse can deliver the information the business user needs to accomplish these tasks. However, users will benefit most when the processes and tools for collection and distribution of information have been chosen with their needs in mind. Architecture is a high-level description of the organization of functional responsibilities within a system. It is not a prescription that cures a specific problem or a road map for successful design. The goal of architecture, in this book, is to convey information about the general structure of systems; it defines the relationships among system components. Client/server is an architecture in the sense that it defines a relationship. It should be kept in mind that architecture is not "the solution" to a business or data processing problem. It is a powerful framework within which the developer can craft any number of different solutions. Just like any construction, building a data warehouse requires an overall plan—an architecture. Actually, two separate architectures should be devised, a technical architecture and an information architecture. The technical architecture provides a blueprint for assembling hardware and software components, defining what and how tools and technologies will be used. It may include middleware and primary database structures as well as which development and access tools to use. It outlines how these components will be integrated to provide a complete support environment for the data warehousing project. The information architecture deals with the warehouse's data; it describes the content, behavior, and interaction of the business objects. Information architecture provides a structure for the information

- 24 -

components of the business: subjects, events, roles, associations, and business rules, and, in doing so, it prescribes the building blocks for applications development. Most applications development occurs outside of information architecture. Developers are free to create the application as though it were the only one they, or anyone else, will ever develop, with no design detailing which technologies should be employed or why, no thought given to how information should be encapsulated and accessed, and no common framework for interaction with existing applications. The intersection of technical and information architectures brings order to this chaos. Business objects, defined by the information architecture and manipulated by the application, use the services of the components defined by the technological architecture. Computing architectures, however, do not address the details of an application design. It is still the responsibility of the programmer to create the best application design possible. Without a coherent architectural plan, any data warehousing project is predestined to disaster.

Role of a Data Warehouse
The goal of integrating data to provide accurate, timely, and useful information to a company's decision makers is not new. Businesses have been seeking to create corporatewide information centers that would enable them to better understand their customers, products, and markets for twenty-five years. Now, business conditions and exciting new technologies, such as the Internet, are creating a new wave of interest in decision-support systems, giving rise to an explosion in the growth of data warehousing. Today's highly competitive marketplace has caused companies to fundamentally alter the way they do business. Centralized markets and competition have created pressure to speed up product cycles and lower costs. Rightsizing, the Internet, business process reengineering, and shorter time-to-market cycles are forcing companies to do more with less. All of these pressures have combined to accelerate the pace of business, requiring organizations to recognize and adapt to change more rapidly. Good strategic planning and timely decision making have become critically important as companies seek to cut costs and gain an advantage over their competition. In this environment, businesses need constant access to useful information. As the need to make quick decisions has gained importance, the data available to support the decisionmaking process has proliferated. The overwhelming market acceptance of relational databases, the Internet as a data source, and the availability of inexpensive platforms combine to allow corporations to accumulate and/or access a massive amount of data. This data can help companies to understand customer patterns, to know who buys what, when, where, and why. There are several technologies that are of significant value for analyzing all this data. Online analytical processing (OLAP) software has matured, providing a significant performance boost and delivering data to users in a form suitable for decision support. Some OLAP products are based on popular relational technology (i.e., ROLAP tools). Others are multidimensional online analytical processing (MOLAP) tools that are based on more recent database technology called multidimensional databases (MDD). Among the technologies showing great promise for data warehousing are the Internet and the World Wide Web. The mere existence of the Internet has revolutionized thought processes. These technologies, when adapted to corporate use, have the capacity to provide every employee throughout a global organization with easy access to company and Internet data. The question still remains as to how these masses of nontechnical but business-savvy knowledge workers will find what they need in a web-enabled distributed data warehouse. Although the inclusion of the Internet exaggerates the problem, the solution is the same for all levels of distributed data warehousing. Data warehousing must not only provide data to knowledge workers, it must also deliver information about the data that defines content and context, giving real meaning and value. This data about data is called metadata, and it is of critical importance to achieving the potential of a data warehouse. Another key component to successful data warehousing is architecture. This architecture must be flexible enough to provide for the inevitable inherent changes the data warehousing environment will require as it matures within a company, and rigid enough to provide a solid framework for warehouse development.

Data Warehouse Definition
The goal of data warehousing is to create competitive advantage for the company that builds one. It does this by tapping and organizing the huge quantities of raw data being captured by operational and other systems. Corporations know that the databases servicing all of the various departments (i.e., marketing, distribution, sales, and finance) contain information that, if properly extracted, analyzed,

- 25 -

combined, and stored, can dramatically increase the effectiveness of the decision-making process. A well-designed and accessible data warehouse can provide a company's knowledge workers with the information they need to ensure corporate success. It is hard to find an organization that is not implementing some kind of a data warehousing solution. Many corporations, however, refer to data warehousing with different terms. And nearly every company implementing a data warehousing project is doing so in a unique way. This diversity is possible because a data warehouse is an architecture, not a product or a database. It is an architecture that defines what information will be delivered to users, how the information will look, and what tools users will employ to access the information. This architecture can assume many forms. Data warehouses assemble data from all over an organization—from legacy systems, desktop databases, spreadsheets, credit reports, sales calls, and more. This collective data structure makes a larger amount of summarized data available to a wider group of users than would be possible with any of the underlying systems. It can be mined or queried in ways that are useful for decision making through a common, centralized database or through distributed databases and easy-to-use query software. How do data warehouses differ from operational systems? Quite simply, operational systems have different structures, needs, requirements, and objectives than do data warehouses. These variations in data, access, and usage prevent organizations from simply co-opting existing operational systems as parts of the distributed data warehouse. For example, operational data is short-lived and changes rapidly, whereas data warehouse information is historical in nature and has a long, static life. Operational systems feature standard, repetitive transactions that use a small amount of data; warehouse systems feature ad hoc queries and reports that access a wide range of data. High availability, rapid response, and real-time updates are critical to the success of operational systems, but are not nearly as important to the data warehouse. The architecture of a data warehouse differs profoundly from the design and structure of an operational system. An operational system has clearly defined input, strict processing requirements, and clearly defined output. It is assumed that the system requirements will remain constant throughout its life cycle. In designing a data warehouse, there are no precisely stated requirements. There is, however, a variety of source data to choose from, many different possible ways to organize and aggregate the data, and an almost infinite variety of possible outputs. An organization must assess not only its current informational needs but also likely future needs. This requires a flexible architecture that ensures the data warehouse can adapt to a changing environment. Operational systems are typically developed by first determining and then defining the business processes in conjunction with the users. With this complete, application code can be written and databases designed. Since the primary goal of an operational system is efficient performance, the design of both applications and databases must be geared to provide the best online transaction processing (OLTP) performance. Operational systems often handle high volumes (i.e., hundreds of thousands of transactions per hour) and must return information in acceptable user-response timeframes, often measured in fractions of a second. By contrast, data warehouses start with a wide variety of data and vague or unknown usage requirements. Data warehouse users seldom know the specifics of any of their queries ahead of time. In fact, as users learn more about the available data and how to use it for decision making, the designer will need to tailor the warehouse to improve performance in view of usage patterns. Typically a user does not know the second question he or she will ask until the first question has been answered. In the area of data mining, which looks for statistical trends and anomalies, users do not even know what the question is until they get the answer. Both operational and warehouse systems play an important role in an organization's success. They need to coexist as equally valuable corporate assets. This can be done by carefully designing and architecting the data warehouse to exploit the power of operational data while meeting the unique needs of the company's knowledge workers. Tons of data are captured by operational systems, but accessing the data and gleaning anything useful from it can be difficult, time-consuming, expensive, and nearly impossible. Most companies are trapped under a deluge of unintelligible data. To break out of this logjam, businesspeople demand ease of access to predigested data, analytical tools, and summary information. Results from the initial

- 26 -

implementation of a data warehouse often show a favorable return on investment, and the competitive advantage gained from a full-scale data warehouse will be far more valuable. To construct a data warehouse, there must first be business interest in the project, a core of potential users with a critically perceived need, and corporate sponsors. To lock in critical business buy-in, find the company's worst "pain" and present the data warehouse as the curative for that pain. Real business pain does not have to be life-threatening and usually is not, but it is truly annoying. Make sure the first stage of warehouse implementation eliminates the pain: Automate the production of manually scheduled reports; reduce error rates across systems by scrubbing data before queries; organize data from the business users' perspective with summarized views and drill-down queries for various analysis; or do whatever else it takes to prove the value of the data warehouse. Data warehouses organize business data to answer questions about why things happen, not just what things happened. The real value of the data warehouse architecture is not just in providing answers to questions but also in encouraging the knowledge worker to formulate even more questions. Focus the data warehouse architecture on getting from what to why as quickly as possible for the best results. How is it possible to design a data warehouse to answer questions about why things happen when all the source data has been collected about what happened? Dimensional modeling is the name of a technique for organizing data in databases in a simple and understandable fashion. When a database can be visualized as a "cube" having three dimensions, or as a geometric solid with four or five or even more dimensions, people can imagine slicing and dicing that figure along each of its axes. Dimensional modeling gives the user the ability to visualize data. The ability to visualize something as abstract as a set of data in a concrete and tangible way is the secret of making the abstract understandable. Try a simple example. Imagine a business where the CEO describes what the company does as follows: We sell products in various markets, and we measure our performance over time. The data warehouse designer listens carefully to these words and adds the appropriate emphasis: We sell PRODUCTS in various MARKETS, and we measure our performance over TIME. Most users find it easy to think of this business as a cube of data (Figure 2-1), with labels on each of the edges of the cube. Any point inside the cube is at the intersection of the coordinates defined by the edges of the cube. For this example, the edges of the cube are labeled as Products, Markets, and Time. The points inside the cube are where the measurements of the business for that combination of Products, Markets, and Time are stored.

Figure 2-1: "Dimensional" representation of data. Compare the dimensional model of the business with a data dependencies model as shown in Figure 22. This is the same business, but instead of talking to the CEO, the warehouse designer spoke with a sales representative and was given an invoice. The designer has drawn a detailed data entity chart that describes how every item on that sales invoice relates to every other item and what all of the many-tomany and many-to-one relationships between data elements are. The picture certainly reveals more detail about the data relationships than the dimensional approach does. However, does it contribute to

- 27 -

understanding the business? Unfortunately, most people cannot hold a diagram like Figure 2-2 in their minds and cannot understand how to navigate it even if they can remember it.

Figure 2-2: "Data dependencies" representation of a sales order. Both the dimensional model of a business and the data dependencies model of a business are capable of storing exactly the same data and are capable of supporting exactly the same final business analyses. It is just that they present the data differently. The dimensional model enables the business user to understand the data and see the why as well as the what. The dimensional model is a topdown model (the designer started by speaking with the CEO), and the data dependencies model is a bottom-up model (the designer started with a salesperson). The data warehousing effort is primarily directed toward building an enterprise data warehouse organized into categories or subjects that are meaningful to business users. The data is defined in a single integrated model (normally a dimensional model) that attempts to meet the needs of the entire enterprise (Figure 2-3). In most data warehouses, data is extracted from operational files and other systems and stored in the warehouse's databases that cannot be changed or deleted. Frequently, the data is timestamped and organized into historical files. A directory is provided to manage the metadata, or the data about the data. Metadata gives the description of and meaning to the data in the warehouse so the user can understand what data is available and where it is housed. The data is then organized into subject areas and provided to the business analysts and executives in a read-only environment.

- 28 -

Figure 2-3: "Enterprise" model. The data warehouse so created is a combination of operational and historical data, repackaged to accommodate knowledge workers who want to report, analyze, or drill down without having to access operational files directly. Like any data warehouse, it has four major architectural components: Data extraction and transformation facilities Data movement and triggering facilities Database management systems Decision-support and analysis tools Most of the effort in building a data warehouse revolves around data integration—making sure that the data has the quality and the structure necessary for inclusion in the warehouse. Several steps are involved in this process: Extracting data from the source systems Transforming data Matching and consolidating data Eliminating data redundancies Validating and cleansing data Loading the data structures Managing the metadata These steps are critical in ensuring that the data populating the warehouse has the accuracy, reliability, and business logic that are absolutely necessary. Inattention to any of these points can spell disaster for the warehousing project. From the business user's perspective, the only thing worse than no information is inaccurate information leading to false conclusions; and poor data integrity will inevitably lead to inaccurate information. Reengineering data for warehouses requires careful planning and design to ensure that the business lines get meaningful information. This planning not only requires that data architects understand the business-line information requirements, they must also understand the structure and meaning of the data elements in the source systems. The appropriate source data must be selected, transformed, integrated, cleansed, and stored in the warehouse, available for the business lines to use.

- 29 -

The use of middleware, also called data integration tools, can be very helpful in managing these data problems. It can reduce the time, complexity, and costs involved with both the initial population and the ongoing maintenance requirements of a data warehouse. Most of the middleware products available today have filtering capabilities and are driven by business-line rules. These tools are especially useful if both a warehouse and data marts are in use, because they can populate both at the same time, ensuring consistency of data across multiple systems. Like many seemingly simple tasks, the actual construction and implementation of a warehouse that meets the objectives defined here is more difficult than it first appears. Figure 2-4 demonstrates the basic architectural constructs required to develop a data warehouse system.

Figure 2-4: Architectural components of a centralized data warehouse. The systems on the left in Figure 2-4, titled Comptrollers, Trading Desk, and Investment Bank, represent standard, individual OLTP systems that are designed to process transactions in each department. In this configuration, data from each of these systems is uploaded into a data warehouse. Along with the data itself, there is information about the data, or metadata. The metadata is available throughout the enterprise and is used to manage the data in the warehouse for decision-support applications. A brief description of each of the architectural components follows.

Enterprise Data Model
The enterprise data model attempts to define the data requirements for the enterprise as a whole. It includes data relevant to the entire organization, as well as data peculiar to individual areas. The enterprise data model is the foundation of the data warehouse, since its constructs will control what information is available to the end user.

Metadata
Since the data warehouse will store and aggregate data from many sources within the organization and sometimes from sources outside the company, it is necessary to gather information about the data being stored. This helps clarify the definition and the significance of the data more fully. For example, is the data sourced from only one place in the organization, or is it a composite from multiple sources? How current is the data? Is it refreshed daily, weekly, monthly, or less frequently? If the data is summarized, where are the individual transactions that were used stored? What formulas were used in calculating the summaries? The metadata is an integral part of the enterprise data model and, in the author's opinion, should be developed in conjunction with it for a data warehouse to be successful.

Data Integration/Upload
Large organizations today can be characterized as an information Tower of Babel, with systems of every sort distributed across the many departments of the organization. The data warehouse, in

- 30 -

providing a single point of consolidation for this divergent data, will need to transfer, or upload, the salient data elements on a regular basis to ensure that the warehouse reflects the current information available to the organization. Information contained in these disparate systems is often redundant, and since the systems were likely designed before the enterprise data model was assembled, the redundant data is usually maintained according to different data dictionaries. This means that even though both the Investment Bank and the Trading Desk know and do business with a Mr. John D. Rockefeller, they likely will have differing data definitions. The duplications and differences in individual data definitions will have to be rationalized, or cleansed, during the uploads. Often small differences in seemingly innocuous data can lead to a significant amount of work in achieving this goal.

Warehouse (Database)
The collective data must be stored somewhere, usually in a central repository. From an architectural perspective, data warehouse developers have to determine the physical storage dimensions of the warehouse (e.g., how big it is and how it should be partitioned) as well as the logical dimensions of the warehouse (e.g., the database technology that should be used). To date, most warehouses have been built using parallel or traditional relational technology.

OLAP
Online analytical processing refers to the ability of the system to support decision making. This technology allows users to view information from many angles, drill down to atomic data, or roll up into aggregations to see trends and historical information. It is the final and arguably most important piece of the data warehouse architecture.

Management
Managing and coordinating the successful integration of disparate source systems into a consistent data model requires a significant management component, as does the warehouse itself. The warehouse must provide current data and ensure the availability of the system, which means managing the scalability of the hardware and software as the dimensions of the system (size of database, number of users, number and complexity of queries) grow.

Traditional Data Warehouse Architecture
The basic model that makes up a centralized data warehouse needs to be flexible to provide for the business needs of the future. The data warehouse must constantly evolve in response to increasingly sophisticated queries. The introduction of new data sources into the warehouse architecture requires an ongoing, elastic approach to design. Building a data warehouse is always an iterative process. A company does not take all of its systems and put them in a warehouse overnight. Typically, construction of a data warehouse starts with one or two data sources, with additional data added gradually, over time. So too, the tools and products that enable companies to implement data warehousing change. New products will be developed and existing products will mature in time to meet new data warehousing requirements. As these new or improved products are incorporated, the data warehouse will change. Business users know they need access to data, and they may even know the first few queries they will place, but the nature of a data warehouse prevents users from knowing everything they will eventually need from it. As users become more familiar with the data contained in the warehouse, their understanding and use of it will change. This evolution may require more data, different data, and/or new data formats. As this feedback loop from users to the designers is established, the data warehouse will change. The company itself will change, too. The business environment in which it functions is changing every day, and the enterprise needs to accommodate new markets, reorganizations, mergers, and rightsizing. As business changes, the data warehouse must change to keep pace.

- 31 -

Previously, it was stated that a data warehouse can be defined as an architecture for delivering information to knowledge workers. It is not a product, but is often instantiated as a collection of products and processes that, working together, form a delivery system for information. As discussed earlier, to be meaningful, this information must be composed of both data and metadata. Together, these elements provide understanding of the data warehouse. To understand how these two elements, data and metadata, flow from the operational systems through the data warehouses and to the knowledge workers, examine an architectural template of a data warehouse. This is called a template because, in the author's experience, no two data warehouses are ever the same. A full-blown architecture, such as described here, will evolve to a mature state over time and with considerable effort, as dictated by corporate necessity. Figure 2-5 depicts an implementation of a data warehouse architecture. Data flow is the main focus of data warehousing. The data is extracted from the source operational systems (on the left), scrubbed, integrated, and transferred into the data warehouse's data structures. Along with operational systems, external data and data from other internal locations (e.g., spreadsheets, departmental databases, and packaged applications) may be supplemental sources of data. A data warehouse of this sort uses a relational database for the warehouse's data structures. In this case, the physical data warehouse acts as an archive and staging area for delivery of the data through focused channels, depicted in Figure 2-5 as departmental data marts.

Figure 2-5: Data warehouse architectural implementation. Subsets of data are extracted, by subject area, and loaded into the data marts. They are specifically designed to provide departmental users with fast access to large amounts of topical data. These data marts often provide several levels of aggregation with drill-down capabilities and employ physical dimensional design techniques suitable for use with OLAP tools. An enterprise's data warehouse architecture may include many data marts. Using sophisticated tools, users can extract information from departmental data marts and easily manipulate the data to perform specific analysis. The metadata from the multiple data sources is integrated into a single enterprise data model. This logical model is not tied to any particular product or implementation of a physical data storage structure. Instead, the model provides data warehouse architects with an overview of the business that enables them to understand the type of data available to the warehouse, the origin of the data, and the relationships among the data elements. It also helps the architects to provide more suitable names for the data elements than those used in the operational systems. From the starting point of this enterprise data model, the physical databases can be designed and the actual data warehouse implemented. This logical business model also serves as a basis for the physical design of any dependent data marts. The actual design and physical implementation of the data warehouse and data marts may be radically different, with varying degrees of data normalization, different levels of aggregation, and even different types of physical data storage. But the logical data model serves the same purpose for both warehouse and marts; it preserves business relationships and data origins and links to the source systems. The metadata contained in the logical model can be made available to specialized data warehousing tools for use in analyzing the data. The data and metadata are thus kept in sync as both flow through the data warehouse distribution channel from source to target data mart to customer.

- 32 -

Metadata can be defined as "data about data," but this definition is sometimes difficult to understand. An example may help to clarify the situation. Any database contains both data and metadata. As an example, a CLIENT table has the following entry: John D. Rockefeller, 100 Park Ave., New York, NY This information is data in the table. However, these column names are the metadata: customer_name,customer_add,customer_city,customer_st The metadata is kept to identify the source of the entry (i.e., these four data elements were all extracted from the Trading Desk client table), and the relationships among the data elements (i.e., customerst is the state in which the client, identified as customer_name, resides). Metadata is essential to bind together the database, the source system, and the application programs that access and update the data. A helpful analogy is found in contrasting arithmetic and algebra. The expressions 5 + 5 3 + 4 2 + 6 are all arithmetic statements that are data to the algebraic formula a + b All respond to the same rule. The same is true of data in a database, whether it is from an operational system, a spreadsheet, or a data warehouse. The data is defined, controlled, and accessed through the formula of the metadata. In the most advanced forms of the traditional data warehouse architecture (Figure 2-6), there are many source systems, multiple extract and load utility programs, a single data warehouse database, and then many more extract and load utility programs to move data from the data warehouse to the dependent data marts. Managing the metadata in this circumstance is a real challenge that can require assistance. In addition, while the metadata defining the warehouse's database and the extract and load utility programs is mainly of interest to data processing professionals, business users are interested in the metadata relating to the data marts to help them construct queries and establish data mining activities. = c = 10

=

7

=

8

- 33 -

Figure 2-6: Traditional data warehousing architecture. In Figure 2-6, a logical model/metadata exists for each specific stage of the data warehouse: source system, physical data warehouse, and data marts. In fact, each stage may have multiple sets of models/metadata if the designer is working with individual data models for each of the source systems and unique models for each data mart. Although this is a curious instance, representing one of the more unusual examples of data warehousing, data warehouse architecture needs to accommodate multiple sets of models/metadata and the mapping between them. For many companies, their on-line transaction processing (OLTP) is the basis for decision support. To facilitate this, the OLTP data is copied to other systems or RDBMS. Refreshed regularly, the data may be a limited subset of the OLTP elements, and might include summary tables, but actually is mapped one-on-one to the OLTP data. Although some would not classify this as data warehousing because of the absence of the traditional "data warehousing process," it is a common mechanism for providing users with the access to operational data they need. Once started on this course, the next step in the process is to recognize that OLTP issues are not applicable to the data warehouse. Data warehouse data is time-dependent, aggregated, summarized, tuned, and usually named differently than the OLTP data elements. Business users' queries frequently span the application boundaries of the OLTP systems, placing greater emphasis on standard names and representations of replicated data. Initial data quality assessments should be made and quality controls instituted where needed prior to loading the data warehouse. Some data may be acquired from non-OLTP sources. Examine a brokerage as an example. The designer recognizes the inherent differences between the OLTP data and data warehousing, but also knows that the business users primarily want access to the OLTP information. The designer uses an existing corporate data model (built from and synchronized with the OLTP systems) and reengineers it to the new DBMS (database management system) environment. The database administrator (DBA) implements, indexes, and tunes the design based on anticipated usage. In this situation, the concept of source-to-target is very important, and therefore the metadata and mapping gain importance. More emphasis is placed on the metadata, as it is used for utilitarian activities (e.g., unloading source data, transforming the source data, and loading the warehouse). Data sources that are non-OLTP are recognized, mapped, and used. As the differences between the data warehouse and the OLTP metadata grow, it becomes imperative to maintain the relationship between the two. To abandon the mapping between OLTP systems and the data warehouse would make the logic in proprietary applications in both source and target systems difficult to maintain and leave them with little or no chance for reuse. In summary, the data models/metadata serve at least three roles in any warehouse: They facilitate automated transformations. They aid the developer to navigate and tune the system. They help users understand what they have to work with.

- 34 -

In all stages of the data warehouse architecture, companies that design with updates and changes in mind build better systems. In this environment, the role of data models/metadata and records of changes and transformations are vital to the success of the data warehouse.

Another Approach
Business buy-in for a centralized data warehouse is not always easy to obtain. As seen, a full-scale implementation requires the construction of a data model, selection and extraction of data from source systems, data integration, loading and maintenance of the warehouse's data structures, installation of user-oriented data access tools, and perhaps several dependent data marts. All of this requires time and money—thousands of worker-hours, often spread over three or more years, and an investment in excess of $3 million. The idea of "starting small" on a data warehouse makes a lot of sense from the company's perspective, allowing it to minimize risk while deriving benefits from the system as early as possible. After all, what is an independent data mart (Figure 2-7) but a "small" data warehouse.

Figure 2-7: "Independent" data mart architecture. However, as the project to build a single independent data mart is launched, it is important to have an overall plan in place to propagate this technology to other departments in an orderly, timely, and integrated fashion. Doing so not only will help to reduce the time and expense of successive installations, but it will also help reduce costs and increase productivity in the operation of the data warehouse itself, should one be eventually built. An integrated architecture should be developed to tie the data marts together to facilitate crossfunctional analysis and maintenance. This is, after all, only common sense: It would be wasteful and foolhardy not to adopt as many elements of the first successful implementation into the subsequent data marts in other departments. Some of the elements to be considered in an overall architecture include: Common data model Extraction tools Transformation tools Processes Data dictionary OLAP tools This raises the thorny question of the enterprise data model. Certainly, it is better to have completed the data model before starting work on the independent data marts. The data model is, after all, an overview of the business and would greatly assist the architect and designer in formulating their plans for the data mart. However, a comprehensive enterprise data model is an expensive and timeconsuming project, and the corporation may not be willing to spend either the time or money before seeing some tangible return on investment. Often, the designer is forced to proceed without the benefit of any data model. While it is possible to successfully design and implement a data mart without a model, this brings an element of risk to the project that it would be better to avoid. At the very least, the enterprise data model/metadata must be developed concurrently with the data marts' projects.

- 35 -

When the decision is made to construct individual data marts for use by specific business areas, an important shift in architectural perspective occurs. The centralized warehouse concept takes a back seat to the individual application. This creates a position where the business application drives the technology, rather than having the technology drive the business application. If data warehouses and data marts are truly tools for management, then economics must be one of the primary factors in determining which to deploy. The success of data warehousing projects depends on a number of factors: 1. Technology. Technology choices are a major factor in the decision to deploy fullscale centralized warehouses or application-specific data marts. For example, roughly one-third of the initial cost of any warehousing or data mart project is spent on hardware and another third on internal development services. Each business must decide what combination of hardware and software best suits their circumstances. 2. Buy-In. A second factor, important in achieving payback, is corporate acceptance of the finished product. Most businesses measure the return on their investment in terms of increased productivity and market gains that can be credited to the use of the warehouse or mart. Therefore, organizations whose warehouse/mart goes unused have incurred the same costs as those whose warehouse/mart is well used, but they have realized much smaller, or nonexistent, benefits. 3. Adoption Rate. An economic death knell for a data mart/data warehouse project is slow corporate adoption of the product. This makes it imperative that the initial stages of the project, even if it is the prototype, be usable and that its availability and utility be well publicized. In fact, prudent use of pilots/prototypes can go a long way toward ensuring continuing funding and corporate acceptance of the overall project. Of course, the subject area should have corporate visibility, the tools must be well suited for the application, and the successful results need to be communicated to the rest of the organization 4. Business Strategy Impact. Perhaps the most important factor is the impact the data warehouse/data mart has on business strategy. The real and most far-reaching benefit of this type of project lies in the solid decision support it offers; therefore, the company's chief decision makers need to be made aware of the impact the project is having. 5. Economics. The economics of data marts versus data warehouses is an almost-tooobvious-to-mention point. In most organizations it is far easier to fund at the departmental level than it is at the enterprise level. This is especially true given the fact that there is no guarantee that the project will succeed, and thus a smaller risk (the data mart) is easier to sell. Regardless of whether a business chooses to construct a global centralized data warehouse or a single independent data mart, it will find itself in need of an n-tiered, or multitiered, computing environment. Ntiered implies a hierarchy with several levels, but it does not exclude a networked data mart layer within the hierarchy. Because even one data mart tends to breed more data marts (which, in turn, usually grow to include a distributed data warehouse), a sound data plan must be in place to provide a strong foundation. It makes no difference whether a corporation starts at the bottom of the hierarchy, with a data mart, or at the top, with a full-scale centralized data warehouse—the corporation must have a goal in mind and a plan for relating the various levels and networks. The data plan cannot be easily constructed or effectively managed without an active metadata catalog. If a company begins its information architecture with a centralized enterprise data warehouse, it will quickly realize the need for subsets of the data and summary information. The easiest way to meet these needs is to spawn dependent data marts, by subject area, from the enterprise data warehouse. This is referred to as top-down development and is the approach recommended by most warehouse gurus. The other approach is to construct an independent data mart first to meet the needs of a specific user group (see Figure 2-7), but this mart must be built according to a data plan and with rollup as a goal. Additional subject-area data marts will be added until finally, the need will arise for the marts to participate in a hierarchy in which detailed information from each mart is extracted and consolidated into an enterprise data warehouse. This method of construction requires very careful thought, planning, and management, but is probably the faster method of arriving at an enterprise view than starting with the centralized data warehouse. Since n-tiered data warehouses can encompass the best of both worlds, enterprise data warehouses and data marts, they are more than just a solution to a decision support problem. They are an important part of an information architecture. This information architecture exists today in nearly every corporation,

- 36 -

just like the operational architecture has existed for some forty years. To support this information architecture, along with robust metadata, a tool that reverse engineers the various data marts' data models into logical units would be of tremendous value. However, the data model/metadata is not the only critical factor; there are other components that are just as necessary. The information side has some of the same requirements as the operational, such as data transfer mechanisms, and some different ones, such as automated data type conversions. A peer-to-peer, secure transport system is a requirement to move large amounts of data safely between the levels of the warehouse. Scheduling becomes paramount in the information architecture to automate the loading, extracting, moving, and delivering of data everywhere. As an infrastructure for the information architecture, there must be a management facility that is able to deploy work to multiple servers, execute the tasks concurrently, automate the execution of multiple dependent tasks, and log, audit, and perform recovery and alerts. Key features of n-tiered data warehouse architectures are: Bottom-up or top-down information strategy Robust, single catalog that not only holds the model/metadata but manages the spawning and rollup Groupware capabilities for sharing information Client/server model with servers located in the business units or departments Distribution and deployment across the Internet and intranet Data distributed to the user in the desired format on a user-defined schedule Subscription service allowing users to request information from any component of the data warehouse Action scheduling by calendar and event Transformation engine that can be run concurrently and on multiple servers Management engine for deployment of tasks, recovery/restart, logging, and load balancing After trying out these different architectures, many companies may come to the conclusion that they need an enterprisewide view of corporate data as well as a focused, single application view. An option is to consider building a total information infrastructure, either a top-down data warehouse architecture with a centralized enterprise data warehouse first and then the dependent data marts later, or a bottomup solution, where the independent data marts are built first and rolled up to the warehouse. In either scenario, the corporation will eventually settle into a framework of an n-tiered data warehouse.

Challenges of a Centralized Data Warehouse
So far, centralized data warehouses have failed to meet users' needs. A major part of the problem is the sheer size of the project. The attendant challenges of trying to track and supervise something as gargantuan as the construction of an enterprisewide consolidation warehouse are outside the normal range of managerial experience. According to industry surveys, fully half of the centralized data warehousing projects fail within their first year, and less than one in twenty ever reach their envisioned conclusion. This says that most "successful" warehousing projects end somewhere well short of the hoped-for enterprise data warehouse, leaving the knowledge workers to work with an incomplete view of the business. This section focuses on the challenges of centralized data warehouses and how they can affect the project.

Data Model/Metadata
Just the thought of undertaking a single data model for the entire corporation is enough to make the author, a battle-scarred veteran of many data modeling projects, get weak-kneed. The road to a single useful model is tortuous, demanding, and above all, long. It is so long, in fact, that even working with a team of experienced modelers, it may take eighteen months or more to complete—too long for most corporate timeframes. Many data warehouse projects end here, at their very start.

- 37 -

Building the enterprise data model must begin with an understanding of the combined information needs of all users who might access the warehouse for decision support, whether they are in research, finance, marketing, sales, or senior management. This is followed by a through analysis of all existing data processing systems and any previous data models (most of which are nonexistent except as embodied in the working systems). Finally, after running these two gauntlets, designers are faced with the task of rationalizing all of the existing models into a single model, filling in the many holes, and ensuring that the metadata is accurate and complete. The importance of the metadata cannot be overemphasized; the eventual success or failure of the data warehousing project will hinge on the metadata's reliability. The finished model needs to fit the needs of all identified uses, and concessions may be needed. Often, these compromises are so drastic that the resulting product is universally panned.

Parochialism
In every organization, there are users and even data processing workers who will try to refuse access to the data they control, or who do not want to cooperate for political reasons. This is especially true in departments that have gained some measure of autonomy from their information technology (IT) department. They have installed local area networks and quasiproduction systems on their own, using their own budget, without the approval, support, or assistance of data processing. It can be very difficult to get the data owners' buy-in to the project so that they will make their documentation and designs available. It is also difficult to gain access to their data and systems if they believe their autonomy and/or their ability to perform their jobs without IT interference is threatened by the data warehouse project. Political agendas are even more difficult to understand and overcome. The data warehousing project will bring change to the organization, and far too many employees view any change as a threat—to their job security, political power, or their position in the corporate pecking order. These employees, business users and technologists alike, will obscure, obfuscate, and outright misinform to maintain the status quo. The best approach is to sell the data warehouse as a personal opportunity to this group. The best insurance against political agendas for a warehouse designer is to always validate all data information.

Implementation Challenges
Another challenge faced during the construction of a centralized data warehouse is the actual implementation. Here, the challenges focus on translating the enterprise data model into a successful prototype and then into a full-blown functional system. Typically, these challenges break down as follows: Implementation Time Frame. The development cycle for a typical centralized data warehousing project is at least three years. The implications of a three-year time frame are significant to any corporation. When a company starts a data warehousing project, it is because decision makers need help now, not three years hence. In three years, a business typically experiences two or more market cycles, the introduction of new products, several reorganizations, and the turnover of a quarter of the staff. Its information needs will have changed significantly. It is almost a certainty that the warehousing project will either deliver little real value or will have been redirected frequently, thereby pushing the end date even farther out. Fully half of the currently completed centralized warehouses are considered failures for just this reason. Database Size. The average data warehouse is based on a substantial subset of the enterprise's operational data that, taken together, affords the business user the current view of the corporate position. The data storage requirements do not stop here; many iterations of this daily data need to be kept to provide a historical record. Most businesses want five or more years of data readily available. Even if monthly or quarterly summaries are kept instead of daily records for historical purposes, the storage requirements are huge. However, the designer can anticipate this requirement and make appropriate plans. What is more difficult to anticipate are all the summary tables, indexes, and other structures that will be needed for decision-support activities. The "completed" data warehouse must be refined to meet the needs of its users, and the enhancements add to the database size without adding any new data. As the need for new reports and summary tables grows, the centralized data warehouse can explode to several times the size of the designer's original estimate.

- 38 -

It is virtually impossible to tell before implementation how popular the system will be—to anticipate the number of users, the type and volume of user queries, and the stress they will place on the system. If the project is a successful one, users almost always demand the addition of yet more data and summary tables. As the size of the database nears 100 gigabytes and its complexity increases, it becomes harder to tune and response times slow. Also, as the volume of queries becomes larger and they become more elaborate, response times lengthen. The situation can become so bad that response times are measured in minutes, not seconds. It is almost as if the success of the system leads to its own demise. Data Model Transformation. As mentioned earlier in this chapter, the enterprise data model is a complex construct. When the centralized warehouse is actually constructed, this model must be transformed into the physical storage structures of the warehouse (when an RDBMS is used, as is most common, those structures are tables). Tables and their indices must be built, primary keys chosen, foreign keys defined, and partitioning schema instantiated. Even a perfect data model, should one exist, cannot be implemented directly. Some data redundancy, absent in the model, is necessary in the physical system to avoid costly joins. Oftentimes, because compromises were made in the design phase, the concepts defined in the data model are found not workable in the implementation phase. Therefore, the model must be revisited by the design team. The more complex the model, the more difficult these situations will be to correct, since interrelationships among data elements often mean that a change in one element propagates changes throughout the system. User/Data Interaction. Implementing directly a complex data model with star or snowflake schemas can lead to another problem. The business users are experts in their own areas, not in data model navigation. Faced with an almost incomprehensible star schema, users get frustrated. They were hoping that the warehouse would allow them to decouple their work from IT, not cause them to require help every time they want to place a query. Even worse, users can misinterpret results if they do not know exactly what data they are getting back from a complex query. A centralized data warehouse is intended to serve everyone in the company and therefore has to contain a complete set of enterprise data. However, no single user has need to access anywhere near the entire scope of data contained within the warehouse. Users can find the abundance of data available disconcerting, and coupled with their confusion about the table schema, the result often is that they are unable to locate the information they want. They have data overload. In other cases, users think they are getting the information they need, but in fact they are working with the wrong data and therefore come up with erroneous results. Since the object of a data warehouse is to provide users with decision support, these particular situations are subversive to the very foundation of the warehouse. Maintenance. Once built, the centralized data warehouse, like any data processing system, must be maintained. The physical size of the database can complicate this routine effort. A warehouse of any size must be reindexed and repartitioned on a regular basis, all of which takes time. The larger the warehouse, the more time these routine tasks require. Denormalization of tables, coupled with extensive use of indexing and summarization to allow users rapid access to the information, can push large data warehouses well into the hundreds of gigabytes and even the multiterabyte range. Not only does this place demands on the number and capacity of disk drives and controllers, it also stresses the processor itself. Some large centralized warehouses reach a point where the amount of time required to upload the integrated data from source systems and update the data warehouse exceeds the available downtime of the system (e.g., overnight or weekends). Companies in this situation are faced with a devil's choice: They might update the data during working hours and, at best, seriously impact response times or, at worst, lose several hours a day of productivity. Or they can reduce the frequency of updates, thereby losing accuracy, currency, and other opportunities. The size of the database also affects the backups and preparations for disaster recovery, both of which are critical if the company intends to protect its investment in the warehouse. These are activities no business can afford to forego, regardless of time constraints. Further exacerbating all of these implementation challenges is the fact that businesses functioning in today's climate are not stable and constant entities. Competition is cutthroat and successful

- 39 -

management is always trying to respond rapidly to changing business conditions. Knowledge workers demand new information and changes in existing reports, as well as new kinds of reports. As queries mutate, DBAs must respond by building new indexes and summaries, constantly tuning the system to support the changing demands. The data warehouse must keep pace with the business or even a stay a step ahead to remain useful.

Distributed Data Warehouse Architecture
The purpose of any database is to integrate and manage the data relevant to a given activity, such as check processing for a bank or securities clearance for a brokerage. The motivation for establishing a centralized data warehouse is to get the data relevant to all of the operations of the corporation gathered together in a single reservoir, so all of the problems associated with decision support in the enterprise can be serviced in a uniform manner. This is to be done through a consistent set of languages, physical data structures, constraint checkers, consistency checkers, development tools, and other data management functions. However, as demonstrated, centralized warehouses are beset with problems that cannot be solved without radical revision of the operating system and/or the basic concepts of data sharing. These issues, and the emergence of the independent data mart as the cost-effective entrance into warehousing, lead the author and others to embrace a distributed data warehouse concept. The distributed data warehouse architecture involves the merging of two diverse concepts—namely, integration through the database element and distribution through the networking element, as shown in Figure 2-8. However, it was the "desktop" computer revolution of the 1980s and the availability of reliable data communications facilities that have made the distributed data warehouse possible.

Figure 2-8: Data integration and distribution. A distributed data warehouse (DDW) can be defined as a logically integrated collection of shared data that is physically distributed across the nodes of a computer network. The terms global and local are

- 40 -

often used when discussing a distributed system in order to distinguish between aspects that refer to a single site (local) and those that refer to the system as a whole (global). For example, the local database refers to the database stored at one specific site in the network (a data mart), whereas the global DDW refers to the logical integration of all the local databases onto a distributed data warehouse. Note that the DDW is a virtual concept, since it does not exist anywhere physically. Three- and four-level architectures for centralized data warehouses have gained widespread acceptance, and most data warehouses adhere to one of these schema. There is no such equivalent for distributed data warehouses. The concept is relatively new, and the few products that exist to facilitate distributed warehousing do not follow any one architectural model. Distributed data warehouses can be divided into three separate groups, based on totally different philosophies, which are suited to quite different needs: Homogeneous distributed data warehouses (Figure 2-9) Heterogeneous distributed data warehouses (Figure 2-10) Single DDBMS distributed data warehouses (Figure 2-11)

Figure 2-9: Homogeneous distributed data warehouse.

Figure 2-10: Heterogeneous distributed data warehouse.

- 41 -

Figure 2-11: Single DDBMS distributed data warehouse. The term distributed data warehouse, as used throughout this book, refers to any and all of the different types, and they will only be differentiated when necessary. Most of the concepts discussed are applicable to all three forms. A homogeneous distributed data warehouse has multiple data collections; it integrates multiple data resources. The homogeneous system resembles a centralized data warehouse, but instead of storing all the data at one site, the data is distributed across a number of sites in the network. Each of these sites can have its own autonomous database and is capable of functioning as an independent data mart. The same type of database structure (i.e., database product) is used at each site, usually relational technology. These independent databases are joined together by a network through a distributed database management tool (Figure 2-12), logically uniting them into an enterprise data warehouse. Users' queries to databases outside of their LAN are managed by the distributed database management tool. It is possible to include a non–data mart database within this system (i.e., a relational production system), provided it uses the same database management system as the other included databases.

Figure 2-12: Distributed database management tool. However, care must be taken in the design of the individual participating databases. They must all have used the same database management system and implemented it in exactly the same way. For example, all must store monetary amounts in the same way (e.g., decimal, 2) and utilize the same rules for null values. A homogeneous distributed system has no facility for data conversion and transformation. This architecture has a number of advantages. Since most of the business users' queries are satisfied by their local data mart (by some estimates, as much as 95 percent), they enjoy all of the advantages of an independent data mart, including rapid response times, local control over the data stored in the mart, and a database tuned to their query pattern. Because they also have access to information stored on other data marts, they enjoy the advantages of an enterprise data warehouse, such as the ability to query across departmental lines, and a good overall view of the business.

- 42 -

The architecture of the distributed database management tool is key to the function of a homogeneous DDW (Figure 2-13). To handle the distribution aspects of the system, two key elements are required beyond the normal relational services—namely, the fragmentation and allocation schemas. The fragmentation schema describes how the global relationships are divided among the local databases. It can be thought of as the "locator" for the information stored across the systems.

Figure 2-13: Architecture of distributed database management software for a homogeneous distributed data warehouse. Figure 2-14 demonstrates an example of a relationship, a "completed trade," which is composed of five separate fragments, each of which could be stored at a different site. To reconstruct the completed trade, the following operations are required: "COMPLETED TRADE"=(A JOIN B) UNION (C JOIN D) UNION E where JOIN and UNION have their normal relational meaning (it must, of course, be possible to reconstruct the global relationship from its fragments using standard relational operators). In practice, this means that the "key" of the relationship must be included in all fragments. Also, A and B, and C and D must be joinable, and the products and E all union compatible.

- 43 -

Figure 2-14: A "completed trade" relationship. Any discussion of the fragmentation schema points out the importance of, and absolute requirement for, a global data model/metadata in a distributed homogeneous system. Without current data models of the individual data marts and an unconditionally reliable global data model, the queries across systems are not possible. The allocation schema is also necessary for a cross-system query. It specifies the location of each fragment, functioning as a "router" for the distributed system. It supports the possibility of data replication within the system by allowing the fragments to exist at more than one site. When replication is present, an optimizer selects the most efficient access path. The heterogeneous distributed data warehouse system (see Figure 2-10) is characterized by the use of different DBMSs at the local sites. While most of these locations function as independent data marts, it is possible to include other data sources, even external ones (via the Internet). Users enjoy the same advantages with a heterogeneous distributed data warehouse as they do with a homogeneous one: rapid response times for most queries, local autonomy, and the ability to query the global warehouse. Because different data sources can be included in the heterogeneous system, users may have enhanced access to corporate data. Again, the architecture of the heterogeneous distributed management tool is key to the functioning of the system. The same two devices, the fragmentation and allocation schema, are employed for fragment location and routing. However, some additional services, which are built around the global data model, are required because of the heterogeneous nature of the system. The global data model/metadata is a logical view of the data within the heterogeneous distributed data warehouse. In this case, it may be only a subset of the union of all the local data models, since local DBMSs are free to decide what portion of their data they wish to contribute to the global data model. An individual node's participation is defined by means of a participation data model, which is imposed on top of the local data model. The participation data models must include the rules that govern the mappings between the local and global levels. For example, rules for unit conversion may be required when one site expresses volume in hundreds and another in thousands. Rules for handling null values may be necessary where one site stores information that is not stored at another. The third, and in many ways most attractive, of the distributed systems in the single DDBMS data warehouses. Using this approach, the warehouse's database "looks" like a centralized data warehouse—that is, from the point of view of the user or developer, there is only one database. All access to the warehouse is handled through the distributed database management system (DDBMS). There are no independent data marts per se in this system. However, it is possible to start small, with the DDBMS installed at a single site, which temporarily functions as a data mart (Figure 2-15).

- 44 -

Additional sites are brought online as the need for additional data and the number of users grows (Figure 2-16). As databases are added, the data is distributed, by placement, replication, or partitioning, across the network.

Figure 2-15: "Starting small" on a DDBMS data warehouse.

Figure 2-16: Expanded DDBMS data warehouse. The users enjoy the benefits of a centralized warehouse (a truly businesswide perspective) and the advantage of a distributed warehouse (rapid response times). The single DDBMS is a centralized data warehouse without the extravagant hardware and development costs. It offers the additional advantage of development in small, incremental stages, reducing the risk level of the project while providing payback within a short period of time. This increases interest in the project and improves the funding possibilities. Because of the nature of the single DDBMS data warehouse, there is no distributed database management tool. The architecture is similar to that of a single database management system, only the data physically resides at multiple locations. No fragmentation schema is necessary, but the allocation schema is of increased importance. There are no individual data models; there is only the global data model/metadata, and it is an absolute requirement. The allocation schema once again contains the physical location of the data stored in the warehouse. It supports the use of data replication, to improve data availability, and table partitioning, to speed response times. When replication is employed, an optimizer chooses the appropriate iteration to provide the best access path for each query. Each of these distributed data warehousing schemes offers advantages to the business that embraces the distributed paradigm. With a distributed data warehouse, it is possible to start with a single data mart and gradually grow into an enterprise data warehouse. Entry costs are low, and even the full-scale distributed warehouse is dramatically less expensive than its centralized counterpart. In some instances, existing databases can even be added as is, without costly extractions and loads. This combination of flexibility and cost-effectiveness makes distributed data warehousing the preferred method for the foreseeable future.

- 45 -

Challenges of a Distributed Data Warehouse
The distributed data warehouse is a relatively new concept. While many organizations have implemented multiple independent data marts, few have taken the next logical step of joining them together. There are several reasons: lack of technical expertise in distributed systems; reluctance to try out new technology; and most important, the limited number of distributed data management tools and systems in the marketplace, none of which can be considered a mature product. In addition, many technologists are unaware of the few tools that are available. A handful of organizations have taken the next step and constructed a distributed data warehouse. Unlike centralized warehouses, which have been well studied, no numbers are available concerning the success or failure of the DDW. Judging by anecdotal evidence and the author's own experiences, DDWs have a high probability of success and, even if they never reach completion, the business has gained one or more functional data marts from the project. However, as with any data warehouse project, there are challenges and pitfalls. This section addresses them and how the decisions made during the project, especially in the design phase, can affect the overall success of the effort.

Distributed Database Management Tool/System
The key component in the interconnected architecture of the distributed data warehouse is, of course, the distributed database management tool or system, depending on the type of architecture employed. It allows the warehouse's data to reside in multiple locations joined together by a network and enables cross-site queries. Use of any of the distributed database types creates a number of challenges common to them all. For example: The distributed database should be transparent to the users while it simultaneously delivers high levels of performance, reliability, and availability. The distributed database must accommodate the heterogeneity of the physical environment. Mix-and-match hardware, differing network protocols, and diverse operating systems are all part of the problem. The distributed database must include systems management tools that will accommodate all the diverse software products and function over the network infrastructure. The distributed database must include facilities for managing (or at least interfacing with) the communications network to ensure security, reliability, availability, and consistent high levels of performance. Choosing an architecture before the first data mart is begun is certainly the best approach. It will eliminate, or at least ameliorate, many of the problems just stated. However, the warehouse designer does not always have that luxury. Frequently the designer must deal with one or more data marts already in place, with their existing hardware, operating systems, DBMS, and attendant software. The designer must then select the best distributed database management tool for the circumstance, keeping in mind the challenges outlined previously. Distributed databases alter the database implementation paradigm, raising some concerns not present with a conventional DBMSs. Some implementation issues are: Capacity planning Predicting the traffic patterns between node sites Maximizing data, object, and program placement Minimizing competition for resources between nodes/sites Supporting disaster recovery efficiently and safely Controlling transaction processing to avoid destructive interference The database administrator charged with resolving these issues should be trained in distributed system management and must work closely with the architect, designer, and chief modeler to achieve success.

- 46 -

Finally, there are issues with the management tools/systems themselves. The distributed database management tools/systems are at best immature products and do not always provide the highest levels of performance. Some of these issues are sufficiently troublesome that, if steps are not taken to avoid the problems, they might cause the warehouse project to fail: Intersite accessing and processing is not always carried out in an efficient manner. Processing may not be distributed properly between sites. Data might not be distributed to best advantage around the various sites of a network. Access (security) may not be adequately controlled to data linked by networks. Skilled DBAs and system programmers can handle all of these problems by manually redistributing data and processes, careful tuning, and when necessary, using each of the individual DBMS's own internal security facilities. A distributed data management tool/system requires a much higher degree of vigilance on the part of its support personnel than does the conventional DBMS to ensure its success.

Data Model/Metadata
Having an enterprise data model at the start of a distributed data warehousing project would be very useful. Unfortunately, this is almost never the case. What the company usually does have instead are models of the existing individual data marts. These need to be carefully reverse engineered into a global data model, resolving all inconsistencies. This "global" data model may be very different from an enterprise data model; it contains only the data and metadata about the existing data marts. For the success of the warehousing project, it is not immediately necessary to extend this model to include the remainder of the business, but it is a useful exercise that aids in the design of additional sites. At the very least, the global model must be kept current, incorporating new data/metadata as sites are added to the warehouse.

Data/User Interface
Like the centralized data warehouse, the distributed warehouse is intended to eventually serve everyone in the company. However, using either a homogeneous or heterogeneous distributed database approach, users have access to a local data mart and access to the entire warehouse. This greatly simplifies things for them; most of their queries require department-specific information and are directed to the local data mart. There is no confusion about which data is needed. It is only when the users venture into the warehouse that there is a chance for trouble. There it is a possibility that they cannot find the correct data or they choose the wrong data and come up with erroneous results. The single distributed database management system looks just like a centralized warehouse to the users. All queries are directed to the warehouse as a whole and, without some assistance, users are likely to get lost while searching for the data they need. Good metadata can function like a roadmap for the users, guiding them to the right locations.

Maintenance
The distributed data warehouse, like any processing system, must be maintained. Here, the physical size of the databases is almost never a consideration. Rather than allow any one site to become overcrowded, a new site is added. However, DDWs do have some system-specific maintenance concerns. Restarts and recoveries must be planned for each individual site and then for the warehouse as a whole. Because identical data is often stored at two or more sites, consistency can be a problem if not constantly monitored. Tuning is also an issue for a distributed data warehouse. Each individual data mart must be tuned, and then the entire warehouse adjusted to ensure adequate performance. Tuning includes more than just indices and summary tables; selected data and even entire tables may be relocated to a different site or duplicated across sites. Static queries should be explained to check for maximum performance and alterations made to the query and/or in the warehouse where necessary. In extreme circumstances, whole processes (e.g., programs, stored procedures, and data) may be moved to improve timing. The company that initiates a distributed warehousing project faces many challenges, venturing into poorly

- 47 -

charted waters. However, the entry cost is modest, the issues are manageable, and the potential rewards well worth the risk.

What to Choose
The arrival of new technologies, the prevalence of the Web and intranets, and the experiences of those who pioneered data warehousing have brought about a new and better way to build a data warehouse. This new architectural approach starts with a limited-scope independent data mart that simultaneously introduces the new technology to the organization, proves the value of the warehousing concept, and produces a substantial return on investment. In parallel with the initial implementations, the business can be developing both an enterprise data model and the appropriate architecture to ensure that the subsequent data mart implementations will blend with the first and that the completed whole will form a corporatewide distributed data warehouse. As the data warehouse market developed over the past fifteen years, many different philosophies and methodologies have evolved, some requiring extensive resources and some with a quick-fix approach. Today, it is apparent that no single architecture is right for every situation. If data warehousing technology is to reach the next stage of its life cycle, universal development, corporations must be allowed to examine their information needs, data source structures, and timelines and requirements and have the freedom to apply whatever warehousing architecture best suits their needs. A few organizations will choose a traditional centralized data warehouse; some will select a single system distributed warehouse, with the remainder opting for homogeneous or heterogeneous data marts joined together to form a distributed data warehouse. All will likely come to the conclusion that an n-tiered architecture, supported by an intranet, will bring the best results and most flexibility for the future. Once requirements are gathered, a strategy outlined, and architectural approach selected, the organization should begin work on its data model/metadata. The data model gives organizations the tool they need to maintain the architecture of the data warehouse. Changes in data warehouse content, physical implementation, available technologies, user requirements, and the business environment can all be addressed in the data warehouse through the logical model. Although modeling is timeconsuming, if the process starts with one subject area at a time, the payoff will be many times the effort in the end. A data warehouse at all stages of development needs to provide not only data but also metadata to be successful. In data warehousing, success is not defined by the quantity of data, but by the users' ability to understand, locate, analyze, and act on the warehouse's contents. Metadata is the key to user/data interaction. The shortest, quickest, and most cost-effective approach in most circumstances is to build from the bottom up, creating independent data marts to meet specific departmental needs, but with the forethought to remain in line with the corporate information strategy. This will yield early, quick success by giving departmental information to a selected group of business users and encourage the enterprisewide information effort. The n-tiered architecture employed in this effort has the flexibility and scalability to support both short-term and long-term objectives. A distributed data warehouse thus begun can deliver better information throughout all its phases to business users and enable informed decisions based on accurate, timely, and precise corporate data.

Chapter 3: Data Placement in Distributed Warehouses—Distribution and Transformation
Overview
One of the greatest challenges facing the organization that chooses to implement a distributed data warehouse is data handling in this relatively new environment. All distributed systems based on relational technology, and warehouses in particular, have the ability to store extremely large quantities of data (100 + terabytes if necessary) at an attractive cost per megabyte. It was likely this cost benefit that lured the company to the distributed warehouse paradigm initially. Along with an almost unlimited storage capacity comes the mixed blessing of multiple processors that are capable of functioning in a truly parallel fashion. These benefits lead to the question of how to distribute data around the various sites in order to take advantage of the "natural" parallelism inherent in distributed relational systems while still preserving the advantages of the individual data marts.

- 48 -

Another fundamental issue in distributed warehousing is one of providing automatic methods of transforming access to data that is stored in an "alien" access language into requests that can be dealt with by the local DBMS. While this is normally only a problem in heterogeneous systems, its resolution has implications important to any distributed warehouse with World Wide Web connections.

The Data Placement Problem
Most distributed data warehouses appear suddenly, overnight in fact, when two or more independent data marts are joined together. The majority of these warehouses are of the homogeneous or heterogeneous type, although some use a single distributed database management system approach (see Chapter 2). Since it was conceived as the union of a group of separate systems, the distributed data warehouse at "birth" already has a built-in data store residing at several different locations. The data objects are predistributed in a fashion that best suits the needs of the independent data marts, normally along departmental lines. There is a great deal of duplication of data across the systems, with key elements, such as customer names and addresses or a product list, resident at all sites. However, the arrangement of data objects that matches the requirements of the data marts might not suit the best interest of the warehouse. One of the easiest ways to understand this is to consider an example—say, a full-service brokerage house doing business as a fifty-person partnership. This brokerage house interconnected its four independent data marts to form a distributed data warehouse. The data marts were constructed, using RDBMSs, in the following fashion: Data Mart 1, Trading Accounts. This first data mart contains the stock (local and foreign) trading activities of all the firm's clients. Data Mart 2, Money Market Accounts. The brokerage makes a money market fund available to its clients for short-term investment of liquid assets. All client monies held by the brokerage, whether for future investment, to cover margin calls, etc., are deposited temporarily in this fund. This data mart holds the record of activities for the money market accounts as well as the "master" client file and its attendant data. Data Mart 3, Bond Accounts. The brokerage also acquires and holds bonds for its clients, and the record of these transactions is kept in this data mart. In terms of total assets, the bond department is as busy as the trading desk and insisted on its own data mart. Data Mart 4, Internal Accounts. This data mart contains the in-house records—brokerage expenses (e.g., rent, utilities, transportation, data processing costs, and accounting services), in-house trading account, in-house bond fund, payroll accounts (brokers are paid a percentage of funds under management or a percentage of each transaction), and partnership accounting, etc. Each of these data marts functions well, providing its users with excellent decision-support capabilities. The brokers and bond traders are especially pleased with their systems, Data Marts 1 and 3, for the instant access to a client's account they provide. However, now that these four data marts have been joined into a data warehouse, the users are expecting more from it. The brokers still want to see the stocks owned by their clients, but they also want to see any bond holdings and their clients' cash position, too. Bond traders are also making similar queries—for clients' complete portfolios. With the distributed data warehouse this access is possible, but it requires three queries to three different systems. Using the relational system, the query for the client, "John D. Rockefeller," would be: SELECT Client_Name (Table A), Client_Stock (Table B) FROM Client Data (Table A), Client Stocks (Table B) WHERE Client_Number (Table B) = Client_Number (Table A) and Client_Name (Table A) = "John D. Rockefeller" + SELECT Client_Name (Table C), Client_Balance (Table D) FROM Client Data (Table C), Client Balance Table (Table D)

- 49 -

WHERE Client_Number (Table D) = Client_Number (Table C) and Client_Name (Table C) = "John D. Rockefeller" + SELECT Client_Name (Table E), Client_Bond (Table F) FROM Client Data (Table E), Client Bond Table (Table F) WHERE Client_Number (Table F) = Client_Number (Table E) and Client_Name (Table E) = "John D. Rockefeller" Tables A, C, and E all contain the same type of data, Client Data: Note Data Mart 1 houses Tables A and B. Data Mart 2 houses Tables C and D. Data Mart 3 houses Tables E and F. While this arrangement of data worked fine for the independent data marts, it is not ideal from the perspective of the new distributed warehouse. There must be a better way to arrange the information to facilitate queries. One possible approach is to combine the stock and bond accounts into a single table and eliminate one of the two copies of the client table. If the resulting combined account structure is too large to be conveniently stored at one site, it can be horizontally partitioned (by account number or client name) and reside at two or more locations (in this example, Data Marts 1 and 3). This redistribution of data yields at least two benefits to the warehouse as a whole: It eliminates the storage of one set of client names and addresses, and it allows the broker and trader requests for portfolio information to be fulfilled with two queries and access to only two systems. With the account records combined and redistributed across two sites, the brokers and traders can have complete portfolio information in one-third of the time it used to take. The portfolio example was relatively easy to solve. The situation is more difficult when it is necessary to distribute the components of more than one relationship across multiple sites. To illustrate this concept, consider an example using the same brokerage house. Only this time, a bond trader knows of a new municipal bond offering by Trenton, New Jersey, about to come to market and would like to alert some clients to the opportunity. Because the bond trader's time is limited, he decides to notify only those people who have purchased municipal bonds in the past; he feels that they will be the ones most inclined to buy a new offering. Also, municipal bonds give the greatest tax advantage to those purchasers living in the community offering the bonds, so he should confine his initial phone calls to only those people with a home address in Trenton or the surrounding area. Using a relational system, the bond trader needs to do the following: SELECT Client_Name (Table 4), Client_Phone_Number (Table 4) Client Securities (Table 1), ZIP Code (Table 2), Bond Table (Table 3), Client Data (Table 4) WHERE Client_ZIP_Code (Table 4) = ZIP_Code (Table 2) and ZIP_Code (Table 2) = City_Name "Trenton" (Table 2) and Client_Bond (Table 1) = Bond_Name (Table 3) and Bond_Name (Table 3) = Bond_Type "Municipal" and Client_Number (Table 4) = Client_Number (Table 1) FROM

- 50 -

As can be seen, to fulfill this query the bond trader will need data from four tables located at three sites to find the clients he seeks: Data Marts 1 and 3 Client Securiti es (Table 1 now contains the combine d custome r account s for the wareho use) and Client Data (Table 4 contains the combine d custome r data file for the wareho use) Bond Table (Table 3 contains all bond names and their type, such as corporat e, Treasur y, and municip al) ZIP Code Table (Table 2 contains all U.S. ZIP codes and their location)

Data Mart 3

Data Mart 2

To maximize the join between the ZIP Code Table and the Client Data Table, the ZIP Code Table could be duplicated and repositioned to reside in both Data Marts 1 and 3, instead of its original location in

- 51 -

Data Mart 2. This would simplify the join and take advantage of the natural parallelism of the distributed environment. The local RD-BMSs at both locations could then process their half of the Client Data Table with the ZIP Code Table simultaneously. By just duplicating and repositioning the ZIP Code Table, total query time is reduced by about 30 percent. The disposition of the Bond Table is somewhat more difficult. It is a very large table containing all the bonds issued in the United States for the past twenty years, as well as some older active issues, and is indexed by bond number. While storage in a distributed system is inexpensive, it is not so cheap that it is practical to duplicate everything everywhere. Horizontal partitioning of the Bond Table would be possible, with the partitions residing in different locations, Data Marts 1 and 3 in this example. However, just because it is possible does not make it the best solution. Some benefit would be derived from the dual processing made possible by the split, but that would be negated by the cross-system traffic it would create. Another approach to the Bond Table is possible. It is currently indexed by bond number, a unique number assigned to each offering of bonds when it is issued. It serves to identify and track the issue during its life cycle. An index could be added to the Bond Table on the Bond Type column, and then the table partitioned in such a manner that all bonds of the same type fall within the same partition (i.e., treasuries and corporate in partition 1, municipal and state in partition 2). Assuming two partitions, the Bond Table could be distributed between Data Marts 1 and 3. Now, only one partition of the Bond Table would participate in the join with the two partitions of the Client Data Table, and the cross-system traffic would be half of what it was before. Processing time for the Client Data/Bond Table would be cut in half. With both improvements in place (ZIP Code Table duplicated and repositioned and Bond Table partitioned and distributed), the total query time is reduced by nearly 80 percent. Of course, this example assumes that these improvements in no way conflict with other queries made to the data warehouse. From these rather contrived and simplified examples it is clear that data placement within a distributed data warehouse can indeed have a significant impact on performance. The technique is made more complex by allowing redundancy (i.e., two copies of the ZIP Code Table), but this "extension" can bring compensating rewards. A methodology, or even a general approach, to solving the data placement problems of a distributed warehouse would be a great help.

A Practical Approach to the Placement Problem
It would be very convenient to have a mathematical solution to these data placement situations. But allocation problems are difficult to solve—it is unlikely that an algebraic expression (an algorithm with polynomial time complexity function) can be formulated to solve them. In fact, C. C. Chang and Phil Shielke demonstrated as early as 1985 that it is almost impossible for an algorithm to provide a complete solution to any of these problems, given realistic-size data structures and an acceptable timeframe. Unfortunately, their work was almost completely forgotten by many in the scramble to find a quick fix to the data placement problem. As of this writing, no such solution exists, nor is one likely to be forthcoming in the foreseeable future. This does not suggest, however, that efficient problem-solving methods should not be sought. Rather, the focus must be on using heuristic methods to unravel the data placement issues. To this end, the author proposes one possible approach for determining data placement in a systematic manner. This methodology has been road tested on several unsuspecting major clients, always with (thankfully) successful results. After all, what is any good theory but the fortuitous outcome of trial and error? When building the data marts/data warehouse, the design team first developed an architecture that served as a guide to the selection and arrangement of the various parts. So, too, with data placement: There needs to be an architecture or at least a general approach that functions as the framework for problem resolution. Like any architecture, this is not the solution to the problem or the blueprint for successful design, but it does act to focus energy into a step-by-step methodology that maximizes the opportunities for success. This architecture can be broken down into two parts, with the first focusing on tuning and data placement to maximize user access and the second part focusing on techniques to minimize load times.

Maximizing User Access
- 52 -

Composed of twelve practical steps, this portion of the architecture should be applied sequentially. It is not that step 1 is more important than step 4; rather, the information acquired in step 1 influences the decisions made in the subsequent steps. Not all steps will apply equally or at all to every distributed data warehouse, but each step should be considered in light of the individual circumstances and the type of distributed system in use.

Step 1: Learn the fundamental goals of this particular distributed data warehouse. No two data warehouses are ever physically exactly the same, and no two ever have
exactly the same objectives. Physical differences are easily discerned; philosophical differences and sometimes even a practical direction are slippery territory. Of course, the company intends to use the warehouse for decision support, but what decisions and who is supported? Is the warehouse primarily a collection of independent data marts, destined to serve individual divisions as a replacement service for monthly and quarterly reports? Does it support the firm's knowledge workers, opening new avenues of investigation for potential products or different marketing approaches? Or is it to function chiefly as the tool of upper management to provide an overall view of the business? The warehouse usage patterns will evolve over time, as will the warehouse itself, but there should be a starting point that establishes the warehouse's biases. Data placement is primarily a tuning device, and it is not possible to tune any system to maximize performance for all (actually, it is possible to tune a system to treat all users moreor-less equally, with the result being equally unacceptable to everyone). It is disastrous to distribute data to support individual data marts in one instance and then redistribute it again to support an overall warehouse function for the next circumstance. A clear direction must be chosen and adhered to consistently, to avoid working at cross-purposes. Step 2: Design to suit the individual database management systems first. This advice applies only to homogeneous and heterogeneous distributed data warehouses (for a single DDBMS, you obviously must design to that product's requirements). Just because the environment is new, a distributed warehouse is not an excuse for the designer forgetting everything she ever knew. The distributed data warehouse is a confederation of stand-alone systems, and the requirements and rules of those systems must be respected in the distributed environment. For example, most of the systems participating in distributed warehouses use a relational database management scheme. Large tables are frequently partitioned for performance reasons in an RDBMS; if a table would normally be partitioned in the stand-alone system, it will still be partitioned in the distributed warehouse. One of the worst mistakes made by novice designers is to violate the rules governing data handling in the local DBMS while tuning the warehouse as a whole. Step 3: Tune the local data marts next. Most queries in a distributed data warehouse are processed locally—by some estimates, as much as 95 percent of all inquiries are handled by the individual data marts. For this reason alone, the performance of the individual systems is extremely important. If users have excellent response times on 90 percent or more of their queries, they will be satisfied with their warehouse. However, in the process of tuning the warehouse as a whole, it is often easy to overlook the denigration of response times in one location. No thought is given to the principle that poor performance at the level of the individual system is magnified by cross-system queries. If one of the data marts involved in a multisystem query is responding slowly, the entire warehouse must wait for it to return the requested information before processing can be completed. What was a localized problem is now a warehousewide issue that can best be resolved by paying attention to the ailing data mart. Always maximize performance at the local level before making the decision to redistribute data. Step 4: Make certain that the local "physical house" is in order. The importance of regularly scheduled data structure reorganizations and a refresh of statistics cannot be overemphasized. In the author's experience, even competent database administrators (DBAs) sometimes overlook this basic necessity or fail to perform it frequently enough, allowing performance to deteriorate as a result. As with step 3, proper data handling in a distributed data warehouse begins with maintenance at the local level. Step 5: Optimize the query language. This is the simplest of the steps and the most frequently overlooked. It is easy for the warehouse designer and the DBAs to assume that the programmers and the interface products employed by the users (SQL when the data storage system is an RDBMS) are producing good code. And, in fact, they might be generating excellent code for local processing and terrible SQL for the extant distributed warehouse. Writing a query for cross-system processing is very different from crafting a single-system inquiry, and programmers and DBAs need training to learn to function efficiently in this new environment. While the various forms of the EXPLAIN command available are useful, most have a single-system bias. At least initially, all static and a portion of dynamic distributed SQL should be checked and improved by someone experienced in multisystem processing and very familiar with the data warehouse. If the local DBMS is equipped for stored procs (procedures), their use should not be overlooked in the distributed system. A stored proc can be called by a query that originated in another system, and they should be used whenever possible to maximize performance. Do not make the mistake of blaming the warehouse for improperly written queries.

- 53 -

Step 6: Apply standard optimization techniques. If the query language has been
optimized and response times are still slow, apply standard optimization techniques to the data in-place before considering more drastic measures. Often the addition of an index or the partitioning of a large table will make a remarkable difference in query performance. Anything that improves the performance of a portion of a complex inquiry generally improves the entire query. Step 7: Position data to be near "complementary" data. Data forms "natural groupings" whose elements are frequently requested together. All members of the group should reside at the same location to avoid the expense of constant cross-system queries. There are two ways to determine which data belongs together: Either have an extensive knowledge of the business the data supports, or understand the interrelationships among the data storage structures (e.g., tables in an RDBMS). Ideally, the warehouse designer has both, but it is often sufficient to just understand the physical table structures. Using an RDBMS, the tables of crucial importance to one another all share a common primary key. Those that are related, but are of lesser importance to one another, have a foreign key relationship. Tables having the same primary key should be stored together. Separating them will create intersystem traffic, a step taken only for the most compelling reasons (see step 9). Foreign key relations, while not as important, should be preserved whenever possible. Step 8: Evaluate "static" tables. Nonvolatile tables are those data that change infrequently, if at all. Make certain that these tables are located at the site where they are most often used—that is, where they have a foreign key relation to one or more of the other tables stored at that location. If they are needed at more than one node, do not hesitate to duplicate them unless they are extremely large. In the second brokerage example examined earlier in this chapter, it was useful to have the ZIP Code Table resident in both Data Marts 1 and 3 to avoid cross-system queries. While intersystem inquiries are expensive, data storage is relatively cheap, and multiple copies of static tables are easy to maintain because they change so seldom. Step 9: Make vertical partitioning work for the warehouse. Data storage structures that share a primary key can be viewed as being vertical partitions of one another; they could be united, by removing some duplicate data, to form one giant table. They all contain information about the same entity, the one identified by the primary key (assuming a business key is used). While this violates all the basic rules of data modeling and table design, it is a helpful logical construct when used to aid in data distribution decisions. In the first brokerage example discussed earlier in this chapter, the Client Data Table, the Client Balance Table, the Client Securities Table, and the Client Bond Table all had the same primary key, Client_Number, and contained data about the firm's customers. Any or all of them could possibly be merged into one structure. To improve performance in the example, the Client Securities Table and Client Bond Table were united to form the Client Data Table. This new table suited the query pattern much better than the old table structures. An occasion may also arise in distributed warehouse design where it is helpful to split apart, or vertically partition, an existing table. This is accomplished by dividing the existing table vertically, moving a portion of the attributes to a new table. Both of the new tables thus created have the same primary key as the original and the same number of rows. These new tables can then be located in the data marts where they are needed.

Step 10: Use horizontal partitioning to take advantage of the natural parallelism of the distributed system. Horizontal partitioning is the practice of dividing a large physical table
into several smaller sections, or partitions, using a key range to define partition members. It is commonly used in almost all RDBMSs to improve system performance, speed up reorganizations, and shorten image copy times, for example. It is also useful in a distributed system to improve performance in an additional way; the various partitions can be positioned at different nodes so they can be queried simultaneously. This technique creates cross-system traffic as the result sets are assembled at a common location and should only be used when the "gain" from the concurrent processing outweighs the "loss" due to intersystem traffic. Step 11: Introduce data redundancy to minimize cross-system traffic. As seen in step 8, duplication of static tables across nodes is an easy implementation of this rule. Duplication of other data can be "expensive" in terms of system resources; duplicate tables must be carefully updated and maintained to protect data integrity and concurrency. For this reason, duplication of "volatile" or "active" tables should be attempted only when the resulting savings in processing costs (i.e., intersystem traffic) justify the additional expense. In this chapter's brokerage examples, there are multiple copies of the Client Data Table. While the solution to the first example included the elimination of one copy of the Client Data Table and the horizontal partitioning and redistribution of another duplicate, there were still two copies of the table available after tuning. Each data mart continued to have all or part of the Client Data Table available to it to reduce cross-system traffic. Step 12: Redistribute data and processes to reduce bottlenecks. A situation can arise where some nodes in the distributed warehouse are much busier than others. In fact, one site may be so busy that it adversely effects response times and/or system availability. If this happens, the problem

- 54 -

can be ameliorated by load balancing, or relocating part of the data stored at the overworked site to another underutilized node. The data should be moved in groups (see step 7) along with its attendant processes (e.g., updates, queries, stored procs, archival activity) to produce the maximum benefits. If the distributed data warehouse uses the data mart architecture, the network might also be reconfigured to move certain users to the new site to minimize cross-system traffic. In some instances, relocating the data is not sufficient; it must be duplicated across nodes and the user population divided to alleviate the problem. It is important to remember that all distributed warehouses change over time; this flexibility is one of their most attractive features. New data marts come online, additional information is acquired, the store of historical data continues to grow, and user requirements change in response to shifts in the business environment. The warehouse designer and DBAs must remain alert to these changes and constantly be ready to adjust the warehouse's configuration to suit the business needs. An unattended warehouse soon becomes obsolete, even useless.

Minimizing Load Times
The second half of the data placement architecture attempts to minimize load times. Almost all data warehouses acquire their information from production systems and other sources, with data transfers occurring on a regularly scheduled basis (e.g., once a quarter, month, week, day, or several times a day, depending on circumstances and the warehouse's purpose). It can happen that there is so much data to transfer, the load time exceeds the available batch window at one node or another. This in turn reduces the availability of part or all of the distributed warehouse. Like tuning for availability, there is no quick fix for minimizing load times. Once again, the proposed solution architecture is presented as a series of steps to be considered in order. Not all steps apply to every distributed warehouse in each circumstance, but all should be reviewed before formulating and implementing a solution. Step 1: Determine what facility is used to accomplish the load. Data loads come in all types: change programs that merely enhance the existing records in a table, load programs that add new records in a table, update programs that both load and enhance a table, and load utilities. The trick is to make certain that the fastest method is chosen for a particular job. Generally, load utilities are the most efficient way to build a new table or add records to an existing one. In some cases, third-party products are more efficient than the utilities that come included with the RDBMS; these products should always be tested before altering the load process in their favor. Changes are best accomplished programmatically, with attention paid to optimizing the query statement. Wherever possible, the same program should not be used to both change and add records; preprocess the data feed to separate the deltas from the new records and use a program for changes and a utility for loads. So too, for maximum efficiency, a program should not update or load more than one table at a time. While every RDBMS is somewhat different, several of them process loads much more rapidly without indexes, which can be built after the load process is complete. Step 2: Determine if the data feed has been preprocessed. All data, before entering the warehouse, has undergone some preprocessing to cleanse, convert, and/or transform it. In some cases, this is not sufficient for optimal loads. The data might also need to be sorted into table sequence, or divided by partition key into multiple files, or split into two or more loads for different tables, and/or checked for referential integrity. Whatever processing can be done outside the warehouse saves time on the loads. Step 3: Assess whether loads can process concurrently at one site. Most RDBMSs offer parallel processing, or the ability to run two or more processes concurrently. At a multitable site, several data loads may be able to run concurrently without fear of lock contention because they access different groups of tables. This technique alone can radically reduce elapsed clock time, but the local RDBMS must be retuned to support parallel processing. Some loads, however, cannot be run concurrently because of referential integrity constraints—either the foreign key tables must be loaded first, imposing a sequence on the loads, or several tables access the same foreign key table while loading to ensure referential integrity, and parallel processing would lead to lock contention. Step 4: Determine whether foreign keys are necessary during load. Referential integrity is extremely important in any system, but it can be very expensive in terms of the time it adds to the load process. Wherever possible, check for referential integrity during the preprocessing phase and then drop the foreign keys during loading. This technique removes the artificial "load sequence" barrier and the "foreign key table" contention associated with parallel loads. Also, determine just how critical foreign keys are to the warehouse in general. While they do ensure referential integrity, is the warehouse paying too high a price for correct data? If the warehouse requires absolute accuracy (i.e.,

- 55 -

the corporate year-end statement is generated from the warehouse), then foreign keys are a must. However, if the warehouse is a decision-support tool, then the very small percentage of error introduced by preprocessing the data instead of depending on foreign keys might be tolerable.

Step 5: Assess whether large tables can be partitioned to allow concurrent processing. Often, the load process bottlenecks at a very large table.This can be resolved by
horizontally partitioning the table in such a way as to permit the individual partitions to be loaded concurrently. Where there was once one enormous load there are now several smaller, faster loads, reducing elapse time by as much as 90 percent. Step 6: Determine how aggregation and summary tables are built and loaded. Often, as a part of the regularly scheduled load process, summary and aggregation tables are constructed. There are several ways to do this, with some methods more efficient than others, in part dependent on the RDBMS used. Generally, it takes less time to perform mathematical computations programmatically than to use the services provided by the RDBMS (this should be tested using the chosen RDBMS before modifying any code). The most time-saving approach is not to build the summary and aggregation tables at load time; instead, create them using VIEWs. While this approach saves a great deal of time during the load process, it drastically slows user access to these tables and should be used cautiously. Step 7: Determine how tables are arranged on the physical storage devices. The application of this technique is completely dependent on the RDBMS chosen and the type and configuration of the physical storage devices. For each given combination of hardware/software there are preferred methods of data distribution across the available storage devices that can be exploited to minimize load times (i.e., placing two tables that load concurrently on different devices). Since the potential savings in elapse time can be substantial (in some cases as much as 70 percent), it is well worth the effort to explore this avenue for load tuning. Step 8: Consider moving some tables to an underutilized site. After applying the other, site-specific techniques for tuning, some nodes could still be overworked. With a distributed system, it is possible to relocate some of the processing to another, underutilized site in the network. To do this, a group of tables (see step 7 in the section on Maximizing User Access) must be moved, along with all their attendant processes. Care must be taken to relocate all the complementary tables together to avoid adversely affecting data access. Often, the network must be reconfigured in support of this data redistribution to keep users and their data together. If the distributed warehouse is based on the data mart paradigm, relocating data outside its home mart will increase network traffic substantially. As a result, this is a last-resort solution.

Step 9: Consider partitioning a large table and distributing the partitions across sites. Partitioning extremely large tables and concurrently processing the loads can drastically reduce
load times (as outlined in step 5 of Minimizing Load Times). This basic technique can be taken one step further by relocating the partitions across the nodes of the network. Although this technique offers some savings over the traditional single-site method (just how much depends on the hardware and software in use), it definitely increases network traffic. Every access to the distributed table generates a multisite query and the extra processing costs of assembling the responses at the local node. Of course, query times might improve because of the parallel processing (see step 10, Maximizing User Access), but this depends on factors such as the number and type of JOINs, the location of the JOINed tables, and network speeds, for example. This technique must be applied cautiously, with thought given to the impact it will have on total data access. As seen, there are circumstances when improving load times de grades data access. With a distributed data warehouse, the designer and DBAs are constantly working to balance the needs of the warehouse with the needs of the users. The warehouse team must be constantly aware that the changes they implement will affect the entire system—there are no insignificant changes. That is why it is so important to understand the overall goals and purpose of the warehouse; this knowledge guides the warehouse team to make the choices best suited for the individual warehouse. No methodology can guarantee success to its practitioners. However, by utilizing both aspects of the data placement architecture described, the designer can be assured that the changes she makes to the distributed system will improve, not denigrate, overall warehouse performance. The steps should be applied in order: Try the basic, or simple, solutions first and progress to the more complex and expensive remedies only when absolutely necessary. Remember, for any warehouse data placement problem there are multiple acceptable solutions, and the best solution is almost always the simplest.

- 56 -

Integration of Diverse Database Systems
Another of the major challenges in distributed warehousing primarily affects only one of the three major types in use today, the heterogeneous system (Figure 3-1). However, most of the homogeneous and single DDBMS warehouses built also have a heterogeneous component—they have World Wide Web interfaces and utilize their Internet connections to acquire at least some data for the warehouse. Because they are reliant on external data stored in DBMSs not of their choosing, these warehouses, like their heterogeneous cousins, must confront the issues inherent in mixed-DBMS processing and find a way to resolve them.

Figure 3-1: Heterogeneous distributed data warehouse. This challenge centers on cross-system queries. When users, connected by a LAN to a local data mart, wish to obtain data from a "remote" site within the warehouse, how do they go about it? Ideally, they just write their queries as if they were asking the question locally; after all, that is the idea behind a data warehouse. The user should not have to know, or even care, where the data is located or how it is stored in order to obtain it. For the distributed data warehouse tool to automatically transform the query written for one data mart into a query, or series of transactions, understandable by another data mart using a different DBMS, it needs to know how the two DBMSs in question relate to one another. The key component in this transformation is the "map" that tells the distributed data warehouse tool how the various data marts interrelate. This map is referred to as the global data model. It is constructed by integrating the component heterogeneous databases into a DBMS-independent universal schema. The global data model provides a virtual representation of all the data structures in the distributed data warehouse and, along with its attendant metadata, provides the backbone for query transformation. Without the global model, access to data in a warehouse is severely restricted. The global data model cannot contain more information than the sum of the information contained in the contributing local schema. Frequently, it contains less, as the local systems choose to make only a portion of their data available to the distributed data warehouse. If individual data models of the contributing local systems exist, they must be melded into a unified whole. This requires a series of data abstractions and transformations to ensure consistency in the unified global model.

Abstractions
The first of these abstractions is aggregation, where a relationship between entities is represented as a higher-level object. For example, an "appointment" entity could be used to represent a potential relationship among a customer, a broker, and the brokerage company; a "derivative" might be a complex trade involving stocks, bonds, and interest rate swaps. Another abstraction is generalization, where a set of generic entities is considered to be a single entity (i.e., "client" might include individual customers as well as corporate entities). Restrictions can be placed on the generalized object or on a class of objects to obtain a subset of particular interest—for example, "bond traders" are a specific group within the generalized "employees."

Transformations
- 57 -

There are other conversions and transformations that must be performed to unite the individual data models. Most are needed for syntactic reasons, such as when items are stored at two or more locations using different measures (e.g., price per unit versus price per hundred). A different type of syntactic transformation would be used where the record structure differs among the various local databases. This can be very simple, as when in one local data mart stores the client's telephone number in the format "area code + telephone number." In another data mart, the area code and telephone number are kept separately. To reconcile the differing record structures, a syntactic transformation is needed. In a more extreme example, the customer data in one data mart may be kept in a database "root" segment, and the securities trades in dependent "child" segments of the same database. The same data is stored in three separate relational tables in another data mart. Here again, a syntactic transformation can resolve the differences. A final example of transformation involves in the resolution of cross-system record duplication. Consider the brokerage business discussed earlier in this chapter where a securities sale is represented by an entry in the Client Stock Table in Data Mart 1. The same sale is also recorded as a credit entry in the Client Balance Table in Data Mart 2. It is not obvious from any syntactic considerations that these records represent the same transaction. The distributed database management system in use would have to "figure out" the meanings of the different structures from syntactic information furnished by the system designer.

Mapping
In addition to aggregations and transformations, special techniques are needed for mapping between the preexisting heterogeneous data models. One of the most important goals of such a mapping is that both the information stored in the local data model and the operators that can manipulate it be "preserved." When the mapping process is complete, the ideal outcome would allow everyone using the warehouse to have access to all the data represented in the local model. They would also be able to use every method of data access supported by the local DBMS without knowing anything about that DBMS (the use of this capability is subject, of course, to the logical and physical constraints of the finished distributed warehouse). The methodology most commonly used for transforming data models is an old and reliable one; a source data description is mapped onto a target data description, in accordance with the rules of the source and target data models. This means that the local data model is mapped, one-to-one, onto the global data schema, with all aggregations and transformations carefully noted in the metadata. These data models, local and global, are considered equivalent if actual physical databases can be generated from them that are "one-to-one" and "into" (i.e., nothing left out). Database states in the source and target data models are said to be equivalent if they can be mapped to the same state in some abstract metamodel. What this actually means is that they both represent the same view of the real world. Formal methods of establishing this equivalency depend on agreement to employ an abstract metamodel. While the concept of the metamodel is generally acknowledged, it has not yet been sanctioned by the modeling community. Therefore, more informal criteria are discussed here to establish the equivalency of states. So, how can the data modeler be certain that the global and local models are really equivalent without generating real databases and comparing them? One way is to design the global model in such a way that any query that will "process" against it will also "process" against the local model (restated, if necessary). For this to happen, the set of operators available for each data model should be as complete as possible (i.e., for each of the objects in the data definition language of the data model, there exists a corresponding way to express it in the data manipulation language). Then, a sequence of events can be defined to transform the operators of the global model into the operators of the local data model. If both data models, global and local, are complete and one-to-one, then they can process the same queries because they are equivalent models. To put the goal for data model transformation in more pragmatic terms, any transaction that runs on the database generated from the source (local) data model should be able to be rewritten to process on the database generated by the target (global) data model. Also, this rewriting of the transaction should occur automatically, performed by the distributed database management tool. Without a serviceable global data model, it is impossible to form a distributed data warehouse from heterogeneous systems.

- 58 -

The Global Data Model
Many methodologies and example schema have been proposed over the past fifteen years for reverse engineering a global data model from local ones. Some methodologies enjoyed brief periods of popularity before fading away, while others never gained even that small measure of acceptance. As diverse as these proposed approaches have been, they all agreed that three basic conditions had to be met to guarantee total inclusion of all kinds of heterogeneous systems into the global data model: The global data model, at any stage of its evolution, must be capable of absorbing new data models (if necessary by extending its DDL and DML using a system of axioms expressed in global data model terms). The information and operators from the local data models must be preserved after inclusion in the global data model (i.e., there must be commutative mappings between the schema and operators and a "one-to-one" and "into" mapping between database states, local and global). The local data models must be synthesized into a unified global data model, not just included as isolated submodels.

Methodologies
Today, most global data models are constructed using one of three generally accepted methodologies. The first such model, called UCDM or the unifying conceptual data model, was developed by starting with the relational data model and extending it incrementally by adding axiomatic extensions equivalent to various well-known data models. For example, to include the network data model in the UCDM, the axioms of uniqueness, conditional uniqueness, and obligation for unique, unique non-null, and mandatory attributes were added. While it is a useful methodology, UCDM is somewhat rigid and difficult to work with. Another, more pragmatic methodology is DAPLEX. It is a special-purpose system designed to be used as both a language and the common data model to which each local data model maps. This system uses the functional model as a pivot representation onto which the heterogeneous systems map on a one-to-one basis. Real-world entities and their properties are represented as DAPLEX entities and functions. Separate views of the integrated model are provided to meet local requirements. Mapping between most well-known data models, and even operating system files, is easy using DAPLEX. For example, a relational model can be included by representing each relationship as a DAPLEX entity and each domain as a DAPLEX single-valued function. The network data model can be described in a similar fashion, and even sets are accommodated by representation as multivalued functions that return member entities. Even with all of its good qualities, DAPLEX is the distant second choice of most modelers because it is difficult to learn and works best only when used in conjunction with a distributed database product called Multibase. By far the most popular methodology, the relational data model without axiomatic extensions, has become the backbone of warehousing systems in the United States. The majority of the data modeling community has judged it adequate for the combined representation of all database systems, and it is used as the underlying structure for the global data model in nearly 98 percent of all warehouses. In fact, it is so widely accepted that many data modelers learn only relational modeling without even knowing that alternatives exist. The relational data model was first proposed by E. F. Codd and C. J. Date in the early 1970s as a new paradigm for representing interrelationships among data, and it was extended shortly thereafter to become the basis for a more efficient system of data storage/retrieval. The impact of the relational model on the data processing industry cannot be overestimated—it revolutionized the way people regarded data, bringing about the realization that properly organized and readily accessible data is a corporate asset. It has spawned dozens of database management systems, from DB2 and Oracle to Microsoft's Sequel Server, and changed forever user expectations. To use the relational model as the basis for a global data model, there need to be relational views of the local systems (often nonrelational in structure) that can be blended together to form the global schema. The problems associated with construction of these local data models varies depending on the type of

- 59 -

DBMS used by the underlying system. There are, however, some general considerations applicable to building a relational data model regardless of the base system. First, all the data sources must be identified and categorized in the local system, and a relational equivalent established. It is normally not difficult to identify the data sources themselves; these are the files and databases used for storage in the local database management system. The data element definitions and formats are somewhat trickier, since so few systems have a functional data dictionary, and many nonrelational systems permit (and even encourage) the storage of duplicate data under the same or different names in several places with differing formats. A data type and length must be chosen for the data element to become a properly defined relational attribute and all its possible sources identified and documented and, where necessary, a conversion factor specified in the metadata. Defining the data type and length ties into the second factor that needs to be established—integrity constraints for the data elements. Are duplicates allowed? Must a number fall within a certain range? Are only three possible values acceptable in a field; if so, what are they? The possible range of allowable values for each data element must be considered and defined. These should be based on the actual data stored in the local system, provided it is "clean" or valid. Considerable research is always required to establish the correct integrity constraints. The relationships among the data elements must also be established and captured in the relational model, and this can be the most difficult task of all. Where the relational model is very good at recording many different kinds of relationships (e.g., one-to-one or one-to- many, foreign key relationships, the interrelationship of data stored in a "star schema"), the relationships in other systems are often vague or nonexistent. Instead, these systems frequently rely on the knowledge of the programmers to provide the necessary connections among the files and databases. This information too must be hunted down and incorporated into the relational model. Every file or database in the local system that is to be mapped onto a relational model generates at least one relationship—a file or database is, by definition, a logically related group of records. They all contain the same type of information about the key elements and can be transferred to the relational model in the form of one or more normalized tables with the same primary key that, in the case of multiple tables, form a snowflake schema. The foreign keys illuminate themselves during the process of "normalization," or removal of data redundancy from records stored in the same table. In DBMSs with parent/child record relationships, there is a natural mapping of a one-to-many relationship between the "parent" table and any "children." The "invisible" relationships known only to the experienced programmers should also be sought and recorded to make the mapping between the local system and the relational model as complete as possible. Generally, relational attributes map to local data items, and relations are mapped to record types or programmer knowledge. Where the mapping is not one-to-one or has been derived from some other source, it must be noted in the metadata. This will allow the completed global data model to play a key role in query transformation. To illustrate how this might work, it is useful to consider an example. Using the same brokerage house as in the previous examples in this chapter, assume that Data Mart 1 has a DBMS that employs DL/I databases for its storage structures. In this case, instead of a Client Data Table and a Client Stocks Table, there is only a Customer database. The other Data Marts all remain the same, using relational database management systems. A global data model, based on a relational schema, has been constructed for the warehouse formed by uniting Data Marts 1, 2, and 3, as shown in Figure 3-2.

Figure 3-2: Global data model for brokerage example. The same query is posed as in the earlier example, requesting portfolio information about the client John D. Rockefeller. It is written in SQL because it is based on the global data model: SELECT Client_Name (Table A), Client_Stock (Table B), Client_Balance (Table D), Client_Bond (Table F) FROM Client Data (Table A), Client Stocks (Table B), Client Balance Table (Table D), Client Bond Table (Table F)

- 60 -

WHERE Client_Number (Table B) = Client_Number (Table A) and Client_Name (Table A) = "John D. Rockefeller" and Client_Number (Table D) = Client_Number (Table A) and Client_Name (Table A) = "John D. Rockefeller" and Client_Number (Table F) = Client_Number (Table A) and Client_Name (Table A) = "John D. Rockefeller" and Sale_Price (Table B) = "" and Last_Update (Table D) = "(today's date)" and Sale_Price (Table F) = "" With assistance from the global data model, the distributed database management tool in use in this warehouse can break this query into three parts, to extract the requested data from each Data Mart. The most difficult transformation is the one addressed to Data Mart 1, which uses the nonrelational DBMS. The metadata for the Client Data Table and the Client Stocks Table indicates that they are actually one DL/I database, Customer, and that the attributes Client_Name and Client_Stock have the same name as the underlying data items. The metadata also indicates that there is a Client Data Table stored in both Data Marts 2 and 3. With this information from the global data model and its attendant metadata, the query can be rewritten as follows: FOR DATA MART 1 GU Client_name = "John D. Rockefeller" Check return code If return code not 0 then go to Customer_Not_Found Else Save Customer data. Loop: GNP If return code not 0 then go to terminate_loop Else if Sale_Price <> "" skip segment Else Save Stock data. End loop. Terminate_loop. FOR DATA MART 2 SELECT Client_Name (Table C), Client_Balance (Table D) FROM Client Data (Table C), Client Balance Table (Table D) WHERE Client_Number (Table D) = Client_Number (Table C) and Client_Name (Table C) = "John D. Rockefeller" and Last_Update (Table D) = "(today's date)"

- 61 -

FOR DATA MART 3 SELECT Client_Name (Table E), Client_Bond (Table F) FROM Client Data (Table E), Client Bond_Table (Table F) Client_Name (Table E) = "John D. Rockefeller" and Sale_Price (Table F) = "" In this oversimplified example, the query transformation was straightforward—the Client_Name could be located in the root segment of the Customer database. The JOIN between the Client Data Table and Client Stocks Table was translated directly into the GNP (Get Next within Parent) command, which would return the "child" segments containing the stocks owned by the client. While not all query transformations are so easily accomplished, most have a one-to-one correspondence between the SQL statements and the commands in the local DBMS. If the global data model has been properly constructed with attention to the accompanying metadata, the transformation can be accomplished automatically. WHERE Client_Number (Table F) = Client_Number (Table E) and

What to Remember
The multiprocessor, multisite configuration of the typical distributed data warehouse makes it the perfect place to store extremely large quantities of data in a very cost-effective manner. However, along with an almost unlimited storage capacity comes the mixed blessing of multiple processors that can function independently of one another. This combination of many storage locations and many processing locations leads to the question of how best to distribute data around the various nodes. The desire to exploit the "natural" parallelism of the distributed systems must be carefully balanced with the advantages of the individual data marts. Unlike the traditional data warehouse whose "birth" is a carefully scripted affair that occurs gently over a period of time, the distributed warehouse appears suddenly, when the network joining two or more independent data marts is enabled. Conceived as the union of a group of separate systems, the distributed data warehouse begins its existence with a built-in data store scattered around the different nodes. The data objects are predistributed in a fashion that is best suited to the needs of the independent data marts. Normally, no thought has been given to optimizing data placement for warehouse performance. How, then, should data be rearranged for a distributed warehouse? Unfortunately, there are no quick and easy solutions, no formula to follow that will lead to optimal data placement. Instead, the warehouse designer should focus on a heuristic approach as the most efficient problem-solving method available. A heuristic approach to data placement can be viewed as the architecture for problem resolution. Like any architecture, it is not the solution to the problem in itself or even the blueprint for the solution. Instead, it provides the framework for a step-by-step approach that leads to success. The first part of this architecture focuses on tuning and data placement to maximize user access, and a series of steps (included in this chapter) that should be applied sequentially. While not all steps apply equally to every distributed data warehouse, each should be considered in light of the individual circumstances and the type of distributed system in use. The second half of the architecture addresses the other side of the data placement problem, data loading. Almost all data warehouses exist as decision-support tools; they do not directly process the company's transactions. They acquire their information from production systems and other sources, with data transfers occurring on a regular schedule. Often, there is so much data to shift that the load process runs longer than the available "batch window" at one node or other. This in turn reduces the availability of part or all of the distributed warehouse.

- 62 -

Once again, the proposed solution architecture is a series of steps that should be applied, in order, to data placement problems. Not all steps apply to every distributed warehouse, but all should be reviewed before formulating and implementing a solution to reduce load times. There are situations where improving transfer times degrade data access. With a distributed data warehouse, the designer and DBAs must constantly work to balance the needs of the warehouse with the needs of the users. The warehouse team must be sensitive to the fact that any shifts in data placement they make will create ripples throughout the entire system. Knowledge of the warehouse's purpose and objectives will guide the warehouse team, enabling them to make the best choices for the circumstances. In addition to the data placement issues caused by lengthy data transfers and access requirements, there are placement problems that arise naturally as the warehouse evolves. The typical distributed warehouse will change over time with the addition of new data marts, supplemental information, and shifting user requirements. The warehouse team must remain alert to these changes and constantly be ready to adjust the warehouse's configuration to suit the business needs. Data warehouses must be adapted to serve their user community; the user community should never have to adapt to the warehouse. No architecture or methodology can guarantee success to its practitioners—a poor implementation and inattention to detail can spoil the best architecture. However, the techniques outlined in this chapter will give the warehouse team a foundation on which they can build practical solutions to all their data placement problems. The steps should always be applied in order so that there is a progression from the basic, or simple, solutions to the more complex and expensive remedies. Remember, for any warehouse data placement problem there are multiple acceptable solutions, and the best is almost always the simplest. Most of the distributed data warehouses built today have a "heterogeneous" component—they currently have or will soon add World Wide Web interfaces, allowing them to utilize their Internet connections to acquire at least some data for the warehouse. This reliance on external data, while very convenient, has its downside; it makes these warehouses vulnerable to the issues inherent in mixed-DBMS processing. The challenge of heterogeneous processing centers on the need to provide automatic transformation of the access to data stored in a "foreign" language into requests that can be dealt with by the local DBMS. This transformation can be accomplished by using the services of the global data model, which is formed by integrating the various databases that compose the distributed warehouse into a DBMSindependent universal schema. The global data model acts as a virtual representation of the warehouse and allows manipulation of the local databases. Without a serviceable global data model, it is impossible to form a distributed data warehouse from heterogeneous systems. The global data model must contain all the information the local systems choose to make available to the distributed data warehouse. To blend the individual data models into a unified whole requires a series of data abstractions and transformations. Methods of mapping between different models must be developed so that all the information and its corresponding operators are preserved. When the mapping process is complete, the global data model should facilitate access to all the shared data using every method supported by the local DBMS. Of the various methods proposed and tried for constructing a global data model from local ones, the relational data model has proven the most durable. It has gained such wide acceptance that it is virtually the only model used for warehouse development. Most data modelers in the United States learn only relational modeling. To use the relational model as the basis for a global data model, the modeling team must first construct relational views of the local systems. These models can later be blended together easily to form the global schema. How difficult it is to build these models varies, depending on the type of DBMS used by the underlying system. There are, however, some general considerations applicable to building a relational data model regardless of the base system: Identify and categorize the data sources in the local system and establish a relational equivalent. Establish integrity constraints for the data elements.

- 63 -

Establish the relationships among the data elements and capture them in the relational model. Generally, local data items map to relational attributes, and record types map to relations. In some systems, entity relationships are not all expressed as record types—only the programmers know how the data items stored in different files relate. Care must be taken to include this specialized knowledge in the local data model. This will enable the completed global data model to play its key role in query transformation. The structure of the typical distributed data warehouse confers many advantages: almost unlimited data storage capacity, true parallel processing capabilities, and great flexibility. It also has some drawbacks: slow and/or costly data access, long data transfer times, and difficulties translating queries to process at remote locations. If the warehouse team is aware of the issues, they can resolve them as part of the development process. The major problems inherent in distributed warehousing can be resolved when the team applies good modeling techniques during the design phase and follows a data placement architecture during the development phase.

Chapter 4: Concurrency Warehouses
Overview

Control

in

Distributed

All database management systems are more or less alike. That is not to say that they are "all created equal"—they most definitely are not. However, almost forty years of experience have taught the vendors that certain basic services are absolutely required of a database management system (DBMS), regardless of its type or intended platform. One of the most important of these basic services is the ability to support multiuser access to the data (i.e., several users simultaneously reading and writing to the database). While it sounds simple, concurrent access is not easily accomplished or regulated, and the problems associated with it are well documented and researched. Better methods have been sought since the introduction of the first true DBMS in the late 1960s, and improvements in concurrent "write" access are still extensively investigated. The recent introduction of new technologies such as distributed database management software and multiprocessor hardware adds new levels of complexity to the issue, elevating concurrency control into the data processing spotlight. Distributed database management software, the engine that powers distributed warehousing, must be able to support multiuser access to data regardless of where the user or the data is located in the system. This is further complicated by the propensity, especially in warehouse implementations, toward maintaining duplicate copies of "popular" data at different sites. To ensure concurrency control, the DDBMS must not only decide which copy of the data will be accessed by a query, it must also find a way to update the other copies of that data if a change is made. Most data warehouses today are implemented using multiprocessor hardware, either SMP (symmetric multiprocessing) or MPP (massively parallel processing). This is especially true for distributed warehouses that are equipped to take maximum advantage of the beneficial price/performance ratio and the massive data handling capabilities of the server-class SMP machines. A typical distributed warehouse might be composed of two to ten multiprocessor servers, each of which houses between two and sixty-four processors. Even the classic, centralized warehouse is probably running on mainframe hardware that utilizes MPP. Sometimes, the SMP architecture is called "shared memory processing," because a group of processors operate cooperatively using a common memory pool. With a shared memory architecture, it is easy to add memory to a common pool, enhancing the flexibility of the architecture. SMP systems are comparatively inexpensive and ideally suited to situations such as distributed warehouses where initial requirements are modest but the developer knows that the environment will eventually need to be scaled up. SMP will be there, ready to accept more processors and/or memory when the need arises. MPP technology is usually encountered in larger, mainframeclass hardware, and many centralized warehouses are beginning to take advantage of its features. Frequently referred to as "share nothing systems," MPP systems have large numbers of processors working in parallel, each with their own system resources. While they lack the easy scalability of SMP, MPP systems are generally faster than their smaller-scale rivals. With the new architectural enhancement known as NUMA (Non Uniform

- 64 -

Memory Access), MPP systems, too, will be able to share common memory while they maintain their low latency. All the major relational vendors are attempting to improve the performance of their products by adding parallel query capabilities that spread the work of a single query across several processors. This will reduce processing times, especially for large bulk warehouse operations that scan great quantities of data. Example of this might be report generation, data maintenance, and data mining operations. Concurrency control, in either of these multiprocessor environments, becomes a very major concern. It is no longer just an issue of proper scheduling that will be handled by the DBMS. The warehousing team—architects, designers, database administrators (DBAs), and programmers—must all be aware of the potential problems and work together to ensure proper warehouse function.

Transactions
Of fundamental importance to concurrency control is the notion of a transaction. A transaction can be defined as the basic unit of work in a DBMS; it is a set of actions that must be carried out together. For example, the transfer of funds from a money market account into a personal checking account could be considered as a transaction; it is not complete until the money is debited from the money market account and deposited into the checking account. It is the DBMS's job to control concurrently executing transactions so they do not get in each other's way. The part of the database management system that performs that task is the scheduler. Serializability is the most common means of proving the correctness of these schedules. Communications between the scheduler and the program (or query) issuing the transaction are handled by the transaction manager, which coordinates database operations on behalf of the applications. Transactions transform a database from one consistent state to another consistent state, although consistency may be violated during execution. As in the funds transfer example, the database is in an inconsistent state during the period between debiting the one account and crediting the other. If a system failure were to occur during this time, then the database would be damaged or inconsistent. It is the responsibility of the DBMS's recovery manager to ensure that all transactions active at the time of failure are rolled back or undone. The effect of a rollback operation is to restore the database to the state it was in prior to the start of the transaction, when there was consistency. The four basic properties, sometimes called the ACID test, common to all transactions are: 1. Atomicity. The "all or nothing" property; a transaction is an indivisible unit. 2. Consistency. Transactions change the database from one consistent state to another consistent state. 3. Independence. Transactions execute independently of another (i.e., the partial effects of incomplete transactions are not visible to other transactions). 4. Durability. Also called persistence; the effects of a successfully completed transaction are permanently recorded in the database and cannot be undone. An application program running in a database environment can be viewed as a series of transactions with nondatabase processing taking place in between transactions (see Figure 4-1). The transaction manager oversees the execution of the transactions and coordinates database requests on their behalf. The scheduler, on the other hand, implements a particular strategy for transaction execution. It tries to maximize concurrency without allowing simultaneously executing transactions to interfere with one another and, in doing so, compromise the consistency or integrity of the database. The transaction manager and scheduler are clearly very closely associated and their functions interrelate. How a transaction manager responds to a database request from an application will depend on the scheduler being used.

- 65 -

Figure 4-1: DBMS transactions in an application program. Transactions issued by concurrent users/programs can be interleaved in only two ways: Either they can execute in turn, end-to-end, in which case only one transaction is active at a time (Figure 4-2), or they can execute concurrently (Figure 4-3). The start of a transaction is signaled by a begin transaction command made explicitly or implicitly by the user/program. The transaction concludes with either a commit command to the DBMS, indicating a successful conclusion, or an abort command, indicating an unsuccessful termination. This abnormal termination (abort command) can be brought about by the transaction itself if it is unable to perform the required action (e.g., $2,500 cannot be withdrawn from the money market account and transferred to checking because there is only $2,200 available in the money market account). Or it can be the result of the execution of the concurrency control algorithm.

Figure 4-2: Serialized transactions.

- 66 -

Figure 4-3: Concurrent transactions. In an environment controlled by a distributed database management system, like a distributed warehouse, a transaction may access data stored at one or more sites. Each transaction must be divided into subtransactions, one for each storage site it will access. These subtransactions are represented by agents at the various sites. A common way of indicating a subtransaction, and the example that will be used in this book, is as follows: The agent of transaction T1 at site A will be referred to as T1A. The agent of transaction T2 at site A will be referred to as T2A. A distributed version of the funds transfer example used earlier can be created by assuming that the money market accounts are stored at site A, and the personal checking accounts are stored at site B. The customer wants to transfer $2,500 from a money market account (X) to a checking account (Y). While this is still a transaction (T1) and is, by definition, an indivisible unit of work, it must be broken down into two subtransactions, or agents, that will be treated as indivisible transactions by their respective processing systems. In this example, the processing flow for transaction T1 would be as follows. EXAMPLE: TRANSACTION T1 (FUNDS TRANSFER) begin transaction T1 begin transaction T1A read balanceX balanceX = balanceX − 2500 if balanceX < 0 then begin print 'insufficient funds' abort T1A end end - if write balancex commit T1A begin transaction T1B read balanceY balanceY = balanceY + 2500 write balanceY

- 67 -

commit T1B commit T1 The subtransactions of the global transaction T1 must not only be synchronized with the other purely local transactions processing concurrently at their site, but also with other global transactions active in the distributed system at the same or other sites. Distribution adds a new dimension to the complexity of concurrency control.

Interference between Concurrent Transactions
There are many ways concurrently executing transactions can interfere with one another and compromise the integrity and consistency of the database. Three major examples of such interference are: Lost updates Violation of integrity constraints Inconsistent retrieval These problems apply equally to centralized DBMSs and distributed systems. The first of these problems, the lost update, occurs when an apparently successfully completed update by one user/program is overridden by the processing of another. For example, transaction T1 (from the funds transfer problem discussed previously) is processing concurrently in a centralized DBMS with another transaction, T2, that is depositing $1,000 to the money market account (accountX) of the customer. Suppose that at the start of the two transactions, the balance in the money market account is $3,000. The increase of $1,000 by transaction T2 could be overwritten by the decrement to $500 by T1, "losing" $1,000 as shown in this example. EXAMPLE: LOST UPDATE CAUSED BY TWO CONCURRENTLY PROCESSING TRANSACTIONS Value of BalanceX $3000 begin transaction T2 begin transaction T1 read balanceX balanceX = balanceX − 2500 if balanceX < 0 then begin print 'insufficient funds' abort T1 end write balanceX read balanceY balanceY = balanceY + 2500 write balanceY commit T1 The $1,000 deposit to the money market account is lost because of the conflict between two concurrently processing transactions. A different type of problem can also arise when two transactions are allowed to execute concurrently without being synchronized, which results in a violation of the integrity constraints governing the $ 500 $3000 $3000 read balanceX balanceX=balanceX+1000 write balanceX commit T2

- 68 -

database. This can be easily understood by looking at another example. Using the brokerage business example (see Chapter 3), a stockbroker manages his clients' accounts by trading stocks in line with their overall preferences. He does not consult each client for every transaction; the stockbroker has a list of their instructions that he uses as a guide for purchases and sales on their behalf. This list is kept in the Client_Preference Table. The stockbroker is aware of a technology IPO coming to market and decides to purchase 1,000 shares on behalf of his client, John D. Rockefeller, because Mr. Rockefeller has some spare funds in his money market account. The broker first checks his client preference list and, finding no objection to technological businesses, makes the purchase. Unknown to the broker, Mr. Rockefeller phones the brokerage that same morning and, unable to speak directly with him, leaves a message with his assistant stating that he does not want to invest any more money in high-tech companies. The assistant makes the change to the table. Unfortunately, the two transactions process at the same time with the following results. EXAMPLE: VIOLATION OF INTEGRITY CAUSED BY TWO NONSYNCHRONIZED TRANSACTIONS EXECUTING CONCURRENTLY begin transaction T1 read stock_type, preference, company_name From Client Preference where Client_Name = 'John D. Rockefeller' if not found abort T1 read balance From Client Balance where Client_Name = 'John D. Rockefeller' balance = balance − 50,000 if balance < 0 then begin print 'insufficient funds' abort T1 end end - if write balance update Client_Stocks where Client_Name = 'John D. Rockefeller' stock = SSY purchase_price = 50 quantity = 1000 commit T1 Note that neither transaction is aware of the actions of the other as they update different data. The broker buys the IPO, thinking he is acting in accordance with his client's wishes when in fact, he is making the sort of investment his client definitely does not want. This may appear to be a trivial begin transaction T2 update Client Preference where Client_Name = 'John D. Rockefeller' stock_type = 'hi tec' preference = 'no' company_name = 'any' commit T2

- 69 -

example, but for the stockbroker it can be very important. Should the IPO decrease in value and the broker sell it at a loss, the client would be able to charge him with mismanagement because he acted contrary to the client's instructions. This could result in the loss of the broker's license. The two examples just discussed—the lost update and the violation of integrity—both centered on transactions that are updating a database. Most concurrency control concentrates on update because transactional interference can corrupt the data. However, transactions that are only reading the database can obtain inaccurate results if they are allowed to read partial results of an incomplete transaction that is simultaneously updating the database. This is often referred to as a "dirty read" or an "unrepeatable read." The simplest example of this sort of problem is a program that summarizes data in a database (e.g., totaling credits or debits). The program will obtain inaccurate results if it is allowed to process with a concurrent program that updates one or more of the fields included in the total. All of the examples given so far pose a serious threat to data integrity and lead to incomplete or inaccurate answers to questions posed by the user/program. Obviously, a way must be found to prevent these types of interference. To this end, all DBMSs have a scheduler whose job it is to avoid concurrency control problems.

Schedules and Serialization
Almost all DBMSs use a concurrency control algorithm whose purpose is to determine the best transaction schedule for avoiding interference. Certainly, if the DBMS allowed only one transaction to process at a time there would be no problems and no need for a scheduler or algorithm. One transaction would have to commit before another could begin. However, to enhance efficiency, DBMSs aim to maximize concurrency or parallelism in the system. They allow transactions that can do so safely to process in parallel. For example, transactions that access completely different portions of the database can execute together. Finding ways to maximize concurrent usage has been the objective of much research in the database community for the past thirty years. A transaction consists of a series of reads and writes to the database. The entire sequence of reads and writes of all concurrent transactions taken together is known as a schedule. A schedule S is generally written as follows: where O1 indicates a read (R) or a write (W) operation. O1 precedes O2, which precedes On, and so on. This ordering is denoted as: Thus, the schedule S for transactions T1 and T2 in the lost update example presented previously would be: where R1 and W1 denote read and write operations by transaction T1. Note that most concurrency control algorithms assume that a transaction reads data before it updates it (i.e., the constrained write rule). The order of interleaving operations from different transactions is crucial to maintaining the consistency of the database. If the order of execution of the operators above were different: then the outcome would be quite different. There would not have been the problem of transaction T1 overwriting the update performed by transaction T2. The objective of the concurrency control algorithm is to generate schedules that are correct (i.e., to leave the database in a consistent state). Of particular interest are serial and serializable schedules. A serial schedule is one in which all reads and writes of each transaction are grouped together so the transactions run sequentially, one after another (Figure 4-4). A schedule S is said to be serializable if all the reads and writes can be reordered in such a way that they are grouped together as in a serial schedule, and the net effect of executing the reordered schedule is the same as that of the original schedule S. A serializable schedule will therefore be equivalent to and have the same effect as some serial schedule.

- 70 -

Figure 4-4: A "serial" schedule. It should be noted, however, that a serializable schedule is not the same as a serial schedule. The objective of the concurrency control algorithm is to produce correct schedules so that the transactions are scheduled in such a way that they transform the database from one consistent state to another and do not interfere with each other. Serializability is taken as proof of correctness. If a concurrency control algorithm generates serializable schedules, then those schedules are guaranteed to be correct. Deciding whether a schedule is equivalent to some serial schedule is difficult. If the constrained write rule is applied, then the algorithm to determine serializability of a schedule is of polynomial complexity. Intuitively, it can be said that two schedules, S1 and S2, are equivalent if their effect on the database is the same. Thus, each read operation on a data item in both of the schedules sees the same value, and the final write operation will be the same in both schedules. More formally, the rules for equivalency of schedules can be stated as follows: Each read operation reads the same values in both schedules (for this to be true, the write operations must be equivalent). The final database state is the same for both schedules. Read operations cannot conflict with one another, and the order of their execution makes no difference in a schedule. However, the execution order of read and write commands do matter, as do the order of write commands. In terms of schedule equivalence, it is the ordering of conflicting operations that must be the same in both schedules. The conflict between a read and a write operation is called a read-write conflict, and the one between two write commands is a write-write conflict. The basic concepts of serializability are the same for both distributed and centralized systems, but with the added complexity imposed by distribution. For a simple example, imagine that two global transactions, T1 and T2, both process at two sites, A and B. Suppose that the transactions process serially with T1A preceding T2A at site A, and T2B preceding T1B at site B. The processing flow would be as shown in the following example. EXAMPLE: SERIALIZABILITY Site A begin transaction T1A read x write x commit T1A begin transaction T2A read x write x Site B begin transaction T2B read y write y commit T2B begin transaction T1B read y write y

commit T1B commit T2A The schedules, SA and SB, would be: Although their agents process serially at each site, the global transactions are not serializable. It is easy to envision how such a circumstance could arrive; if the global transactions T1 and T2 were launched simultaneously by different users, one at site A and the other at site B, then the schedulers operating

- 71 -

independently at each site could schedule them this way. So, for distributed transactions, it is necessary to have serializability of all local schedules (both purely local and the local agents of global transactions) and global serializability for all global transactions. Effectively, this means that all subtransactions of global transactions appear in the same order in the equivalent serial schedule at all sites. For the global transactions, TX and TY, processing at sites A through N where they have agents, this can be expressed as follows: if TXA < TYA then TXN < TYN (local ordering at site N) and T1 < T2 (global ordering) In a centralized DBMS, there is a single scheduler that is responsible for synchronizing transactions and for ensuring that only serializable schedules are generated. In a system using a single system distributed management tool, there is a global scheduler and transaction manager, and there may also be local schedulers and transaction managers, depending on the product used. Scheduling in a single DDBMS is very similar to that in the normal DBMS because of the centralized global scheduling facility. In a heterogeneous or homogeneous distributed system, however, the DBMSs' schedulers and transaction managers themselves are distributed across the nodes in the network (Figure 4-5). At each node, there must also be a "global" scheduler and transaction manager to act as coordinator for those transactions initiated locally that need to process at more than one site. The alternative would be to appoint one site as coordinator for all global transactions. While this would simplify many of the problems associated with concurrency control in a distributed environment, it is a somewhat unrealistic approach because the site soon becomes a bottleneck. If it fails, becomes overloaded, or is disconnected from the network for any reason, no global transaction could process anywhere in the system.

Figure 4-5: Distributed DBMS schedulers and transaction managers. With distributed transaction control, the following sequence of steps is required to process a global transaction: 1. A global transaction is initiated at site A by the global transaction manager. 2. Using information about the location of data from the global data model/metadata, the global transaction manager divides the transaction into a series of agents at each relevant site. 3. The global communications manager at site A sends these agents to the appropriate sites. 4. Once all agents have completed processing, the results are communicated back to site A by the global communications managers. Notice that the agents do not communicate directly with each other; they communicate through the coordinator.

- 72 -

This scenario ignores many issues, especially query decomposition and optimization. Each agent, from the point of view of the local transaction manager at its processing site, is itself an atomic transaction. It will terminate with either a commit or an abort operation. Still, the global transaction is also atomic, and its needs take precedence over the agents. When an agent is complete, it goes into a wait state and sends a message to the coordinating global transaction manager, indicating that it is ready to either commit or abort. The coordinating global transaction manager analyzes the responses from all the agents and then decides whether to commit or roll back the global transaction. It is only after that decision is made that the processing of the agents can be completed. The decision is broadcast to all agents, which in turn perform the processing as directed and notify the requesting site. The global transaction manager can pursue a number of different decision strategies. The simplest is unanimity; if all agents return a commit, then the global transaction is committed. If all agents return an abort, then the global transaction and all agents are rolled back. However, if the agent responses are mixed, the decision is not as simple. The safest approach, of course, would be to abort all in this circumstance. If the database is highly replicated, a consensus approach might be taken, where the global transaction is committed if the majority of the agents "vote" to commit (and the unsuccessful agents abort). This technique must be applied with care and an understanding of how the result set may be affected.

Concurrency Control Techniques
There are three primary concurrency control techniques designed to allow transactions to execute safely in parallel: Locking methods Timestamp methods Optimistic methods These methods were developed for centralized DBMSs and then extended to include the distributed environment. Both locking and timestamping are considered conservative approaches because they cause transactions to be delayed in case they might conflict with other transactions at some time in the future. Optimistic methods are based on the premise that conflict is rare, so they allow transactions to proceed unsynchronized, only checking for conflicts when a transaction commits.

Locking Protocols
Of the three, locking is the most frequently used method for concurrency control. There are several variations, but all share the fundamental premise that a transaction must claim a read (shared) or write (exclusive) lock on a data item before performing the operation. Since read operations cannot conflict, it is permissible for two or more transactions to hold a read lock simultaneously on a data item. A write lock gives a transaction exclusive access to the locked item. As long as a transaction holds a write lock on a data item, no other transaction can read that item or write to it. A transaction holds a lock until it explicitly releases it, and it is only then that the result of the write operation are made available to other transactions. Some systems allow upgrading and downgrading of locks. So, if a transaction holds a read lock on a data item, then it can upgrade that lock to write if no other transaction holds any lock on that item. In effect, this allows a transaction to "inspect" an item before deciding what action it will take. In systems where upgrading is not supported, a transaction must seek and hold write locks on any data items it might update during the course of its processing. This has the effect of reducing the concurrency in the system. With downgrading, a transaction may downgrade a lock from write to read after it has updated a data item, increasing the concurrency in the system. The size or granularity of the data item that can be locked in a single operation will greatly affect performance of the concurrency control algorithm. Consider the instance where a transaction needs to update one data element in a table or a field in a single record of a database. At one extreme, the concurrency control application might allow the transaction to take a write lock on just the data element or field in question. On the other hand, it might force the transaction to lock the entire table or database. In the latter case, no other transaction could process against the table or database until the first transaction was complete, which would clearly be undesirable. However, if a transaction were updating 90 percent of the data in a table, it would be better for it to lock the whole table instead of locking each

- 73 -

individual data element, one at a time. Ideally, the DDBMS will support a range of granularity, with at least row-, page-, and table-level locking. Many systems will automatically upgrade locks, from page to table, when a certain percentage of data is being updated. The most common locking protocol is known as two-phase locking (2PL). It is so-named because transactions using it go through two distinct phases during execution—a growing phase, when it acquires locks, and a shrinking phase, while it releases those locks. The rules for transactions that practice two-phase locking are: The transaction is well formed (i.e., it acquires a lock on an object before operating on it, and it releases all locks when processing is complete). Compatibility rules for locking are observed (i.e., no conflicting locks are held). Once a transaction has released a lock, no new locks are acquired. All write locks are released together when the transaction commits. The last rule ensures transaction atomicity; if this rule is not enforced, other transactions would be able to "see" uncommitted results. However, upgrading and downgrading of locks is allowed under 2PL, with the restriction that downgrading is only permitted during the shrinking phase. The main problem with 2PL in a distributed environment is the enormous message overhead incurred. Consider, for example, a global transaction that processes at N sites. The successful execution of this transaction would generate at least 4N messages: N "begin transaction" messages from coordinator to agents (for 5 sites, there are 5 agents and 5 messages) N "ready to commit" subtransaction messages to the coordinator (for 5 sites, there are 5 subtransactions and 5 messages) N "global commit" transaction messages to agents (for 5 sites, there are 5 messages) N local subtransaction commit successful messages to the coordinator (for 5 sites, there are 5 messages) Actually, there will probably be more than 4N messages because most messages require an acknowledgment of receipt (doubling the number of messages). When there are many global transactions involving numerous sites, the overhead becomes unacceptable. Fortunately, this does not happen often. One of the primary reasons for using a distributed system is to store the most frequently accessed data "locally," so the majority of transactions can be processed at their place of origin. This makes implementing distributed 2PL a realistic proposition. Also, the messages can be sent from the controller to the remote sites in "parallel," highlighting the importance of careful data distribution. This brings up another aspect of data distribution—data replication across the nodes of a distributed system. If a data item is updated, then it is highly desirable that all copies of it be updated simultaneously. One way to accomplish this is to have the update of a replicated item spawn a global transaction that updates all duplicate copies. When using a locking protocol such as 2PL, each local transaction manager at a replicant site must acquire a write lock on the copy of the data item and hold it until all copies have been updated. This method ensures data consistency across the distributed environment. Another, simpler approach to this problem is to designate one copy of the data as the primary copy and all duplicates as slave copies. Updating transactions must access the primary copy and make their changes there, locking only that copy of the data for the duration of the update. Once the transaction commits and the lock is released, the update can be propagated to the slave copies. This should occur as soon as possible to keep other transactions from reading out-of-date data, but it need not be a global transaction. With this approach, only the primary copy is guaranteed to be accurate, and transactions requiring current data must read it rather than one of the slave copies. There are times when more than one transaction needs to lock the same data item. A transaction (T1) can be viewed as a series of read and write operations as it progresses through its processing, and it acquires various locks along the way. If it obeys two-phase locking protocols, it will hold all write locks until it commits. However, if another transaction (T2) requests a write lock on one of the same data items, one of two possibilities can occur: T2 is placed on a queue, waiting for T1 to release its lock.

- 74 -

T2 is aborted and rolled back. With the first option, T2 retains any other locks it might hold and enters a wait state. In the case of the second option, T2 must release all of its locks and restart. For a complex transaction, especially a global one in a distributed system, the overhead of having to restart would be very high (the rollback of the agent at one site would trigger the rollback of all agents at all sites). This approach is called a deadlock prevention protocol. By allowing T2 to retain its locks in the first approach, a potentially dangerous situation that could lead to deadlock is created. Deadlock occurs when multiple transactions are queued up waiting for a lock to be released. Where there is a possibility of this happening, a deadlock detection protocol is required that will be invoked periodically to check for deadlocks. It is also possible, using locking protocols, for a transaction to be either repeatedly rolled back or left in a wait state indefinitely, unable to acquire locks, even though the system in not deadlocked. This situation is called, appropriately enough, a livelock, since the transaction is blocked while the rest of the transactions are "live" and continue normal operations. For example, consider a transaction processing in a brokerage data warehouse, whose purpose is to determine how many shares of IBM common stock are owned by the firm's clients. At the same time this transaction is counting shares, the normal trading operations of the company are proceeding, with shares of stock, some of it IBM's, being bought and sold all the time. The "counting" transaction will need to acquire locks on all client stock holdings in order to achieve a consistent view of the table, but may have trouble doing so in light of all the other processing. If it is unable to acquire a full lockset in a reasonable period of time, it is considered to be livelocked. To avoid this, most schedulers employ a priority system where the longer a transaction has to wait the higher its priority becomes. Eventually, the "counting" transaction has the highest priority in the system and can take the necessary locks. A deadlock condition in a centralized DBMS is normally detected by means of a wait-for graph. In a wait-for graph, transactions (or their agents) are represented by nodes and blocked requests for locks are represented by edges. Figure 4-6 shows a wait-for graph in which there is a deadlock between T1 and T2. T1 is waiting for a lock on data item XA, which is currently held by T2. The wait-for graph (G) can also be represented symbolically as follows:

It is possible for a deadlock to occur between two transactions indirectly through a chain of intermediate transactions. For example:

Using wait-for graphs, detection of deadlocks in a centralized system is straightforward; if the graph forms a circular shape, there is a deadlock. Deadlock must be resolved by preempting or aborting one of the transactions (aborting any transaction will break the circle). However, the transaction making the request that creates the deadlock is the one normally chosen for rollback.

Figure 4-6: Deadlock situation on a wait-for graph. Deadlock detection in a distributed system is more complex since the chain of transactions can involve a number of different sites (Figure 4-7). Additional arcs are inserted into the wait-for graph to account for

- 75 -

the agent of a global transaction waiting for the completion of another agent of the same global transaction at a different site. A symbolic representation of Figure 4-7 would be as follows:

Global deadlocks cannot be detected using only local wait-for graphs. There are three approaches to detecting a global deadlock situation in a DDBMS: Centralized Hierarchical Distributed

Figure 4-7: Deadlock situation in a distributed system. With centralized deadlock detection, all the local wait-for graphs are merged into one, called the deadlock detection site, and examined for cyclic structures. The communications overhead for constructing a global wait-for graph is potentially very high if there are many sites and many agents involved. As with any centralized solution to a distributed problem, the deadlock detection site can become a bottleneck and, if it should go offline for any reason, there needs to be a backup site to ensure continued processing. When hierarchical deadlock detection is employed, the sites in the network must be organized into a hierarchy so that a blocked site sends its local wait-for graph to the site above it in the hierarchy. Figure 4-8 shows a proposed hierarchy for an eight-site, A through H, network. The bottom of the figure shows the sites themselves, where local wait-for graphs are created. The next level up shows where deadlock detection is accomplished for each pair of sites (it is located at one of the two named nodes). The third level performs deadlock detection for half of the network at each site and the final, or root level, is essentially a global deadlock detection site. Hierarchical deadlock detection reduces communications costs compared with centralized detection, but it is difficult to implement, especially in the face of site or communications failures.

Figure 4-8: Hierarchical deadlock detection.

- 76 -

One of the most well known distributed deadlock detection methods was developed by Obermarck and was popularized by IBM Corp. in its much-discussed but never released product R*. This method highlights one of the problems associated with global deadlock detection—how to prove that all actual deadlocks will be detected and that no false deadlocks are accidentally included. Ron Obermarck's method may include some false deadlocks because it assumes that a snapshot of the system's state is captured in the wait-for graphs. However, if transactions are allowed to abort spontaneously (for reasons other than concurrency control), then false deadlocks are inevitable. With Obermarck's method of distributed deadlock detection, an additional node, labeled EXT, is added to a local wait-for graph to indicate an agent at a remote site. When a transaction spawns an agent at a remote site, an EXT is added to both the local and remote site graphs. The global wait-for graph in Figure 4-7 would be built on the local wait-for graphs shown in Figure 4-9. These graphs, respectively, can be depicted symbolically as follows:

This does not necessarily imply that there is a deadlock since the EXT nodes could represent completely disjoint agents, but cyclic forms must appear in the graph if there is a genuine deadlock. To decide if there is a real deadlock, the local graphs must be merged and evaluated. The result will be the same graph as in Figure 4-7, and the cycle indicating a genuine deadlock will emerge: In general, when the EXT extension appears in the local wait-for graph of site X, that graph should be sent to site Y, the node it is waiting for, where the two local graphs can be combined and evaluated. If no cyclic form is detected, processing can continue. The process stops when a cycle is found, and one of the transactions is rolled back and restarted. Obermarck's method has become one of the most popular approaches to concurrency control for distributed database systems.

Figure 4-9: Obermarck's distributed deadlock detection wait-for graph. However, even with the use of the external agent, deadlock detection in a distributed system is potentially very costly. In a distributed system, it would be far too expensive to take the standard centralized approach of checking for deadlocks every time a transaction has a lock request refused. Yet it is difficult to know just when to check. If it is known that there is a lot of contention in a distributed system, it might be worthwhile checking for deadlock every time an external agent appears in a cycle in a wait-for graph at one of the local systems. Another possible approach is to use a timeout mechanism where deadlock detection is initiated only after a local system has "hung" for a selected period of time. However, distributed systems are prone to all kinds of delays, especially in communications, that have nothing to do with deadlocks, so this method is far from foolproof. Timeouts in a distributed system are just not as reliable an indicator of deadlocks as they are in centralized systems. Concurrency control algorithms in which the possibility (not necessarily the reality) of deadlocks is detected and avoided are known as deadlock prevention protocols. How, then, does the transaction manager decide to allow a transaction, T1, which has requested a lock on data item X (currently locked by transaction T2), to wait and guarantee that this waiting will not give rise to a deadlock? One way is to order the data by forcing locks to be acquired in a certain data-dependent order. This method, however, is almost impossible to define, since users tend to access the database through views that can be

- 77 -

defined across any subset of the database. A more realistic approach is to order the transactions and ensure that all conflicting operations are performed in sequence, according to the defined order. Deadlock is thus prevented because transactions are only allowed to wait under circumstances that will maintain the ordering. The ordering scheme is generally based on timestamps. There are two possible ways to order the transactions using this method: Wait-die (where the older transactions wait for the younger ones to either commit or roll back) Wound-wait (where the younger transactions wait for the older ones to either commit or roll back) Using either of these schemes, an aborted and restarted transaction must retain its original timestamp. If it does not, it will be repeatedly rolled back. Effectively, the timestamp mechanism supports a priority system where older transactions have a higher priority than younger transactions, or vice versa. Because this methodology uses locks as its primary concurrency control mechanism, it is classified as lock-based rather than timestamp-based.

Timestamping
Actual timestamp methods of concurrency control are quite different from locking methods. Since there are no locks involved there can be no deadlocking. Locking mechanisms usually involve transactions waiting if they make conflicting requests. With timestamping, there is no waiting; transactions involved in conflict are rolled back and restarted. The basic goal of the timestamping methodologies is to order transactions globally so that older transactions (those with smaller timestamps) get priority in the event there is a conflict. If a transaction attempts to read or write an item, it will only be allowed to do so if the last update of that data item was performed by an older transaction. Otherwise, the transaction is aborted and restarted, acquiring a new timestamp. Unlike the locking methodology discussed previously, a restarted transaction must be assigned a new timestamp each time it starts. If it is not, a situation could arise where the transaction would never be able to process because the data items it is trying to lock will have been updated by younger transactions. It should be noted that timestamping methods produce serializable schedules. Timestamps are used to order transactions with respect to each other, so they must have unique timestamps. In a centralized system, this can be accomplished easily using the system clock to stamp the transaction at initiation. Alternatively, a counter can be used, operating like a take-a-number queuing mechanism. When a transaction is launched, it is assigned the next number in sequence by the counter. To avoid very large timestamps, the counter can be periodically reset to zero. In a distributed system, there is generally no global system clock and no universal counter. Transactions are launched from the individual nodes, each of which has its own clock; there is no guarantee that the clocks are even synchronized. In a centralized system, events do not occur simultaneously, especially with the transaction manager that can launch only one transaction at a time. In contrast, in a distributed system, two or more transactions can start at the same time at different sites in the network. Yet there must be some way to order these simultaneous events with respect to one another. One possible way is to define "global time" in the network. Most often, this is done with a concatenation of the local time and system identifier: site clock, site identifier "Clock" need not mean the system clock; a counter or other timekeeping mechanism can be used in its place. For the purposes of this discussion, all counting/timekeeping devices will be referred to as "clock" and the value of the counter will be called the "timestamp." Two rules are sufficient to ensure ordering of events both with respect to other local events and to events occurring at other sites: The local event clock advances at least one unit for every event occurring at that site (event means a transaction and/or message).

- 78 -

Intersite messages are timestamped by the sender and the receiver adjusts his clock to an equal to or greater value than that of the sender's upon receipt of the message. The first rule ensures that if e1 occurs before e2 at a given location, then the timestamp of e1 will be smaller than the timestamp of e2. The second rule maintains some consistency among system clocks. Just how does timestamping work to ensure concurrency control? With lock-based protocols, all updates are write-locked until the transaction finishes processing and commits, protecting other transactions from "seeing" partially updated data. With timestamping, there are no locks, so this protection is gone. A different approach must be found in order to hide partially updated data. This can be done by using prewrites (i.e., deferred updates), where updates of uncommitted transactions are not written to the database. Instead, they are written to a buffer and only flushed out to the database when the transaction commits. This has the advantage that when a transaction is aborted and restarted, no physical changes need to be made to the database. The process of using deferred updates is not quite as straightforward as it seems. It is possible that there could be a number of prewrites pending in the buffer for a given data item, and serializability demands that the corresponding writes take place in timestamp order. Therefore, when a transaction, T1, attempts a write operation on data item X at commit, it must first check to make sure that no other writes by an older transaction are pending in the buffer. If any such writes are found, then T1 must wait until the other transaction has committed or restarted. Similarly, in the case of T1 reading data item X, the system must check not only that the last update of X was by a transaction older than T1, but also that there are no writes pending in the buffer by an older transaction. The main problem with basic timestamping is that the absence of "locking delays" is paid for by costly restarts whenever conflicts are detected. A modification to basic timestamping, known as conservative timestamping, substantially reduces the degree of concurrency but eliminates the need for restarts. With this approach, transactions wait until the system knows it cannot receive a conflicting request from an older transaction. To apply conservative timestamping, each site in the distributed system must maintain several pairs of queues—one read queue and one write queue for every node. Each read queue contains requests originating at remote sites to read local data (with all read requests from the same remote system waiting on the same queue), while each write queue maintains the same information for write requests. Individual read and write requests are labeled with the timestamp of the issuing global transaction and are kept on the queue in timestamp order (the oldest is at the head of the line). For this method to work, the following rules must be observed: All sites agree to commit transactions in timestamp order. Transactions do not spawn agents, just issue remote read and write requests. Read and write requests from a remote site must arrive in timestamp order (transactions must complete all reads before issuing any writes). If necessary, younger transactions at the remote site will wait for an older transaction (at the same site) to finish issuing requests before beginning. All queues are nonempty, because if one queue is empty (queue for site N) and an update request is received from a remote system (site M), there is no guarantee that another request could be received from a different remote system (site N) at a future time, which was issued earlier than the request now processing (from site M). Conservative timestamping depends on there being a lot of intersystem traffic; otherwise, there could be an empty update queue at a site, which would cause it to "hang." A simple solution to this problem would be for each of the sites in a distributed system to periodically send timestamped null update requests to one another in the absence of genuine requests. These null requests would ensure the other sites that the sender does not intend to issue any requests with an older timestamp than the null request. Conversely, a blocked site could issue a request for a null update to fill its empty queue. While it does not allow the same high degree of concurrency as the lock-based methods, conservative timestamping does have the advantage of cost-effective function in a distributed systems with high volumes of intersystem traffic. The case can be made that given a very busy, high-contention network, it is by far the more practical choice for maintenance-free operation.

Optimistic Methods
- 79 -

On the other end of the spectrum of distributed systems are those with low-contention or where readonly transactions predominate. For these systems, optimistic concurrency control techniques seem attractive. Optimistic methods are based on the idea that conflict is rare and the best approach is to allow transactions to proceed unimpeded without any waiting. Only when a transaction wants to commit should the system check for conflicts and, if one is detected, force the transaction to restart. To ensure atomicity of the transaction, all updates are made to transaction-local copies of the data and are propagated to the database at commit, after no conflicts are detected. Problem resolution by restart is a very expensive method in a distributed system and is only practical if it occurs infrequently. With the optimistic method, transactions execute in a three-phase manner: Read Phase. This is the body of the transaction up to commit (any updates are written to transaction-local copies of the data). Validation Phase. The results of the transaction are examined for conflicts; if conflicts are detected, then the transaction rolls back. Write Phase. If the transaction is validated, then the updates are propagated from the local copy of the data to the database. The read phase is straightforward, except that the "writes" are internal to the transaction. The validation proceeds in two phases. First, in the local phase, all subtransactions are validated locally (the local schedule is serializable). If local validation fails at any site, then the global transaction is aborted. If all local schedules pass inspection, then the second phase, the global phase, continues. The global phase ensures that the same serialization is used at all sites. This can be accomplished by seeing that all agents of global transactions that precede the transaction being validated have completed processing. If any have not, then the transaction must wait until they do (or until it times out and is aborted). A timeout function is necessary because this waiting would otherwise cause deadlocks. After validation, the write phase can proceed using two-phase commit. As attractive as it seems, no major distributed database product has chosen, as of this writing, to use optimistic concurrency control (IMS Fast Path is one of the very few centralized systems that employs a form of this approach). Studies have shown that unless contention is extremely low other concurrency control strategies outperform optimistic methods by a wide margin. This is largely due to the enormous overhead (especially for distributed systems) of restarting transactions once they have reached commit.

Special Case Concurrency Control
Distributed database management systems differ from their centralized forebears in many ways. One of the most common differences is in data replication. Where centralized systems often house multiple copies of the same data due to poor application/database design, they almost never keep that data under the same name in a duplicate structure. Distributed systems often store several exact copies of the same data for reasons of performance and/or fault tolerance. Performance is improved by locating data at all sites where it is needed, rather than having to access it remotely. Fault tolerance is provided by replicated tables (or databases) since a copy of the data remains available in case of local site or network failure. Where a DDBMS supports data replication, the system must ensure that all copies of the data are consistent. This can be done by treating the update of replicated data as a global transaction, which can be handled by the scheduler in the normal fashion. Such an approach requires that all sites with duplicates of the data in question be operational and connected to the network. In the event of a network or site failure, it is not possible to update the duplicate data at all. This approach runs somewhat counter to the fault-tolerant purposes of a distributed network. It is therefore common to adopt a consensus approach to updating replicate data. When the global scheduler instructs all sites to commit the update, it takes a poll. If the majority of the sites are available, then the update is committed. Those sites not participating (because of site or network failure) are simply notified of the update when they come back online. Some particular problems with replicated data can arise due to network partitioning and communications failures. DDBMSs depend on the ability of all sites in the network to be able to communicate reliably with one another. Most networks today are very reliable because the protocols used guarantee correct transmission of messages in the correct order. In the event of line failure, many networks support automatic rerouting of messages. However, communications failures still occur. Such a failure can result in the network being partitioned into two or more subnetworks. When this happens, sites within a partition can still communicate with each other, but not with sites in other partitions (see Figure 4-10).

- 80 -

Figure 4-10: Failure-induced subnetworks. If replicated data was updated during such an outage, the duplicates in the one partition, subnetwork X, would be altered to match, but not those in the other, subnetwork Y. It would be possible for a second transaction to also process during this time and update the same replicated data items in subnetwork Y. The two network partitions now have different values for replicated data items. These two transactions executed totally independent of one another, and because of the network failure, no communications between sites was possible. The versions of the replicated data have diverged, resulting in a consistency problem. There are various methods for resolving inconsistencies when the partitions are reunited. They fall into two groups: those that adopt an optimistic approach and those that take a pessimistic approach. Optimistic approaches are based on the premise that conflicts are rare. The emphasis is placed on availability over consistency. In the event of a network failure, updates are allowed to proceed independently in the resulting partitions. Very complex strategies are required for detecting and resolving inconsistencies when the partitions are reunited—strategies that can be so complex, in fact, that no one to date has chosen to employ this method in a distributed system. Pessimistic methods for supporting update of replicated data during network partitioning adopt a very conservative approach. They are based on the premise that data integrity is more important than data availability and therefore conflict should be expected. They sacrifice a degree of availability for the sake of guaranteeing consistency. They avoid the possibility of conflicts when partitions are reunited by confining updates during partition to a single distinguished or majority partition. Updates to the majority partition are simply propagated to the other partitions when the network failure is resolved. Several options are available that limit update to a single partition. For instance: 1. Primary/Slave. Every data item has one copy designated as the primary copy and all other duplicates are slave copies. Updates are directed only to the primary copy, then propagated to the slaves. All reads must first acquire a read lock on the primary copy before reading any copy. If the primary site itself fails, it is possible to promote one of the slave copies to primary, assuming the distributed management product is able to detect that the problem is a site failure and not a communications problem. This promotion is generally accomplished using a voting strategy (see option 2). 2. Voting. Using the voting (or quorum consensus) strategy, a transaction is permitted to update a data item only if it has access to and can lock a majority of the copies. This majority is known as a write quorum. When the transaction achieves a majority, all the sites are updated at once, and the change propagated to any other sites. A similar system, called a read quorum, operates for reads to prevent transactions reading out-of-date versions of a data item. If data integrity is to be assured, a majority of sites must be represented in the read quorum. 3. Missing Writes. While the voting strategy provides greater data availability than the primary/slave in the event of failure, this availability is achieved at a high cost. During normal operations, the transaction must acquire read or write quorums for every operation. The missing writes method reverses this situation by involving much less overhead during normal operations at the expense of higher overhead when things go wrong. Using this

- 81 -

strategy, transactions operate in one of two modes—normal mode when all copies are available and failure mode when one or more sites are unavailable. Timeouts are used to detect failures, so a transaction in normal mode can switch to failure mode when it does not receive an answer from a site in a reasonable period of time. During failure mode execution, a voting strategy is used. The techniques described thus far apply mainly to homogeneous distributed systems or those with a very limited degree of heterogeneity. In these systems, a global transaction manager (or the local transaction manager temporarily acting in a global capacity) is aware of and can synchronize all transactions in the system. In a truly heterogeneous distributed system, preexisting autonomous databases have been integrated. To ensure concurrency control, there must be a mechanism to synchronize global transactions with local transactions that are under the control of the local DBMS. While the global transactions can be handled by any of the methods discussed thus far in this chapter, it is impossible to synchronize them with the local transactions and retain any degree of local autonomy. Once a global transaction submits an agent to a local system, it loses all control over it. The local DBMS assumes all responsibility and will decide, quite independently of that global transaction's other agents, what should happen in the local system. Therefore, some subtransactions could commit while others abort, destroying the atomicity of the global transaction and compromising the consistency of the distributed database. The problems associated with providing support for global transactions in the presence of local transactions can be summarized into three points: Maintenance of atomicity for both global transactions and their agents Serialization of local and global transactions Detection and prevention of global deadlocks Most existing heterogeneous distributed database management products support read operations only; all updates must occur locally. Even with this severe restriction, the problem of dirty (unrepeatable) reads must be addressed. Read-only transactions requiring a consistent view of data have to take into account the possibility that local transactions may be processing against the data they are trying to read. It might be possible to support global updates in a heterogeneous environment, but only if some of the constraints are relaxed. Any of the following strategies might be employed: Reduce local autonomy, yielding some authority to a centralized control mechanism. Restrict the types of updates. Allow only one global transaction in the system at a time, via centralized control. Relax strict serializability rules. Use application-specific methods of concurrency control. Of these strategies, the trade-off most often chosen is between preserving local autonomy and providing full functionality. One way this might be accomplished involves the local transaction manager yielding the local wait-for graphs to the global transaction manager for analysis and action. The alternate would be to restrict the function of global transactions (perhaps to read-only or single-site updates). In fact, this is often the most practical thing to do since the local nodes are managed by proprietary DBMSs whose internal structures (transaction managers and schedulers) cannot be altered. Another way of preserving local autonomy while still processing global transactions is to adopt a method of concurrency control based on quasi-serializability. Global transactions are essentially hierarchical in nature (i.e., they can be broken down into subtransactions). This fact, taken in conjunction with the desire to preserve local autonomy, makes it very difficult to produce schedules that are globally serializable. However, if global and local transactions could be treated differently, as they are with quasi-serializability, joint processing might be possible. This method of concurrency control requires that all global transactions be submitted serially while local transactions execute in the normal way (i.e., are serializable). Local autonomy is maintained, but no global integrity constraints and no referential integrity between different agents of the same transaction are enforced.

- 82 -

The last method of concurrency control involves using application-specific methods. This method allows transactions to be interleaved with other transactions subject to certain constraints. Rather than having to wait for a transaction to complete, this method exploits advance knowledge of the transaction's operations to allow other transactions to process simultaneously whenever possible. Transaction failure is handled by providing a counterstep for every step in the transaction. Transactions are divided into two or more sets, with the transactions in one group able to process concurrently and those in the other being incompatible, unable to be interleaved together. This method can be taken a step further by dividing the individual transactions into a series of atomic steps divided by break-points. The breakpoints represent places where other transactions can safely be interleaved, allowing a greater degree of concurrency control. This method uses a new type of schedule, called a relatively consistent schedule, which is nonserializable but still guarantees consistency. No matter which method is chosen, concurrency control in a distributed database environment, and especially in one involving heterogeneous systems, comes at a high cost. Because it is simply too important to ignore, concurrency control becomes one of the major obstacles that must be overcome for a distributed system to function successfully.

Data Warehousing and Concurrency Control
One of the most important basic services provided by a DBMS, and one of the most difficult to achieve, is the ability to support multiuser access to the data. Improvements in concurrent "read" and "write" access have been sought since the introduction of the first DBMS, IMS, in the late 1960s. The current interest in data warehousing, combined with the natural desire to exploit the best, most cost-effective methods available, has sparked renewed interest in concurrency control. The problems of concurrency control are complicated in a distributed warehouse environment by the fact that there are many "local" users accessing data in their departmental data mart at the same time as others are processing "global" queries against the warehouse as a whole. An additional intricacy arises from the fact that most data warehouses constructed today use multiprocessor hardware, either SMP or MPP. This pairing of distributed software with multiprocessor hardware complicates concurrency control beyond being just an issue of proper scheduling that will be handled by the DBMS. A team of architects, designers, system programmers, DBAs, and applications programmers must work in tandem to identify problems and select and implement a successful concurrency control strategy. Key to devising a concurrency control plan is an understanding of the transaction, the basic unit of work in a DBMS environment, and the classes of problems that can arise when transactions are allowed to process without any attempt at synchronization. But almost all DBMSs try to accommodate simultaneous transactions by means of a schedule and a concurrency control algorithm. A schedule is the entire series, in order, of all the reads and writes of the transactions processing together. The concurrency control algorithm tries to inter-leave the read and write operations of a transaction with those of other concurrently executing transactions in such a way that they can process together without interference. The concurrency control algorithm is responsible for producing correct schedules where the consistency of both the transactions and the database are guaranteed. Correct schedules are normally those that are serializable, or equivalent to running the transactions sequentially, one after another. While it is possible to have a correct schedule that is nonserializable, serializability is taken as proof of correctness because it is well known and easy to apply. The three principal methods for concurrency control, described in depth in this chapter, are: Locking Timestamping Optimistic methods Both locking and timestamping are considered conservative approaches because they cause transactions to be delayed if they might conflict with other transactions at some future time. Optimistic methods are based on the premise that conflict is rare and therefore transactions are allowed to proceed unsynchronized, with checks for conflicts done only when a transaction commits. Of the three techniques, locking is the most frequently used method for concurrency control. There are several variations, but all are based on the premise that a transaction must claim a read or write lock on a data item prior to performing the operation. While it is permissible for two or more transactions to hold a read lock simultaneously, a write lock gives a transaction exclusive access to an item, and for its

- 83 -

duration no other transaction can read or write to that data item. A transaction holds a lock until it explicitly releases it, and it is only then that the results of the write operation are made available to other transactions. There are times when more than one transaction needs to lock the same data item. When this happens, it can be handled in one of two ways: The second transaction is placed on a queue, waiting for the first to release its lock. The second transaction is aborted and rolled back. With the first option, the requesting transaction retains any other locks it might hold and enters a wait state. Allowing the waiting transaction to retain its locks can lead to a potentially dangerous situation known as deadlock. Deadlock occurs when multiple transactions are queued up waiting for a lock to be released and, because of conflicting requests, the waiting transaction is frozen. Where there is a possibility of this happening, a deadlock detection protocol that utilizes wait-for graphs must be invoked periodically to check for and cure any conflicts. With the second option, the requesting transaction must release all of its locks and restart. For a complex transaction, especially a global one in a distributed system, the overhead of having to restart could be very high. Timestamping methods of concurrency control are very different from locking strategies. The basic goal of the timestamping methodologies is to order transactions globally so that older transactions (those with smaller timestamps) always get priority. A transaction can read or write an item only if the last update of that data item was performed by an older transaction. Because there are no locks, deadlocks cannot occur, but transactions are frequently aborted and restarted to resolve potential conflicts. Timestamping methods produce serializable schedules. Optimistic methods take the position that conflict in a database system is rare, so it is best to allow transactions to proceed unimpeded without any waiting. Only when a transaction is ready to commit should there be any check for conflicts and, if one is detected, the transaction should abort and restart. To ensure atomicity of the transaction, all updates during processing occur internally, to transactionlocal copies of the data, and are propagated to the database after commit, when the system is sure there are no conflicts. Studies have shown that unless contention in the system is extremely low, other concurrency control strategies outperform optimistic methods by a wide margin. This is largely due to the enormous overhead of using restart for problem resolution in distributed systems. The unique difficulties of distributed database management systems, especially those containing replicated data, must be addressed with their own strategies for concurrency control. When faced with site or network failures, the distributed system can adopt one of several voting methods that allow it to continue process transactions until full function returns. The problems associated with simultaneously processing global and local transactions in a distributed environment can be summarized as follows: Maintenance of atomicity for both global transactions and their agents Serialization of local and global transactions Detection and prevention of global deadlocks While it is possible to support global updates in a distributed environment, it can only be accomplished with the loss of some autonomy by the local DBMSs or by placing restrictions on the transactions themselves (e.g., read-only, single-system update). The more heterogeneous the distributed system, the more difficult it is to control global transactions and, as a result, concurrency rates diminish. No matter which method is chosen, concurrency control in a distributed database environment comes at a high cost; but regardless of the time, effort, and system compromises involved, it is worth the effort. To provide acceptable levels of service to the users, all distributed warehouses must be able to process transactions simultaneously—in the same processor (i.e., the same "box") and across the nodes of the network. Effective concurrency control makes this possible.

Chapter 5: The Web and the Data Warehouse

- 84 -

Overview
Information in the hands of decision makers is a powerful tool. Corporate America recognizes this fact and, to meet the almost insatiable appetite for more and better data, businesses are extracting information from operational systems and placing it into data warehouses at a phenomenal rate. The result is that users receive the informational equivalent of a "fast-food meal" that costs the company five-star restaurant prices. First in the list of expenses is the warehouse itself. Most data warehouses are still centralized, depending on costly mainframe computers to house the core of the warehouse. To populate the warehouse, the data must be extracted from production systems and other sources and cleansed, formatted, organized for user access, and loaded. From there, it is extracted again and formatted a second time for download into the local data marts. But the warehouse and data marts do not operate alone; analytic software has to be loaded onto every user's PC to facilitate data access and handling. Since user requirements, and even the users themselves, change constantly, the result is a significant support burden. Networks must be constructed and dial-up capabilities added to provide support for those whose work takes them away from the base system. The price tag for all this can run into the millions of dollars, far more than many businesses are able to comfortably afford. The World Wide Web can hold the key to more affordable warehousing for those willing to look and learn. It is the model of a successful distributed environment, and the use of web browsers has taught everyone a better way to deploy software and access data aross an organization. The Internet, and especially intranets, can introduce a new level of collaborative analysis and information sharing among decision makers when it replaces older, more costly, conventional networks. The Web can also change the way data is accessed and analyzed in support of mission-critical decisions. Data published on the Web is easily accessible and, once obtained, ideally suited for query, reporting, and analysis by nontechnical users. For corporations, this could be an alternative, low-cost mechanism for making data available to those individuals who need it. For users, the switch to a webbased environment means a much friendlier and richer pool of accessible information. The World Wide Web represents a paradigm shift in technology that is gradually changing everyone's life. But where did all this new technology come from? Driving the transformation is the Internet, which is really just a chaotic collection of computers, network connections, and telecommunications lines (it loses its ability to impress when viewed from the perspective of a badly organized heterogeneous system). The Internet was initially designed in the 1960s to meet the needs of the United States Department of Defense (DOD) for secure communications. As a result, certain aspects of the Internet, such as the packet-oriented communications protocol used (see Chapter 8), were intended to prevent interruption of service during a military confrontation. Although it never really fulfilled its original purpose, the Internet's "second life" has revolutionized communications around the world. For its first twenty years of existence, only a privileged few at military installations and universities had access to the Internet. During that time, various technologies emerged, one at a time, which gradually increased the number of people with access. The development of powerful personal computers with graphical interfaces (e.g., Microsoft Windows and Motif), the rise of service providers (such as the pioneering CompuServe), and improved bandwidth with cheaper transmission rates all contributed to the popularization of the Internet. However, it has only been recently that the number of users "on the Net" has exploded. This growth is due in large part to the development of some relatively simple software that makes navigating the Internet easy. The first of these developments is the hyperlink, which turned the Internet into the host of the World Wide Web. Hyperlinks freed users from knowing the exact location of each document and the network path that leads to the site where it is stored. Users can simply click on highlighted text or an icon (with an attached hyperlink) to navigate to another set of information located literally anywhere. But what really sparked the incredible growth in Internet usage in the mid-1990s was the design and commercial success of a friendly user interface to the Internet known as Mosaic. While the infrastructure had been in place for some time and users could theoretically access the vast store of information that is the Internet with arcane operating system commands, very few did so because it was just too difficult. Mosaic changed all that. Originally developed at the University of Illinois, Mosaic was quickly refined and commercialized as a browser by Netscape. Within a matter of months it gained wide-spread public acceptance and a host of imitators. The ease of use resulting from the pairing of hyperlinks with browsers has caused phenomenal growth in Internet usage.

- 85 -

Emerging Trends
There are millions of users on the Internet every day, all clamoring for enhanced services. They want improvements in usability, manageability, performance, and security. As new products are developed and proven better than existing ones, they are quickly adopted by the web subculture. For example, the HyperText Markup Language (HTML), still the standard way to access the Web, is rapidly giving way to newer enhancements. Although HTML and the protocol used for HTML communication, HyperText Transfer Protocol (HTTP), are easy to use, HTML places limits on the interaction between user and application. Users may only send and receive information as document forms, with no ability to drag-and-drop or to manipulate objects on the screen. New extensions and enhancements to HTML are making the user interface come alive with reusable application components that run on the local PC. Among the most well known of these are Java (Sun Microsystems), Virtual Reality Modeling Language or VRML (Silicon Graphics), ActiveX (Microsoft), and many others. Within a matter of months, these will have superseded HTML as the preferred method of access. So, too, are database technologies and access tools making revolutionary advances under pressure from newer technologies. The relational database management system (RDBMS), which appeared in the early 1970s, simplified access to data for programmers with its structured query language (SQL). RDBMSs allowed developers to build data access tools independent of the structure of the database. Before the emergence of the mathematically elegant relational model, all data access mechanisms were database specific and, in many cases, application dependent. Data was almost never shared between applications. This functional separation of the data access mechanism from the database itself, along with the availability of low-cost desktop processing power, led to another commercially successful trend, client/server technology. Popularized in the mid-1980s, client/server decoupled the access tool processing from the processing of the database engine. Communications were established across corporate networks using vendor-specific middleware and network protocols such as Token Ring, TCP/IP, SPX/IPX, and others. Client/server was so successful that, by the end of the 1980s, all the major relational vendors had ported their products to the new environment. Along with the proliferation of client/server platforms and RDBMSs came the concept of the open system, where customers were free to mix-and-match multivendor (i.e., heterogeneous) components such as hardware, network protocols, operating systems, databases, access tools and other elements. Industry standards quickly evolved to bring some order to the resulting chaos, ensuring a degree of interoperability between components. By 1990, most corporations had in place an infrastructure for bringing users and data together. However, access to data was still limited in the following ways: Canned forms and reports were still used at the workstation to access data. These static windows gave users a limited view of the available information with no flexibility to adapt the forms. From the users' perspective, this was not much of an improvement over the old printed reports they used to get delivered on schedule to their desk. Because of the rapidly changing business environment and frequent business reorganizations, there was a need to drastically shorten the applications development cycle. Companies' IT departments had trouble keeping pace. In some cases, applications development time was estimated to be longer than the application's useful life, or the period during which it would have been relevant. Custom tools appeared that were designed to appeal to specialized user groups, but these tools had limited scope or capacity, and most required extensive IT involvement to set up and maintain. As time goes on and the business-tool industry matures, new technologies are emerging to replace the old. The first tools available performed only one function (e.g., query or analysis or reporting). If users wanted to access the warehouse, they had a tool on their PC designed to do just that. But if they also wanted to analyze the data and report on the results, they needed two additional programs to accomplish this task (provided the user could import the data from one application into another, which itself was often a very difficult task). In spite of these difficulties, the numbers of users involved in decision-support functions continued to increase as data warehouses proliferated. In response to increased demand, tool manufacturers finally have begun integrating query, analysis, and reporting into

- 86 -

one product. State-of-the-art products today are the OLAP (online analytical processing) and ROLAP (relational online analytical processing) tools. Not only are the tools changing, the entire infrastructure is shifting again at the start of the new millennium. In an effort to keep pace with technological and business pressures, corporations have to rethink their approach to data processing. What was "state of the art" in 1990 now seems old and hopelessly outdated. A new architecture is called for to meet the new demands that will be placed on it. This does not mean that all the old hardware will have to be junked; quite the contrary. With careful planning, current equipment can be incorporated and repositioned to better serve the organization's needs. To develop a new architecture, some decisions must be made. One is how best to distribute and maintain the software that runs on the desktop computers. There are three basic approaches to this issue: Fat Clients. Fat-client configurations are designed to take advantage of the low-cost processing power of the desktop PC. They are essentially client/server applications that can be enhanced to take advantage of the Web. All decision-support software resides on the desktop and is maintained there. These machines can be easily converted from wide area networks to using the Internet/intranet for communications. Thin Clients. For environments where it is difficult or costly to manage applications distributed across a large number of user sites, there is the thin-client approach. Using this model, the software on the users' desktop is limited to those components that relate to presentation only. All other activity takes place on the back-end system as directed by the user, with only the result-set shipped down line to the desktop PC. There are many that argue that application processing should be separate from presentation activities, and they point to the thin-client approach as the ideal. Multitiered Approach. In addition to the question of software installation and maintenance, another important issue is the speed at which a user's request is processed. Performance concerns in the Internet/intranet environment are driving companies toward modularization of the current two-tier (front-end browser and back-end HTML server) environment. Using a multitiered approach, HTTP server activity is separated from application-related processing (which occurs on an intermediate server). While other factors such as bandwidth and equipment used can affect performance, the architecture selected remains the most important issue. The three models just described are not mutually exclusive. A company might choose a combination of fat and thin clients, as well as taking a multitiered approach to systems development. Regardless of the architecture chosen, the company will need to select an array of applications designed to facilitate data access and analysis. To better keep pace with today's rapidly changing business environment, these tools should also be Internet/intranet compatible. This first generation of Internet decision-support aids must meet certain criteria. They must: Support interactive analysis. Be able to keep pace with the infrastructure. Ensure security. Give users flexibility. Decision support is not just a single function; it is an iterative process consisting of query, analysis, and reporting. The process begins with the user submitting a request for data to the warehouse/ data mart. When the query set is returned to the user's PC, it is in the form of a "report." The user analyzes the data by viewing it from different perspectives and varying levels of detail. During the course of this analysis, the user may decide that additional information is needed and request more data from the source. When finished, the user formats a report before distributing his study. Any decision-support tool must allow the user to carry out these activities efficiently. While the World Wide Web promises to change the way people access data, analyze information, and share their conclusions with others, it has for the moment some limits, as does the intranet-enabled corporate infrastructure. But it is also necessary to keep in mind while designing a data warehouse and choosing decision-support tools that those limits will change during the life of a system built today. It is best to choose hardware and software that play to the strengths of the Internet/intranet, remembering that even more capability will be offered in the near future. Above all, the decision-support tools chosen

- 87 -

should not be hardware dependent; as the Web evolves, so will the corporate infrastructure and existing equipment will be replaced with the newer, faster, and better models. To protect its investment, a company must select tools that will work anywhere. The Web is best suited currently for the publication of HTML pages, navigation across documents (i.e., surfing), and the downloading of these documents across the network. For the moment, network delays limit certain decision-support activities, such as drilling down on a cell in a spreadsheet. Users may not be willing to wait twenty to sixty seconds (typical response time on the Internet/intranet) for detailed data to be returned. Improvements in network speeds and modems will soon eliminate this concern, so it would be prudent to include this capability (if desired) and others in decision-support tools purchased today. A corporate decision-support environment, like a warehouse, must maintain security for sensitive business-related and personal data. Security requirements include the ability to control: Publication rights for users who want to post data on the Web Access rights to information published on the Web A third point—not as critical as the two listed, but still a consideration—is that existing security be reusable. The technology department should not have to redo the entire scheme of an application when it becomes web-enabled, so the decision-support tools must play their part in keeping the network safe. The role of security in an Internet environment cannot be ignored (see Chapter 8). To do so is foolish beyond belief. A last consideration in choosing decision-support tools must be their adaptability in a changing environment. The Internet is a dynamic, ever-changing, evolving environment. Organizations that deploy web-enabled applications need to ensure that the choices they make now do not trap them in a technological dead end in the near future. A corporation needs to take a broad view of its intranet—it is not just a place to store and distribute text files and e-mail; it can also be the infrastructure for a comprehensive decision-support environment. Technology has allowed users to become knowledge sharers, and it can be deployed to yield the powerful advantage of shared problem resolution to those prepared to exploit it. Knowledge sharing requires the free flow of all types of information among users, not only text files but also interactive analysis capability that encourages the exchange of ideas and experiences. By putting a centralized data warehouse on an intranet (Figure 5-1) and deploying a series of data marts that also function as web servers, an organization gains immediate economic and rapid application deployment benefits. Of longer-term interest, users are able to collaborate more freely on business issues, which can lead to a better and more rapid response on critical decisions.

Figure 5-1: Centralized data warehouse in an intranet environment.

- 88 -

For the business without a prior investment in a centralized warehouse or that already has data marts in place, now is the time to begin constructing a distributed data warehouse (Figure 5-2). Based on an intranet infrastructure, a distributed system can be developed quickly and economically and will confer on its corporate owner all the benefits associated with its more traditional cousin. Because it is extremely flexible, a distributed warehouse conforms readily to meet almost any business need, even the more unusual special-situation requirements ignored by the centralized warehouse. Regardless of whether it is used to support a distributed warehouse or to enhance the capabilities of a centralized one, the advent of the intranet could be the basis for rethinking the enterprise information infrastructure.

Figure 5-2: A distributed data warehouse in an intranet environment.

Intranet Basics
To understand the attraction of an intranet, it is first useful to look at the logic of why a data warehouse or almost any business computer system is built. Data warehouses are essentially all about reports. All the information that is put into the warehouse is only there to be taken out again, albeit in a different form. The people who need that data are usually business users. Their job is to make decisions for the corporation, not to manage the business's data or write application programs. They frequently lack technical training, and there really should be no need for them to acquire it. Communication between the user and the intranet is handled by a web browser, which makes for a simple, single point of contact. The point-and-click paradigm of a browser is such that anyone who can operate a mouse can use it easily. With proper web page design and appropriate security, business users can point-and-click their way to the exact data they need regardless of its location in the network. The user does not need to write a program, learn a new tool, or even know a "transport language"; with minimal (or no) training the user is productive. By using client workstations with windowing systems, the interface can be simplified even more. A web page link can be placed on the desktop as a shortcut; by double-clicking on this link, the user signals the browser to automatically load the web page. In this way a intranet simplifies data access for users so they can spend more time acting on the information and less time looking for it. Intranets can be viewed as next-generation client/server systems with a number of standardized characteristics. Like traditional client/ server, intranets have three basic elements: The client The communications protocol The server Each of these elements makes its own contribution to information delivery in a data warehouse environment (Figure 5-3).

- 89 -

Figure 5-3: Web-enabled data warehouse. The client side of a client/server intranet environment is the browser. It receives HTML web pages from the web server and formats them at the client site for presentation. The presentation characteristics of a web page are controlled completely by the browser; most settings can be altered to suit the user, such as a change of background color or font type. This feature is part of the client/server intranet architecture. The web server delivers the basic page information while the browser determines how that information will be presented, based on the client hardware/software settings characteristics. The server component of an intranet is called the web server. It acts much like a file server, responding to requests from its clients. The web server differs from the conventional file server in that it is responsible for passing requests on to other application servers when necessary. The method of communication, or communication protocol, is often the most challenging part of client/server technology. The Internet uses TCP/IP communications, and it is the protocol provided with most platforms (e.g., Windows NT or Windows 2000) purchased today. In its current form, TCP/IP is easy to deploy and use. Working on top of the communications protocol is the transport language used by the client and server to communicate. In traditional client/server systems, transport languages can cause problems for users because most of them are difficult to use or at least require some training. With TCP/IP as the communications protocol for a client/server intranet, the transport language is HTTP, which requires no special user training. As shown, an intranet architecture can greatly enhance user access to data, but this alone cannot account for the sudden popularity of corporate intranets. Why are corporations rushing to imitate this particular feature of the World Wide Web? There are many reasons: Intranets are cost-effective, easy to implement, easy to use, and a particularly efficient way to make all kinds of information available to the people who need it. Many corporations have already installed web browsers on the desktop, and the

- 90 -

change to an intranet allows them to leverage that investment. Using nothing more than that standard browser, a user on an intranet can access any information in the environment. Most intranets currently manage unstructured content such as text pages, images, and even audio files as static HTML documents. A data warehouse stores structured content and raw alphanumeric data, but with the right tools and correct architecture, a data warehouse can be accessed through the corporate intranet. This "web-enabled" warehouse forms the foundation of a comprehensive enterprise information infrastructure that can confer three advantages to its corporate owner: Improved cost/benefit ratio of an intranet architecture Enhanced decision support due to information integration Improved user collaboration on projects and key business decisions The cost of traditional client/server computing is high when all factors are considered. There is not just the price of the client PC and the file server to be tallied; there are also communications, support, and other hidden costs that must be added to the total. In fact, numerous studies conducted within the last twenty-four months suggest that client/server computing is just as costly or even more expensive than mainframe computing. Certainly, the personal computer has "bulked up" with faster clock speeds, extra memory, giant disk drives, DVD readers, software and user-managed files, and databases of considerable size. The result is a fat-client architecture. An intranet can change the economics of supporting a large community of users. An intranet can certainly reduce communications costs, and many speculate that it may also reduce costs on the client side by utilizing an inexpensive "intranet device" in place of a personal computer. This is possible because intranets are usually based on an architecture that calls for a thin client, which can be a very simple personal computer or even a dumb terminal. Whether corporations ever adopt intranet devices remains to be seen; at a minimum, the intranet will prolong the life of the current round of personal computers because it eliminates the need to upgrade constantly. The thin-client model (Figure 5-4) calls for server distribution of application software. Java permits software to be served to an intranet browser in code fragments or applets. The only portion of the software that must be installed on the client is the browser. Any application software is acquired only as needed for a specific purpose. Since the application software resides on the server and nowhere else, it is much easier to maintain, upgrade, and administer. The economics of an intranet are such that a corporation has reduced communications costs, less expensive thin-client hardware, and in most cases, reduced application software costs.

Figure 5-4: Thin client. Perhaps the most valuable asset of any business is its data. The operational data is acquired and used in the production systems during the course of the enterprise's day-to-day activities. When a business

- 91 -

builds a data warehouse, it is organizing that data in a way that is useful to decision makers. By making that data warehouse a part of the enterprise information infrastructure running on an intranet, the business is providing its key decision makers with the access and tools they need to evaluate the data in new ways (Figure 5-5). Users can toggle between different views of data or between structured and unstructured data adding a new dimension to their analysis.

Figure 5-5: Enterprise information structure. But a web-enabled data warehouse is not just about providing access to data; it is also about communication at an interactive level. No one would dispute that decision making improves with accurate, timely, and complete information. Decision making is also influenced by how the ideas and experiences of a workgroup are exchanged in the analysis and problem-resolution process. A regular LAN-based application has many communications restraints that prevent the free flow of ideas within a group. An intranet has no such restrictions and encourages information-enriched communications and collaborative problem solving. Today, most users can communicate using the corporate e-mail system. While it permits text files to be exchanged, e-mail does not facilitate true collaboration. Sending an EXCEL spreadsheet on Lotus Notes is a step closer, but the emphasis is still on textual file sharing. What is needed is a means of communication that allows the recipient to continue the analysis started by someone else, or to branch off in a new direction if he desired. For example, if a user receives a report from a coworker, she should immediately be able to drill down or up on any report dimension, or add additional calculations, and then pass the altered report on to others. This requires dynamic report creation based on the data stored in the web-enabled warehouse. While this example is beyond the capabilities of most text-oriented groupware products available today, it is not fantasy. An enterprise information infrastructure has the capacity to provide truly interactive collaboration and much more.

The Advantages of Intranets and Data Warehousing
Computers have been a valuable business tool for more than forty years. In their earliest form, computers used flat files, similar to lists, for data storage. The next generation of computing brought the concept of databases with it, simplifying information storage and access for the programmer. Out of the database idea grew relational technologies, which changed forever the role of data processing in a corporate environment. Suddenly, corporate data became a valuable asset, a source of information that decision makers consulted to guide the company. Without relational technologies, data warehousing would not be possible. But it is the next generation in computing, the Internet/intranet, that will make data warehousing cost-effective and available to all those who can benefit from access to its contents. An intranet can deliver vital information and reports to each desktop in an organization at the click of a mouse, regardless of the user's location or the location of the data. Even remote users, traveling or just temporarily away from their desks, can access data through dial-up connections or the World Wide Web, provided correct security procedures are observed. An intranet offers other, subtler advantages, too. An intranet disseminates a uniform view of information to all users, producing a high degree of coherence for the entire organization. The communications, reports formats, and interfaces are consistent, making the information they contain easy to read and

- 92 -

understand. Everyone has access to the same data (regardless of whether it is stored in a data mart or a distributed or centralized warehouse), presented in an identical fashion, thus avoiding misunderstandings and confusion. Because each desktop has the ability to design, extract, and mine corporate data and is equipped with the tools to generate reports and other time-critical information, an intranet architecture improves the decision-making process. Users are empowered with the knowledge necessary for faster and betterinformed business decisions. As one of its most important advantages, an intranet provides a flexible and scalable nonproprietary solution for a distributed data warehouse implementation. It enables the integration of a diverse computing environment into a cohesive information network. An intranet makes data stored at any node equally available to all authorized users regardless of their location in the company and can be easily extended to serve remote corporate locations and business partners through a wide area network (WAN), an extranet, or even a virtual private network (VPN). In this fashion, external users can access data and drill down or print reports through proxy servers located outside the corporate firewall. Even customers can have limited access through the World Wide Web, visiting the company's home page for information about products and services. All of this accessibility makes an intranet seem terribly insecure. That, however, is not the case. A webenabled distributed warehouse can actually be more secure than other warehouse implementations. An intranet both strengthens security and simplifies its administration by permitting a "second line of defense." In addition to overall intranet security, each data mart can employ its own security measures in keeping with the sensitivity of the data stored there. Whereas a business partner might be entitled to access the corporate shipping records housed in Data Mart 1, he could be prevented from seeing the personnel records and financial documents residing in Data Mart 2 by limiting his access to that location. With a little preplanning on the designer's part, all sensitive corporate data is grouped together at a limited number of sites, safe behind a double wall of defense. As a key component of any intranet, the browser offers at least two sizable advantages. The browser can act as a universal applications delivery platform, making it possible for the business to create and integrate new, more robust applications quickly and economically, without most of the usual postdeployment maintenance costs. Because all users have access to the same applications, training and support costs are also reduced. The second advantage to browsers is their ease of use. The intuitive nature of their interface virtually eliminates the need for user training and drastically lowers the number of help desk calls. In summary, an intranet is flexible, scalable, cost-effective, and easy to implement, maintain, and use— the best architecture to maximize the benefits of a corporate data warehouse.

The Challenges of Intranets and Data Warehousing
Almost all data warehouses employ relational database management systems for data storage. Structured query language (SQL) is used to retrieve the highly structured data organized in rows and columns in tables. To date, intranets have been used primarily to access unstructured data managed as HTML documents. The challenge in putting a data warehouse on an intranet is in properly enabling SQL access to the warehouse from HTML browsers. For this implementation to succeed, at least three application layers are needed (Figure 5-6). They are: Analytic layer File management layer Security layer

- 93 -

Figure 5-6: The three application layers enabling SQL in an intranet environment.

Analytic Layer
Putting structured data on an intranet requires a server-resident analytic layer whose job it is to generate SQL as needed, perform computations, and format reports based on user requests. In fact, a specialized web server is required to support data warehouse access by a user employing an HTML browser. Because the analytic layer typically makes heavy demands on both the relational database management product and the web server, there should be a high-speed network connection between the analytic and database layers, or they may reside on the same machine if the hardware is sufficiently robust. In either case, the web server must be configured to support the higher processing loads associated with an analytic layer. The analytic layer shares some capabilities with standard spreadsheet software. Using a spreadsheet, the user has the ability to create custom calculations based on data stored in the cells. For example, similar information is stored in rows numbered 1, 2, 3, and so on, and individual examples of data elements are kept in columns named A, B, and C. By combining the two dimensions, a unique address is created that can be used in a mathematical formula—A1 + B1. The number of calculated rows and columns in a spreadsheet application can exceed the number of the original stored elements. A data warehouse has more than two dimensions—for example, it stores multiple information such as product, market, time period, customer, vendor, and so on. The combination of values for each of the dimensions again provides a unique address. The analytic layer of the structured content web server allows users to apply calculations based on warehouse dimensions to create useful reports. Additionally, once complete, calculations can be shared with other users, just as calculations in a spreadsheet can be stored and passed on. And, like a spreadsheet, the calculation logic is maintained no matter how often the data warehouse is updated. The resulting reports often contain more calculated elements than raw data. For that reason, a robust analytic layer is needed in front of the data warehouse; without it, the user's reporting capabilities are limited to a simple list of stored data elements. The analytic layer is key to addressing the business questions the users must answer.

File Management Layer
This type of decision support requires both interactive analysis and knowledge sharing that is only possible with an active file management layer. A report created by one user becomes more valuable when shared with others to gain their insights and ideas. The recipients should be able to continue the analysis begun by the report's author, making the process an interactive exchange. When necessary, many users can pursue different analysis paths from the same starting point. A file management layer allows users to continue work on the same copy of a report, or to make their own copy for individual analysis. To meet the challenge of providing interactive analysis of warehouse data, users must be able to access "public files" and manage their own personal files over an intranet connection. A sophisticated server-based file management layer is required to support user collaboration and maintain security at this level.

- 94 -

Security Layer
While corporations welcome the opportunity to open their information repositories to in-depth analysis by their employees and even some business partners and customers, they fear such access will leave them vulnerable to security breaches. Accessibility is, after all, the web's greatest benefit and its greatest liability. The challenge for anyone building an intranet-enabled distributed warehouse and web site is to achieve a reasonable level of security without impeding opportunity. The sharing of information and collaboration among employees immediately raises data security issues. The data warehouse contains all kinds of information about the business, some of it proprietary and highly sensitive. Only a very few users should have access to everything; most users should be provided with access to only those portions of the warehouse relevant to their position. The data must be secure, but not so highly controlled that the value of the warehouse goes unrealized. This delicate balance is sometimes difficult to achieve. Another security measure, encryption, can provide a higher level of protection than is generally available to business applications. Data passing between the client and server can be encrypted by using any one of a number of vendor products. This enables a business to run its applications over unsecured communication lines without worrying about someone tapping into the network and viewing the transmission. A multitiered architecture, with the gateway server behind a firewall, can ensure that security needs are met. As additional precautions, a server can monitor the identity of users and their usage habits, and the relational product used for data storage has its own security that can be brought into play to limit access to the data itself. As an example of the need for a robust analytic layer, a file management layer, and a security layer, consider the following: A department head in the brokerage firm (introduced as an example in Chapter 3), who is also a full partner, decides to create a budget analysis. Covered in this report is a breakdown of the expenses of each area by manager, including staffing costs. When the partner chooses to share her analysis with the managers, it is acceptable for them to view the summary numbers for all areas but not to drill down on areas other than their own and view the individual salaries of employees working for other managers. It is contrary to company policy for managers to know the salaries of anyone not working directly for them. This presents an interesting dilemma: Users not authorized to access certain data must still be able to view the report but not to drill down to the forbidden details For this reason, reports created from data stored in the warehouse cannot be simply shared as HTML text files. All reports must contain the underlying logic, giving the recipient the ability to analyze and modify the report if they are authorized to do so. Before the user can even view the report, much less modify it, the authorization level for that user needs to be verified. For effective decision support, reports need to be shared throughout a group and across the company by appropriate individuals. If the recipients of the report are not authorized, then their access to the underlying logic can be denied.

Common Gateway Interfaces
An integral part of any Internet/intranet solution is the Common Gateway Interface (CGI). It should be examined and understood before attempting any web-enabled warehousing solution. CGI is the original standard for creating plug-in enhancements for web servers. CGI is the interface between the web site's HTTP server and the other resources of the server's host computer. CGI is not a language or protocol in the strict sense of the word; it is just a commonly named set of variables and agreed-upon conventions for passing information back and forth from the client to the server. The Common Gateway Interface is the "magic" that makes the World Wide Web interactive. It enables users to browse, fill out forms, and submit data to the host. CGI works by using the operating system's standard input/output and environmental variables to pass information between a form, the server, and the plug-in. While binary programs can be used as CGI plug-ins, scripting languages such as Perl are commonly used for CGI. Such scripting languages offer the advantages of portability and ease of modification (even remotely), and they require no compilation or interpretation by the server. There is a downside to using CGI. CGI scripts typically need to be loaded and unloaded from memory each time they are executed. This, along with the use of standard input/output and environmental variables, seriously impacts the performance of the server. CGI also opens a huge hole in a security system that must be plugged or, at least, defended (see Chapter 8).

- 95 -

The Common Gateway Interface facility of web server software (Figure 5-7) provides a method to execute server-resident applications. Constructing programs for an intranet requires a well-thought-out security strategy as well as the appropriate application architecture. Most web applications provide all users with the same access permissions to the reachable files on the server. But business users require a more refined security approach. A system that maps users to their server account by verifying user names and passwords, called Password Authentication Protocol (PAP), can be used to provide granular security. Access to server applications and files are based on user ID, group assignment, and permission level. Of course, for this to work, users must be mapped to the appropriate user or group in the relational database to control the data that the user can access. Because the number of users can be large, the administration of this system must be centralized and rigidly maintained.

Figure 5-7: The Common Gateway Interface. Another issue with the CGI is that it does not maintain a continuous connection between client and server, or server and data warehouse. As a result, it is impossible to support an application that requires multiple interactive queries—a data warehousing requirement. One possible approach to this problem is to employ a message-based protocol between the client and the server-resident analytic layer. By mapping the user to a server account and starting a process that executes as the user, a continuous connection is maintained between the analytic layer and the database that can support iterative queries during the lifetime of the process. For example, an HTML form can be used to request the user's name and password (and database user name and password, if different). This information is passed as parameters or through environmental variables to a CGI program. That application then verifies the user name and password and starts a process that executes as that user. This process could connect to the warehouse using its Application Program Interface (API) and the supplied user name and password to log into the relational database management system. Once the connection to the RDBMS is established, any number of queries can be processed and the result-sets returned to the analytic layer. A final output report can be generated, converted to an HTML document, and sent back to the client for display to the user. A set of replacements for CGI is being adopted on some platforms that allow the plug-in to remain in memory and use more direct methods of interfacing. On a whole, these replacements provide better performance on the web server than CGI. Microsoft's Internet Server API and Netscape's API are two of more than a dozen alternatives available now. Regardless of whether standard CGI or one of the newer replacements is used, an additional component is needed between the web server and the application layer for a successful warehouse implementation. This ingredient is called a web gateway, and it acts as a translator between the HTML language used by web servers and the data warehouse application API/CGI. When users log in to the network, they first view a web page that contains standard HTML and application-specific tags (Figure 5-7). These tags guide the web gateway in what buttons, dialogues, or objects to present to the user. If the user issues a request to update or query the warehouse, that request goes to the web server, which passes it, along with the tags, to the web gateway (Figure 5-8), where the request is translated into data warehouse application commands.

- 96 -

Figure 5-8: Web gateway. The request results come back from the warehouse server through the web gateway, where it is translated into an HTML table with hidden HTML form controls that contain data for each of the cells. These form controls are important because they can interact with VBScript or JavaScript; these scripts can interrogate and manipulate the controls to perform client-side processing. Ideally, the web gateway will support both the intranet and the Internet (with access for remote users through the corporate home page) through direct or dial-up links. Since the web gateway generally sends only small amounts of information over the network, performance is excellent even for dial-up users with their slower transmission rates.

The Future of Web-Enabled Data Warehousing
A data warehouse should not be viewed as just a product or collection of products, or even a standard answer to an end-user data access problem. Rather, a data warehouse is the properly architected solution in which individual requirements, such as browsing capabilities and report creation, are handled in the method best suited to the characteristics of a specific environment. For this reason, no two warehouses will ever be exactly alike; each is an individual creation, designed to service a unique mix of business user needs and corporate structure. That is not to say, however, that many warehouses cannot share a similar basic architecture. The threetier or n-tiered taxonomy discussed in this chapter can be adapted to suit most of the major classes of data warehousing architecture. It can include the relatively simple centralized reporting system intended primarily for end-user computing and report generation, a moderately complex environment encompassing multiple applications and dependent data marts, and the more sophisticated distributed warehouse with a mix of relational data and text and image documents. Data warehousing is not new. Almost all large corporations and many midsize ones have at least attempted a data warehouse, but only a small percentage of them succeeded in a meaningful way. Over the next few years, the growth of warehousing is going to be enormous, with new products and technologies becoming available almost daily, most of them web-enabled. To get the maximum benefit during this period, data warehouse planners and developers will need a clear idea what they are looking for. They need to choose strategies and methods that will yield performance today and flexibility tomorrow. Because of the growing need for warehousing solutions and the relatively low success rate of their implementation, the data warehousing concept has received a lot of attention from the database community. Everyone knows that data warehousing will provide the means for querying the vast amounts of information that has been accumulating but has never been put to any practical use. Until fairly recently, it was impossible to provide a system that could support efficient use of these mounds of

- 97 -

data. Data warehousing can provide corporate users with the means to perform such tasks as data mining to find those valuable tidbits of information hiding in the sea of data. In fact, there are now two viable architectures for large-scale data warehousing—the more traditional, centralized model and the new, distributed paradigm. With either method, decision makers now have the vehicle to process in a meaningful way the terabytes of data that are the by-product of the past twenty years of automation. The primary difference between the two approaches to data warehousing is one of perspective: Centralized warehousing is rooted in the past but able to incorporate many of new, web-based technologies, whereas distributed warehousing is future-oriented, built on the best of the current Internet technologies and positioned to incorporate whatever new advances are forthcoming. Where will the future of web-enabled data warehousing lead? For the near future, the answer will be more complexity and more third-party vendor solutions—at least until the industry begins to normalize itself and a few architectures and enabling technologies become standards. The whole issue of standards continues to evolve. CGI versus its proposed replacements, two-tier versus n-tiered architecture, Java versus ActiveX—these are just a few of the many areas where the marketplace needs to make choices. This process of choice is one of trade-offs. It's disturbing to see developers move away from the standards that made the Internet the great environment it is today, but they are doing so to get around many of the limitations that traditional web technology places in their way. Once again, the industry seems to be moving toward the quick-fix for applications development, just as it did for client/server, only this time with more complexity. History is once again repeating itself. If the past is any predictor of the future, a consensus will eventually be reached to the benefit of all. Until then, warehouse developers should fasten their seat belts—it's going to be a bumpy and fascinating ride!

Chapter 6: Data Marts
Overview
Regrettably, centralized data warehousing has failed to live up to its promise. Instead of providing instant access to mission-critical data for the company's decision makers, the typical centralized data warehouse is stuck somewhere in the development phase. At best, this process is risky, long to implement, difficult to manage, and very, very costly. This traditional warehouse implementation is mainframe-centric, enterprisewide in scope, and top-down driven. The entire focus of the warehouse is at the enterprise level. This results in one giant database holding all of the decision-support information for all user requirements—reports, analysis, and ad hoc queries—regardless of area of interest. Separate databases are maintained for the corporation's operational systems. The enterprisewide nature of these implementations imposes enormously complex issues on the data warehouse. Every decision must go through multiple levels of agreement, getting a sign-off at every level of each division, because the warehouse is intended for everyone's use. Often the most trivial item requires six months or more of meetings and paperwork before all areas of the corporation sign off on it. Then, the impact of that one simple change may be enormous because changing one item can affect everything else. The lifeblood of any warehouse is the ability to change to meet new business needs so it can rapidly deliver valuable information to corporate decision makers. The centralized warehouse, however, tends to become bound by its own architecture and enterprisewide nature and unable to adapt to new situations as needed. This isn't the only problem with a centralized warehouse. Traditional warehouses are built to accommodate predetermined queries, for which they are tuned in advance. But users seldom know exactly what questions they want to ask, and every answer leads to a new question. This results in poor response times, frustrating developers and users alike. There is an almost constant need for new summary and reporting tables to improve warehouse response, but this can cause an explosion in storage requirements. The additional storage is costly and adds to the management burden. As a result of these challenges to the centralized data warehouse, a new architectural construct has developed that shares the decision-support goals of its predecessor. This distributed data warehouse focuses its approach on data marts, built rapidly with cost-effective, scalable distributed technology. Data marts can provide dramatic returns on investment without the high cost and risk of a centralized

- 98 -

enterprise data warehouse project. Organizations can rapidly develop decision-support applications that are able to change and grow as business needs develop and change. A data mart is a decision-support application system that focuses on solving the specific business problems of a given department or area within the company. A data mart should be constructed with an enterprise data model in hand (to ensure that the finished product can later be integrated with other marts), contain consistent business information, and have the ability to grow. A centralized data warehouse is not a prerequisite for a data mart; in fact, in many ways, it can be an impediment. Data marts are characterized by their rapid response to ad hoc queries and their low construction costs. In many cases, the expenses for an enterprisewide distributed warehouse composed of several data marts are less than half that of a comparable centralized data warehouse. Distributed warehouses, especially those that are web-enabled, are also easier and cheaper to maintain, and they can provide substantial bonus savings on the equipment needed for user interface.

Data Mart Solutions
Clearly, stand-alone data marts cannot meet all of a business's needs for enterprisewide decision support. As companies build data marts that satisfy users' localized application needs, they begin to learn what data elements are most valuable, what information needs to be most current, and where additional details or external data would be useful. Over time, the data processing department may even determine that some data should be stored centrally, because it is key to all of the data marts. By implementing departmental data marts, the enterprise is actually building the corporate warehouse slowly, in steps, in direct response to the needs of its various groups. As demand and requirements grow, the company can construct more data marts until eventually, over time, it will have developed a multitiered, distributed warehouse. In some organizations, the decision-making process is highly decentralized, perhaps even compartmentalized, and as a result, the company will be slow to develop interest in linking the individual data marts together. In others, where top-down decision making is strong, there will be pressure to network the data marts almost from the start. The key to success is to start with a pragmatic approach, focusing on the needs of a single user group and meeting those needs with a data mart. This will ensure a "quick win" for the new mart and help to build user demand for more data marts and eventually the distributed warehouse. The concept of data marts derives from the fact that any single user has limited data needs. Even though there is always a need for cross-functional analysis, the scope of the data requirements is still narrow in most cases. By concentrating on the needs of individual departments, one at a time, the scope of the distributed warehouse itself is more sharply defined, and this naturally limits the type and quantity of data stored in the data marts. It is a more targeted approach than the traditional warehouse, which in its unfocused way tries to be everything to everyone. With the more clearly defined approach, the warehouse can be completed quickly and remains flexible enough to respond to changing business needs. Figure 6-1 illustrates this approach, with several targeted data marts networked into a corporate data warehouse.

Figure 6-1: Networked data marts.

- 99 -

What Is a Data Mart?
A data mart is an application-focused miniature data warehouse, built rapidly to support a single line of business. Data marts share all the other characteristics of a data warehouse, such as subject-oriented data that is nonvolatile, time-variant, and integrated. Rather than representing an overview of the complete corporate data, however, the data mart contains a limited subset of data that is specifically of interest to one department or division of that enterprise.

Dependent Data Marts
The typical data warehouse is the center of the decision-support universe for the company it serves, containing integrated historical data that is communal to the entire organization. The warehouse's stores include both summarized data and detailed information, in addition to the metadata that describes the content and source of the data that enters into the warehouse. From this centralized warehouse, data may flow to various departments for their customized decisionsupport usage. These departmental decision-support databases are a type of data mart, called a dependent data mart, that has at its architectural foundation the centralized enterprise data warehouse. These data marts exist primarily as a means of improving performance for the users. While the data is stored at a very granular level in the centralized data warehouse, it is kept at a more refined level in the dependent data marts. In an organization that has several dependent data marts, each contains a different combination and selection of the same data found in the centralized data warehouse. In some cases, the detailed data is handled differently in the data mart than it is in the centralized warehouse to provide a customized look at the information for the user. Each data mart may also structure the same data differently, depending on the requirements of its users. In every case, the centralized data warehouse provides the detailed foundation for all of the data, both granular and aggregated, found in all of the dependent data marts. Because of the single data warehouse foundation that they share, dependent data marts have a mutual heritage and can always be reconciled at the most basic level. Several factors have led to the popularity of the centralized warehouse/dependent data mart combination. As long as the centralized data warehouse does not contain a large amount of data, it is able to serve the needs of many different departments as a basis for their decision-support processing. But successful data warehouses never stay "empty" for long and, for various reasons, soon acquire vast amounts of data. As the warehouse grows, the demand for dependent data marts soon mounts up. The larger and more successful the warehouse is, the fiercer the competition becomes for access to its resources. More and more departmental decision-support processing is done with the aid of the warehouse, to the point where resource consumption becomes a real problem. It quickly becomes nearly impossible to customize and/or summarize the data in the same way it was done when the warehouse was small, placing the burden for dealing with raw data on the user. It is unlikely that the user has the time or resources to deal with the information in its unrefined form. To make matters worse, the software tools available to assist users with access and analysis of large amounts of data are not nearly as elegant or simple as the software designed for smaller quantities of data. Dependent data marts have become the natural extension of the centralized warehouse. They are attractive because: Having its own data mart allows a department to customize the information the way it wants as it flows into the mart from the warehouse. Since there is no need for the data in the data mart to service the entire corporation, the department can summarize, select, sort, and structure its own data with no consideration for the rest of the company. The amount of historical data required is a function of the department, not the corporation. In almost every case, the department will choose to store less historical data than is kept in the centralized data warehouse. In those rare instances where more historical data is needed, the department can store it in the exact form it requires to best fulfill its needs. The department can do whatever decision-support processing it wants whenever it wants with no impact on resource utilization for the rest of the company.

- 100 -

