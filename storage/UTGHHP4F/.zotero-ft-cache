IBML
Data Modeling Techniques for Data Warehousing
Chuck Ballard, Dirk Herreman, Don Schau, Rhonda Bell, Eunsaeng Kim, Ann Valencic

International Technical Support Organization http://www.redbooks.ibm.com

SG24-2238-00

IBML

International Technical Support Organization

SG24-2238-00

Data Modeling Techniques for Data Warehousing February 1998

Take Note! Before using this information and the product it supports, be sure to read the general information in Appendix B, “Special Notices” on page 183.

First Edition (February 1998)
Comments may be addressed to: IBM Corporation, International Technical Support Organization Dept. QXXE Building 80-E2 650 Harry Road San Jose, California 95120-6099 When you send information to IBM, you grant IBM a non-exclusive right to use or distribute the information in any way it believes appropriate without incurring any obligation to you. © Copyright International Business Machines Corporation 1998. All rights reserved. Note to U.S. Government Users — Documentation related to restricted rights — Use, duplication or disclosure is subject to restrictions set forth in GSA ADP Schedule Contract with IBM Corp.

Contents
Figures Tables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ix xi xiii xiii xiv 1 2 2 5 5 5 6 9 10 11 12 13 15 15 15 17 18 18 19 20 21 23 23 24 24 24 25 25 26 27 28 28 30 30 31 31 31 32

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Preface . . . . . . . . . . . . . . . . The Team That Wrote This Redbook . . . . . . . . Comments Welcome Chapter 1. Introduction . . . . . 1.1 Who Should Read This Book 1.2 Structure of This Book . . . Chapter 2. Data Warehousing 2.1 A Solution, Not a Product 2.2 Why Data Warehousing? . . . . . . 2.3 Short History

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 3. Data Analysis Techniques 3.1 Query and Reporting . . . . . . . . . . 3.2 Multidimensional Analysis 3.3 Data Mining . . . . . . . . . . . . 3.4 Importance to Modeling . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 4. Data Warehousing Architecture and Implementation Choices 4.1 Architecture Choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Global Warehouse Architecture . . . . . . . . . . . . . . . . . . . 4.1.2 Independent Data Mart Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.3 Interconnected Data Mart Architecture 4.2 Implementation Choices . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 Top Down Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 Bottom Up Implementation 4.2.3 A Combined Approach . . . . . . . . . . . . . . . . . . . . . . . . Chapter 5. Architecting the Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Structuring the Data . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Real-Time Data 5.1.2 Derived Data . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.3 Reconciled Data . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Enterprise Data Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Phased Enterprise Data Modeling 5.2.2 A Simple Enterprise Data Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.3 The Benefits of EDM . . . . . . . . . . . . . . . . . . . . . . 5.3 Data Granularity Model 5.3.1 Granularity of Data in the Data Warehouse . . . . . . . . 5.3.2 Multigranularity Modeling in the Corporate Environment 5.4 Logical Data Partitioning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1 Partitioning the Data 5.4.1.1 The Goals of Partitioning . . . . . . . . . . . . . . . . 5.4.1.2 The Criteria of Partitioning . . . . . . . . . . . . . . . 5.4.2 Subject Area . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

© Copyright IBM Corp. 1998

iii

Chapter 6. Data Modeling for a Data Warehouse . . . . 6.1 Why Data Modeling Is Important . . . . . . . . . . . . Visualization of the business world . . . . . . . . The essence of the data warehouse architecture Different approaches of data modeling . . . . . . 6.2 Data Modeling Techniques . . . . . . . . . . . . . . . 6.3 ER Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.1 Basic Concepts 6.3.1.1 Entity . . . . . . . . . . . . . . . . . . . . . . . 6.3.1.2 Relationship . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.1.3 Attributes 6.3.1.4 Other Concepts . . . . . . . . . . . . . . . . . 6.3.2 Advanced Topics in ER Modeling . . . . . . . . . 6.3.2.1 Supertype and Subtype . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.2.2 Constraints 6.3.2.3 Derived Attributes and Derivation Functions 6.4 Dimensional Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.1 Basic Concepts 6.4.1.1 Fact . . . . . . . . . . . . . . . . . . . . . . . . 6.4.1.2 Dimension . . . . . . . . . . . . . . . . . . . . Dimension Members . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dimension Hierarchies 6.4.1.3 Measure . . . . . . . . . . . . . . . . . . . . . 6.4.2 Visualization of a Dimensional Model . . . . . . . . . . . . . . . . . . 6.4.3 Basic Operations for OLAP . . . . . . . . . . . . 6.4.3.1 Drill Down and Roll Up . . . . . . . . . . . . . . . . . 6.4.3.2 Slice and Dice 6.4.4 Star and Snowflake Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.4.1 Star Model 6.4.4.2 Snowflake Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4.5 Data Consolidation 6.5 ER Modeling and Dimensional Modeling . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

35 35 35 36 36 36 37 37 37 38 38 39 39 39 40 41 42 42 42 42 43 43 43 43 44 44 45 45 46 46 47 47 49 50 51 51 52 53 53 53 54 55 55 57 58 58 60 60 64 65 66 68 69 69

Chapter 7. The Process of Data Warehousing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1 Manage the Project . . . . . . . . . . . . . . . . . . . . . . 7.2 Define the Project . . . . . . . . . . . . . . . . . . 7.3 Requirements Gathering 7.3.1 Source-Driven Requirements Gathering . . . . . . . 7.3.2 User-Driven Requirements Gathering . . . . . . . . . . . . . . . . . . . . . . . . 7.3.3 The CelDial Case Study 7.4 Modeling the Data Warehouse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.1 Creating an ER Model 7.4.2 Creating a Dimensional Model . . . . . . . . . . . . 7.4.2.1 Dimensions and Measures . . . . . . . . . . . . . . . . . . . . . . . . 7.4.2.2 Adding a Time Dimension . . . . . . . . . . . . . . . . . . . 7.4.2.3 Creating Facts 7.4.2.4 Granularity, Additivity, and Merging Facts . . . Granularity and Additivity . . . . . . . . . . . . . . . . Fact Consolidation . . . . . . . . . . . . . . . . . . . . 7.4.2.5 Integration with Existing Models . . . . . . . . . 7.4.2.6 Sizing Your Model . . . . . . . . . . . . . . . . . 7.4.3 Don′t Forget the Metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.4 Validating the Model 7.5 Design the Warehouse . . . . . . . . . . . . . . . . . . . . 7.5.1 Data Warehouse Design versus Operational Design

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

iv

Data Modeling Techniques for Data Warehousing

7.5.2 Identifying the Sources . . . . . . 7.5.3 Cleaning the Data . . . . . . . . . 7.5.4 Transforming the Data . . . . . . 7.5.4.1 Capturing the Source Data . 7.5.4.2 Generating Keys . . . . . . . 7.5.4.3 Getting from Source to Target 7.5.5 Designing Subsidiary Targets . . 7.5.6 Validating the Design . . . . . . . . . . . 7.5.7 What About Data Mining? 7.5.7.1 Data Scoping . . . . . . . . . . . . . . . . . 7.5.7.2 Data Selection . . . . . . . . 7.5.7.3 Data Cleaning 7.5.7.4 Data Transformation . . . . . 7.5.7.5 Data Summarization . . . . . 7.6 The Dynamic Warehouse Model . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

71 72 72 73 73 74 76 77 77 78 78 78 79 79 79

Chapter 8. Data Warehouse Modeling Techniques . . . . . . . . . . . . . . . . . 8.1 Data Warehouse Modeling and OLTP Database Modeling 8.1.1 Origin of the Modeling Differences . . . . . . . . . . . . . . . . . 8.1.2 Base Properties of a Data Warehouse . . . . . . . . . . . . . . . 8.1.3 The Data Warehouse Computing Context . . . . . . . . . . . . . . . . . . . . 8.1.4 Setting Up a Data Warehouse Modeling Approach 8.2 Principal Data Warehouse Modeling Techniques . . . . . . . . . . . 8.3 Data Warehouse Modeling for Data Marts . . . . . . . . . . . . . . . 8.4 Dimensional Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4.1 Requirements Gathering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4.1.1 Process Oriented Requirements . . . . . . . . . . . . . 8.4.1.2 Information-Oriented Requirements 8.4.2 Requirements Analysis . . . . . . . . . . . . . . . . . . . . . . . . 8.4.2.1 Determining Candidate Measures, Dimensions, and Facts Candidate Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Candidate Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . Candidate Facts . . . . . . . . . . . 8.4.2.2 Creating the Initial Dimensional Model . . . . . . . . . . . . . . . . Establishing the Business Directory Determining Facts and Dimension Keys . . . . . . . . . . . . . . Determining Representative Dimensions and Detailed Versus Consolidated Facts . . . . . . . . . . . . . . . . . . . . . . . . . . Dimensions and Their Roles in a Dimensional Model . . . . . . . . . . . . . . . . . . . . . . . . . . . Getting the Measures Right . . Fact Attributes Other Than Dimension Keys and Measures 8.4.3 Requirements Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4.4 Requirements Modeling - CelDial Case Study Example 8.4.4.1 Modeling of Nontemporal Dimensions . . . . . . . . . . . . The Product Dimension . . . . . . . . . . . . . . . . . . . . . . . . Analyzing the Extended Product Dimension . . . . . . . . . . Looking for Fundamental Aggregation Paths . . . . . . . . . . The Manufacturing Dimension . . . . . . . . . . . . . . . . . . . . The Customer Dimension . . . . . . . . . . . . . . . . . . . . . . . The Sales Organization Dimension . . . . . . . . . . . . . . . . . The Time Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4.4.2 Developing the Basis of a Time Dimension Model . . . . . About Aggregation Paths above Week . . . . . . . . . . . . . . . Business Time Periods and Business-Related Time Attributes Making the Time Dimension Model More Generic . . . . . . . .

. . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

81 81 . 82 . 82 . 84 . 85 . 86 . 86 . 88 . 92 . 93 . 95 . 96 . 98 . 98 . 99 100 105 105 106 109 111 112 114 115 117 120 121 123 124 125 126 126 127 127 128 130 131

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Contents

v

Flattening the Time Dimension Model into a Dimension Table The Time Dimension As a Means for Consistency . . . . . . . Lower Levels of Time Granularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4.4.3 Modeling Slow-Varying Dimensions About Keys in Dimensions of a Data Warehouse . . . . . . . . Dealing with Attribute Changes in Slow-Varying Dimensions Modeling Time-Variancy of the Dimension Hierarchy . . . . . 8.4.4.4 Temporal Data Modeling . . . . . . . . . . . . . . . . . . . Preliminary Considerations . . . . . . . . . . . . . . . . . . . . . Time Stamp Interpretations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Instant and Interval Time Stamps . . . . . . . . . . . . . . Base Temporal Modeling Techniques . . . . . . . . . . . . . . . . Adding Time Stamps to Entities . . . . . . . . . . . . . . . . . . . . Restructuring the Entities Adding Entities for Transactions and Events . . . . . . . . . . . . . . . . . Grouping Time-Variant Classes of Attributes . . . . . . . . . . . Advanced Temporal Modeling Techniques Adding Temporal Constraints to a Model . . . . . . . . . . . Modeling Lifespan Histories of Database Objects . . . . . . Modeling Time-Variancy at the Schema Level . . . . . . . . Some Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4.4.5 Selecting a Data Warehouse Modeling Approach Considerations for ER Modeling . . . . . . . . . . . . . . . . . . Considerations for Dimensional Modeling . . . . . . . . . . . . Two-Tiered Data Modeling . . . . . . . . . . . . . . . . . . . . . Dimensional Modeling Supporting Drill Across . . . . . . . . . Modeling Corporate Historical Databases . . . . . . . . . . . . Chapter 9. Selecting a Modeling Tool 9.1 Diagram Notation . . . . . . . . . 9.1.1 ER Modeling . . . . . . . . . . . . 9.1.2 Dimensional Modeling 9.2 Reverse Engineering . . . . . . . 9.3 Forward Engineering . . . . . . . . . . 9.4 Source to Target Mapping . . 9.5 Data Dictionary (Repository) . . . . . . . . . . . . . 9.6 Reporting . . . . . . . . . . . . . . . . 9.7 Tools

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

132 132 133 133 133 135 137 139 141 143 144 145 145 146 148 149 149 149 150 150 150 151 152 152 152 153 153 155 155 155 156 156 156 157 157 158 158 159 159 161 161 162 163 163 163 164 164 165 165 165 166 167

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 10. Populating the Data Warehouse 10.1 Capture . . . . . . . . . . . . . . . . . . 10.2 Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3 Apply . . . . . . . . 10.4 Importance to Modeling Appendix A. The CelDial Case Study A.1 CelDial - The Company . . . . . A.2 Project Definition . . . . . . . . . . . A.3 Defining the Business Need A.3.1 Life Cycle of a Product . . . . . . . . A.3.2 Anatomy of a Sale A.3.3 Structure of the Organization A.3.4 Defining Cost and Revenue A.3.5 What Do the Users Want? . . . . . . . . . . A.4 Getting the Data

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

vi

Data Modeling Techniques for Data Warehousing

A.5 CelDial Dimensional Models - Proposed Solution A.6 CelDial Metadata - Proposed Solution . . . . . . Appendix B. Special Notices

. . . . . . . . . . . . . . . . . . . . . . . . . . .

167 170 183 185 185 185 185 185 186 189 189 190 191 193 195 197

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

Appendix C. Related Publications . . . . . . . . . . . . . . . . . . . . . . . C.1 International Technical Support Organization Publications . . . . . . C.2 Redbooks on CD-ROMs . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Other Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3.1 Books . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3.2 Journal Articles, Technical Reports, and Miscellaneous Sources How to Get ITSO Redbooks . . . . . . . . . . How IBM Employees Can Get ITSO Redbooks How Customers Can Get ITSO Redbooks . . . . . . . . . . . . . IBM Redbook Order Form Glossary Index

. . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ITSO Redbook Evaluation

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Contents

vii

viii

Data Modeling Techniques for Data Warehousing

Figures
1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Query and Reporting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Drill-Down and Roll-Up Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Data Mining . . . . . . . . . . . . . . . . . . . . . . . Global Warehouse Architecture Data Mart Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . Top Down Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Bottom Up Implementation The Phased Enterprise Data Model (EDM) . . . . . . . . . . . . . . . . . A Simple Enterprise Data Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Granularity of Data: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A Sample ER Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . Supertype and Subtype Multiple Hierarchies in a Time Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The Cube: A Metaphor for a Dimensional Model . . . . . . . . . . . . . . . . . . . . . Example of Drill Down and Roll Up Example of Slice and Dice . . . . . . . . . . . . . . . . . . . . . . . . . . . Star Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Snowflake Model . . . . . . . . . . . . . . . . . Data Warehouse Development Life Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Two Approaches . . . . . . . . . . . . . . . . . . . . . . Corporate Dimensions: Step One . . . . . . . . . . . . . . . . . . . . . . Corporate Dimensions: Step Two . . . . . . . . . . . Dimensions of CelDial Required for the Case Study Initial Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Intermediate Facts Merging Fact 3 into Fact 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . Merging Fact 4 into the Result of Fact 2 and Fact 3 . . . . . . . . . . . . Final Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inventory Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Sales Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Warehouse Metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dimensional and ER Views of Product-Related Data . . . . . . . . . . . . . . . . . . The Complete Metadata Diagram for the Data Warehouse . Metadata Changes in the Production Data Warehouse Environment Use of the Warehouse Model throughout the Life Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . Base Properties of a Data Warehouse . . . . . . . . . . . . . . . . . . . . Data Warehouse Computing Context Data Marts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dimensional Modeling Activities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Schematic Notation Technique for Requirements Analysis . . . . . . . . . . . . . . . . . . . . . . Requirements Analysis Activities Requirements Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . Requirements Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . Categories of (Informal) End-User Requirements . . . . . . . . . . . . . Data Models in the Data Warehouse Modeling Process . . . . . . . . . . . . . . . . . . . . . . . . . . Overview of Initial Dimensional Modeling Notation Technique for Schematically Documenting Initial Dimensional . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Models . . . . . . . . . Facts Representing Business Transactions and Events . . . . . . . . . . . . . Inventory Fact Representing the Inventory State
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9 10 12 13 16 17 19 20 25 27 29 38 41 43 44 45 46 47 48 49 52 54 55 58 59 61 62 62 63 64 64 68 70 77 80 80 83 84 87 89 90 90 91 91 93 96 97 97 102 103

.

© Copyright IBM Corp. 1998

ix

51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90. 91. 92. 93. 94.

Inventory Fact Representing the Inventory State Changes . . . . . . . Initial Dimensional Models for Sales and Inventory . . . . . . . . . . . . Inventory State Fact at Product Component and Inventory Location Granularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inventory State Change Fact Made Unique through Adding the Inventory Movement Transaction Dimension Key . . . . . . . . . . . . . Determinant Sets of Dimension Keys for the Sales and Inventory Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . for the CelDial Case Corporate Sales and Retail Sales Facts and Their Associated . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dimensions Two Solutions for the Consolidated Sales Fact and How the Dimensions Can Be Modeled . . . . . . . . . . . . . . . . . . . . . . . . . . . Dimension Keys and Their Roles for Facts in Dimensional Models Degenerate Keys, Status Tracking Attributes, and Supportive Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . in the CelDial Model . . . . . . . . . . . . . . . . . . . . . . Requirements Validation Process Requirements Modeling Activities . . . . . . . . . . . . . . . . . . . . . . Star Model for the Sales and Inventory Facts in the CelDial Case Study Snowflake Model for the Sales and Inventory Facts in the CelDial Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Study . . . . . . . . . . . . Roll Up and Drill Down against the Inventory Fact . . . . . . Sample CelDial Dimension with Parallel Aggregation Paths Inventory and Sales Facts and Their Dimensions in the CelDial Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Study Inventory Fact and Associated Dimensions in the Extended CelDial Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Sales Fact and Associated Dimensions in the Extended CelDial Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Study . . . . . . . . . . . . . Base Calendar Elements of the Time Dimension . . . . . . . . . . . . . . . About Aggregation Paths from Week to Year Business-Related Time Dimension Model Artifacts . . . . . . . . . . . . The Time Dimension Model Incorporating Several Business-Related . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Model Artifacts . . . . . . The Time Dimension Model with Generic Business Periods The Flattened Time Dimension Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Time Variancy Issues of Keys in Dimensions Dealing with Attribute Changes in Slow-Varying Dimensions . . . . . . . . . . . . . . . . Modeling Time-Variancy of the Dimension Hierarchy Modeling Hierarchy Changes in Slow-Varying Dimensions . . . . . . . Adding Time As a Dimension to a Nontemporal Data Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . Nontemporal Model for MovieDB Temporal Modeling Styles . . . . . . . . . . . . . . . . . . . . . . . . . . . Continuous History Model . . . . . . . . . . . . . . . . . . . . . . . . . . . Different Interpretations of Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Instant and Interval Time Stamps Adding Time Stamps to the MovieDB Entities . . . . . . . . . . . . . . . Redundancy Caused by Merging Volatility Classes . . . . . . . . . . . . Director and Movie Volatility Classes . . . . . . . . . . . . . . . . . . . . Temporal Model for MovieDB . . . . . . . . . . . . . . . . . . . . . . . . . Grouping of Time-Variant Classes of Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Populating the Data Warehouse . . . . . . . . . . . . . . . . . . . . . . . . . . CelDial Organization Chart Subset of CelDial Corporate ER Model . . . . . . . . . . . . . . . . . . . Dimensional Model for CelDial Product Sales . . . . . . . . . . . . . . . Dimensional Model for CelDial Product Inventory . . . . . . . . . . . . .

104 105 107 108 109 110 111 112 115 116 117 118 118 119 120 120 122 123 127 129 130 131 131 132 134 136 138 139 140 141 142 143 143 144 145 147 148 149 149 159 166 168 169 170

x

Data Modeling Techniques for Data Warehousing

Tables
1. 2. 3. Dimensions, Measures, and Related Questions . . . Size Estimates for CelDial′ s Warehouse . . . . . . . . . . . . . . . Capture Techniques
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

56 66 160

© Copyright IBM Corp. 1998

xi

xii

Data Modeling Techniques for Data Warehousing

Preface
This redbook gives detail coverage to the topic of data modeling techniques for data warehousing, within the context of the overall data warehouse development process. The process of data warehouse modeling, including the steps required before and after the actual modeling step, is discussed. Detailed coverage of modeling techniques is presented in an evolutionary way through a gradual, but well-managed, expansion of the content of the actual data model. Coverage is also given to other important aspects of data warehousing that affect, or are affected by, the modeling process. These include architecting the warehouse and populating the data warehouse. Guidelines for selecting a data modeling tool that is appropriate for data warehousing are presented.

The Team That Wrote This Redbook
This redbook was produced by a team of specialists from around the world working for the IBM International Technical Support Organization San Jose center. Chuck Ballard was the project manager for the development of the book and is currently a data warehousing consultant at the IBM International Technical Support Organization-San Jose center. He develops, staffs, and manages projects to explore current topics in data warehousing that result in the delivery of technical workshops, papers, and IBM Redbooks. Chuck writes extensively and lectures worldwide on the subject of data warehousing. Before joining the ITSO, he worked at the IBM Santa Teresa Development Lab, where he was responsible for developing strategies, programs, and market support deliverables on data warehousing. Dirk Herreman is a senior data warehousing consultant for CIMAD Consultants in Belgium. He leads a team of data warehouse consultants, data warehouse modelers, and data and system architects for data warehousing and operates with CIMAD Consultants within IBM′s Global Services. Dirk has more than 15 years of experience with databases, most of it from an application development point of view. For the last couple of years in particular, his work has focused primarily on the development of process and architecture models and the associated techniques for evolutionary data warehouse development. As a result of this work, Dirk and his team are now the prime developers of course and workshop materials for IBM′s worldwide education curriculum for data warehouse enablement. He holds a degree in mathematics and in computer sciences from the State University of Ghent, Belgium. Don Schau is an Information Consultant for the City of Winnipeg. He holds a diploma in analysis and programming from Red River Community College. He has 20 years of experience in data processing, the last 8 in data and database management, with a focus on data warehousing in the past 2 years. His areas of expertise include data modeling and data and database management. Don currently resides in Winnipeg, Manitoba, Canada with his wife, Shelley, and their four children. Rhonda Bell is an I/T Architect in the Business Intelligence Services Practice for IBM Global Services based in Austin, Texas. She has 5 years of experience in data processing. Rhonda holds a degree in computer information systems from

© Copyright IBM Corp. 1998

xiii

Southwest Texas State University. Her areas of expertise include data modeling and client/server and data warehouse design and development. Eunsaeng Kim is an Advisory Sales Specialist in Banking, Finance and Securities Industry (BFSI) for IBM Korea. He has seven years of experience in data processing, the last five years in banking data warehouse modeling and implementation for four Korean commercial banks. He holds a degree in economics from Seoul National University in Seoul, Korea. His areas of expertise include data modeling, data warehousing, and business subjects in banking and finance industry. Eunsaeng currently resides in Seoul, Korea with his wife, Eunkyung and their two sons. Ann Valencic is a Senior Systems Specialist in the Software Services Group in IBM Australia. She has 12 years of experience in data processing, specializing in database and data warehouse. Ann′s areas of expertise include database design and performance tuning.

Comments Welcome
Your comments are important to us! We want our redbooks to be as helpful as possible. Please send us your comments about this or other redbooks in one of the following ways:
•

Fax the evaluation form found in “ITSO Redbook Evaluation” on page 197 to the fax number shown on the form. Use the electronic evaluation form found on the Redbooks Web sites: For Internet users For IBM Intranet users

•

http://www.redbooks.ibm.com http://w3.itso.ibm.com

•

Send us a note at the following address:

redbook@vnet.ibm.com

xiv

Data Modeling Techniques for Data Warehousing

Chapter 1. Introduction
Businesses of all sizes and in different industries, as well as government agencies, are finding that they can realize significant benefits by implementing a data warehouse. It is generally accepted that data warehousing provides an excellent approach for transforming the vast amounts of data that exist in these organizations into useful and reliable information for getting answers to their questions and to support the decision making process. A data warehouse provides the base for the powerful data analysis techniques that are available today such as data mining and multidimensional analysis, as well as the more traditional query and reporting. Making use of these techniques along with data warehousing can result in easier access to the information you need for more informed decision making. The question most asked now is, How do I build a data warehouse? This is a question that is not so easy to answer. As you will see in this book, there are many approaches to building one. However, at the end of all the research, planning, and architecting, you will come to realize that it all starts with a firm foundation. Whether you are building a large centralized data warehouse, one or more smaller distributed data warehouses (sometimes called data marts), or some combination of the two, you will always come to the point where you must decide on how the data is to be structured. This is, after all, one of the most key concepts in data warehousing and what differentiates it from the more typical operational database and decision support application building. That is, you structure the data and build applications around it rather than structuring applications and bringing data to them. How will you structure the data in your data warehouse? The purpose of this book is to help you with that decision. It all revolves around data modeling. Everyone will have to develop a data model; the decision is how much effort to expend on the task and what type of data model should be used. There are new data modeling techniques that have become popular in recent years and provide excellent support for data warehousing. This book discusses those techniques and offers some considerations for their selection in a data warehousing environment. Data warehouse modeling is a process that produces abstract data models for one or more database components of the data warehouse. It is one part of the overall data warehouse development process, which is comprised of other major processes such as data warehouse architecture, design, and construction. We consider the data warehouse modeling process to consist of all tasks related to requirements gathering, analysis, validation, and modeling. Typically for data warehouse development, these tasks are difficult to separate. The book covers data warehouse design only at a superficial level. This may suggest a rather broad gap between modeling and design activities, which in reality certainly is not the case. The separation between modeling and design is done for practical reasons: it is our intention to cover the modeling activities and techniques quite extensively. Therefore, covering data warehouse design as extensively simply could not be done within the scope of this book. The need to model data warehouse databases in a way that differs from modeling operational databases has been promoted in many textbooks. Some trend-setting authors and data warehouse consultants have taken this point to what we consider to be the extreme. That is, they are presenting what they are
© Copyright IBM Corp. 1998

1

calling a totally new approach to data modeling. It is called dimensional data modeling, or fact/dimension modeling. Fancy names have been invented to refer to different types of dimensional models, such as star models and snowflake models. Numerous arguments have been presented against traditional entity-relationship (ER) modeling, when used for modeling data in the data warehouse. Rather than taking this more extreme position, we believe that every technique has its area of usability. For example, we do support the many criticisms of ER modeling when considered in a specific context of data warehouse data modeling, and there are also criticisms of dimensional modeling. There are many types of data warehouse applications for which ER modeling is not well suited, especially those that address the needs of a well-identified community of data analysts interested primarily in analyzing their business measures in their business context. Likewise, there are data warehouse applications that are not well supported at all by star or snowflake models alone. For example, dimensional modeling is not very suitable for making large, corporatewide data models for a data warehouse. With the changing data warehouse landscape and the need for data warehouse modeling, the new modeling approaches and the controversies surrounding traditional modeling and the dimensional modeling approach all merit investigation. And that is another purpose of this book. Because it presents details of data warehouse modeling processes and techniques, the book can also be used as an initiating textbook for those who want to learn data warehouse modeling.

1.1 Who Should Read This Book
This book is intended for those involved in the development, implementation, maintenance, and administration of data warehouses. It is also applicable for project planners and managers involved in data warehousing. To benefit from this book, the reader should have, at least, a basic understanding of ER modeling. It is worthwhile for those responsible for developing a data warehouse to progress sequentially through the entire book. Those less directly involved in data warehouse modeling should refer to 1.2, “Structure of This Book” to determine which chapters will be of interest.

1.2 Structure of This Book
In Chapter 2, “Data Warehousing” on page 5, we begin with an exploration of the evolution of the concept of data warehousing, as it relates to data modeling for the data warehouse. We discuss the subject of data marts and distinguish them from data warehouses. After having read Chapter 1, you should have a clear perception of data modeling in the context of data mart and/or data warehouse development. Chapter 3, “Data Analysis Techniques” on page 9 surveys several methods of data analysis in data warehousing. Query and reporting, multidimensional analysis, and data mining run the spectrum of being analyst driven to analyst assisted to data driven. Because of this spectrum, each of the data analysis methods affects data modeling.

2

Data Modeling Techniques for Data Warehousing

Chapter 4, “Data Warehousing Architecture and Implementation Choices” on page 15 discusses the architecture and implementation choices available for data warehousing. The architecture of the data warehouse environment is based on where the data warehouses and/or data marts reside and where the control of the data exists. Three architecture choices are presented: the global warehouse, independent data marts, and interconnected data marts. There are several ways to implement these architecture choices: top down, bottom up, or stand alone. These three implementation choices offer flexibility in choosing an architecture and deploying the resources to create the data warehouse and/or data marts within the organization. Chapter 5, “Architecting the Data” on page 23 addresses the approaches and techniques suitable for architecting the data in the data warehouse. Information requirements can be satisfied by three types of business data: real-time, reconciled, and derived. The Enterprise Data Model (EDM) could be very helpful in data warehouse data modeling, if you have one. For example, from the EDM you could derive the general scope and understanding of the business requirements, and you could link the EDM to the physical area of interest. Also discussed in this chapter is the importance of data granularity, or level of detail of the data. Chapter 6, “Data Modeling for a Data Warehouse” on page 35 presents the basics of data modeling for the data warehouse. Two major approaches are described. First we present the highlights of ER modeling, identify the major components of ER models, and describe their properties. Next, we introduce the basic concepts of dimensional modeling and present and position two fundamental approaches: Star modeling and Snowflake. We also position the different approaches by contrasting ER and dimensional modeling, and Stars and Snowflakes. We also identify how and when the different approaches can be used as complementary, and how the different models and techniques can be mapped. In Chapter 7, “The Process of Data Warehousing” on page 49, we present a process model for data warehouse modeling. This is one of the core chapters of this book. Data modeling techniques are covered extensively in Chapter 8, “Data Warehouse Modeling Techniques” on page 81, but they can only be appreciated and well used if they are part of a well-managed data warehouse modeling process. The process model we use as the base for this book is an evolutionary, user-centric approach. It is one that focuses on end-user requirements first (rather than on the data sources) and recognizes that data warehouses and data marts typically are developed with a bottom-up approach. Chapter 8, “Data Warehouse Modeling Techniques” on page 81 covers the core data modeling techniques for the data warehouse development process. The chapter has two major sections. In the first section, we present the techniques suitable for developing a data warehouse or a data mart that suits the needs of a particular community of end users or data analysts. In the second section, we explore the data warehouse modeling techniques suitable for expanding the scope of a data mart or a data warehouse. The techniques presented in this chapter are of particular interest for those organizations that develop their data marts or data warehouses in an evolutionary way; that is, through a gradual, but well-managed, expansion of the scope of content of what has already been implemented.

Chapter 1. Introduction

3

Chapter 9, “Selecting a Modeling Tool” on page 155, an overview of the functions that a data modeling tool, or suite of tools, must support for modeling the data warehouse is presented. Also presented is a partial list of tools available at the time this redbook was written. Chapter 10, “Populating the Data Warehouse” on page 159 discusses the process of populating the data warehouse or data mart. Populating is the process of getting the source data from the operational and external systems into the data warehouse and data marts. This process consists of a capture step, a transform step, and an apply step. Also discussed in this chapter is the effect of modeling on the populating process, and, conversely, the effect of populating on modeling.

4

Data Modeling Techniques for Data Warehousing

Chapter 2. Data Warehousing
In this chapter we position data warehousing as more than just a product, or set of products—it is a solution! It is an information environment that is separate from the more typical transaction-oriented operational environment. Data warehousing is, in and of itself, an information environment that is evolving as a critical resource in today′s organizations.

2.1 A Solution, Not a Product
Often we think that a data warehouse is a product, or group of products, that we can buy to help get answers to our questions and improve our decision-making capability. But, it is not so simple. A data warehouse can help us get answers for better decision making, but it is only one part of a more global set of processes. As examples, where did the data in the data warehouse come from? How did it get into the data warehouse? How is it maintained? How is the data structured in the data warehouse? What is actually in the data warehouse? These are all questions that must be answered before a data warehouse can be built. We prefer to discuss the more global environment, and we refer to it as data warehousing. Data warehousing is the design and implementation of processes, tools, and facilities to manage and deliver complete, timely, accurate, and understandable information for decision making. It includes all the activities that make it possible for an organization to create, manage, and maintain a data warehouse or data mart.

2.2 Why Data Warehousing?
The concept of data warehousing has evolved out of the need for easy access to a structured store of quality data that can be used for decision making. It is globally accepted that information is a very powerful asset that can provide significant benefits to any organization and a competitive advantage in the business world. Organizations have vast amounts of data but have found it increasingly difficult to access it and make use of it. This is because it is in many different formats, exists on many different platforms, and resides in many different file and database structures developed by different vendors. Thus organizations have had to write and maintain perhaps hundreds of programs that are used to extract, prepare, and consolidate data for use by many different applications for analysis and reporting. Also, decision makers often want to dig deeper into the data once initial findings are made. This would typically require modification of the extract programs or development of new ones. This process is costly, inefficient, and very time consuming. Data warehousing offers a better approach. Data warehousing implements the process to access heterogeneous data sources; clean, filter, and transform the data; and store the data in a structure that is easy to access, understand, and use. The data is then used for query, reporting, and data analysis. As such, the access, use, technology, and performance requirements are completely different from those in a transaction-oriented operational environment. The volume of data in data warehousing can be very high, particularly when considering the requirements

© Copyright IBM Corp. 1998

5

for historical data analysis. Data analysis programs are often required to scan vast amounts of that data, which could result in a negative impact on operational applications, which are more performance sensitive. Therefore, there is a requirement to separate the two environments to minimize conflicts and degradation of performance in the operational environment.

2.3 Short History
The origin of the concept of data warehousing can be traced back to the early 1980s, when relational database management systems emerged as commercial products. The foundation of the relational model with its simplicity, together with the query capabilities provided by the SQL language, supported the growing interest in what then was called end-user computing or decision support . To support end-user computing environments, data was extracted from the organization′s online databases and stored in newly created database systems dedicated to supporting ad hoc end-user queries and reporting functions of all kinds. One of the prime concerns underlying the creation of these systems was the performance impact of end-user computing on the operational data processing systems. This concern prompted the requirement to separate end-user computing systems from transactional processing systems. In those early days of data warehousing, the extracts of operational data were usually snapshots or subsets of the operational data. These snapshots were loaded in an end-user computing (or decision support) database system on a regular basis, perhaps once a week or once per month. Sometimes a limited number of versions of these snapshots were even accumulated in the system while access was provided to end users equipped with query and reporting tools. Data modeling for these decision support database systems was not much of a concern. Data models for these decision support systems typically matched the data models of the operational systems because, after all, they were extracted snapshots anyhow. One of the frequently occurring remodeling issues then was to ″normalize″ the data to eliminate the nasty effects of design techniques that had been applied on the operational systems to maximize their performance, to eliminate code tables that were difficult to understand, along with other local cleanup activities. But by and large, the decision support data models were technical in nature and primarily concerned with providing data available in the operational application systems to the decision support environment. The role and purpose of data warehouses in the data processing industry have evolved considerably since those early days and are still evolving rapidly. Comparing today′s data warehouses with the early days′ decision support databases should be done with great care. Data warehouses should no longer be identified with database systems that support end-user queries and reporting functions. They should no longer be conceived as snapshots of operational data. Data warehouse databases should be considered as new sources of information, conceived for use by the whole organization or for smaller communities of users and data analysts within the organization. Simply reengineering source data models in the traditional way will no longer satisfy the requirements for data warehousing. Developing data warehouses requires a much more thoughtfully applied set of modeling techniques and a much closer working relationship with the business side of the organization. Data warehouses should also be conceived of as sources of new information. This statement sounds controversial at first, because there is global agreement that data warehouses are read-only database systems. The point is, that by

6

Data Modeling Techniques for Data Warehousing

accumulating and consolidating data from different sources, and by keeping this historical data in the warehouse, new information about the business, competitors, customers, suppliers, the behavior of the organization ′s business processes, and so forth, can be unveiled. The value of a data warehouse is no longer in being able to do ad hoc query and reporting. The real value is realized when someone gets to work with the data in the warehouse and discovers things that make a difference for the organization, whatever the objective of the analytical work may be. To achieve such interesting results, simply reengineering the source data models will not do.

Chapter 2. Data Warehousing

7

8

Data Modeling Techniques for Data Warehousing

Chapter 3. Data Analysis Techniques
A data warehouse is built to provide an easy to access source of high quality data. It is a means to an end, not the end itself. That end is typically the need to perform analysis and decision making through the use of that source of data. There are several techniques for data analysis that are in common use today. They are query and reporting, multidimensional analysis, and data mining (see Figure 1). They are used to formulate and display query results, to analyze data content by viewing it from different perspectives, and to discover patterns and clustering attributes in the data that will provide further insight into the data content.

Figure 1. Data Analysis.

Several methods of data analysis are in c o m m o n use.

The techniques of data analysis can impact the type of data model selected and its content. For example, if the intent is simply to provide query and reporting capability, a data model that structures the data in more of a normalized fashion would probably provide the fastest and easiest access to the data. Query and reporting capability primarily consists of selecting associated data elements, perhaps summarizing them and grouping them by some category, and presenting the results. Executing this type of capability typically might lead to the use of more direct table scans. For this type of capability, perhaps an ER model with a normalized and/or denormalized data structure would be most appropriate. If the objective is to perform multidimensional data analysis, a dimensional data model would be more appropriate. This type of analysis requires that the data model support a structure that enables fast and easy access to the data on the basis of any of numerous combinations of analysis dimensions. For example, you may want to know how many of a specific product were sold on a specific day, in a specific store, in a specific price range. Then for further analysis you may want to know how many stores sold a specific product, in a specific price range, on a specific day. These two questions require similar information, but one viewed from a product perspective and the other viewed from a store perspective. Multidimensional analysis requires a data model that will enable the data to easily and quickly be viewed from many possible perspectives, or dimensions.
© Copyright IBM Corp. 1998

9

Since a number of dimensions are being used, the model must provide a way for fast access to the data. If a highly normalized data structure were used, many joins would be required between the tables holding the different dimension data, and they could significantly impact performance. In this case, a dimensional data model would be most appropriate. An understanding of the data and its use will impact the choice of a data model. It also seems clear that, in most implementations, multiple types of data models might be used to best satisfy the varying requirements of the data warehouse.

3.1 Query and Reporting
Query and reporting analysis is the process of posing a question to be answered, retrieving relevant data from the data warehouse, transforming it into the appropriate context, and displaying it in a readable format. It is driven by analysts who must pose those questions to receive an answer. You will find that this is quite different, for example, from data mining, which is data driven. Refer to Figure 4 on page 13. Traditionally, queries have dealt with two dimensions, or two factors, at a time. For example, one might ask, ″How much of that product has been sold this week?″ Subsequent queries would then be posed to perhaps determine how much of the product was sold by a particular store. Figure 2 depicts the process flow in query and reporting. Query definition is the process of taking a business question or hypothesis and translating it into a query format that can be used by a particular decision support tool. When the query is executed, the tool generates the appropriate language commands to access and retrieve the requested data, which is returned in what is typically called an answer set . The data analyst then performs the required calculations and manipulations on the answer set to achieve the desired results. Those results are then formatted to fit into a display or report template that has been selected for ease of understanding by the end user. This template could consist of combinations of text, graphic images, video, and audio. Finally, the report is delivered to the end user on the desired output medium, which could be printed on paper, visualized on a computer display device, or presented audibly.

Figure 2. Query and Reporting. report delivery.

The process of query and reporting starts with query definition and ends with

10

Data Modeling Techniques for Data Warehousing

End users are primarily interested in processing numeric values, which they use to analyze the behavior of business processes, such as sales revenue and shipment quantities. They may also calculate, or investigate, quality measures such as customer satisfaction rates, delays in the business processes, and late or wrong shipments. They might also analyze the effects of business transactions or events, analyze trends, or extrapolate their predictions for the future. Often the data displayed will cause the user to formulate another query to clarify the answer set or gather more detailed information. This process continues until the desired results are reached.

3.2 Multidimensional Analysis
Multidimensional analysis has become a popular way to extend the capabilities of query and reporting. That is, rather than submitting multiple queries, data is structured to enable fast and easy access to answers to the questions that are typically asked. For example, the data would be structured to include answers to the question, ″How much of each of our products was sold on a particular day, by a particular sales person, in a particular store?″ Each separate part of that query is called a dimension . By precalculating answers to each subquery within the larger context, many answers can be readily available because the results are not recalculated with each query; they are simply accessed and displayed. For example, by having the results to the above query, one would automatically have the answer to any of the subqueries. That is, we would already know the answer to the subquery, ″How much of a particular product was sold by a particular salesperson?″ Having the data categorized by these different factors, or dimensions, makes it easier to understand, particularly by business-oriented users of the data. Dimensions can have individual entities or a hierarchy of entities, such as region, store, and department. Multidimensional analysis enables users to look at a large number of interdependent factors involved in a business problem and to view the data in complex relationships. End users are interested in exploring the data at different levels of detail, which is determined dynamically. The complex relationships can be analyzed through an iterative process that includes drilling down to lower levels of detail or rolling up to higher levels of summarization and aggregation. Figure 3 on page 12 demonstrates that the user can start by viewing the total sales for the organization and drill down to view the sales by continent, region, country, and finally by customer. Or, the user could start at customer and roll up through the different levels to finally reach total sales. Pivoting in the data can also be used. This is a data analysis operation whereby the user takes a different viewpoint than is typical on the results of the analysis, changing the way the dimensions are arranged in the result. Like query and reporting, multidimensional analysis continues until no more drilling down or rolling up is performed.

Chapter 3. Data Analysis Techniques

11

Figure 3. Drill-Down and Roll-Up Analysis. multidimensional analysis.

End users can perform drill down or roll up when using

3.3 Data Mining
Data mining is a relatively new data analysis technique. It is very different from query and reporting and multidimensional analysis in that is uses what is called a discovery technique . That is, you do not ask a particular question of the data but rather use specific algorithms that analyze the data and report what they have discovered. Unlike query and reporting and multidimensional analysis where the user has to create and execute queries based on hypotheses, data mining searches for answers to questions that may have not been previously asked. This discovery could take the form of finding significance in relationships between certain data elements, a clustering together of specific data elements, or other patterns in the usage of specific sets of data elements. After finding these patterns, the algorithms can infer rules. These rules can then be used to generate a model that can predict a desired behavior, identify relationships among the data, discover patterns, and group clusters of records with similar attributes. Data mining is most typically used for statistical data analysis and knowledge discovery. Statistical data analysis detects unusual patterns in data and applies statistical and mathematical modeling techniques to explain the patterns. The models are then used to forecast and predict. Types of statistical data analysis techniques include linear and nonlinear analysis, regression analysis, multivariant analysis, and time series analysis. Knowledge discovery extracts implicit, previously unknown information from the data. This often results in uncovering unknown business facts. Data mining is data driven (see Figure 4 on page 13). There is a high level of complexity in stored data and data interrelations in the data warehouse that are difficult to discover without data mining. Data mining offers new insights into the business that may not be discovered with query and reporting or multidimensional analysis. Data mining can help discover new insights about the business by giving us answers to questions we might never have thought to ask.

12

Data Modeling Techniques for Data Warehousing

Figure 4. Data Mining. questions.

Data M i n i n g focuses on analyzing the data content rather than simply responding to

3.4 Importance to Modeling
The type of analysis that will be done with the data warehouse can determine the type of model and the model′s contents. Because query and reporting and multidimensional analysis require summarization and explicit metadata, it is important that the model contain these elements. Also, multidimensional analysis usually entails drilling down and rolling up, so these characteristics need to be in the model as well. A clean and clear data warehouse model is a requirement, else the end users′ tasks will become too complex, and end users will stop trusting the contents of the data warehouse and the information drawn from it because of highly inconsistent results. Data mining, however, usually works best with the lowest level of detail available. Thus, if the data warehouse is used for data mining, a low level of detail data should be included in the model.

Chapter 3. Data Analysis Techniques

13

14

Data Modeling Techniques for Data Warehousing

Chapter 4. Data Warehousing Architecture and Implementation Choices
In this chapter we discuss the architecture and implementation choices available for data warehousing. During the discussions we may use the term data mart . Data marts, simply defined, are smaller data warehouses that can function independently or can be interconnected to form a global integrated data warehouse. However, in this book, unless noted otherwise, use of the term data warehouse also implies data mart. Although it is not always the case, choosing an architecture should be done prior to beginning implementation. The architecture can be determined, or modified, after implementation begins. However, a longer delay typically means an increased volume of rework. And, everyone knows that it is more time consuming and difficult to do rework after the fact than to do it right, or very close to right, the first time. The architecture choice selected is a management decision that will be based on such factors as the current infrastructure, business environment, desired management and control structure, commitment to and scope of the implementation effort, capability of the technical environment the organization employs, and resources available. The implementation approach selected is also a management decision, and one that can have a dramatic impact on the success of a data warehousing project. The variables affected by that choice are time to completion, return-on-investment, speed of benefit realization, user satisfaction, potential implementation rework, resource requirements needed at any point-in-time, and the data warehouse architecture selected.

4.1 Architecture Choices
Selection of an architecture will determine, or be determined by, where the data warehouses and/or data marts themselves will reside and where the control resides. For example, the data can reside in a central location that is managed centrally. Or, the data can reside in distributed local and/or remote locations that are either managed centrally or independently. The architecture choices we consider in this book are global, independent, interconnected, or some combination of all three. The implementation choices to be considered are top down, bottom up, or a combination of both. It should be understood that the architecture choices and the implementation choices can also be used in combinations. For example, a data warehouse architecture could be physically distributed, managed centrally, and implemented from the bottom up starting with data marts that service a particular workgroup, department, or line of business.

4.1.1 Global Warehouse Architecture
A global data warehouse is considered one that will support all, or a large part, of the corporation that has the requirement for a more fully integrated data warehouse with a high degree of data access and usage across departments or lines-of-business. That is, it is designed and constructed based on the needs of the enterprise as a whole. It could be considered to be a common repository for

© Copyright IBM Corp. 1998

15

decision support data that is available across the entire organization, or a large subset thereof. A common misconception is that a global data warehouse is centralized. The term global is used here to reflect the scope of data access and usage, not the physical structure. The global data warehouse can be physically centralized or physically distributed throughout the organization. A physically centralized global warehouse is to be used by the entire organization that resides in a single location and is managed by the Information Systems (IS) department. A distributed global warehouse is also to be used by the entire organization, but it distributes the data across multiple physical locations within the organization and is managed by the IS department. When we say that the IS department manages the data warehouse, we do not necessarily mean that it controls the data warehouse. For example, the distributed locations could be controlled by a particular department or line of business. That is, they decide what data goes into the data warehouse, when it is updated, which other departments or lines of business can access it, which individuals in those departments can access it, and so forth. However, to manage the implementation of these choices requires support in a more global context, and that support would typically be provided by IS. For example, IS would typically manage network connections. Figure 5 shows the two ways that a global warehouse can be implemented. In the top part of the figure, you see that the data warehouse is distributed across three physical locations. In the bottom part of the figure, the data warehouse resides in a single, centralized location.

Figure 5. Global Warehouse Architecture.

The two primary architecture approaches.

Data for the data warehouse is typically extracted from operational systems and possibly from data sources external to the organization with batch processes during off-peak operational hours. It is then filtered to eliminate any unwanted data items and transformed to meet the data quality and usability requirements. It is then loaded into the appropriate data warehouse databases for access by end users.

16

Data Modeling Techniques for Data Warehousing

A global warehouse architecture enables end users to have more of an enterprisewide or corporatewide view of the data. It should be certain that this is a requirement, however, because this type of environment can be very time consuming and costly to implement.

4.1.2 Independent Data Mart Architecture
An independent data mart architecture implies stand-alone data marts that are controlled by a particular workgroup, department, or line of business and are built solely to meet their needs. There may, in fact, not even be any connectivity with data marts in other workgroups, departments, or lines of business. For example, data for these data marts may be generated internally. The data may be extracted from operational systems but would then require the support of IS. IS would not control the implementation but would simply help manage the environment. Data could also be extracted from sources of data external to the organization. In this case IS could be involved unless the appropriate skills were available within the workgroup, department, or line of business. The top part of Figure 6 depicts the independent data mart structure. Although the figure depicts the data coming from operational or external data sources, it could also come from a global data warehouse if one exists. The independent data mart architecture requires some technical skills to implement, but the resources and personnel could be owned and managed by the workgroup, department, or line of business. These types of implementation typically have minimal impact on IS resources and can result in a very fast implementation. However, the minimal integration and lack of a more global view of the data can be a constraint. That is, the data in any particular data mart will be accessible only to those in the workgroup, department, or line of business that owns the data mart. Be sure that this is a known and accepted situation.

Figure 6. Data Mart Architectures.

They can be independent or interconnected.

Chapter 4. Data Warehousing Architecture and Implementation Choices

17

4.1.3 Interconnected Data Mart Architecture
An interconnected data mart architecture is basically a distributed implementation. Although separate data marts are implemented in a particular workgroup, department, or line of business, they can be integrated, or interconnected, to provide a more enterprisewide or corporatewide view of the data. In fact, at the highest level of integration, they can become the global data warehouse. Therefore, end users in one department can access and use the data on a data mart in another department. This architecture is depicted in the bottom of Figure 6 on page 17. Although the figure depicts the data coming from operational or external data sources, it could also come from a global data warehouse if one exists. This architecture brings with it many other functions and capabilities that can be selected. Be aware, however, that these additional choices can bring with them additional integration requirements and complexity as compared to the independent data mart architecture. For example, you will now need to consider who controls and manages the environment. You will need to consider the need for another tier in the architecture to contain, for example, data common to multiple data marts. Or, you may need to elect a data sharing schema across the data marts. Either of these choices adds a degree of complexity to the architecture. But, on the positive side, there can be significant benefit to the more global view of the data. Interconnected data marts can be independently controlled by a workgroup, department, or line of business. They decide what source data to load into the data mart, when to update it, who can access it, and where it resides. They may also elect to provide the tools and skills necessary to implement the data mart themselves. In this case, minimal resources would be required from IS. IS could, for example, provide help in cross-department security, backup and recovery, and the network connectivity aspects of the implementation. In contrast, interconnected data marts could be controlled and managed by IS. Each workgroup, department, or line of business would have its own data mart, but the tools, skills, and resources necessary to implement the data marts would be provided by IS.

4.2 Implementation Choices
Several approaches can be used to implement the architectures discussed in 4.1, “Architecture Choices” on page 15. The approaches to be discussed in this book are top down, bottom up, or a combination of both. These implementation choices offer flexibility in determining the criteria that are important in any particular implementation. The choice of an implementation approach is influenced by such factors as the current IS infrastructure, resources available, the architecture selected, scope of the implementation, the need for more global data access across the organization, return-on-investment requirements, and speed of implementation.

18

Data Modeling Techniques for Data Warehousing

4.2.1 Top Down Implementation
A top down implementation requires more planning and design work to be completed at the beginning of the project. This brings with it the need to involve people from each of the workgroups, departments, or lines of business that will be participating in the data warehouse implementation. Decisions concerning data sources to be used, security, data structure, data quality, data standards, and an overall data model will typically need to be completed before actual implementation begins. The top down implementation can also imply more of a need for an enterprisewide or corporatewide data warehouse with a higher degree of cross workgroup, department, or line of business access to the data. This approach is depicted in Figure 7. As shown, with this approach, it is more typical to structure a global data warehouse. If data marts are included in the configuration, they are typically built afterward. And, they are more typically populated from the global data warehouse rather than directly from the operational or external data sources.

Figure 7. Top Down Implementation.

Creating a corporate infrastructure first.

A top down implementation can result in more consistent data definitions and the enforcement of business rules across the organization, from the beginning. However, the cost of the initial planning and design can be significant. It is a time-consuming process and can delay actual implementation, benefits, and return-on-investment. For example, it is difficult and time consuming to determine, and get agreement on, the data definitions and business rules among all the different workgroups, departments, and lines of business participating. Developing a global data model is also a lengthy task. In many organizations, management is becoming less and less willing to accept these delays. The top down implementation approach can work well when there is a good centralized IS organization that is responsible for all hardware and other computer resources. In many organizations, the workgroups, departments, or lines of business may not have the resources to implement their own data marts. Top down implementation will also be difficult to implement in organizations where the workgroup, department, or line of business has its own IS resources. They are typically unwilling to wait for a more global infrastructure to be put in place.

Chapter 4. Data Warehousing Architecture and Implementation Choices

19

4.2.2 Bottom Up Implementation
A bottom up implementation involves the planning and designing of data marts without waiting for a more global infrastructure to be put in place. This does not mean that a more global infrastructure will not be developed; it will be built incrementally as initial data mart implementations expand. This approach is more widely accepted today than the top down approach because immediate results from the data marts can be realized and used as justification for expanding to a more global implementation. Figure 8 depicts the bottom up approach. In contrast to the top down approach, data marts can be built before, or in parallel with, a global data warehouse. And as the figure shows, data marts can be populated either from a global data warehouse or directly from the operational or external data sources.

Figure 8. Bottom Up Implementation.

Starts with a data mart and expands over time.

The bottom up implementation approach has become the choice of many organizations, especially business management, because of the faster payback. It enables faster results because data marts have a less complex design than a global data warehouse. In addition, the initial implementation is usually less expensive in terms of hardware and other resources than deploying the global data warehouse. Along with the positive aspects of the bottom up approach are some considerations. For example, as more data marts are created, data redundancy and inconsistency between the data marts can occur. With careful planning, monitoring, and design guidelines, this can be minimized. Multiple data marts may bring with them an increased load on operational systems because more data extract operations are required. Integration of the data marts into a more global environment, if that is the desire, can be difficult unless some degree of planning has been done. Some rework may also be required as the implementation grows and new issues are uncovered that force a change to the existing areas of the implementation. These are all considerations to be carefully understood before selecting the bottom up approach.

20

Data Modeling Techniques for Data Warehousing

4.2.3 A Combined Approach
As we have seen, there are both positive and negative considerations when implementing with the top down or the bottom up approach. In many cases the best approach may be a combination of the two. This can be a difficult balancing act, but with a good project manager it can be done. One of the keys to this approach is to determine the degree of planning and design that is required for the global approach to support integration as the data marts are being built with the bottom up approach. Develop a base level infrastructure definition for the global data warehouse, being careful to stay, initially, at a business level. For example, as a first step simply identify the lines of business that will be participating. A high level view of the business processes and data areas of interest to them will provide the elements for a plan for implementation of the data marts. As data marts are implemented, develop a plan for how to handle the data elements that are needed by multiple data marts. This could be the start of a more global data warehouse structure or simply a common data store accessible by all the data marts. It some cases it may be appropriate to duplicate the data across multiple data marts. This is a trade-off decision between storage space, ease of access, and the impact of data redundancy along with the requirement to keep the data in the multiple data marts at the same level of consistency. There are many issues to be resolved in any data warehousing implementation. Using the combined approach can enable resolution of these issues as they are encountered, and in the smaller scope of a data mart rather than a global data warehouse. Careful monitoring of the implementation processes and management of the issues could result in gaining the best benefits of both implementation techniques.

Chapter 4. Data Warehousing Architecture and Implementation Choices

21

22

Data Modeling Techniques for Data Warehousing

Chapter 5. Architecting the Data
A data warehouse is, by definition, a subject-oriented, integrated, time-variant collection of data to enable decision making across a disparate group of users. One of the most basic concepts of data warehousing is to clean, filter, transform, summarize, and aggregate the data, and then put it in a structure for easy access and analysis by those users. But, that structure must first be defined and that is the task of the data warehouse model. In modeling a data warehouse, we begin by architecting the data. By architecting the data, we structure and locate it according to its characteristics. In this chapter, we review the types of data used in data warehousing and provide some basic hints and tips for architecting that data. We then discuss approaches to developing a data warehouse data model along with some of the considerations. Having an enterprise data model (EDM) available would be very helpful, but not required, in developing the data warehouse data model. For example, from the EDM you can derive the general scope and understanding of the business requirements. The EDM would also let you relate the data elements and the physical design to a specific area of interest. Data granularity is one of the most important criteria in architecting the data. On one hand, having data of a high granularity can support any query. However, having a large volume of data that must be manipulated and managed could be an issue as it would impact response times. On the other hand, having data of a low granularity would support only specific queries. But, with the reduced volume of data, you would realize significant improvements in performance. The size of a data warehouse varies, but they are typically quite large. This is especially true as you consider the impact of storing volumes of historical data. To deal with this issue you have to consider data partitioning in the data architecture. We consider both logical and physical partitioning to better understand and maintain the data. In logical partitioning of data, you should consider the concept of subject areas . This concept is typically used in most information engineering (IE) methodologies. We discuss subject areas and their different definitions in more detail later in this chapter.

5.1 Structuring the Data
In structuring the data, for data warehousing, we can distinguish three basic types of data that can be used to satisfy the requirements of an organization:
• • •

Real-time data Derived data Reconciled data

In this section, we describe these three types of data according to usage, scope, and currency. You can configure an appropriate data warehouse based on these three data types, with consideration for the requirements of any particular implementation effort. Depending on the nature of the operational systems, the type of business, and the number of users that access the data warehouse, you

© Copyright IBM Corp. 1998

23

can combine the three types of data to create the most appropriate architecture for the data warehouse.

5.1.1 Real-Time Data
Real-time data represents the current status of the business. It is typically used by operational applications to run the business and is constantly changing as operational transactions are processed. Real-time data is at a detailed level, meaning high granularity, and is usually accessed in read/write mode by the operational transactions. Not confined to operational systems, real-time data is extracted and distributed to informational systems throughout the organization. For example, in the banking industry, where real-time data is critical for operational management and tactical decision making, an independent system, the so-called deferred or delayed system, delivers the data from the operational systems to the informational systems (data warehouses) for data analysis and more strategic decision making. To use real-time data in a data warehouse, typically it first must be cleansed to ensure appropriate data quality, perhaps summarized, and transformed into a format more easily understood and manipulated by business analysts. This is because the real-time data contains all the individual, transactional, and detailed data values as well as other data valuable only to the operational systems that must be filtered out. In addition, because it may come from multiple different systems, real-time data may not be consistent in representation and meaning. As an example, the units of measure, currency, and exchange rates may differ among systems. These anomalies must be reconciled before loading into the data warehouse.

5.1.2 Derived Data
Derived data is data that has been created perhaps by summarizing, averaging, or aggregating the real-time data through some process. Derived data can be either detailed or summarized, based on requirements. It can represent a view of the business at a specific point in time or be a historical record of the business over some period of time. Derived data is traditionally used for data analysis and decision making. Data analysts seldom need large volumes of detailed data; rather they need summaries that are much easier for manipulation and use. Manipulating large volumes of atomic data can also require tremendous processing resources. Considering the requirements for improved query processing capability, an efficient approach is to precalculate derived data elements and summarize the detailed data to better meet user requirements. Efficiently processing large volumes of data in an appropriate amount of time is one of the most important issues to resolve.

5.1.3 Reconciled Data
Reconciled data is real-time data that has been cleansed, adjusted, or enhanced to provide an integrated source of quality data that can be used by data analysts. The basic requirement for data quality is consistency. In addition, we can create and maintain historical data while reconciling the data. Thus, we can say reconciled data is a special type of derived data.

24

Data Modeling Techniques for Data Warehousing

Reconciled data is seldom explicitly defined. It is usually a logical result of derivation operations. Sometimes reconciled data is stored only as temporary files that are required to transform operational data for consistency.

5.2 Enterprise Data Model
An EDM is a consistent definition of all of the data elements common to the business, from a high-level business view to a generic logical data design. It includes links to the physical data designs of individual applications. Through an EDM, you can derive the general scope and understanding of the business requirements.

5.2.1 Phased Enterprise Data Modeling
Many methodologies for enterprise data modeling have been published. Some publications propose a three-tiered methodology such as conceptual, logical, and physical data model. In IBM′s Worldwide Solution Design and Delivery Method (WSDDM), five tiers are described for information engineering approaches:
• • • • •

ISP - Information System Planning BAA - Business Area Analysis BSD - Business System Design BSI - Business System Implementation BSM - Business System Maintenance

Despite the differences of the number of tiers, the common thread is that every methodology focuses on the phased or layered approach. The phases include the tasks for information planning, business analyzing, logical data modeling, and physical data design as shown on Figure 9.

Figure 9. The Phased Enterprise Data Model (EDM)

The size of the phases in Figure 9 represents the amount of information to be included in that phase of the model. That is, the pyramid shape implies that the amount of information is minimal at the planning phase but increases remarkably at the physical data design phase. The information planning phase at the top of the pyramid provides the highly consolidated view of the business. In that view, we can identify some number of business concepts, usually in the range of 10 to 30. Those business concepts

Chapter 5. Architecting the Data

25

can be categorized as a business entity , super entity , or subject area in which an organization is interested and about which it maintains data elements. Examples of those are customer, product, organization, schedule, activity, and policy. The purpose of this phase is to set up the scope and architecture of a data warehouse and to provide a single, comprehensive point of view for the other phases. The business analyzing phase provides a means of further defining the contents of the primary concepts and categorizing those contents according to various business rules. This phase is described in business terms so that business people who have no modeling training can understand it. The purpose of this phase is to gather and arrange business requirements and define the business terms specifically. The logical data modeling phase is primarily enterprisewide in scope and generic to all applications located below it in the pyramid. The logical data model typically consists of several hundred entities. It is a complete model that is in third normal form and contains the identification and definition of all entities, relationships, and attributes. For further specific analysis, the entities of the logical data model are sometimes partitioned into views by subject areas or by applications. Some methodologies divide this phase into two phases:
• •

Generic logical data model - the enterprise level Logical application model - application level of data view

The physical data design applies physical constraints, such as space, performance, and the physical distribution of data. The purpose of this phase is to design for the actual physical implementation.

5.2.2 A Simple Enterprise Data Model
In general it is not possible, or practical, to assign resources to all of the development phases concurrently when constructing an EDM. However, some of the core components that are required for data warehouse modeling can be extracted and grouped and used as a phased approach. In this book we call that phased approach a simple EDM . Figure 10 on page 27 shows an example of a simple EDM that consists of subject areas and the relationships among them. We suggest drawing a simple EDM diagram for each subject you select for your data warehouse model. For a simple EDM, make a list of subject areas, typically less than 25. Then, draw a subject area model of the organization by defining the business relationships among the subject areas. When you have completed the subject area model, you will need to define the contents of each subject area. For example, when you define customer , you cannot simply say that customer is the person in an organization that has a business relationship with your organization. For example, you must make it clear whether the person includes a prospect or ex-customer. When referring to the organization , be clear as to whether it can be only a registered business, and not simply a social or civic interest group. If possible, draw an ER diagram for each subject area. Do not be too concerned about the details, such as relationship name and cardinality. Just identify the primary entities and the relationships among them.

26

Data Modeling Techniques for Data Warehousing

Figure 10. A Simple Enterprise Data Model

The objective of a simple EDM is to scope a specific area of interest and develop a general understanding of it. It will be very helpful in the development of your data warehouse model.

5.2.3 The Benefits of EDM
Compared to an application or departmental model, an EDM has these benefits:
•

• •

Provides a single development base for applications and promotes the integration of existing applications Supports the sharing of data among different areas of the business Provides a single set of consistent data definitions

The benefits of the EDM are being challenged today because a number of organizations have attempted to create them and have been largely unsuccessful. The following are some of the reasons for this lack of success:
•

The scope of EDM tends to cover the entire enterprise. Therefore, the size of the project tended to be so big that it seldom delivered the proper results in a reasonable period of time. To deliver the value of EDM to the business, all areas of the organization must be involved concurrently, which is an unrealistic expectation. The people required in an EDM project must have both a broad understanding of the business and a detailed knowledge of a specific business area. It is difficult to find such people, but even if you can, it is more difficult to get them assigned to the modeling task rather than performing their standard business duties.

•

•

Chapter 5. Architecting the Data

27

The above reasons are certainly cause for concern, but we consider them challenges rather than reasons to avoid pursuit of an EDM. It is still a valuable item to have and can be very helpful in creating the data warehouse model. To help ease the effort of creating an EDM, many industry-specific template data models are available to use as a starting point. For example, there is the Financial Services Data Model (FSDM) for the finance industry available from IBM. Through customizing the templates, you can reduce the modeling period and required resources while at the same time experience the stable benefits of an EDM. If an organization has no EDM and no plans to create one, you can still receive many of the benefits by creating a simple EDM. Whether the scope of the data warehouse is for the entire enterprise or for a specific business area, a simple EDM adds value. If you already have several data models for specific applications, you can make use of them in creating the simple EDM. For example, you can extract common components from application data models and integrate them into the simple EDM. Integration is always a virtue in data warehousing.

5.3 Data Granularity Model
In the physical design phase for data modeling, one of the most important aspects of the design is related to the granularity of the data. In this section we describe what we mean by granularity in the context of a data warehouse and explain how to structure data to minimize or eliminate any loss of information from using this valuable construct.

5.3.1 Granularity of Data in the Data Warehouse
Granularity of data in the data warehouse is concerned with the level of summarization of the data elements. It refers then, actually, to the level of detail available in the data elements. The more detail data that is available, the lower the level of granularity. Conversely, the lower the level of detail, the higher the level of granularity (or level of summarization of the data elements). Granularity is important in data warehouse modeling because it offers the opportunity for trade-off between important issues in data warehousing. For example, one trade-off could be performance versus volume of data (and the related cost of storing that data). Another example might be a trade-off between the ability to access data at a very detailed level versus performance and the cost of storing and accessing large volumes of data. Selecting the appropriate level of granularity significantly affects the volume of data in the data warehouse. Along with that, selecting the appropriate level of granularity determines the capability of the data warehouse to enable answers to different types of queries. To help make this clear, refer to the example shown in Figure 11 on page 29. Here we are looking at transaction data for a bank account. On the left side of the figure, let′s say that 50 is the average number of transaction per account and the size of the record for a transaction is 150 bytes. As the result, it would require about 7.5 KB to keep the very detailed transaction records to the end of the month. On the right side of the figure, a less detailed set of data (with a higher level of granularity) is shown in the form of summary by account per month. Here, all the transactions for an account are summarized in only one record. The summary record would require longer record size, perhaps 200 bytes instead of the 150 bytes of the raw transaction, but the result is a significant savings in storage space.

28

Data Modeling Techniques for Data Warehousing

Figure 11. Granularity of Data:.

The Level of Detail Trade-off

In terms of disk space and volume of data, a higher granularity provides a more efficient way of storing data than a lower granularity. You would also have to consider the disk space for the index of the data as well. This makes the space savings even greater. Perhaps a greater concern is with the manipulation of large volumes of data. This can impact performance at the cost of more processing power. There are always trade-offs to be made in data processing, and this is no exception. For example, as the granularity becomes higher, the ability to answer different types of queries (that require data at a more detailed level) diminishes. If you have very low level of granularity, you can support any queries using that data at the cost of increased storage space and diminished performance. Let′s look again at the example in Figure 11. With a low level of granularity you could answer the query, ″How many credit transactions were there for John′ s demand deposit account in the San Jose branch last week?″ With the higher level of granularity, you cannot answer that question because the data is summarized by month rather than by week. If the granularity does not impact the ability to answer a specific query, the amount of system resources required for that same query could still differ considerably. Suppose that you have two tables with different levels of granularity, such as transaction details and monthly account summary. To answer a query about the monthly report for channel utilization by accounts, you could use either of those two tables without any dependency on the level of granularity. However, using the detailed transaction table requires a significantly higher volume of disk activity to scan all the data as well as additional processing power for calculation of the results. Using the monthly account summary table would require much less resource. In deciding on the level of granularity, you must always consider the trade-off between the cost of the volume of data and the ability to answer queries.

Chapter 5. Architecting the Data

29

5.3.2 Multigranularity Modeling in the Corporate Environment
In organizations that have large volumes of data, multiple levels of granularity could be considered to overcome the trade-offs. For example, we could divide the data in a data warehouse into detailed raw data and summarized data. Detailed raw data is the lowest level of detailed transaction data without any aggregation and summarization. At this level, the data volume could be extremely large. It may actually have to be on a separate storage medium such as magnetic tape or an optical disk device when it is not being used. The data could be loaded to disk for easy and faster access only during those times when it is required. Summarized data is transaction data aggregated at the level required for the most typically used queries. In the banking example used previously, this might be at the level of customer accounts. A much lower volume of data is required for the summarized data source as compared to the detailed raw data. Of course, there is a limit to the number of queries and level of detail that can be extracted from the summarized data. By creating two levels of granularity in a data warehouse, you can overcome the trade-off between volume of data and query capability. The summarized level of data supports almost all queries with the reduced amount of resources, and the detailed raw data supports the limited number of queries requiring a detailed level of data. What we mean by summarized may still not be clear. The issue here is about what the criteria will be for determining the level of summarization that should be used in various situations. The answer requires a certain amount of intuition and experience in the business. For example, if you summarize the data at a very low level of detail, there will be few differences from the detailed raw data. If you summarize the data at too high a level of detail, many queries must be satisfied by using the detailed raw data. Therefore, in the beginning, simply using intuition may be the rule. Then, over time, analytical iterative processes can be refined to enhance or verify the intuition. Collecting statistics on the usage of the various sources of data will provide input for the processes. By structuring the data into multiple levels of summarized data, you can extend the analysis of dual levels of granularity into multiple levels of granularity based on the business requirements and the capacity of the data warehouse of each organization. You will find more detail and examples of techniques for implementing multigranularity modeling in Chapter 8, “Data Warehouse Modeling Techniques” on page 81.

5.4 Logical Data Partitioning Model
To better understand, maintain, and navigate the data warehouse, we can define both logical and physical partitions. Physical partitioning can be designed according to the physical implementation requirements and constraints. In data warehouse modeling, logical data partitioning is very important because it affects physical partitioning not only for overall structure but also detailed table partitioning. In this section we describe why and how the data is partitioned. The subject area is the most common criterion for determining overall logical data partitioning. We can define a subject area as a portion of a data warehouse that is classified by a specific consistent perspective. The perspective is usually

30

Data Modeling Techniques for Data Warehousing

based on the characteristics of the data, such as customer, product, or account. Sometimes, however, other criteria such as time period, geography, and organizational unit become the measure for partitioning.

5.4.1 Partitioning the Data
The term partition was originally concerned with the physical status of a data structure that has been divided into two or more separate structures. However, sometimes logical partitioning of the data is required to better understand and use the data. In that case, the descriptions of logical partitioning overlap with physical partitioning.

5.4.1.1 The Goals of Partitioning
Partitioning the data in the data warehouse enables the accomplishment of several critical goals. For example, it can:
• • • •

Provide flexible access to data Provide easy and efficient data management services Ensure scalability of the data warehouse Enable elements of the data warehouse to be portable. That is, certain elements of the data warehouse can be shared with other physical warehouses or archived on other storage media.

We usually partition large volumes of current detail data by splitting it into smaller pieces. Doing that helps make the data easier to:
• • • • • •

Restructure Index Sequentially scan Reorganize Recover Monitor

5.4.1.2 The Criteria of Partitioning
For the question of how to partition the data in a data warehouse, there are a number of important criteria to consider. As examples, the data can be partitioned according to several of the following criteria:
• • • • •

Time period (date, month, or quarter) Geography (location) Product (more generically, by line of business) Organizational unit A combination of the above

The choice of criteria is based on the business requirements and physical database constraints. Nevertheless, time period must always be considered when you decide to partition data. Every database management system (DBMS) has its own specific way of implementing physical partitioning, and they all can be quite different. And, a very important consideration when selecting the DBMS on which the data resides is support for partition indexing. Instead of DBMS or system level of partitioning, you can consider partitioning by application. This would provide flexibility in defining data over time, and portability in moving to the other data warehouses. Notice that the issue of partitioning is closely related to

Chapter 5. Architecting the Data

31

multidimensional modeling, data granularity modeling, and the capabilities of a particular DBMS to support data warehousing.

5.4.2 Subject Area
When you consider the partitioning of the data in a data warehouse, the most common criterion is subject area. As you will remember, a data warehouse is subject oriented; that is, it is oriented to specific selected subject areas in the organization such as customer and product. This is quite different from partitioning in the operational environment. In the operational environment, partitioning is more typically by application or function because the operational environment has been built around transaction-oriented applications that perform a specific set of functions. And, typically, the objective is to perform those functions as quickly as possible. If there are queries performed in the operational environment, they are more tactical in nature and are to answer a question concerned with that instant in time. An example might be, ″Is the check for Mr. Smith payable or not?″ Queries in the data warehouse environment are more strategic in nature and are to answer questions concerned with a larger scope. An example might be ″What products are selling well?″ or ″Where are my weakest sales offices?″ To answer those questions, the data warehouse should be structured and oriented to subject areas such as product or organization. As such, subject areas are the most common unit of logical partitioning in the data warehouse. Subject areas are roughly classified by the topics of interest to the business. To extract a candidate list of potential subject areas, you should first consider what your business interests are. Examples are customers, profit, sales, organizations, and products. To help in determining the subject areas, you could use a technique that has been successful for many organizations, namely, the 5W1H rule ; that is, the when, where, who, what, why, and how of your business interests. For example, for answering the who question, your business interests might be in customer, employee, manager, supplier, business partner, and competitor. After you extract a list of candidate subject areas, you decompose, rearrange, select, and redefine them more clearly. As a result, you can get a list of subject areas that best represent your organization. We suggest that you make a hierarchy or grouping with them to provide a clear definition of what they are and how they relate to each other. As a practical example of subject areas, consider the following list taken from the FSDM:
• • • • • • • • •

Arrangement Business direction item Classification Condition Event Involved party Location Product Resource item

The above list of nine subject areas can be decomposed into several other subject areas. For example, arrangement consists of several subject areas such as customer arrangement, facility arrangement, and security arrangement.

32

Data Modeling Techniques for Data Warehousing

Once you have a list of subject areas, you have to define the business relationships among them. The relationships are good starting points for determining the dimensions that might be used in a dimensional data warehouse model because a subject area is a perspective of the business about which you are interested. In data warehouse modeling, subject areas help define the following criteria:
• • • •

Unit of the data model Unit of an implementation project Unit of management of the data Basis for the integration of multiple implementations

Assuming that the main role of subject area is the determination of the unit for effective analysis, modeling, and implementation of the data warehouse, then the other criteria such as business function, process, specific applications, or organizational unit can be the measure for the subject area. In dimensional modeling, the best unit of analysis is the business process area in which the organization has the most interest. For a practical implementation of a data warehouse, it is suggested that the unit of measure be the business process area.

Chapter 5. Architecting the Data

33

34

Data Modeling Techniques for Data Warehousing

Chapter 6. Data Modeling for a Data Warehouse
This chapter provides you with a basic understanding of data modeling, specifically for the purpose of implementing a data warehouse. Data warehousing has become generally accepted as the best approach for providing an integrated, consistent source of data for use in data analysis and business decision making. However, data warehousing can present complex issues and require significant time and resources to implement. This is especially true when implementing on a corporatewide basis. To receive benefits faster, the implementation approach of choice has become bottom up with data marts. Implementing in these small increments of small scope provides a larger return-on-investment in a short amount of time. Implementing data marts does not preclude the implementation of a global data warehouse. It has been shown that data marts can scale up or be integrated to provide a global data warehouse solution for an organization. Whether you approach data warehousing from a global perspective or begin by implementing data marts, the benefits from data warehousing are significant. The question then becomes, How should the data warehouse databases be designed to best support the needs of the data warehouse users? Answering that question is the task of the data modeler. Data modeling is, by necessity, part of every data processing task, and data warehousing is no exception. As we discuss this topic, unless otherwise specified, the term data warehouse also implies data mart . We consider two basic data modeling techniques in this book: ER modeling and dimensional modeling. In the operational environment, the ER modeling technique has been the technique of choice. With the advent of data warehousing, the requirement has emerged for a technique that supports a data analysis environment. Although ER models can be used to support a data warehouse environment, there is now an increased interest in dimensional modeling for that task. In this chapter, we review why data modeling is important for data warehousing. Then we describe the basic concepts and characteristics of ER modeling and dimensional modeling.

6.1 Why Data Modeling Is Important
Visualization of the business world: Generally speaking, a model is an abstraction and reflection of the real world. Modeling gives us the ability to visualize what we cannot yet realize. It is the same with data modeling.
Traditionally, data modelers have made use of the ER diagram, developed as part of the data modeling process, as a communication media with the business end users. The ER diagram is a tool that can help in the analysis of business requirements and in the design of the resulting data structure. Dimensional modeling gives us an improved capability to visualize the very abstract questions that the business end users are required to answer. Utilizing dimensional modeling, end users can easily understand and navigate the data structure and fully exploit the data.

© Copyright IBM Corp. 1998

35

Actually, data is simply a record of all business activities, resources, and results of the organization. The data model is a well-organized abstraction of that data. So, it is quite natural that the data model has become the best method to understand and manage the business of the organization. Without a data model, it would be very difficult to organize the structure and contents of the data in the data warehouse.

The essence of the data warehouse architecture: In addition to the benefit of visualization, the data model plays the role of a guideline, or plan, to implement the data warehouse. Traditionally, ER modeling has primarily focused on eliminating data redundancy and keeping consistency among the different data sources and applications. Consolidating the data models of each business area before the real implementation can help assure that the result will be an effective data warehouse and can help reduce the cost of implementation. Different approaches of data modeling: ER and dimensional modeling, although related, are very different from each other. There is much debate as to which method is best and the conditions under which a particular technique should be selected. There can be no definite answer on which is best, but there are guidelines on which would be the better selection in a particular set of circumstances or in a particular environment. In the following sections, we review and define the modeling techniques and provide some selection guidelines.

6.2 Data Modeling Techniques
Two data modeling techniques that are relevant in a data warehousing environment are ER modeling and dimensional modeling. ER modeling produces a data model of the specific area of interest, using two basic concepts: entities and the relationships between those entities. Detailed ER models also contain attributes , which can be properties of either the entities or the relationships. The ER model is an abstraction tool because it can be used to understand and simplify the ambiguous data relationships in the business world and complex systems environments. Dimensional modeling uses three basic concepts: measures , facts , and dimensions . Dimensional modeling is powerful in representing the requirements of the business user in the context of database tables. Both ER and dimensional modeling can be used to create an abstract model of a specific subject. However, each has its own limited set of modeling concepts and associated notation conventions. Consequently, the techniques look different, and they are indeed different in terms of semantic representation. The following sections describe the modeling concepts and notation conventions for both ER modeling and dimensional modeling that will be used throughout this book.

36

Data Modeling Techniques for Data Warehousing

6.3 ER Modeling
A prerequisite for reading this book is a basic knowledge of ER modeling. Therefore we do not focus on that traditional technique. We simply define the necessary terms to form some consensus and present notation conventions used in the rest of this book.

6.3.1 Basic Concepts
An ER model is represented by an ER diagram, which uses three basic graphic symbols to conceptualize the data: entity, relationship, and attribute.

6.3.1.1 Entity
An entity is defined to be a person, place, thing, or event of interest to the business or the organization. An entity represents a class of objects, which are things in the real world that can be observed and classified by their properties and characteristics. In some books on IE, the term entity type is used to represent classes of objects and entity for an instance of an entity type. In this book, we will use them interchangeably. Even though it can differ across the modeling phases, usually an entity has its own business definition and a clear boundary definition that is required to describe what is included and what is not. In a practical modeling project, the project members share a definition template for integration and a consistent entity definition in a model. In high-level business modeling an entity can be very generic, but an entity must be quite specific in the detailed logical modeling. Figure 12 on page 38 shows an example of entities in an ER diagram. A rectangle represents an entity and, in this book, the entity name is notated by capital letters. In Figure 12 on page 38 there are four entities: PRODUCT, PRODUCT MODEL, PRODUCT COMPONENT, and COMPONENT. The four diagonal lines on the corners of the PRODUCT COMPONENT entity represent the notation for an associative entity. An associative entity is usually to resolve the many-to-many relationship between two entities. PRODUCT MODEL and COMPONENT are independent of each other but have a business relationship between them. A product model consists of many components and a component is related to many product models. With just this business rule, we cannot tell which components make up a product model. To do that you can define a resolving entity. For example, consider PRODUCT COMPONENT in Figure 12 on page 38. The PRODUCT COMPONENT entity can provide the information about which components are related to which product model. In ER modeling, naming entities is important for an easy and clear understanding and communications. Usually, the entity name is expressed grammatically in the form of a noun rather than a verb. The criteria for selecting an entity name is how well the name represents the characteristics and scope of the entity. In the detailed ER model, defining a unique identifier of an entity is the most critical task. These unique identifiers are called candidate keys . From them we can select the key that is most commonly used to identify the entity. It is called the primary key .

Chapter 6. Data Modeling for a Data Warehouse

37

Figure 12. A Sample ER Model.

Entity, relationship, and attributes in an ER diagram.

6.3.1.2 Relationship
A relationship is represented with lines drawn between entities. It depicts the structural interaction and association among the entities in a model. A relationship is designated grammatically by a verb, such as owns, belongs , and has . The relationship between two entities can be defined in terms of the cardinality. This is the maximum number of instances of one entity that are related to a single instance in another table and vice versa. The possible cardinalities are: one-to-one (1:1), one-to-many (1:M), and many-to-many (M:M). In a detailed (normalized) ER model, any M:M relationship is not shown because it is resolved to an associative entity. Figure 12 shows examples of relationships. A high-level ER diagram has relationship names, but in a detailed ER diagram, the developers usually do not define the relationship name. In Figure 12, the line between COMPONENT and PRODUCT COMPONENT is a relationship. The notation (cross lines and short lines) on the relationship represents the cardinality. When a relationship of an entity is related to itself, we can say that the relationship is recursive . Recursive relationships are usually developed either into associative entities or an attribute that references the other instance of the same entity. When the cardinality of an entity is one-to-many, very often the relationship represents the dependent relationship of an entity to the other entity. In that case, the primary key of the parent entity is inherited into the dependent entity as some part of the primary key.

6.3.1.3 Attributes
Attributes describe the characteristics of properties of the entities. In Figure 12, Product ID, Description, and Picture are attributes of the PRODUCT entity. For clarification, attribute naming conventions are very important. An attribute name should be unique in an entity and should be self-explanatory. For example, simply saying date1 or date2 is not allowed, we must clearly define each. As examples, they could be defined as the order date and delivery date.

38

Data Modeling Techniques for Data Warehousing

When an instance has no value for an attribute, the minimum cardinality of the attribute is zero, which means either nullable or optional . In Figure 12, you can see the characters P, m, o, and F . They stand for primary key, mandatory, optional, and foreign key . The Picture attribute of the PRODUCT entity is optional, which means it is nullable. A foreign key of an entity is defined to be the primary key of another entity. The Product ID attribute of the PRODUCT MODEL entity is a foreign key because it is the primary key of the PRODUCT entity. Foreign keys are useful to determine relationships such as the referential integrity between entities. In ER modeling, if the maximum cardinality of an attribute is more than 1, the modeler will try to normalize the entity and finally elevate the attribute to another entity. Therefore, normally the maximum cardinality of an attribute is 1.

6.3.1.4 Other Concepts
A concept that seems frustrating to users is domain . However, it is actually a very simple concept. A domain consists of all the possible acceptable values and categories that are allowed for an attribute. Simply, a domain is just the whole set of the real possible occurrences. The format or data type, such as integer, date, and character, provides a clear definition of domain. For the enumerative type of domain, the possible instances should be defined. The practical benefits of domain is that it is imperative for building the data dictionary or repository, and consequently for implementing the database. For example, suppose that we have a new attribute called product type in the PRODUCT entity and the number of product types is fixed and with a value of CellPhone and Pager. The product types attribute forms an enumerative domain with instances of CellPhone and Pager, and this information should be included in the data dictionary. The attribute first shop date of the PRODUCT MODEL entity can be any date within specific conditions. For this kind of restrictive domain, the instances cannot be fixed, and the range or conditions should be included in the data dictionary. Another important concept in ER modeling is normalization . Normalization is a process for assigning attributes to entities in a way that reduces data redundancy, avoids data anomalies, provides a solid architecture for updating data, and reinforces the long-term integrity of the data model. The third normal form is usually adequate. A process for resolving the many-to-many relationships is an example of normalization.

6.3.2 Advanced Topics in ER Modeling
In addition to the basic ER modeling concepts, three others are important for this book:
• • •

Supertype and subtype Constraint statement Derivation

6.3.2.1 Supertype and Subtype
Entities can have subtypes and supertypes. The relationship between a supertype entity and its subtype entity is an Isa relationship. An Isa relationship is used where one entity is a generalization of several more specialized entities. Figure 13 on page 41 shows an example of supertype and subtype. In the figure, SALES OUTLET is the supertype of RETAIL STORE and CORPORATE SALES OFFICE. And, RETAIL STORE and CORPORATE SALES OFFICE are subtypes of SALES OUTLET. The notation of supertype and subtype is

Chapter 6. Data Modeling for a Data Warehouse

39

represented by a triangle on the relationship. This notation is used by the IBM DataAtlas product. Each subtype entity inherits attributes from its supertype entity. In addition to that, each subtype entity has its own distinctive attributes. In the example, subentities have Region ID and Outlet ID as inherited attributes. And, the subentities have their own attributes such as number of cash registers and floor space of the RETAIL STORE subentity. The practical benefits of supertyping and subtyping are that it makes a data model more directly expressive. In Figure 13 on page 41, by just looking at the ER diagram we can understand that sales outlets are composed of retail stores and corporate sales offices. The other benefits of supertyping and subtyping are that it makes a data model more ready to support flexible database developments. To transform supertype and subtype entities into tables, we can think of several implementation choices. We can make only one table within which an attribute is the indicator and many attributes are nullable. Otherwise, we can have only subtype tables to which all attributes of supertype are inherited. Another choice is to make tables for each entity. Each choice has its considerations. Through supertyping and subtyping, a very flexible data model can be implemented. Subtyping also makes the relationship clear. For example, suppose that we have a SALESPERSON entity and only corporate sales offices can officially have a salesperson. Without subtyping of SALES OUTLET into CORPORATE SALES OFFICE and RETAIL STORE, there is no way to express the constraints explicitly using ER notations. Sometimes inappropriate use of supertyping and/or subtyping in ER modeling can cause problems. For example, a person can be a salesperson for the CelDial company as well as a customer. We might define person as being a supertype of employee and customer. But, in the practical world, it is not true. If we want a very generic model, we would better design a contract or association entity between person and company , or just leave it as customer and salesperson entities.

6.3.2.2 Constraints
Some constraints can be represented by relationships in the model. Basic referential integrity rules can be identified by relationships and their cardinalities. However, the more specific constraints such as ″Only when the occurrences of the parent entity ACCOUNT are checking accounts, can the occurrence of the child entity CHECK ACCOUNT DETAILS exist″ are not represented on an ER diagram. Such constraints can be added explicitly in the model by adding a constraint statement. This is particularly useful when we will have to show the temporal constraints, which also cannot be captured by relationship. For example, some occurrences of an entity have to be deleted when an occurrence of the other related entity is updated to a specific status. To define the life cycle of an entity, we need a constraint statement. Showing these types of specific conditions on an ER diagram is difficult. If you define the basics of a language for expressing constraint statements, it will be very useful for communications among developers. For example, you could make a template for constraint statement with these titles:
• • •

Constraint name and type Related objects (entity, relationship, attribute) Definition and descriptions

40

Data Modeling Techniques for Data Warehousing

Figure 13. Supertype and Subtype
•

Examples of the whole fixed number of instances

6.3.2.3 Derived Attributes and Derivation Functions
Derived attributes are less common in ER modeling for traditional OLTP applications, because they usually avoid having derived attributes at all. Data warehouse models, however, tend to include more derived attributes explicitly in the model. You can define a way to represent the derivation formula in the form of a statement. Through this form, you identify that an attribute is derived as well as providing explicitly the derivation function that is associated with the attribute. As a matter of fact, all summarized attributes in the data warehouse are derived, because the data warehouse collects and consolidates data from source databases. As a consequence, the metadata should contain all of these derivation policies explicitly and users should have access to it. For example, you can write a detailed derivation statement as follows:
• •

•

•

•

Entity and attribute name - SALES.Sales Volume. Derivation source - Sales Operational Database for each region. Related tables - SALES HISTORY, ..... Derivation function - summarization of the gross sales of all sales outlets (formula: sales volume - returned volume - loss volume). Returned volume is counted only for the month. Derivation frequency - weekly (after closing of operational journaling on Saturday night) Others

Of course, you must not clutter up your ER model by explicitly presenting the derivation functions for all attributes. You need some compromise. Perhaps you can associate attributes derived from other attributes in the data warehouse with a derivation function that is explicitly added to the model. In any case, presenting the derivation functions is restricted to only where it helps to understand the model.

Chapter 6. Data Modeling for a Data Warehouse

41

6.4 Dimensional Modeling
In some respects, dimensional modeling is simpler, more expressive, and easier to understand than ER modeling. But, dimensional modeling is a relatively new concept and not firmly defined yet in details, especially when compared to ER modeling techniques. This section presents the terminology that we use in this book as we discuss dimensional modeling. For more detailed techniques, methodologies, and hints, refer to Chapter 8, “Data Warehouse Modeling Techniques” on page 81.

6.4.1 Basic Concepts
Dimensional modeling is a technique for conceptualizing and visualizing data models as a set of measures that are described by common aspects of the business. It is especially useful for summarizing and rearranging the data and presenting views of the data to support data analysis. Dimensional modeling focuses on numeric data, such as values, counts, weights, balances, and occurrences. Dimensional modeling has several basic concepts:
• • •

Facts Dimensions Measures (variables)

6.4.1.1 Fact
A fact is a collection of related data items, consisting of measures and context data. Each fact typically represents a business item, a business transaction, or an event that can be used in analyzing the business or business processes. In a data warehouse, facts are implemented in the core tables in which all of the numeric data is stored.

6.4.1.2 Dimension
A dimension is a collection of members or units of the same type of views. In a diagram, a dimension is usually represented by an axis. In a dimensional model, every data point in the fact table is associated with one and only one member from each of the multiple dimensions. That is, dimensions determine the contextual background for the facts. Many analytical processes are used to quantify the impact of dimensions on the facts. Dimensions are the parameters over which we want to perform Online Analytical Processing (OLAP). For example, in a database for analyzing all sales of products, common dimensions could be:
• • • • •

Time Location/region Customers Salesperson Scenarios such as actual, budgeted, or estimated numbers

Dimensions can usually be mapped to nonnumeric, informative entities such as branch or employee.

42

Data Modeling Techniques for Data Warehousing

Dimension Members: A dimension contains many dimension members . A dimension member is a distinct name or identifier used to determine a data item′s position. For example, all months, quarters, and years make up a time dimension, and all cities, regions, and countries make up a geography dimension. Dimension Hierarchies: We can arrange the members of a dimension into one or more hierarchies. Each hierarchy can also have multiple hierarchy levels. Every member of a dimension does not locate on one hierarchy structure.
A good example to consider is the time dimension hierarchy as shown in Figure 14. The reason we define two hierarchies for time dimension is because a week can span two months, quarters, and higher levels. Therefore, weeks cannot be added up to equal a month, and so forth. If there is no practical benefit in analyzing the data on a weekly basis, you would not need to define another hierarchy for week.

Figure 14. Multiple Hierarchies in a Time Dimension

6.4.1.3 Measure
A measure is a numeric attribute of a fact, representing the performance or behavior of the business relative to the dimensions. The actual numbers are called as variables . For example, measures are the sales in money, the sales volume, the quantity supplied, the supply cost, the transaction amount, and so forth. A measure is determined by combinations of the members of the dimensions and is located on facts.

6.4.2 Visualization of a Dimensional Model
The most popular way of visualizing a dimensional model is to draw a cube. We can represent a three-dimensional model using a cube. Usually a dimensional model consists of more than three dimensions and is referred to as a hypercube . However, a hypercube is difficult to visualize, so a cube is the more commonly used term. In Figure 15 on page 44, the measurement is the volume of production, which is determined by the combination of three dimensions: location, product, and time. The location dimension and product dimension have their own two levels of hierarchy. For example, the location dimension has the region level and plant

Chapter 6. Data Modeling for a Data Warehouse

43

level. In each dimension, there are members such as the east region and west region of the location dimension. Although not shown in the figure, the time dimension has its numbers, such as 1996 and 1997. Each subcube has its own numbers, which represent the volume of production as a measurement. For example, in a specific time period (not expressed in the figure), the Armonk plant in East region has produced 11,000 CellPhones, of model number 1001.

Figure 15. The Cube: A Metaphor for a Dimensional Model

6.4.3 Basic Operations for OLAP
Dimensional modeling is primarily to support OLAP and decision making. Let′ s review some of the basic concepts of OLAP to get a little better grasp of OLAP business requirements so that we can model the data warehouse more effectively. Four types of operations are used in OLAP to analyze data. As we consider granularity, we can perform the operations of drill down and roll up . To browse along the dimensions, we use the operations slice and dice . Let′s explore what those terms really mean.

6.4.3.1 Drill Down and Roll Up
Drill down and roll up are the operations for moving the view down and up along the dimensional hierarchy levels. With drill-down capability, users can navigate to higher levels of detail. With roll-up capability, users can zoom out to see a summarized level of data. The navigation path is determined by the hierarchies within dimensions. As an example, look at Figure 16 on page 45. While you analyze the monthly production report of the west region plants, you might like to review the recent trends by looking at past performance by quarter. You would be performing a roll-up operation by looking at the quarterly data. You may then wonder why the San Jose plant produced less than Boulder and would need more detailed information. You could then use the drill down-operation on the report by Team within a Plant to understand how the productivity of Team 2 (which is lower in all cases than the productivity for Team 1) can be improved.

44

Data Modeling Techniques for Data Warehousing

Figure 16. Example of Drill Down and Roll Up

6.4.3.2 Slice and Dice
Slice and dice are the operations for browsing the data through the visualized cube. Slicing cuts through the cube so that users can focus on some specific perspectives. Dicing rotates the cube to another perspective so that users can be more specific with the data analysis. Let′s look at another example, using Figure 17 on page 46. You may be analyzing the production report of a specific month by plant and product, so you get the quarterly view of gross production by plant. You can then change the dimension from product to time, which is dicing. Now, you want to focus on the CellPhone only, rather than gross production. To do this, you can cut off the cube only for the CellPhone for the same dimensions, which is slicing.
Those are some of the key operations used in data analysis. To enable those types of operations requires that the data be stored in a specific way, and that is in a dimensional model.

6.4.4 Star and Snowflake Models
There are two basic models that can be used in dimensional modeling:
• •

Star model Snowflake model

Sometimes, the constellation model or multistar model is introduced as an extension of star and snowflake, but we will confine our discussion to the two basic structures. That is sufficient to explain the issues in dimensional modeling. This section presents only a basic introduction to the dimensional modeling techniques. For a detailed description, refer to Chapter 8, “Data Warehouse Modeling Techniques” on page 81.

Chapter 6. Data Modeling for a Data Warehouse

45

Figure 17. Example of Slice and Dice

6.4.4.1 Star Model
Star schema has become a common term used to connote a dimensional model. Database designers have long used the term star schema to describe dimensional models because the resulting structure looks like a star and the logical diagram looks like the physical schema. Business users feel uncomfortable with the term schema , so they have embraced the more simple sounding term of star model. In this book, we will also use the term star model .
The star model is the basic structure for a dimensional model. It typically has one large central table (called the fact table ) and a set of smaller tables (called the dimension tables ) arranged in a radial pattern around the fact table. Figure 18 on page 47 shows an example of a star schema. It depicts sales as a fact table in the center. Arranged around the fact table are the dimension tables of time, customer, seller, manufacturing location , and product . Whereas the traditional ER model has an even and balanced style of entities and complex relationships among entities, the dimensional model is very asymmetric. Even though the fact table in the dimensional model is joined with all the other dimension tables, there is only a single join line connecting the fact table to the dimension tables.

6.4.4.2 Snowflake Model
Dimensional modeling typically begins by identifying facts and dimensions, after the business requirements have been gathered. The initial dimensional model is usually starlike in appearance, with one fact in the center and one level of several dimensions around it. The snowflake model is the result of decomposing one or more of the dimensions, which sometimes have hierarchies themselves. We can define the many-to-one relationships among members within a dimension table as a

46

Data Modeling Techniques for Data Warehousing

Figure 18. Star Model.

separate dimension table, forming a hierarchy. For example, the seller dimension in Figure 18 on page 47 is decomposed into subdimensions outlet, region, and outlet type in Figure 19 on page 48. This type of model is derived from the star schema and, as can be seen, looks like a snowflake. The decomposed snowflake structure visualizes the hierarchical structure of dimensions very well. The snowflake model is easy for data modelers to understand and for database designers to use for the analysis of dimensions. However, the snowflake structure seems more complex and could tend to make the business users feel more uncomfortable working with it than with the simpler star model. Developers can also elect the snowflake because it typically saves data storage. Consider a banking application where there is a very large account table for one of the dimensions. You can easily expect to save quite a bit of space in a table of that size by not storing the very frequently repeated text fields, but rather putting them once in a subdimension table. Although the snowflake model does save space, it is generally not significant when compared to the fact table. Most database designers do not consider the savings in space to be a major decision criterion in the selection of a modeling technique.

6.4.5 Data Consolidation
Another major criterion for the use of OLAP is the fast response time for ad hoc queries. However, there could still be performance issues depending on the structure and volume of data. For a consistently fast response time, data consolidation (precalculation or preaggregation) is required. By precalculating and storing all subtotals before the query is issued, you can reduce the number of records to be retrieved for the query and maintain consistent and fast performance. The trade-off is that you will have to know how the users typically make their queries to understand how to consolidate. When users drill down to details, they typically move along the levels of a dimension hierarchy. Therefore, that provides the paths to consolidate or precalculate the data.

6.5 ER Modeling and Dimensional Modeling
The two techniques for data modeling in a data warehouse environment sometimes look very different from each other, but they have many similarities. Dimensional modeling can use the same notation, such as entity, relationship, attribute, and primary key. And, in general, you can say that a fact is just an entity in which the primary key is a combination of foreign keys, and the foreign

Chapter 6. Data Modeling for a Data Warehouse

47

Figure 19. Snowflake Model

keys reference the dimensions. Therefore, we could say that dimensional modeling is a special form of ER modeling. An ER model provides the structure and content definition of the informational needs of the corporation, which is the base for designing the data warehouse. This chapter defines the basic differences between the two primary data modeling techniques used in data warehousing. A conclusion that can be drawn from the discussion is that the two techniques have their own strengths and weaknesses, and either can be used in the appropriate situation.

48

Data Modeling Techniques for Data Warehousing

Chapter 7. The Process of Data Warehousing
This chapter presents a basic methodology for developing a data warehouse. The ideas presented generally apply equally to a data warehouse or a data mart. Therefore, when we use the term data warehouse you can infer data mart . If something applies only to one or the other, that will be explicitly stated. We focus on the process of data modeling for the data warehouse and provide an extended section on the subject but discuss it in the larger context of data warehouse development. The process of developing a data warehouse is similar in many respects to any other development project. Therefore, the process follows a similar path. What follows is a typical, and likely familiar, development cycle with emphasis on how the different components of the cycle affect your data warehouse modeling efforts. Figure 20 shows a typical data warehouse development cycle.

Figure 20. Data Warehouse Development Life Cycle

It is certainly true that there is no one correct or definitive life cycle for developing a data warehouse. We have chosen one simply because it seems to work well for us. Because our focus is really on modeling, the specific life cycle is not an issue here. What is essential is that we identify what you need to know to create an effective model for your data warehouse environment. There are a number of considerations that must be taken into account as we discuss the data warehouse development life cycle. We need not dwell on them, but be aware of how they affect the development effort and understand how they will affect the overall data warehouse design and model.
•

The life cycle diagram in Figure 20 seems to infer a single instance of a data warehouse. Clearly, this should be considered a logical view. That is, there could be multiple physical instances of a data warehouse involved in the environment. As an example, consider an implementation where there are multiple data marts. In this case you would iterate through the tasks in the life cycle for each data mart. This approach, however, brings with it an additional consideration, namely, the integration of the data marts. This integration can have an impact on the physical data, with considerations for

© Copyright IBM Corp. 1998

49

redundancy, inconsistency, and currency levels. Integration is also especially important because it can require integration of the data models for each of the data marts as well. If dimensional modeling were being used, the integration might take place at the dimension level. Perhaps there could be a more global model that contains the dimensions for the organization. Then when data marts, or multiple instances of a data warehouse, are implemented, the dimensions used could be subsets of those in the global model. This would enable easier integration and consistency in the implementation. Data marts can be dependent or independent. In the previous consideration we addressed dependent data marts with their need for integration. Independent data marts are basically smaller in scope data warehouses that are stand-alone. In this case the data models can also be independent, but you must understand that this type of implementation can result in data redundancy, inconsistency, and currency levels.

•

The key message of the life cycle diagram is the iterative nature of data warehouse development. This, more than anything else, distinguishes the life cycle of a data warehouse project from other development projects. Whereas all projects have some degree of iteration, data warehouse projects take iteration to the extreme to enable fast delivery of portions of a warehouse. Thus portions of a data warehouse can be delivered while others are still being developed. In most cases, providing the user with some data warehouse function generates immediate benefits. Delivery of a data warehouse is not typically an all-or-nothing proposition. Because the emphasis of this book is on modeling for the data warehouse, we have left out discussion about infrastructure acquisition. Although this would certainly be part of any typical data warehouse effort, it does not directly impact the modeling process. Within each step of the process a number of techniques are identified for creating the model. As the focus here is on what to do more than how to do it, very little detail is given for these techniques. A separate chapter (see Chapter 8, “Data Warehouse Modeling Techniques” on page 81) is provided for those requiring detailed knowledge of the techniques outlined here.

7.1 Manage the Project
On the left side of the diagram in Figure 20 on page 49, you see a line entitled Manage the Project . As with any development project, there must be a management component, and this component exists from the beginning to the end of the project. The development of a data warehouse is no different in this respect. However, it is a project management component and not a data warehouse management component. The difference is that management of a project is finite in scope and is concerned with the building of the data warehouse, whereas management of a data warehouse is ongoing (just as management of any other aspect of your organization, such as inventory or facilities) and is concerned with the execution of the data warehousing processes.

50

Data Modeling Techniques for Data Warehousing

7.2 Define the Project
In a typical project, high-level objectives are defined during the project definition phase. As well, limits are set on what will be delivered. This is commonly called the scope of the project. In data warehouse development, although the project objectives need to be specific, the data warehouse requirements are typically defined in general statements. They should answer such questions as, ″What do I want to analyze, and why do I want to analyze it?″ By answering the why question, we get an understanding of the requirements that must be addressed and begin to gain insight into the users′ information requirements. Data warehouse requirements contrast with typical application requirements, which will generally contain specific statements about which processes need to be automated. It is important that the requirements for data warehouse development not be too specific. If they are too specific, they may influence the way the data warehouse is designed to the point of excluding factors that seem irrelevant but may be key to the analysis being conducted. One of the main reasons for defining the scope of a project is to prevent constant change throughout the life cycle as new requirements arise. In data warehousing, defining the scope requires special care. It is still true that you want to prevent your target from constantly changing as new requirements arise. However, two of the keys to a valuable data warehouse are its flexibility and its ability to handle the as yet unknown query. Therefore, it is essential that the scope be defined to recognize that the delivered data warehouse will likely be somewhat broader than indicated by the initial requirements. You are walking a tightrope between a scope that leads to an ever-changing target, incapable of being pinned down and declared complete, and one so rigid that it cannot adjust to the users′ ever-changing requirements.

7.3 Requirements Gathering
The traditional development cycle focuses on automating the process, making it faster and more efficient. The data warehouse development cycle focuses on facilitating the analysis that will change the process to make it more effective. Efficiency measures how much effort is required to meet a goal. Effectiveness measures how well a goal is being met against a set of expectations. The requirements identified at this point in the development cycle are used to build the data warehouse model. But, the requirements of an organization change over time, and what is true one day is no longer valid the next. How then, do you know when you have successfully identified the user′ s requirements? Although there is no definitive test, we propose that if your requirements address the following questions, you probably have enough information to begin modeling:
• • • • • •

Who (people, groups, organizations) is of interest to the user? What (functions) is the user trying to analyze? Why does the user need the data? When (for what point in time) does the data need to be recorded? Where (geographically, organizationally) do relevant processes occur? How do we measure the performance or state of the functions being analyzed?

Chapter 7. The Process of Data Warehousing

51

There are many methods for deriving business requirements. In general, these methods can be placed in one of two categories: source-driven requirements gathering and user-driven requirements gathering (see Figure 21 on page 52).

Figure 21. Two Approaches.

Source-Driven and User-Driven Requirements Gathering

7.3.1 Source-Driven Requirements Gathering
Source-driven requirements gathering, as the name implies, is a method based on defining the requirements by using the source data in production operational systems. This is done by analyzing an ER model of source data if one is available or the actual physical record layouts and selecting data elements deemed to be of interest. The major advantage of this approach is that you know from the beginning that you can supply all the data because you are already limiting yourself to what is available. A second benefit is that you can minimize the time required by the users in the early stages of the project. Of course there are also disadvantages to this approach. By minimizing user involvement, you increase the risk of producing an incorrect set of requirements. Depending on the volume of source data you have, and the availability of ER models for it, this can also be a very time-consuming approach. Perhaps most important, some of the user′s key requirements may need data that is currently unavailable. Without the opportunity to identify such requirements, there is no chance to investigate what is involved in obtaining external data. External data is data that exists outside the organization. Even so, external data can often be of significant value to the business users. Even though steps should be taken to ensure the quality of such data, there is no reason to arbitrarily exclude it from being used. The result of the source-driven approach is to provide the user with what you have. We believe there are at least two cases where this is appropriate. First, relative to dimensional modeling, it can be used to drive out a fairly comprehensive list of the major dimensions of interest to the organization. If you ultimately plan to have an organizationwide data warehouse, this could minimize the proliferation of duplicate dimensions across separately developed data marts. Second, analyzing relationships in the source data can identify areas on which to focus your data warehouse development efforts.

52

Data Modeling Techniques for Data Warehousing

7.3.2 User-Driven Requirements Gathering
User-driven requirements gathering is a method based on defining the requirements by investigating the functions the users perform. This is usually done through a series of meetings and/or interviews with users. The major advantage to this approach is that the focus is on providing what is needed, rather than what is available. In general, this approach has a smaller scope than the source-driven approach. Therefore, it generally produces a useful data warehouse in a shorter timespan. On the negative side, expectations must be closely managed. The users must clearly understand that it is possible that some of the data they need can simply not be made available. This is important because you do not want to limit what the user asks for. Outside-the-box thinking should be promoted when defining requirements for a data warehouse. This will prevent you from eliminating requirements simply because you think they might not be possible. If a user is too tightly focused, it is possible to miss useful data that is available in the production systems. We believe user-driven requirements gathering is the approach of choice, especially when developing data marts. For a full-scale data warehouse, we believe it would be worthwhile to use the source-driven approach to break the project into manageable pieces, which may be defined as subject areas. The user-driven approach could then be used to gather the requirements for each subject area.

7.3.3 The CelDial Case Study
Throughout this chapter, we reference a case study (see Appendix A, “The CelDial Case Study” on page 163) to illustrate the steps in the process of creating a data warehouse model. In that case study, we create a set of corporatewide dimensions, using the source-driven requirements gathering approach. We then take the user-driven requirements gathering approach to define specific dimensional models. As each step in the process is presented, some component of the model is created. It would be well worthwhile to review that case study before continuing.

7.4 Modeling the Data Warehouse
Modeling the target warehouse data is the process of translating requirements into a picture along with the supporting metadata that represents those requirements. Although we separate the requirements and modeling discussions for readability purposes, in reality these steps often overlap. As soon as some initial requirements are documented, an initial model starts to take shape. As the requirements become more complete, so too does the model. We must also point out that there is a distinction between completing the modeling phase and completing the model. At the end of the modeling phase, you have a complete picture of the requirements. However, only part of the metadata will have been documented. A model cannot truly be considered complete until the remainder of the metadata is identified and documented during the design phase.

Chapter 7. The Process of Data Warehousing

53

For a discussion on selection of a modeling technique, refer to Chapter 8, “Data Warehouse Modeling Techniques” on page 81. The remainder of this section demonstrates the steps to follow in building a model of your data warehouse.

7.4.1 Creating an ER Model
We believe that ER modeling is generally well understood. In the circumstance that the physical data warehouse implementation is different enough from the dimensional model to warrant the creation of an ER model, standard ER modeling techniques apply. Defining the dimensions for your organization is a worthwhile exercise. Creation of successive data marts will be easier if much of the dimension data already exists. Let′s use the case study ER model (see Figure 92 on page 168) as an example. The first step is to remove all the entities that act as associative entities and all subtype entities. In the case study this includes Product Component, Inventory, Order Line, Order, Retail Store, and Corporate Sales Office . Be careful to create all the many-to-many relationships that replace these entities (see Figure 22).

Figure 22. Corporate Dimensions: Step One. relationships from an ER model.

Removing subtypes and many-to-many

The next step is to roll up the entities at the end of each of the many-to-many relationships into single entities. For each new entity, consider which attributes in the original entities would be useful constraints on the new dimension. Remember to consider attributes of any subtype entities removed in the first step. As well, because the model is a logical representation, we remove the individual keys and replace them with a generic key for each dimension (see Figure 23 on page 55). Physical keys will be assigned during the design phase.

54

Data Modeling Techniques for Data Warehousing

In our case study example, note that rolling the salesperson up into the sales dimension implies (correctly) that the relationships among outlet, salesperson, and customer roll up into the sales to customer relationship. The many-to-many relationship between customer and sales prevents the erroneous rollup of customer into sales person and ultimately into sales.

Figure 23. Corporate Dimensions: Step Two. organization.

Fully attributed dimensions for the

7.4.2 Creating a Dimensional Model
The purpose of a data model is to represent a set of requirements for data in a clear and concise manner. In the case of a dimensional model, it is essential that the representation can be understood by the user. This model will be the basis for the analysis undertaken by a user and, if implemented properly, is how the user will see the data. Although the structure should look like the model to the user, it may be physically implemented differently based on the technology used to create, maintain, and access it. We discuss this translation and completion of the model later in this chapter (see 7.5, “Design the Warehouse” on page 69). The remainder of this section documents a set of steps to create a dimensional model that will be used to create the target data warehouse for the user ′s data analysis requirements.

7.4.2.1 Dimensions and Measures
A user typically needs to evaluate, or analyze, some aspect of the organization′ s business. The requirements that have been collected must represent the two key elements of this analysis: what is being analyzed, and the evaluation criteria for what is being analyzed. We refer to the evaluation criteria as measures and what is being analyzed as dimensions. Our first step in creating a model is to identify the measures and dimensions within our requirements. A set of questions is defined in the case study that we

Chapter 7. The Process of Data Warehousing

55

use as our sample requirements (see A.3.5, “What Do the Users Want?” on page 166). We restate these here: 1. What is the average quantity on-hand and reorder level this month for each model in each manufacturing plant? 2. What is the total cost and revenue for each model sold today, summarized by outlet, outlet type, region, and corporate sales levels? 3. What is the total cost and revenue for each model sold today, summarized by manufacturing plant and region? 4. What percentage of models are eligible for discounting and of those, what percentage is actually discounted when sold, by store, for all sales this week? This month? 5. For each model sold this month, what is the percentage sold retail, the percentage sold corporately through an order desk, and the percentage sold corporately by a salesperson? 6. Which models and products have not sold in the last week? In the last month? 7. What are the top five models sold last month by total revenue? By quantity sold? By total cost? 8. Which sales outlets had no sales recorded last month for each of the models in each of the three top five lists? 9. Which sales persons had no sales recorded last month for each of the models in each of the three top five lists? By analyzing these questions, we define the dimensions and measures needed to meet the requirements (see Table 1).
Table 1. Dimensions, Measures, and Related Questions
Dimensions and Measures Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9

Dimensions
Sales Manufacturing Product X X X X X X X X X X X X X X X

Measures
Average quantity on hand Total cost Total revenue Quantity sold Percentage of models eligible for discount Percentage of models eligible for discount that are actually discounted Percentage of a model sold through a retail outlet Percentage of a model sold through a corporate sales office order desk Percentage of a model sold through a sales person X X X X X X X X X X X X X

Because we have already created the dimensions of CelDial (see Figure 23 on page 55), we do not go through the steps here to roll up the lower level entities

56

Data Modeling Techniques for Data Warehousing

into each dimension. We only list the dimensions relevant to our requirements. If we did not have a corporate set of requirements to use here, we would have used the requirements generated from the questions in 7.4.2.1, “Dimensions and Measures” on page 55. This would have been a time-consuming exercise, but more importantly we would have had an incomplete set of dimensions and data. For example, we would have been unaware of the existence of the Customer and Component dimensions and the Number of Cash Registers and Floor Space attributes of the Sales dimension (see Figure 23 on page 55). At this point we review the dimensions to ensure we have the data we need to answer our questions. No additional attributes are required for the sales and manufacturing dimensions. However, the product dimension as it stands cannot answer questions 2 and 3. To meet this need, we add the unit cost of a model to the product dimension. The derivation rule for this is defined in the case study (see A.3.4, “Defining Cost and Revenue” on page 165). Based on the case study, there is interest in knowing the unit cost of a model at a point in time. We therefore conclude that a history of unit cost is necessary and add begin and end dates to fill out the product dimension (see Figure 24 on page 58).

7.4.2.2 Adding a Time Dimension
To properly evaluate any data it must be set in its proper context. This context always contains an element of time. Therefore we recommend the creation of a time dimension once for the organization. Be aware that adding time to another dimension as we did with product is a separate discussion. Here we only discuss time as a dimension of its own. For most organizations, the lowest level of time that is relevant is an individual day. This is true for CelDial and so we choose day as our lowest level of granularity. Analyzing the requirements we can see a need for reporting by day, week, and month. Because we do not have more information about CelDial, we will not consider adding other attributes such as period, quarter, year, and day of week. When you initially create your time dimension, consider additional attributes such as those above and any others that may apply to your organization. We now have a time dimension that meets CelDial′s analysis requirements. This completes the dimensions we need to meet the documented case study requirements (see Figure 24 on page 58).

Chapter 7. The Process of Data Warehousing

57

Figure 24. Dimensions of CelDial Required for the Case Study

7.4.2.3 Creating Facts
Together, one set of dimensions and its associated measures make up what we call a fact . Organizing the dimensions and measures into facts is the next step. This is the process of grouping dimensions and measures together in a manner that can address the specified requirements. We will create an initial fact for each of the queries in the case study. For any measures that describe exactly the same set of dimensions, we will create only one fact (see Figure 25 on page 59). Note that questions 6, 8, and 9 have no measures associated with them (see Table 1 on page 56). Had we not merged question 6 with questions 5 and 7 into fact 4, and questions 8 and 9 with question 2 into fact 2, these would produce facts containing no measures. Such facts are called factless facts because they only record that an event, in this case the sale of a product at a point in time (facts 2 and 3) at a specific location (fact 2 only), has occurred. No other measurement is required.

7.4.2.4 Granularity, Additivity, and Merging Facts
The granularity of a fact is the level of detail at which it is recorded. If data is to be analyzed effectively, it must all be at the same level of granularity. As a general rule, data should be kept at the highest (most detailed) level of granularity. This is because you cannot change data to a higher level than what you have decided to keep. You can, however, always roll up (summarize) the data to create a table with a lower level of granularity. Closely related to the granularity issue is that of additivity , the ability of measures to be summarized. Measures fall into three categories: fully additive, nonadditive, and semiadditive. An example of a nonadditive measure is a

58

Data Modeling Techniques for Data Warehousing

Figure 25. Initial Facts

percentage. You simply cannot add the percentages from two facts together and come up with a meaningful result. An example of a semiadditive measure is a balance. Although you can add the balances from two accounts to get a total balance, you cannot add two balances from the same account at two different points in time. Because the balance is additive only across some dimensions, we call it a semiadditive measure. A value that can be added across all dimensions is considered to be fully additive. Additivity becomes important when you consider the possible summarizations that will occur on a fact table. Generally, it is desirable for measures to be fully additive. When they are not, you should consider breaking them down into their atomic elements. Once you have assessed the granularity and additivity that exists in your facts, consider the possibility of merging facts. This may require changing the granularity of a particular fact. Usually, merging facts will expand the range of analyses that can be performed on the fact. This is because merging facts often implies adding dimensions to a fact. Let′s review the granularity and additivity of the facts we have generated and then consider the possibility of merging facts.

Chapter 7. The Process of Data Warehousing

59

Granularity and Additivity: When reviewing fact 1, we immediately discover a problem with granularity. The average quantity on hand is a monthly figure, whereas the total cost and total revenue are daily figures. We must either split this into two facts or make the time dimension consistent. This means bringing time down to the lowest level of the two in question, which is day. However, because average quantity on hand is nonadditive, we have to store actual quantity on hand and let the query calculate an average. Making quantity on hand fully additive increases our range of analysis, so this seems to be the best choice.
Fact 2 also has a problem with the time dimension in that two different levels of granularity (day for query 2 and month for queries 8 and 9) exist. However, because all of the measures are fully additive, we will simply set the grain of time to a day. The grain is the fundamental atomic level of data to be represented in the fact table. A query can then handle any summarization to the monthly level. Facts 3 and 4 both present difficult choices. As with the previous facts, we have two distinct grains of time. However, in this case neither grain can roll up into the other. To change the grain means setting time at the day level and summarizing up for both week and month. This becomes difficult because many of the measures are non additive. This seems to speak against changing the time granularity and in favor of splitting into multiple facts. The result, however, would be two facts containing exactly the same measures, with only the grain of the time dimension making a difference. This would certainly prove confusing when an analyst is trying to determine which fact to query against. Our preference would be to set the granularity at the day level and store the atomic elements of the percentage calculations for both of these facts. This is based on the idea that if the measures are the same, then having two facts is an unnecessary redundancy. Therefore in fact 3 we will replace the percentages with: quantity of model sold through a retail outlet, quantity of a model sold through a corporate sales office, and quantity of a model sold through a salesperson. Together with total quantity sold, which already exists in this fact, these measures will allow the percentages to be calculated. Similarly, in fact 4 we will replace the percentages with: number of models eligible for discount, quantity of models eligible for discount actually sold, and quantity of models sold at a discount.

Fact Consolidation: We have resolved the granularity and additivity problems with our facts (see Figure 26 on page 61). It is time to consider which, if any, can be consolidated. There are three main reasons for consolidating facts. First, it is easier for a user to find the data needed to satisfy a query if there are fewer places to look. Second, when you merge facts you expand the analysis potential because you can relate more measures to more dimensions at a higher level of granularity. Of course this is true only if it is valid to relate the dimensions and measures you are merging. Third, the fewer facts you have, the less administration there is to do.
The first step in evaluating the merge possibilities is to determine for each measure which additional dimensions can be added to increase its granularity. Reviewing fact 1 we see that total cost and total revenue could be further broken down by the sales dimension. However, the same cannot be said for quantity on hand or reorder level. In fact, there is no finer breakdown for quantity on hand

60

Data Modeling Techniques for Data Warehousing

Figure 26. Intermediate Facts.

Resolution of Granularity and Additivity Issues

than product and manufacturing. Therefore, we recommend no changes for fact 1. Fact 2 already has all the dimensions present in facts 3 and 4. Therefore no further dimensions are necessary to allow a merger with facts 3 and 4. Let′ s examine facts 3 and 4 to see whether this merger is possible. Adding the sales dimension to fact 3 necessitates some interesting changes. Certainly the total cost, total revenue, and total quantity sold can be further refined by adding the sales dimension. However, the sales dimension contains both outlet type and salesperson data. Using this structure we can classify the total quantity sold, negating the need to store the three individual totals. Facts 2 and 3 should definitely be merged (see Figure 27 on page 62).

Chapter 7. The Process of Data Warehousing

61

Figure 27. Merging Fact 3 into Fact 2

Adding the product dimension to fact 4 to facilitate the consolidation with fact 2 has similar results to our efforts with fact 3. The number of models eligible for discount can be calculated directly from the product dimension. Therefore it is no longer needed in the consolidated fact. Because the product dimension tells us whether an individual model is eligible for discount, we can use the total quantity sold (consolidated from fact 3) to represent the quantity of models eligible for discount actually sold. We have two options for quantity of models sold at a discount. One is to keep it as it is, which would meet the need. Another option is to record the discount amount and generate the quantity sold at a discount by adding up the quantity sold where the discount amount is not zero. Although this is not strictly required to meet the requirements, the potential for additional analysis makes this option attractive, and it is the one we recommend. The result is the consolidation of facts 2, 3, and 4 (see Figure 28).

Figure 28. Merging Fact 4 into the Result of Fact 2 and Fact 3

62

Data Modeling Techniques for Data Warehousing

One last step should be followed before declaring our model complete. The facts should be reviewed for opportunities to add other dimensions, thus increasing the potential for valuable analysis. Again we see that fact 1 cannot be broken down any further. However, fact 2 still presents some opportunities. Both the manufacturing and customer dimensions can be applied to fact 2. The only other dimension we currently know about is the component dimension. Clearly it cannot be applied to fact 2. However, when we look at all the dimensions in fact 2, we see that there is one other possibility. The dimensions of fact 2 are: sales, product, manufacturing, customer, and time. All of these dimensions can be identified at the time an order is placed. The order would add one last level of granularity to fact 2. Although we have not, up to this point, considered order as a dimension, we do so now because of its effect on the granularity of fact 2. Because all of the measures in the fact are valid for an order, and because the increased granularity increases analysis potential, we recommend adding order to fact 2 as a dimension. Order is a dimension without attributes, so no dimension is actually created. We simply add order key to the fact. When we add a dimension in this manner, we refer to it as a degenerate dimension. An interesting side effect of adding manufacturing to fact 2 is that we can now answer query 3 (from the requirements list in 7.4.2.1, “Dimensions and Measures” on page 55) through either fact. This gives us the option of removing total cost and total revenue from fact 1. For now, however, we will leave them there in case there is some value in relating inventory levels to sales activity. Up to this point we have referred to our facts simply by number. Typically we name facts by what they represent. Since fact 1 represents inventory, we will call it the inventory fact. We will refer to fact 2 as the sale fact. Of course, this may be confusing as we already have a sales dimension. To clarify this, we will rename the sales dimension to the seller dimension. We now have our completely modeled facts (see Figure 29).

Figure 29. Final Facts

Chapter 7. The Process of Data Warehousing

63

7.4.2.5 Integration with Existing Models
Once you have completed your facts and dimensions, follow the same process of setting granularity, additivity, and consolidation with data from your existing warehouse if you have one. The only significant difference is that you will be restricted in your ability to change already existing facts, dimensions, and measures. This is why it is so important to carry the process as far as possible up front. In the case study we do not have an existing warehouse. Therefore, at this point we simply connect our dimensions to our facts to complete the pictures of our inventory model (see Figure 30) and our sales model (see Figure 31).

Figure 30. Inventory Model

Figure 31. Sales Model

64

Data Modeling Techniques for Data Warehousing

7.4.2.6 Sizing Your Model
Now that we have a model we can estimate how big our warehouse will be. At this point we can only give rough estimates of the size of our warehouse. We have enough information to estimate the size of the data in each of our tables. However, until some decisions are made regarding how these tables are physically implemented, items such as overhead and indices cannot be included. To calculate the size of the data in a table, rows times the length of each row. For our by adding 4 bytes for each numeric or date for a character attribute, and the number of by 2 and rounded up. we simply multiply the number of case study, we calculate row length attribute, the number of characters digits in a decimal attribute divided

The method for determining the number of rows for each table varies. For seller, manufacturing, and customer, because we do not keep a history of changes, we simply use the total number of rows from the operational systems. For manufacturing there are seven plants. CelDial serves 3000 customers. There are three corporate sales offices, 15 retail stores, and 30 salespeople, for a total of 48 sellers. For the remaining entities, the number of rows is dependent on how long we want to keep the data. In the case study, three complete years of data are required. Therefore no data can be deleted until the end of the fourth year. (If we needed three continuous years of data we could delete daily, weekly, or monthly all data that is more than three years old.) There will be only one row per day for the time entity. Over four years, this will be 1,461 rows (4 years x 365 days + 1 day for the leap year). There are 300 models of product. We must add to this a row for each of 10 changes per week. The result is 2,380 rows (300 models + 10 changes x 52 weeks x 4 years). Our inventory fact will contain 3,068,100 rows (7 plants x 300 models x 1,461 days). To determine the number of rows in our sales fact, we calculate corporate and retail sales separately because they have different volumes and days of operation. There will be 5,200,000 rows for corporate sales (500 sales x 10 models x 5 days x 52 weeks x 4 years). There will be 2,912,000 rows for retail sales (1000 sales x 2 models x 7 days x 52 weeks x 4 years). The total number of rows for the sales fact will be 8,112,000 (5,200,000 corporate sales rows + 2,912,000 retail sales rows). Multiplying the length of a row by the number of rows in each table and then adding these results together gives us a total of 431 MB (see Table 2 on page 66). Note that the size of the dimensions has no impact on the size of the warehouse when measured in megabytes. It is typical for the dimensions to be orders of magnitude smaller than facts. For this reason, sizing of the fact tables is often the only estimating done at this point. We only calculate the dimensions here for illustrative purposes.

Chapter 7. The Process of Data Warehousing

65

Table 2. Size Estimates for CelDial ′ s Warehouse
Table Manufacturing Seller Time Product Customer Inventory Sale Total Row Length 64 107 15 76 94 30 43 -Number of Rows 7 48 1461 2,380 3,000 3,068,100 8,112,000 -Size (bytes) 448 5,136 21,900 180,880 282,000 92,043,000 348,816,000 441,349,364 Size 0.4 K B 5 KB 21.4 K B 176.6 K B 275.4 K B 89.9 M B 340.6 M B 431 M B

This is a very preliminary estimate, to be sure. It does, however, enable technical staff to begin planning for its infrastructure requirements. This is important because, with the compressed life cycle of a warehouse, you will be needing that infrastructure soon.

7.4.3 Don′t Forget the Metadata
In the traditional development cycle, a model sees only sparse use after completion, typically when changes need to be made, or when other projects require the data. In the warehouse, however, your model is used on a continuous basis. The users of the warehouse constantly reference the model to determine the data they want to use to analyze the organization. The rate of change of the data structure in a warehouse is much greater than that of operational data structures. Therefore, the technical users of the warehouse (administrators, modelers, designers, etc.) will also use your model on a regular basis. This is where the metadata comes in. Far from just a pretty picture, the model must be a complete representation of the data you are storing, or it will be of little use to anybody. At this point you cannot define all of the metadata. However, this does not mean you should wait until you can. To properly understand the model, and be able to confirm that it meets requirements, a user must have access to the metadata that describes the warehouse in business terms that are easily understood. Therefore, nontechnical metadata should be documented at this point. During the design phase, the technical metadata will be added to it. At the warehouse level, a list should be provided of what is available in the warehouse. This list should contain the models, dimensions, facts, and measures available as these will all be used as initial entry points when a user begins analyzing data. For each model, provide a name, definition, and purpose. The name simply gives the user something to focus on when searching. Usually, it is the same as the fact. The definition identifies what is modeled, and the purpose describes what the model is used for. The metadata for the model should also contain a list of dimensions, facts, and measures associated with it, as well as the name of a contact person so that users can get additional information when they have questions about the model.

66

Data Modeling Techniques for Data Warehousing

A name, definition, and aliases must be provided for all dimensions, dimension attributes, facts, and measures. Aliases are necessary because it is often difficult to come to agreement on a common name for any widely used object. For dimensions and facts a contact person should be provided. Metadata about a dimension should also include hierarchy, change rules, load frequency, and the attributes, facts, and measures associated with the dimension. The hierarchy defines the relationships between attributes of the dimension that identify the different levels that exist within it. For example, in the seller dimension we have the sales region, outlet type (corporate or retail), outlet, and salesperson as a hierarchy. This documents the roll-up structure of the dimension. Change rules identify how changes to attributes within a dimension are dealt with. In some instances, these rules can be different for individual attributes. Record change rules with the attributes when this is the case. The load frequency allows the user to understand whether data will be available when needed. The attributes of a dimension are used to identify which facts the user wants to analyze. For example, in our case study, analyzing cost and revenue for the products that are very expensive to make might be done by only including facts where the unit cost attribute in the product dimension was greater than $500.00. For attributes to be used effectively, metadata about them should include the data type, domain, and derivation rules. At this point, a general indication of the data type (character, date, numeric, etc.) is sufficient. Exact data type definition can be done during design. The domain of an attribute defines the set of valid values that it can hold. For attributes that contain derived values, the rules for determining the value must be documented. Metadata about a fact should include the load frequency, the measures and dimensions associated with the fact, and the grain of time for the fact. Although it is possible to derive the grain of time for a fact through its relationship to the time dimension, it is worthwhile explicitly stating it here. It is essential for proper analysis that this grain be understood. Metadata about a measure should include its data type, domain, derivation rules, and the facts and dimensions associated with the measure. To sum up the metadata required at this point, we provide a graphic representation of the metadata and the access paths users might travel when analyzing their data (see Figure 32 on page 68).

Chapter 7. The Process of Data Warehousing

67

Figure 32. Warehouse Metadata. modeling phase

Metadata and user access paths at the end of the

7.4.4 Validating the Model
Before investing a lot of time and effort in designing your warehouse, it is a good idea to validate your model with the user. The purpose of such a review is twofold. First, it serves to confirm that the model can actually meet the user′ s requirements. Second, and just as important, a review should confirm that the user can understand the model. Remember that once the warehouse is implemented, the user will be relying on the model on a regular basis to access data in the warehouse. No matter how well the model meets the user′ s requirements, your warehouse will fail if the user cannot understand the model and, consequently, cannot access the data. Validation at this point is done at a high level. This model is reviewed with the user to confirm that it is understandable. Together with the user, test the model by resolving how you will answer some of the questions identified in the requirements. It is almost certain that the model will not meet all of the user′s requirements. This does not mean that you stop and go back to the beginning. Expect on your first cut of the model to meet perhaps 50% of the requirements. Take this 50% (or however much is validated) of the model and start working on the design. The remainder should be sent back to the requirements gathering stage. Either the requirements need to be better understood or, as is often the case, they have changed and need to be redefined. Usually, this will lead to additions, and possibly changes, to the model already created. In the mean time, the validated

68

Data Modeling Techniques for Data Warehousing

portion of the model will go through the design phase and begin providing benefits to the user. The iteration of development and the continued creation of partially complete models are the key elements that provide the ability to rapidly develop data warehouses.

7.5 Design the Warehouse
From the modeling perspective, our main focus in discussing the design phase of data warehouse development is identifying the additional metadata required to make the model complete. Aside from the metadata, there are a few cases where the design can impact the model. We identify these as well. We do not go beyond the design steps to discuss specific design techniques as this is beyond the scope of the book. We begin this section with a short discussion on the differences in design for operational systems and data warehouse systems. This is followed with sections for each step in the design process. The focus of these sections is on the impact that the design techniques have on the model and its metadata. As the creation of a data mining application is primarily a design, not a modeling, function, we close this section with a discussion of data mining development.

7.5.1 Data Warehouse Design versus Operational Design
Once a model is created and validated, it is analyzed to determine the best way to physically implement it. Although similar in nature to modeling and design in the operational world, the actual steps in data warehousing are different. The discussion here assumes that operational models are typically ER models and the data warehousing models are dimensional models. You have probably already noticed that the data warehouse model looks more physical in nature than a model of an operational system. Probably the feature that most differentiates the data warehouse model from the logical operational model is the denormalization of the dimensions. Compare the product dimension in the dimensional model to the relevant entities in the ER model (see Figure 33 on page 70).

Chapter 7. The Process of Data Warehousing

69

Figure 33. Dimensional and ER Views of Product-Related Data

The reason for this difference is the different role the model plays in the data warehouse. To the user, the data must look like the data warehouse model. In the operational world, a user does not generally use the model to access the data. The operational model is only used as a tool to capture requirements, not to access data. Data warehouse design also has a different focus from operational design. Design in an operational system is concerned with creating a database that will perform well based on a well-defined set of access paths. Data warehouse design is concerned with creating a process that will retrieve and transform operational data into useful and timely warehouse data. This is not to imply that there is no concern for performance in a data warehouse. On the contrary, due to the amount of data typically present in a

70

Data Modeling Techniques for Data Warehousing

data warehouse, performance is an essential consideration. However, performance considerations cannot be handled in a data warehouse in the same way they are handled in operational systems. Access paths have already been built into the model due to the nature of dimensional modeling. The unpredictable nature of data warehouse queries limits how much further you can design for performance. After implementation, additional tuning may be possible based on monitoring usage patterns. One area where design can impact performance is renormalizing, or snowflaking, dimensions. This decision should be made based on how the specific query tools you choose will access the dimensions. Some tools enable the user to view the contents of a dimension more efficiently if it is snowflaked while for other tools the opposite is true. As well, the choice to snowflake will also have a tool-dependent impact on the join techniques used to relate a set of dimensions to a fact. Regardless of the design decision made, the model should remain the same. From the user perspective, each dimension should have a single consolidated image.

7.5.2 Identifying the Sources
Once the validated portion of the model passes on to the design stage, the first step is to identify the sources of the data that will be used to load the model. These sources should then be mapped to the target warehouse data model. Mapping should be done for each dimension, dimension attribute, fact, and measure. For dimensions and facts, only the source entities (for example, relational tables, flat files, IMS DBDs and segments) need be documented. For dimension attributes and measures, along with the source entities, the specific source attributes (such as columns and fields) must be documented. Conversion and derivation algorithms must also be included in the metadata. At the dimension attribute and measure level, this includes data type conversion, algorithms for merging and splitting source attributes, calculations that must be performed, domain conversions, and source selection logic. A domain conversion is the changing of the domain in the source system to a new set of values in the target. For example, in the operational system you may use codes for gender, such as 1=female and 2=male. You may want to convert this to female and male in the target system. Such a conversion should be documented in the metadata. In some cases you may choose to load your target attribute from different source attributes based on certain conditions. Suppose you have a distributed sales organization and each location has its own customer file. However, your accounts receivable system is centralized. If you try to relate customer payments to sales data, you will likely have to pull some customer data from different locations based on where the customer does business. Source selection logic such as this must be included in the metadata. At the fact and dimension level, conversion and derivation metadata includes the logic for merging and splitting rows of data in the source, the rules for joining multiple sources, and the logic followed to determine which of multiple sources will be used. Identifying sources can also cause changes to your model. This will occur when you cannot find a valid source. Two possibilities exist. First, there simply is no

Chapter 7. The Process of Data Warehousing

71

source that comes close to meeting the user′s requirements. This should be very rare, but it is possible. If only a portion of the model is affected, remove that component and continue designing the remainder. Whatever portion of the model cannot be sourced must return to the requirements stage to redefine the need in a manner that can be met. A more likely scenario is that there will be a source that comes close but is not exactly what the user had in mind. In the case study we have a product description but no model description. The model code is available to select individual models for analysis, but it is hardly user friendly. However, rather than not meet the requirement to perform analysis by model, model code will be used. If user knowledge of source systems is high, this may occur during the modeling stage, but often it occurs during design. All of the metadata regarding data sources must be documented in the data warehouse model (see Figure 34 on page 77).

7.5.3 Cleaning the Data
Data cleaning has three basic components: validation of data, data enhancement, and error handling. Validation of data consists of a number of checks, including:
• • • •

Valid values for an attribute (domain check) Attribute valid in context of the rest of the row Attribute valid in context of related rows in this or other tables Relationship between rows in this and other tables valid (foreign key check)

This is not an exhaustive list. It is only meant to highlight the basic concepts of data validation. Data enhancement is the process of cleaning valid data to make it more meaningful. The most common example is name and address information. Often we store name and address information for customers in multiple locations. Over time, these tend to become unsynchronized. Merging data for the customer is often difficult because the data we use to match the different images of the customer no longer matches. Data enhancement resynchronizes this data. Error handling is a process that determines what to do with less than perfect data. Data may be rejected, stored for repair in a holding area, or passed on with its imperfections to the data warehouse. From a data model perspective, we only care about the data that is passed on to the data warehouse. The metadata for imperfect data should include statements about the data quality (types of errors) to be expected and the data accuracy (frequency of errors) of the data (see Figure 34 on page 77).

7.5.4 Transforming the Data
Data transformation is a critical step in any data warehouse development effort. Two major decisions must be made at this point: how to capture the source data, and a method for assigning keys to the target data. Along with these two decisions, you must generate a plan documenting the steps to get the data from source to target. From a modeling perspective, this is simply adding more metadata.

72

Data Modeling Techniques for Data Warehousing

7.5.4.1 Capturing the Source Data
The first step in transformation is capturing the source data. Initially, a full copy of the data is required. Once this initial copy has been loaded, a means of maintaining it must be devised. There are four primary methods of capturing data:
• • • •

Full refresh Log capture Time-stamped source Change transaction files

A full refresh, as the name implies, is simply a full copy of the data to be moved into the target data warehouse. This copy may replace what is in the data warehouse, add a complete new copy at the new point in time, or be compared to the target data to produce a record of changes in the target. The other three methods focus on capturing only what has changed in the source data. Log capture extracts relevant changes from the DBMS′s log files. If source data has been time stamped, the extract process can select only data that has changed since the previous extract was run. Some systems will produce a file of changes that have been made in the source. An extract can use this in the same manner it would use a log file. From a modeling perspective, the method used should be documented in the metadata for the model. As well, the schedule of the extract should be documented at this point. Later, in the production environment, actual extract statistics will be added to this metadata (see Figure 34 on page 77).

7.5.4.2 Generating Keys
Key selection in the data warehouse is a difficult issue. It involves a trade-off between performance and management. Key selection applies mainly to dimensions. The keys chosen for the dimensions must be the foreign keys of the fact. There are two choices for dimension keys. Either an arbitrary key can be assigned, or identifiers from the operational system can be used. An arbitrary key is usually just a sequential number where the next available number is assigned when a new key is required. To uniquely represent a dimension using identifiers from an operational system usually requires a composite key. A composite key is a key made up of multiple columns. An arbitrary key is one column and is almost always smaller than an operationally derived key. Therefore arbitrary keys will generally perform joins faster. Generation of an arbitrary key is slightly more complex. If you get your key from the operational system, there is no need to determine the next available key. The exception to this is where history of a dimension is kept. In this case, when you use identifiers from an operational system, you must add an additional key because keys must be unique. One option is an arbitrary sequence number. Another is to add begin and end time stamps to the dimension key. Both of these options also work for an arbitrary key, but it is simpler just to generate a new arbitrary key when an entry in a dimension changes.

Chapter 7. The Process of Data Warehousing

73

Once the history issue is considered, it certainly seems as if an arbitrary key is the way to go. However, the last factor in key selection is its impact on the fact table. When a fact is created, the key from each dimension must be assigned to it. If operationally derived keys, with time stamps for history, are used in the dimensions, there is no additional work when a fact is created. The linkage happens automatically. With arbitrary keys, or arbitrary history identifiers, a key must be assigned to a fact at the time the fact is created. There are two ways to assign keys. One is to maintain a translation table of operational and data warehouse keys. The other is to store the operational keys and, if necessary, time stamps, as attribute data on the dimension. The above discussion also applies to degenerate keys on the fact. The only difference is that there is no need to join on a degenerate key, thus diminishing the performance impact of an arbitrary key. The issue is more likely to come down to whether a user may need to know the value of a degenerate key for analysis purposes or that it is simply recorded to create the desired level of granularity. The choice, then, is between better performance of an arbitrary key and easier maintenance of an operational key. The questions of how much better performance and how much more maintenance must be evaluated in your own organization. Regardless of the choice you make, the keys, and the process that generates them, must be documented in the metadata (see Figure 34 on page 77). This data is necessary for the technical staff who administer and maintain the data warehouse. If the tools you use do not hide join processing, the user may need to understand this also. However, it is not recommended that a user be required to have this knowledge.

7.5.4.3 Getting from Source to Target
It is often the case that getting from source to target is a multiple step process. Rarely can it be completed in one step. Among the many reasons for creating a multiple step process to get from source to target are these:
• • • •

Sources to be merged are in different locations Not all data can be merged at once as some tables require outer joins Sources are stored on multiple incompatible technologies Complex summarization and derivation must take place

The point is simply that the process must be documented. The metadata for a model must include not only the steps of the process, but the contents of each step, as well as the reasons for it. It should look something like this: 1. Step 1 - Get Product Changes Objective of step Create a table containing rows where product information has changed Inputs to step Change transaction log for Products and Models, Product Component table, Component table, and the Product dimension table Transformations performed For each change record, read the related product component and component rows. For each product model, the cost of each

74

Data Modeling Techniques for Data Warehousing

component is multiplied by the number of components used to manufacture the model. The sum of results for all components that make up the model is the cost of that model. A key is generated for each record consisting of a sequential number starting with the next number after the highest used in the product dimension table. Write a record to the output table containing the generated key, the product and model keys, the current date, product description, model code, unit cost, suggested wholesale price, suggested retail price, and eligible for volume discount code. Outputs of step A work table containing new rows for the product dimension where there has been a change in a product or model 2. Step 2 - Get Component Changes Objective of step Create a table containing rows where component information has changed Inputs to step Change transaction log for Product Components and Components, Product table, Product Model table, the Product dimension table, and the work table from step 1 Transformations performed For each change record, check that the product and model exist in the work table. If they do, the component change is already recorded so ignore the change record. If not, read the product and model tables for related information. For each product model, the cost of each component is multiplied by the number of components used to manufacture the model. The sum of results for all components that make up the model is the cost of that model. A key is generated for each record consisting of a sequential number starting with the next number after the highest used in the product dimension table. Add a record to the work table containing the generated key, the product and model keys, the current date, product description, model code, unit cost, suggested wholesale price, suggested retail price, and eligible for volume discount code. Outputs of step A work table containing additional new rows for the product dimension where there has been a change in the product component table or the component table 3. Step 3 - Update Product Dimension Objective of step Add changes to the Product dimension Inputs to step Work table from step 2 Transformations performed For each row in the work table, a row is inserted into the product dimension. The effective to date is set to null. The effective to date of the previously current row is set to the day before the effective from date of the new row. A row is also written to a translation table containing the generated key, product key, model key, and change date.

Chapter 7. The Process of Data Warehousing

75

Outputs of step A translation table for use in assigning keys to facts and an updated product dimension We do not suggest that this is the best (or even a good) transform method. The purpose here is to point out the type of metadata that should be recorded (see Figure 34 on page 77).

7.5.5 Designing Subsidiary Targets
Subsidiary targets are targets derived from the originally designed fact and dimension tables. The reason for developing such targets is performance. If, for example, a user frequently runs a query that sums across one dimension and scans the entire fact table, it is likely that a subsidiary target should be created with the dimension removed and measures summed to produce a table with less rows for this query. Creating a subsidiary dimension should only be done if the original dimension will not join properly with a subsidiary fact. This is likely to be a tool-dependent decision. Because this is a performance issue, rules should be defined for when a subsidiary target will be considered. Consider a maximum allowable time for a query before an aggregate is deemed necessary. You may also create a sliding scale of time it takes to run a query versus the frequency of the query. Metadata for subsidiary targets should be the same as for the original facts and dimensions, with only the aggregates themselves being different. However, if your suite of tools can hide the subsidiary targets from the user and select them when appropriate based on the query, the metadata should be made visible only for technical purposes. The metadata should contain the reasons for creating the subsidiary target (see Figure 34 on page 77). Often it is not possible to predict which subsidiary targets will be necessary at the design stage. These targets should not be created unless there is a clear justification. Rather than commit significant resources to them at this time, consider creating them as a result of monitoring efforts in the post-implementation environment.

76

Data Modeling Techniques for Data Warehousing

Figure 34. The Complete Metadata Diagram for the Data Warehouse

7.5.6 Validating the Design
During the design stage you will create a test version of the production environment. When it comes time to validate the design with the user, hands-on testing is the best approach. Let the user try to answer questions through manipulation of the test target. Document any areas where the test target cannot provide the data requested. Aside from testing, review with the user any additions and changes to the model that have resulted from the design phase to ensure they are understandable. Similar to the model validation step, pass what works on to the implementation phase. What does not work should be returned to the requirements phase for clarification and reentry into modeling.

7.5.7 What About Data Mining?
Decisions in data warehouse modeling would typically not be affected by a decision to support data mining. However, the discussion on data mining, as one of the key data analysis techniques, is presented here for your information and completeness. As stated previously, data mining is about creating hypotheses, not testing them. It is important to make this distinction. If you are really testing hypotheses, the

Chapter 7. The Process of Data Warehousing

77

dimensional model will meet your requirements. It cannot, however, safely create a hypothesis. The reason for this is that by defining the dimensions of the data and organizing dimensions and measure into facts, you are building the hypotheses based on known rules and relationships. Once done, you have created a paradigm. To create a hypothesis, you must be able to work outside the paradigm, searching for patterns hidden in the unknown depths of the data. There are, in general, four steps in the process of making data available for mining: data scoping, data selection, data cleaning, and data transformation. In some cases, a fifth step, data summarization, may be necessary.

7.5.7.1 Data Scoping
Even within the scope of your data warehouse project, when mining data you want to define a data scope, or possibly multiple data scopes. Because patterns are based on various forms of statistical analysis, you must define a scope in which a statistically significant pattern is likely to emerge. For example, buying patterns that show different products being purchased together may differ greatly in different geographical locations. To simply lump all of the data together may hide all of the patterns that exist in each location. Of course, by imposing such a scope you are defining some, though not all, of the business rules. It is therefore important that data scoping be done in concert with someone knowledgeable in both the business and in statistical analysis so that artificial patterns are not imposed and real patterns are not lost.

7.5.7.2 Data Selection
Data selection consists of identifying the source data that will be mined. Generally, the main focus will be on a transaction file. Once the transaction file is selected, related data may be added to your scope. The related data will consist of master files relevant to the transaction. In some cases, you will want to go beyond the directly related data and delve into other operational systems. For example, if you are doing sales analysis, you may want to include store staff scheduling data, to determine whether staffing levels, or even individual staff, create a pattern of sales of particular products, product combinations, or levels of sales. Clearly this data will not be part of your transaction, and it is quite likely the data is not stored in the same operational system.

7.5.7.3 Data Cleaning
Once you have scoped and selected the data to be mined, you must analyze it for quality. When cleaning data that will be mined, use extreme caution. The simple act of cleaning the data can remove or introduce patterns. The first type of data cleaning (see 7.5.3, “Cleaning the Data” on page 72) is data validation. Validating the contents of a source field or column is very important when preparing data for mining. For example, if a gender code has valid values of M and F, all other values should be corrected. If this is not possible, you may want to document a margin of error for any patterns generated that relate to gender. You may also want to determine whether there are any patterns related to the bad data that can reveal an underlying cause. Documenting relationships is the act of defining the relationships when adding in data such as the sales schedules in our data selection example. An algorithm must be developed to determine what part of the schedule gets recorded with a particular transaction. Although it seems clear that a sales transaction must be related to the schedule by the date and time of the sale, this may not be enough. What if some salespeople tend to start earlier than their shift and leave a little

78

Data Modeling Techniques for Data Warehousing

earlier? As long as it all balances out, it may be easier for staff to leave the scheduling system alone, but your patterns could be distorted by such an unknown. Of course, you may not be able to correct the problem with this example. The point is simply that you must be able to document the relationship to be able to correctly transform the data for mining purposes. The second type of data cleaning (see 7.5.3, “Cleaning the Data” on page 72), data enhancement, is risky when preparing data for mining. It is certainly important to be able to relate all images of a customer. However, the differences that exist in your data may also expose hidden patterns. You should proceed with enhancement cautiously.

7.5.7.4 Data Transformation
Depending on the capabilities of the tools you select to perform data mining, a set of relational tables or a large flat file may meet your requirements. Regardless, data transformation is the act of retrieving the data identified in the scoping and selection processes, creating the relationships and performing some of the validation documented in the cleaning process, and producing the file or tables to be mined. We say ″some of the validation″ because data that is truly incorrect should be fixed in the source operational system before transformation, unless you need to find patterns to indicate the cause of the errors. Such pattern searching should only be necessary, and indeed possible, if there is a high degree of error in the source data.

7.5.7.5 Data Summarization
There may be cases where you cannot relate the transaction data to other data at the granularity of the transaction; for example, the data needed to set the scope at the right level is not contained in the original transaction data. In such cases, you may consider summarizing data to allow the relationships to be built. However, be aware that altering your data in this way may remove the detail needed to produce the very patterns for which you are searching. You may want to consider mining at two levels when this summarization appears to be necessary.

7.6 The Dynamic Warehouse Model
In an operational system, shortly after implementation the system stabilizes and the model becomes static, until the next development initiative. But, the data warehouse is more dynamic, and it is possible for the model to change with no additional development initiative simply because of usage patterns. Metadata is constantly added to the data warehouse from four sources (see Figure 35 on page 80). Monitoring of the warehouse provides usage statistics. The transform process adds metadata about what and how much data was loaded and when it was loaded. An archive process will record what data has been removed from the warehouse, when it was removed, and where it is stored. A purge process will remove data and update the metadata to reflect what remains in the data warehouse.

Chapter 7. The Process of Data Warehousing

79

Figure 35. Metadata Changes in the Production Data Warehouse Environment

Based on usage and performance statistics or requests from users a data warehouse administrator may add or remove aggregates or alter archive and purge schedules. Such changes must also be reflected in the metadata (see Figure 34 on page 77). Once a data warehouse is implemented, usage of it will spawn new requests and requirements. This will start another cycle of development, continuing the iterative and evolutionary process of building the data warehouse. As you can see, the data model is a living part of a data warehouse. Through the entire life cycle of the data warehouse, the data model is both maintained and used (see Figure 36). The process of data warehouse modeling can be truly endless.

Figure 36. Use of the Warehouse Model throughout the Life Cycle

80

Data Modeling Techniques for Data Warehousing

Chapter 8. Data Warehouse Modeling Techniques
Data warehouse modeling is the process of building a model for the data that is to be stored in the data warehouse. The model produced is an abstract model, and in this sense, it is a representation of reality, or at least a part of reality which the data warehouse is assumed to support. When considered like this, data warehouse modeling seems to resemble traditional database modeling, which most of us are familiar with in the context of database development for operational applications (OLTP database development). This resemblance should be considered with great care, however, because there are a number of significant differences between data warehouse modeling and OLTP database modeling. These differences impact not only the modeling process but also the modeling techniques to be used. In Chapter 7, “The Process of Data Warehousing,” the basic issues and steps of a data warehouse modeling process were described. This chapter focuses entirely on the techniques involved in a data warehouse modeling process. It extends and complements Chapter 7, “The Process of Data Warehousing” in several ways:
•

•

•

Whereas Chapter 7, “The Process of Data Warehousing” focuses on the modeling process, this chapter focuses on data warehouse modeling techniques. Whereas Chapter 7, “The Process of Data Warehousing” in large part deals with the basic issues of dimensional modeling and illustrates how end-user requirements can be captured and somehow formalized in what is an initial dimensional model, this chapter investigates data warehouse modeling beyond these aspects. Whereas Chapter 7, “The Process of Data Warehousing” considers data warehouse modeling primarily from the point of view of rapid development of an independent data mart, this chapter is concerned with making data warehouse models that are suitable for integration with other data marts or can be deployed within a corporate data warehouse environment. Although we focus on modeling techniques, we try to present the techniques as part of a structured approach to data warehouse modeling. However, more work is required in this area to develop a methodological approach to data warehouse modeling.

8.1 Data Warehouse Modeling and OLTP Database Modeling
Before studying data warehouse modeling techniques, it is worthwhile investigating the differences between data warehouse modeling and OLTP database modeling. This will give you a better idea of why new or adapted techniques are required for performing data warehouse modeling will help you understand how to set up a data warehouse modeling approach or methodology.

© Copyright IBM Corp. 1998

81

8.1.1 Origin of the Modeling Differences
There are three main reasons why data warehouse modeling requires modeling techniques other than OLTP database modeling or why traditional modeling techniques, when used in data warehouse development projects, require a significantly different focus.
•

•

•

A data warehouse has base properties that make it fundamentally different from OLTP databases. In the next section, these properties and the impact they have on data warehouse modeling are investigated further. The computing context in which a data warehouse resides differs from the context in which OLTP databases reside. Users of OLTP applications are ″shielded″ from the database structure because they interact through user interfaces and use application services for working with the databases. Users of a data warehouse, however, are much more directly involved with the data warehouse model and the way data is organized in the warehouse. Failing to make models that are simple to understand and directly represent the end user′s perception of reality is one of the worst things that can happen to a data warehouse enablement project. Inherent to data warehouse enablement is the fuzziness and incompleteness of end-user requirements and the continuous evolution of the data warehouse. These incomplete requirements call for a flexible modeling process and for techniques which are appropriate for evolutionary development. The risks of flexible and evolutionary software development are incoherence and inconsistency of the end result. These issues certainly require attention when performing data warehouse modeling.

Most of the above reasons for why data warehouse modeling is different from OLTP database modeling also apply in the context of data mart development. Although the development of data marts may appear to be less complicated than the development of corporate data warehouses, many of the properties of a data warehouse that make modeling so different from OLTP database modeling also apply for data mart development projects. In addition, the impact of end users and end-user requirements on the modeling process and techniques applied for data marts become even more important for data warehouses.

8.1.2 Base Properties of a Data Warehouse
Some of the most significant differences between data warehouse modeling and OLTP database modeling are related to the base properties of a data warehouse, which are summarized in Figure 37 on page 83.

82

Data Modeling Techniques for Data Warehousing

Figure 37. Base Properties of a Data Warehouse.

A data warehouse is an integrated collection of databases rather than a single database. It should be conceived as the single source of information for all decision support processing and all informational applications throughout the organization. A data warehouse is an organic ′thing′, and it tends to become big, if not big from the beginning. In addition to the obvious requirement that a data warehouse should satisfy the needs of end users, there is also a great need to achieve maximum consistency throughout the whole data warehouse environment, at the level of primitive data and derived data, and also within the information derivation processes themselves. A data warehouse contains data that belongs to different information subject areas, which can be the basis for logically partitioning the data warehouse in several different (conceptual or even physical) databases. A data warehouse also contains different categories of data. It contains primitive data (the ″System of Record″) either represented and organized as an accumulation of captured source data changes, business events and transactions, or as an interpreted and well-structured historical database. In many cases both representations of primitive data are present in the data warehouse and are positioned and mapped to form an integrated collection of data that represents ″the corporate memory.″ Another major category of data in the data warehouse is that which is condensed and aggregated in information analysis databases having a format and layout that is directly suitable for end users to interpret and use. A data warehouse also usually contains ″support databases,″ which are not directly of interest to end users for their data analysis activities but are important components in the process of capturing source data and delivering consistent information to end users. Clearly, data warehouse modeling must consist of different kinds of modeling techniques. The System of Record is usually best if not modeled using the same modeling techniques as the end-user-oriented information analysis databases. If, in addition, one considers that end users may be dealing with decision support tools (query and reporting, OLAP, data mining, ...) and informational applications that have different usage and development characteristics, it becomes clear that data warehouse modeling is in fact a compilation of different modeling techniques, each with its own area of applicability.

Chapter 8. Data Warehouse Modeling Techniques

83

8.1.3 The Data Warehouse Computing Context
Data warehouses have to be developed within a computing context that differs from the context in which OLTP database applications are developed (see Figure 38).

Figure 38. Data Warehouse Computing Context.

There is a fundamental difference between the way end users use OLTP databases and data warehouses. OLTP users are shielded from the databases by an application layer. They perform tasks, usually consisting of a fixed number of predefined database operations, which are part of a fixed transaction workflow. Data warehouse applications are totally different. They are data-centric rather than process-centric. End users deal almost directly with the data and there are no fixed workflows (with a few exceptions here and there). End users are not interested in recording data in the warehouse: they want to get information out of the warehouse. They raise questions against the warehouse, test and verify hypotheses with information they drag out of the warehouse, reconstruct chains of events, which they then analyze possibly to detect patterns or seasonal trends, and make extrapolations and projections for the future. Data warehouses are very much open collections of data, and end-user involvement in the enablement process is known to be a vital element of success. In addition, good data warehouses realize what could be called the information supermarket principle , whereby end users freely access the data warehouse when they need information for their own purposes. Figure 38 also points to the other side of the coin. Data warehouse developers, including those who do data warehouse modeling, do have to take into account that the data warehouse ″input context″ consists of a legacy data processing environment residing in a legacy business process environment. Required data may not be available or perhaps cannot be captured at the sufficient level of detail, unless money and effort are spent changing the legacy input environment. Data warehouse enablement projects therefore often get involved with business process and source application reengineering.

84

Data Modeling Techniques for Data Warehousing

