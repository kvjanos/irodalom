

Research and Trends in Data Mining Technologies and Applications
Davd Tanar, Monash Unversty, Australa

IDeA GRoup publIshInG
Hershey • London • Melbourne • Singapore


Acquisition Editor: Senior Managing Editor: Managing Editor: Assistant Managing Editor: Development Editor: Copy Editor: Typesetter: Cover Design: Printed at: Kristin Klinger Jennifer Neidig Sara Reed Sharon Berger Kristin Roth Maria Boyer Jamie Snavely Lisa Tosheff Integrated Book Technology

Published in the United States of America by Idea Group Publishing (an imprint of Idea Group Inc.) 701 E. Chocolate Avenue Hershey PA 17033 Tel: 717-533-8845 Fax: 717-533-8661 E-mail: cust@idea-group.com Web site: http://www.idea-group.com and in the United Kingdom by Idea Group Publishing (an imprint of Idea Group Inc.) 3 Henrietta Street Covent Garden London WC2E 8LU Tel: 44 20 7240 0856 Fax: 44 20 7379 0609 Web site: http://www.eurospanonline.com Copyright © 2007 by Idea Group Inc. All rights reserved. No part of this book may be reproduced in any form or by any means, electronic or mechanical, including photocopying, without written permission from the publisher. Product or company names used in this book are for identification purposes only. Inclusion of the names of the products or companies does not indicate a claim of ownership by IGI of the trademark or registered trademark. Library of Congress Cataloging-in-Publication Data Research and trends in data mining technologies and applications / David Taniar, editor. p. cm. -- (Advanced topics in data warehousing and mining ; vol. 1) Includes bibliographical references and index. Summary: “This book focuses on the integration between the fields of data warehousing and data mining, with emphasis on applicability to real-world problems; it book provides an international perspective, highlighting solutions to some of researchers’ toughest challenges. Developments in the knowledge discovery process, data models, structures and design offer answers and solutions”--Provided by publisher. ISBN 1-59904-271-1 (hardcover) -- ISBN 1-59904-272-X (softcover) -- ISBN 1-59904-273-8 (ebook) 1. Data mining. 2. Data warehousing. 3. Web databases. I. Taniar, David. QA76.9.D343.R49 2006 005.74--dc22 2006032160 British Cataloguing in Publication Data A Cataloguing in Publication record for this book is available from the British Library. All work contributed to this book is new, previously-unpublished material. The views expressed in this book are those of the authors, but not necessarily of the publisher.



Research and Trends in Data Mining Technologies and Applications Table of Contents

Preface. ............................................................................................................. vi .

Section.I:. Data.Warehousing.and.Mining Chapter.I Combining.Data.Warehousing.and.Data.Mining.Techniques.for. Web.Log.Analysis.............................................................................................. 1 Torben Bach Pedersen, Aalborg University, Denmark Jesper Thorhauge, Conzentrate, Denmark Søren E. Jespersen, Linkage, Denmark Chapter.II Computing.Dense.Cubes.Embedded.in.Sparse.Data................................... 29 Lixin Fu, The University of North Carolina at Greensboro, USA Chapter.III Exploring.Similarities.Across.High-Dimensional.Datasets......................... 53 Karlton Sequeira, Rensselaer Polytechnic Institute, USA Mohammed Zaki, Rensselaer Polytechnic Institute, USA

iv

Section II: Patterns Chapter IV Pattern Comparison in Data Mining: A Survey .......................................... 86 	 Irene Ntoutsi, University of Piraeus, Greece Nikos Pelekis, University of Piraeus, Greece Yannis Theodoridis, University of Piraeus, Greece Chapter V Mining Frequent Patterns Using Self-Organizing Map ........................... 121 	 Fedja Hadzic, University of Technology Sydney, Australia Tharam Dillon, University of Technology Sydney, Australia Henry Tan, University of Technology Sydney, Australia Ling Feng, University of Twente, The Netherlands Elizabeth Chang, Curtin University of Technology, Australia Chapter VI An Efficient Compression Technique for Vertical Mining Methods ....... 143 	 Mafruz Zaman Ashrafi, Monash University, Australia David Taniar, Monash University, Australia Kate Smith, Monash University, Australia Section III: Data Mining in Bioinformatics Chapter VII A Tutorial on Hierachical Classification with Applications in Bioinformatics .............................................................................................. 175 	 Alex Freitas, University of Kent, UK André C.P.L.F. de Carvalho, University of São Paulo, Brazil Chapter VIII Topological Analysis and Sub-Network Mining of Protein-Protein Interactions ....................................................................... 209 	 Daniel Wu, Drexel University, USA Xiaohua Hu, Drexel University, USA

v

Section IV: Data Mining Techiques Chapter IX Introduction to Data Mining Techniques via Multiple Criteria Optimization Approaches and Applications .............................................. 242 	 Yong Shi, University of the Chinese Academy of Sciences, China & University of Nebraska at Omaha, USA Yi Peng, University of Nebraska at Omaha, USA Gang Kou, University of Nebraka at Omaha, USA Zhengxin Chen, University of Nebraska at Omaha, USA Chapter X Linguistic Rule Extraction from Support Vector Machine Classifiers ...................................................................................................... 276 	 Xiuju Fu, Institute of High Performance Computing, Singapore Lipo Wang, Nanyang Technological University, Singapore GihGuang Hung, Institute of High Performance Computing, Singapore Liping Goh, Institute of High Performance Computing, Singapore Chapter XI Graph-Based Data Mining .......................................................................... 291 	 Wenyuan Li, Nanyang Technological University, Singapore Wee-Keong Ng, Nanyang Technological University, Singapore Kok-Leong Ong, Deakin University, Australia Chapter XII Facilitating and Improving the Use of Web Services with Data Mining ......................................................................................... 308 	 Richi Nayak, Queensland University, Australia About the Authors ........................................................................................ 327 Index .............................................................................................................. 336

v

preface

In enterprises, a large volume of data has been collected and stored in data warehouses. Advances in data gathering, storage, and distribution have created a need for integrating data warehousing and data mining techniques. Mining data warehouses raises unique issues and requires special attention. Data warehousing and data mining are inter-related, and require holistic techniques from the two disciplines. The “Advanced Topics in Data Warehousing and Mining” series comes into place to address some issues related to mining data warehouses. To start this series, this volume 1, includes 12 chapters in four sections, contributed by authors and editorial board members from the International Journal of Data Warehousing and Mining. Section I, on Data Warehousing and Mining, consists of three chapters covering data mining techniques applied to data warehouse Web logs, data cubes, and highdimensional datasets. Chapter.I, “Combining Data Warehousing and Data Mining Techniques for Web Log Analysis” by Torben Bach Pedersen (Aalborg University, Denmark), Jesper Thorhauge (Conzentrate, Demark), and Søren E. Jespersen (Linkage, Denmark), brings together data warehousing and data mining by focusing on data that has been collected in Web server logs. This data will only be useful if high-level knowledge about user navigation patterns can be analyzed and extracted. There are several approaches to analyze Web logs. They propose a hybrid method that combines data warehouse Web log schemas and a data mining technique called Hyper Probabilistic Grammars, resulting in a fast and flexible Web log analysis. Further enhancement to this hybrid method is also outlined. Chapter. II, “Computing Dense Cubes Embedded in Sparse Data” by Lixin Fu (The University of North Carolina at Greensboro, USA) focuses on a sparse input dataset of high-dimensional data whereby both number of dimensions and cardinalities of dimensions are large, but a dense aggregate dataset. The chapter proposes

v

a new dynamic data structure called Restricted Sparse Statistics Trees and a cube evaluation algorithm to efficiently compute dense sub-cubes embedded in highdimensional sparse input datasets. The proposed algorithm is scalable, as well as incrementally maintainable, which is suitable for data warehousing and the analysis of streaming data. Chapter.III, “Exploring Similarities Across High-Dimensional Datasets” by Karlton Sequeira and Mohammed Zaki (Rensselaer Polytechnic Institute, USA), concentrates on the problem whereby related data collected from a number of different sources may not be able to share the entire dataset. They however may only be willing to share a condensed version of their datasets, due to various reasons, such as privacy issues, dataset size, and so forth. To solve this problem, they propose a framework for constructing condensed models of datasets and algorithms to find similar substructure in pairs of such models. Their analysis shows that they could find more interesting results from the combined model than those obtained from independent analysis of the original datasets. Section II, on Patterns, consists of three chapters covering pattern comparisons, frequent patterns, and vertical mining patterns. Chapter.IV, “Pattern Comparison in Data Mining: A Survey” by Irene Ntoutsi, Nikos Pelekis, and Yannis Theodoridis (University of Piraeus, Greece), provides a thorough survey on pattern comparison. Pattern comparison aims at evaluating how close to each other two patterns are. In this chapter, the authors focus on pattern comparison in frequent itemsets and association rules, clusters and clusterings, and decision trees. Chapter. V, “Mining Frequent Patterns Using Self-Organizing Map” by Fedja Hadzic, Tharam Dillon, and Henry Tan (University of Technology Sydney, Australia), along with Ling Feng (University of Twente, The Netherlands), and Elizabeth Chang (Curtin University of Technology, Australia) investigates a non-traditional approach of extracting frequent patterns using self-organizing map (SOM), which is an unsupervised neural network technique. SOM has normally been used for clustering, but not association rules and frequent itemsets generation. This chapter discusses issues of using a SOM clustering technique for the purpose of generating association rules. It also includes some case studies comparing the SOM approach and the traditional association rule approach. Chapter.VI, “An Efficient Compression Technique for Vertical Mining Methods” by Mafruz Zaman Ashrafi and Kate Smith (Monash University, Australia), highlights the performance problems of many efficient association rule mining algorithms, particularly when the dataset is large or the user-specified support is low, due to a poor treatment of main memory. To solve this problem, this chapter proposes an algorithm for vertical association rule mining that compresses a vertical dataset in an efficient manner by utilizing bit vectors. The performance shows that the proposed compression ratio offers better results than those of the other well-known techniques.

v

Section III, on Data Mining in Bioinformatics, presents data mining applications in the bioinformatics domain. This part consists of two chapters covering hierarchical classification, and topological analysis and sub-network mining. Chapter. VII, “A Tutorial on Hierarchical Classification with Applications in Bioinformatics” by Alex Freitas (University of Kent, UK) and André C.P.L.F. de Carvalho (University of São Paulo, Brazil), presents a comprehensive tutorial on complex classification problems suitable for bioinformatics applications, particularly the prediction of protein function, whereby the predicted classes are hierarchical in nature. Chapter.VIII, “Topological Analysis and Sub-Network Mining of Protein: Protein Interactions” by.Daniel Wu and Xiaohua Hu (Drexel University, USA), reports a comprehensive evaluation of the topological structure of protein-protein interaction networks across different species and confidence levels, by mining and analyzing graphs constructed from public domain bioinformatics datasets. The authors also report some statistical analysis whereby the results obtained are far from a power law, contradicting many published results. The final section of this volume, Data Mining Techniques, consists of four chapters, covering data mining techniques using multiple criteria optimization, support vector machine classifiers, graph-based mining, and Web services. Chapter.IX, “Introduction to Data Mining Techniques via Multiple Criteria Optimization Approaches and Applications” by Yong Shi (University of the Chinese Academy of Sciences, China and University of Nebraska at Omaha, USA) and Yi Peng, Gang Kou, and Zhengxin Chen (University of Nebraska at Omaha, USA), gives an overview of a series of multiple criteria optimization-based data mining methods that utilize the multiple criteria programming (like multiple criteria linear programming, multiple criteria quadratic programming, and multiple criteria fuzzy linear programming) to solve data mining problems. The chapter includes some case studies, including credit card scoring, HIV-1, and network introduction detection. The authors also highlight some research challenges and opportunities in these areas. Chapter. X, “Linguistic Rule Extraction from Support Vector Machine Classifiers” by Xiuju Fu (Institute of High Performance Computing, Singapore), Lipo Wang (Nanyang Technological University, Singapore), GihGuang Hung (Institute of High Performance Computing, Singapore), and Liping Goh (Institute of High Performance Computing, Singapore), shows how decisions from an SVM classifier can be decoded into linguistic rules based on the information provided by support vectors and decision function. They show that the rule extraction results from the proposed method could follow SVM classifier decisions very well. Chapter.XI, “Graph-Based Data Mining” by Wenyuan Li (Richardson, USA) and Wee-Keong Ng (Nanyang Technological University, Singapore), along with KokLeong Ong (Deakin University, Australia), systematically reviews theories and

x

techniques in graph mining. This chapter particularly focuses on approaches that are potentially valuable to graph-based data mining. Finally, Chapter.XII, “Facilitating and Improving the Use of Web Services with Data Mining” by Richi Nayak (Queensland University, Australia), presents an overview of an area in which data mining may be useful, especially in the area of Web services. This chapter examines how some of the issues of Web services can be addressed through data mining. Overall, this volume covers important foundations to research and applications in data mining, covering patterns and techniques, as well as issues of mining data warehouses and an important application domain, namely bioinformatics. The different types of chapters, some of which are surveys and tutorials, while others propose novel techniques and algorithms, show a full spectrum of the coverage of this important topic.

David Taniar Editor-in-Chief, Journal of International Journal of Data Warehousing and Mining 2006

x

Section I Data Warehousing and Mining

Combnng Data Warehousng and Data Mnng Technques 

Chapter.I

Combining.Data. Warehousing.and. Data.Mining.Techniques. for.Web.Log.Analysis
Torben Bach Pedersen, Aalborg Unversty, Denmark Jesper Thorhauge, Conzentrate, Denmark Søren E. Jespersen, Lnkage, Denmark

Abstract
Enormous amounts of information about Web site user behavior are collected in Web server logs. However, this information is only useful if it can be queried and analyzed to provide high-level knowledge about user navigation patterns, a task that requires powerful techniques.This chapter presents a number of approaches that combine data warehousing and data mining techniques in order to analyze Web logs. After introducing the well-known click and session data warehouse (DW) schemas, the chapter presents the subsession schema, which allows fast queries on sequences
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

of page visits. Then, the chapter presents the so-called “hybrid” technique, which combines DW Web log schemas with a data mining technique called Hypertext Probabilistic Grammars, hereby providing fast and flexible constraint-based Web log analysis. Finally, the chapter presents a “post-check enhanced” improvement of the hybrid technique.

Introduction
With the large number of companies using the Internet to distribute and collect information, knowledge discovery on the Webor Web mininghas become an important research area. Web mining can be divided into three areas, namely Web content mining, Web structure mining, and Web usage mining (also called Web log mining) (Cooley, Srivastava, & Mobasher, 1997). Web content mining focuses on discovery of information stored on the Internetthat is, the various search engines. Web structure mining can be used when improving the structural design of a Web site. Web usage mining, the main topic of this chapter, focuses on knowledge discovery from the usage of individual Web sites. Web usage mining is mainly based on the activities recorded in the Web log, the log file written by the Web server recording individual requests made to the server. An important notion in a Web log is the existence of user sessions. A user session is a sequence of requests from a single user within a certain time window. Of particular interest is the discovery of frequently performed sequences of actions by the Web userthat is, frequent sequences of visited Web pages. The work presented in this chapter has to a large extent been motivated by collaboration with the e-learning company Zenaria (www.zenaria.com). Zenaria is in the business of creating e-learning, namely interactive stories told through a series of video-sequences. The story is formed by a user first viewing a video-sequence and then choosing between some predefined options, based on the video-sequence. Depending on the user’s choice, a new video-sequence is shown and new options are presented. The choices of a user will form a complete storya walkthrough reflecting the choices made by the individual users. Traditionally, stories have been distributed on CD-ROM, and a consultant has evaluated the walkthroughs by observing the users. However, Zenaria now wants to distribute its stories using the Internet (i.e., a walkthrough will correspond to a Web session). Thus, the consultants will now have to use Web usage mining technology to evaluate the walkthroughs. Data warehousing and database management system (DBMS) technologies excel in handling large amounts of data with good performance and ease-of-use. These technologies support a wide range of complex analysis queries, such as aggregation queries with constraints very well due to techniques such as bitmapped indices, maCopyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

terialized views, and algebraic query optimization. However, certain very specialized analysis queries are not well supported in standard systems. On the other hand, data mining techniques generally handle one type of very specialized analysis query very well, but this often comes at the expense of generality, ease-of-use, and scalability since data must be managed separately in dedicated file structures outside a DBMS. Thus, a lot of advantages can be gained by combining data warehousing/DBMS and data mining technologies (Han, 1997; Hinneburg, Lehner, & Habich, 2000; Sarawagi, Thomas, & Agrawal, 1998). Much work has been performed on extracting various patterns from Web logs, and the application of the discovered knowledge range from improving the design and structure of a Web site to enabling companies to provide more targeted marketing. One line of work features techniques for working directly on the log file (Cooley et al., 1997; Cooley et al., 1999). Another line of work concentrates on creating aggregated structures of the information in the Web log (Spiliopoulou et al., 1998; Pei et al., 1999). The Hypertext Probabilistic Grammar framework (Borges et al., 1999), utilizing the theory of grammars, is such an aggregated structure. Yet another line of work focuses on using database technology in the clickstream analysis (Andersen et al., 2000; Büchner, Anand, Mulvenna, & Hughes, 2000), building a so-called “data Webhouse” (Kimball & Merz, 2000). Several database schemas have been suggested, for example, the click fact star schema where the individual click is the primary fact (Kimball & Merz, 2000). Several commercial tools for analyzing Web logs exist (Wu, Yu, & Ballman, 1998; Spiliopoulou et al., 1998), but their focus is mostly on statistical measures (e.g., most frequently visited pages), and they provide only limited facilities for clickstream analysis. Finally, a prominent line of work focuses on mining sequential patterns in general sequence databases (Agrawal et al., 1995; Agrawal et al., 1996; Pei et al., 1999; Pei, Han, B. Mortazavi-asl, & Zhu, 2000). However, all the mentioned approaches have inherent weaknesses in that they either have huge storage requirements, slow performance due to many scans over the data, or problems when additional information (e.g., user demographics) are introduced into the analysis. Another line of research tries to provide database/data warehouse support for generic data mining techniques, but does not consider Web log mining (Han, 1997; Hinneburg et al., 2000; Sarawagi et al., 1998). This chapter presents a number of approaches that combine data warehousing and data mining techniques in order to analyze Web logs. After introducing the well-known click and session DW schemas (Kimball & Merz, 2000), the chapter presents the subsession schema which allows fast queries on sequences of page visits (Andersen et al., 2000). Then, the chapter presents the so-called hybrid technique (Jespersen, Thorhauge, & Pedersen, 2002), which combines DW Web log schemas (Kimball & Merz, 2000) with a data mining technique called Hypertext Probabilistic Grammars (Borges, 2000), hereby providing fast and flexible constraint-based Web log analysis. Finally, the chapter presents a “post-check enhanced” improvement of the hybrid technique. This chapter is the first to provide an integrated presentation
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

of approaches combining data warehousing and data mining techniques for Web log analysis. The chapter is organized as follows. The next section presents the well-known click and session fact schemas. Then, the subsession schema is presented, followed by a section on the hybrid approach. Finally, the post-check enhancement of the hybrid approach is presented, followed by a conclusion.

Click and Session Fact Schemas
The Click Fact Schema
The click fact schema uses the individual click on the Web site as the essential fact in the data warehouse (Kimball & Merz, 2000). This will preserve most of the information found in a Web log in the data warehouse. In our case, the click fact schema contains the following dimensions: URL dimension (the Web pages) (note that the fact table has references to both the requested page and the referring page, also known as the “referrer”), Date dimension, TimeOfDay dimension, Session dimension, and Timespan dimension. We could easily add any desired additional information as extra dimensions, for example, a User dimension capturing user demographics. An example Click Fact schema can be seen in Figure 1.

Figure 1. Click fact schema

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

A strong point of this approach is that almost all information from the Web log is retained within the data warehouse. Individual clicks within the individual sessions can be tracked, and very detailed click information is available. This approach can utilize existing OLAP techniques such as pre-aggregation to efficiently extract knowledge on individual pages. When querying for sequences of clicks, using the click fact schema, several join and self-join operations are needed to extract even relative short sequences (Andersen et al., 2000), severely degrading performance for large fact tables.

The.Session.Fact.Schema
The session fact schema uses the entire session as the primary fact in the data warehouse (Kimball & Merz, 2000), thereby ignoring the individual clicks within a session. Only the requests for the start and end page are stored as a field value on the individual session fact entry. The approach is therefore not suitable for querying for detailed information about sequences of clicks or even individual clicks in a session. Queries on entire sessions are however quite efficient (Andersen et al., 2000). An example session fact schema can be seen in Figure 2. We will not present strong and weak points for this approach, since it is unable to perform analysis of sequences of clicks, which is a key requirement for us.

Figure 2. Session fact schema

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

The.Subsession.Fact.Schema
The subsession fact schema was first proposed by Andersen et al. (2000) and is aimed specifically at clickstream analysis on sequences of clicks. The approach introduces the concept of using subsessions as the factthat is, explicitly storing all possible subsequences of clicks from a session in the data warehouse. This means that for all sessions, subsessions of all lengths are generated and stored explicitly in the data warehouse. In our case, the subsession fact schema has the dimensions Date, Session, TimeOfDay, Timespan, as well as the URL_Sequence dimension capturing the corresponding sequence of Web page requests. An example subsession fact schema can be seen in Figure 3. Let us compare the SQL queries used to analyze subsessions in the click fact schema and the subsession fact schema. The SQL query used to find the most common subsessions of lengths 2-5 in a click fact table, sorted with the most frequent longest subsessions first, is seen in Figure 4. The || operator denotes concatenation. This query is almost one page of extremely complicated SQL, making it very hard to write correctly for everyone but the most seasoned SQL veterans. Even more importantly, the query has very bad query performance as the (huge) fact table has to be joined to itself up to five times, as well as to five (smaller) dimension tables, requiring a lot of server resources. Because of the complexity of the query, it cannot be efficiently supported by materialized views as found in current DBMSs. Fur-

Figure 3. Subsession fact schema

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

Figure 4. Subsession SQL query for click fact schema (taken from Andersen et al., 2000)
SELECT us.url_sequence AS start_url,u2.url_name as end_url, s.length, COUNT(*) AS occurences FROM subsession_fact s,url_sequence us WHERE us.subsession_key=s.subsession_key AND length<=5
SELECT url_sequence,length,occurences FROM ( (SELECT u1.url_name||u2.url_name as url_sequence, 2 AS length, COUNT(*) AS occurences FROM url_dimension u1,url_dimension u2,click_fact c1,click_fact c2 WHERE c1.number_in_session=c2.number_in_session-1 AND c1.session_key = c2.session_key AND c1.url_key=u1.url_key AND c2.url_key=u2.url_key GROUP BY url_sequence,length) UNION ALL (SELECT u1.url_name||u2.url_name|| u3.url_name as url_sequence, 3 AS length, COUNT(*) AS occurences FROM url_dimension u1,url_dimension u2,url_dimension u3, click_fact c1,click_fact c2,click_fact c3 WHERE c1.number_in_session=c2.number_in_session-1 AND c2.number_in_session=c3.number_in_session-1 AND c1.session_key = c2.session_key AND c2.session_key = c3.session_key AND c1.url_key=u1.url_key AND c2.url_key=u2.url_key AND c3.url_key=u3.url_key GROUP BY url_sequence,length) UNION ALL (SELECT u1.url_name||u2.url_name||u3.url_name||u4.url_name AS url_sequence, 4 AS length, COUNT(*) AS occurences FROM url_dimension u1,url_dimension u2,url_dimension u3,url_dimension u4, click_fact c1,click_fact c2,click_fact c3,click_fact c4 WHERE c1.number_in_session=c2.number_in_session-1 AND c2.number_in_session=c3.number_in_session-1 AND c3.number_in_session=c4.number_in_session-1 AND c1.session_key = c2.session_key AND c2.session_key = c3.session_key AND c3.session_key = c4.session_key AND c1.url_key=u1.url_key AND c2.url_key=u2.url_key AND c3.url_key=u3.url_key AND c4.url_key=u4.url_key GROUP BY url_sequence,length) UNION ALL (SELECT u1.url_name|| u2.url_name||u3.url_name||u4.url_name||u5.url_name AS url_sequence, 5 AS length, COUNT(*) AS occurrences FROM url_dimension u1,url_dimension u2,url_dimension u3,url_dimension u4, url_dimension u5,click_fact c1,click_fact, c2,click_fact c3,click_fact c4,click_fact c5 WHERE c1.number_in_session=c2.number_in_session-1 AND c2.number_in_session=c3.number_in_session-1 AND c3.number_in_session=c4.number_in_session-1 AND c4.number_in_session=c5.number_in_session-1 AND c1.session_key = c2.session_key AND c2.session_key = c3.session_key AND c3.session_key = c4.session_key AND c4.session_key = c5.session_key c1.url_key=u1.url_key AND c2.url_key=u2.url_key) AND c3.url_key=u3.url_key AND c4.url_key=u4.url_key) AND c5.url_key=u5.url_key GROUP BY url_sequence,length) ) ORDER BY occurences DESC,length DESC,url_sequence ASC

thermore, in the sequence join conditions, for example, “c2.number_in_session=c3. number_in_session-1”, the “-1” part of the condition means using indices joining will not be applicable, leading to very long query processing times. If we wanted to find subsessions of length 2-8 instead, the query would be twice as long, with even worse performance.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

In contrast, the simple query seen in Figure 4 finds the same answer using a subsession fact table. The query only joins the fact table with one small dimension table, and the query can be efficiently supported by materialized views. As we have seen, storing subsessions explicitly in the database instead of implicitly (through join-operations) will allow for better performance on queries concerning sequences of clicks. There is a tradeoff between the amount of storage used and query performance, but if a Web site has relatively short sessions, the storage overhead is manageable (Andersen et al., 2000). This technique can answer sequencerelated queries much faster than, for example, the click fact schema (Andersen et al., 2000). However, as hinted above, the longer the sessions, the more storage is needed for storing the information for all subsessions. The number of subsessions generated from a session of length n can be estimated using the formula 0.5*(n-1)2 (Andersen et al., 2000). A study shows that the storage overhead can vary between a factor of 4 and a factor above 20, depending on the characteristics of the sessions on a Web site (Andersen et al., 2000). Some methods for reducing the number of subsessions that are to be stored have been suggested, including ignoring either very short or very long subsessions or ignoring certain subsessions. For example, if only subsessions of length 2-10 are stored, the number of subsession facts will only be nine times the number of click facts, meaning that it is feasible to store them all.

Hypertext.Probabilistic.Grammars
The nature of Web sites, Web pages, and link navigation has a nice parallel in formal language theory which proves rather intuitive and presents a model for extracting information about user sessions. The model uses a Hypertext Probabilistic Grammar (HPG) (Levene et al., 1999) that rests upon the well-established theoretical area of languages and grammars. We will present this parallel using the example Web to the left in Figure 5.

Figure 5. Example Web site and corresponding HPG

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

The left side of Figure 5 shows a number of Web pages and the links that connect them. As can be seen, the structure is very similar to a grammar with a number of states and a number of productions leading from one state to another. It is this parallel that the model explores. The model uses all Web pages as states (this is only true if the HPG is created with a so-called “history depth” of 1, to be addressed below) and adds two additional artificial states, the start state S and the end state F, to form all states of the grammar. We will throughout the chapter use the terms state and page interchangeably. From the processing of the sessions in the Web log, each state will be marked with the number of times it has been requested. The probability of a production is assigned based on the information in the Web log so that the probability of a production is proportional to the number of times the given link was traversed relative to the number of times the state on the left side of the production was visited. Note that not all links within a Web site may have been traversed, so some of the links might not be represented in an HPG. The probability of a string in the language of the HPG can be found by multiplying the probabilities of the productions needed to generate the string. Note that Web pages might be linked in a circular fashion and therefore the language of the HPG could be infinite. A HPG therefore specifies a threshold η against which all strings are evaluated. If the probability of the string is below the threshold, the string will not be included in the language of the HPG (with the assigned threshold). This will generate a complete language for a given HPG with a given threshold, Lη. Mining an HPG is essentially the process of extracting high-probability strings from the grammar. These strings are called rules (the notion of a rule and a string will be used interchangeably). These rules will describe the most preferred trails on the Web site since they are traversed with a high probability. Mining can be done using both a breath-first and a depth-first search algorithm (Borges, 2000). A parameter α is used when mining an HPG to allow for mining of rules “inside” the grammarthat is, rules with a leftmost state that has not necessarily been the first request in any session. This is done by assigning probability to productions from the start state to all other states depending on α and whether or not the state is first in a session. An example HPG is shown to the right in Figure 5. Note that, as mentioned above, not all links on the Web pages are necessarily represented in the grammar. The link A1→A3 from Figure 5 (left) has not been traversed and is therefore not represented in the grammar. In Figure 5, a Web page maps to a state in the grammar. The HPG can also be generated with a history-depth N above 1. With a history-depth of, for example, 2, each state represents two Web pages requested in sequence. The structure of the HPG remains the same, but each state now has a “memory” of the N last states traversed. With N=3, a state might then be named A1A3A5, for the traversal A1→A3→A5. The mining of rules on the HPG using a simple breath-first search algorithm has been shown to be too un-precise for extracting a manageable number of rules. Heuristics have been proposed to allow for a more fine-grained mining of rules from an HPG (Borges et al., 2000). The heuristics are
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

0 Pedersen, Thorhauge, & Jespersen

aimed at specifying controls that more accurately and intuitively present relevant rules mined from an HPG and allow for, for example, generation of longer rules and for only returning a subset of the complete ruleset.

Representing.Additional.Information
An HPG has no memory of detailed click information in the states. Thus, if rules using additional informationfor example, sessions for users with specific demographic parameterswere to be mined, each production should be split into a number of middle-states, where each middle-state would represent some specific combination of the parameters. Each middle-state should then be labeled with the respective combinations of information. This is illustrated in Figure 6, where A and B are original states and 1 to 7 represent new middle-states. Note that the probability of going from a middle-state to B is 1 for each middle-state. Note also that Figure 6 only illustrates additional information grouped into seven different categories, which, for example, could be some demographic information about the user. If several different kinds of information were to be represented, the number of middle states would increase exponentially, since all combinations of the different kinds of parameters should potentially be represented for all states in the HPG, thus creating a problem with state explosion. For instance, if the HPG should represent the gender and marital status of users, each production could potentially be split into four middle-states. If the HPG should also represent whether a user had children, each production needs to be split into eight middle states. This factor increases with the cardinality of each additional parameter. This solution scales very poorly. For instance, representing gender, age in years, salary (grouped into ten categories), number of children (five categories), job status (10 categories), and

Figure 6. HPG with additional information

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

years of working experience could easily require each production to be split into over four million (2∗110∗10∗5∗10∗40) middle states. Doing this for an HPG that includes only ten interconnected states would require over 400 million states (including middle-states) in the full HPG. This is clearly not a scalable solution, since the number of clicks represented might not even be 400 million. Furthermore, the existing algorithms (Borges, 2000) should be expanded to be able to work on this new type of state in the HPG. Alternatively, a single middle-state could be inserted in each production, containing a structure indicating the distribution of clicks over the parameters. This would effectively reduce the state-explosion but require significant changes to the existing mining algorithms to include parameters in the mining process and to support, for example, mining of rules for a range of parameter-values. We now discuss pros and cons of the HPG approach.

Pros
The size of an HPG is not proportional to the number of sessions it represents, but to the number of states and productions in the grammar, which makes the model very compact, scalable, and self-contained. Additional sessions can easily be added to an existing HPG and thereby allow for an HPG to grow in the number of sessions represented without growing in size. Performing data mining on an HPG outputs a number of rules describing the most preferred trails on the Web site using probability, which is a relatively intuitive measure of the usage of a Web site. Mining the HPG for a small, high-probability set of rules requires the usage of the described heuristics, which provides the user of the HPG with several parameters in order to tune the ruleset to his or her liking and extract rules for a very specific area.

Cons
The HPG does not preserve the ordering of clicks which is found in the sessions added (using a history depth above 1 will preserve some ordering). Therefore the rules mined could potentially be false trails in the sense that none or few sessions include the trail, but a large number of sessions include the different parts of the trail. In order to extract rules for a specific subset of all sessions in the HPG, a specialized HPG for exactly these sessions needs to be created. This is because a “complete” HPG does not have a memory of individual clicks and their session, as described above. A collection of sessions can potentially contain a very large number of different subsets, so building and storing specialized HPGs for every subset is not a scalable option, as mentioned above. Therefore, if more specialized HPGs are needed, there would be a need for storing the Web log information so
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

that the desired session could be extracted later on and a specialized HPG could be built and mined.

The.Hybrid.Approach
Overview
As mentioned in the preceding sections, each approach has some inherent strengths and weaknesses. The click and subsession fact approaches handle additional information easily, but result either in huge I/O or huge storage requirements. On the other hand, an HPG can efficiently mine long rules for a large collection of sessions, but is not able to represent additional information in a scalable way. To remedy this situation, we now present the hybrid approach for extracting information about the use of a Web site, utilizing the potential for mining very specific rules present in the HPG approach while still allowing for the representation of additional information using a click fact schema in a data warehouse, utilizing existing DW technology. The main idea is to create HPGs on demand, where each dynamically created HPG represents a specialized part of the information in the Web log. Our proposal for a hybrid approach combines the click fact schema with the HPG model, creating a flexible technique able to answer both general and detailed queries regarding Web usage. Using the detailed information from the click fact schema means that almost no data is lost in the conversion from Web log to database. However, we also need some kind of abstraction or simplification applied to the query results. In short, we want to be able to specify exactly what subset of the information we want to discover knowledge for. An HPG provides us with a simple technique to create an overview from detailed information. The scalability issues and somewhat lack of flexibility in the HPG model must though also be kept in mind as we want to be flexible with regards to querying possibilities. The concept of the hybrid approach is shown in Figure 7.

Figure 7. Overview of the hybrid approach
Clickfact Schema
Click Extraction

Hypertext Probabilistic Grammar

Mined Rules

Database

Constraints

HPG Construction

HPG Mining

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

As mentioned above, creating specialized HPGs would require storing the original Web log information and creating each specialized HPG when required. Storing the original Web log file is not very efficient since, for example, a lot of non-optimized string-processing would be required, so some other format needs to be devised. The click fact schema described above provides this detailed level, since it preserves the ordering of the Web log and furthermore offers database functionality such as backup, optimized querying, and the possibility of OLAP techniques such as preaggregation to be applied. However, a more subtle feature of the click fact schema actually offers itself directly to the HPG model and proves useful in our solution. The database schema for the click fact table includes unique keys for both the referrer and the destination for each click. These two keys uniquely identify a specific production within a grammar since each key is a reference to a page and thus a state. Thereby we are able to extract all productions from the click fact table simply by returning all combinations of url_key and referrer_key. Each occurrence of a specific combination of keys will represent a single traversal of the corresponding link on the Web site. Retrieving all states from an HPG is immediately possible from the click fact schema. The url_dimension table holds information about each individual page on the Web site, therefore a single query could easily retrieve all states in the grammar and a count of how many times the state was visited, both in total and as first or last in a session. The queries can be used to initialize an HPG. This would normally be done using an algorithm iterating over all states in the sessions (Borges, 2000), but using the database representation, the required information can be retrieved in a few simple database queries. Note that some post-processing of the query results are necessary for a nice in-memory representation. Creating specialized HPGs is indeed possible with this approach. Inserting a constraint layer between the database software and the creation process for an HPG will allow for restrictions on the information represented in the HPG. Extending the queries described above to only extract information for clicks with certain characteristics will allow for creation of an HPG only representing this information. Thereby rules mined on this HPG will be solely for clicks with the specific characteristics. Using this concept, specialized HPGs can be created on-demand from the database. For instance, the consultant might be interested in learning about the characteristics of walkthroughs for male, senior salesmen in a company. The consultant will specify this constraint, and the queries above are modified to only extract the productions and the states that apply to the constraint. The queries will therefore only extract the sessions generated by male, senior salesmen in the company, and the HPG built from these sessions will produce rules telling about the characteristic behavior of male, senior salesmen. This approach utilizes some of the techniques earlier described, but combines them to utilize the strong sides and avoid some of the weak sides.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

Pros
The limitations of the “simple” HPG framework of not being able to efficiently represent additional information are avoided with this hybrid approach. The ability to easily generate a specialized HPG overcomes the shortcomings of not being able to store all possible specialized HPGs. Saving the Web log information in the click fact table (and thus in the database) gives us a tool for storing information which arguably is preferable to storing the original log file. A DBMS has many techniques for restoring, querying, and analyzing the information with considerable performance gains over processing on raw textual data such as a log file. Combining the click fact schema, which offers a detailed level of information, and the HPG framework, which offers a more generalized and compact view of the data, will allow for different views on the same data within the same model, without storing information redundantly on non-volatile storage (the extracted HPGs only exist in main memory).

Cons
As the hybrid approach mines results using the HPG, false trails might be presented to the user, which is a characteristic inherited from the general HPG approach. This is obviously a critical issue since this might lead to misinterpretations of the data. Using a history depth greater than 1 might reduce the number of false trails.

Open Issues
The performance of generating specialized HPGs using queries on top of a database is an open issue that will be explored next. The number of rules mined from an HPG should not be too large and the rules should not be too short, so some of the heuristics mentioned above need to be used when mining the HPG to be able to present the information to the user in a manageable way.

Prototype.Implementation
The architecture of the hybrid approach prototype is seen in Figure 8. All modules within the system are implemented in Java. The prototype works as follows. First, the Web log file from the Web server is converted into an XML-based format. Then a Quilt query (Chamberlin, Robie, & Florescu, 2000) is executed on the XML file, resulting in a new XML file. An XML parser is then invoked which parses the XML into the click fact schema contained within the database. This part of the system is
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

Figure 8. Prototype architecture

called the data warehouse loader. We then use a simple Graphical User Interface (GUI) to control how the SQL query extracting data from the database should be constructed. The SQL generator then constructs four SQL queries which are used to query the database. We call this part the constraint layer. The results from these queries are used to construct an HPG structure held in main memory. A BFS mining algorithm then extracts rules from the HPG. This part is named HPG. The system is designing to read information from a Web log. The typical format of a Web log is the Common Log Format (CLF) or the Extended Common Log Format (ECLF). Such Web logs contain the URL requested, the IP address (or a resolved name, if possible) of the computer making the request, and timestamps. An entry in a CLF log file for a single request is seen below:
ask.cs.auc.dk - - [/Oct/00:0:: +000] “GET /educaton/datnf HTTP/.0” 0 

The identification of users is non-trivial as above. The prototype does not implement any logic to handle user identification, but simply assumes that an IP maps directly to a user. Also, the prototype does not at present include any means to avoid proxy caching of data. Instead of writing a data warehouse loader reading the CLF format directly, we have chosen to convert it into a more high-level format, based on XML. Once in this format, we are able to import and process our log data in a vast variety of programs which provides flexibility. The first step in loading the data into the data warehouse is cleansing the log data. Instead of cleansing directly on the log
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

file using a temporary table in the database, or using other proposed techniques (Cooley et al., 1999), we use the Quilt query language. Quilt is the predecessor of the XQuery language proposed by the W3C and is an SQL-like language for querying an XML structure. We use two Quilt queries to produce an XML file cleansed for irrelevant information and grouped into sessions for each host. When the Web log is transformed into XML, we are ready to load it into the data warehouse. Using a SAX parser, we have implemented separate transformers from XML to the click and subsession warehouse schemas described above. Provided with the XML file, the transformers parse our XML structure into the data warehouse schemas, and the loading is completed. No scheme for handling the uncertainty associated with the decision-time is implemented in the prototype, but it could readily be extended to assume that a predefined timespan was used handling, for example, streaming time for each request. Based on previous work (Cooley et al., 1999), we have chosen to spawn a new session if the dwell time in a session exceeded 25 minutes. The spawning of a new session is handled by the log transformers in the prototype. The main idea of combining a data warehouse with the HPG technique is the ability to constrain the data on which the HPG is built. We need SQL queries to extract our constrained set of data and then pass it on to a mechanism initializing our HPG. However, the extracted data must first be divided into session-specific and clickspecific information. This distinction is very important, as the constructed HPG could otherwise be incorrect.

Session Specific
Dimensions which are specific to an entire session will, when constrained on one or more of their attributes, always return entire sessions as the result. One such dimension is the session dimension. If the click fact schema is constrained to only return clicks referencing a subset of all sessions in the session dimension, it is assured that the clicks returned will form complete sessions. Also, if we assume that all sessions start and finish on the same date, the date dimension will also be session specific. In an HPG context it means that the constructed HPG never has any disconnected statesstates where no productions are going either to or from.

Click Specific
Dimensions containing information about a single click will, if the click fact table is constrained on a subset of these keys, produce a set of single clicks which are probably not forming complete sessions. The probability of this will increase as the cardinality of the attribute grows or the number of selected attributes shrinks. For instance, when constraining on three URLs from a set of 1,000 in the URL
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

Figure 9. Two-step extraction process

dimension, the clicks returned will properly not constitute complete sessions, and the probability of false trails dramatically increase. Furthermore, the HPG produced will then consist of three states with potentially no productions between them and some productions leading to states not included in this HPG. These three states are disconnected states. To be able to derive any rules from an HPG, we need to have states with productions connecting them. As we want the approach to provide the overview as well as the detail, these two types of dimensions must be dealt with before the HPG is built. The solution proposed here is to constrain the data in two stages. The two stages (see Figure 9) can briefly be described as follows. First, we retrieve a temporary result using dimensions which are thought to be click specific (1a). The temporary result comes from joining the click-specific dimensions with the click fact schema on all constrained attributes. The distinct session keys from the temporary result (1b) can be used to get the subset of all clicks having these session keys, which is done in step 2. These distinct keys are session specific and will assure an interconnected HPG. Second, we constrain the result using dimensions which are thought to be session specific and the distinct session keys (2a). All constraints must be fulfilled on the dimension when joining with the click fact schema. The collection of retrieved clicks is interconnected (2b). Note that both steps in practice will be performed as one query. When executing queries on the database, java.util.ResultSet objects are returned. The HPG’s initializing method is then provided with the respective ResultSet objects and dealt with internally in each method. The method used to initialize the HPG state set will initially run through the ResultSet object provided and create a State object for each row in
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

the ResultSet object. Transparency between the specific SQL query and the result used for HPG initialization is maintained, as the ResultSet object is well defined and not affected by a change in the SQL query. The HPG initialization method is at this stage provided with data, and as such, the extraction of data from the data warehouse is completed. The HPG implementation consists of two parts: the HPG building method and a Breadth First Search (BFS) mining algorithm. The BFS algorithm is chosen instead of the Depth First Search (DFS) algorithm, as the latter has higher memory consumption (Borges, 2000). The only difference between the prototype HPG implementation and work by Borges (2000) is that rules and productions are stored in main memory instead of a database. The HPG implemented in the prototype holds two simple lists in main memory, one for states and one for productions. These lists are initialized from the results from the constraint layer, and the mining process works entirely on these lists.

Experimental.Evaluation
We now briefly discuss the results of our experimental evaluation (additional details and figures can be found in Jespersen et al., 2002). In evaluating the hybrid approach, we decided to evaluate the performance of the creation of HPGs on-the-fly both by itself and against a straightforward SQL-query on a subsession schema. The experiments were performed on both a non-optimized and an optimized implementation of the hybrid approach. We decided not to evaluate against a straight forward click fact schema since they perform poorly on sequences of clicks (Andersen et al., 2000) and would perform too poorly to be used for online mining. The goal of the experiments is to retrieve information concerning the performance of creating HPGs on-the-fly on top of a database. We have used the Web log from the Computer Science Department at Aalborg University for October 2001, containing 33,000 sessions, with a total of 115,000 valid clicks divided among 27,400 unique pages. The DBMS used in the evaluation is MySQL 4.0.1 running on an Intel 933 Mhz machine. The prototype described above is running on an AMD Athlon 1800 Mhz machine. We adopted the suggested convention (Andersen et al., 2000) of limiting the subsession length to 10 clicks to avoid a massive blowup in number of subsessions generated. Insertion of subsessions yielded a total of 620,000 entries in the subsession fact table divided among 490,000 different subsessions. In the process of extracting rules from the data warehouse, three main tasks are performed. First, the database is queried for data used to initialize the HPG. Second, the HPG is built in main memory based on the extracted data, and third, the BFS mining algorithm extracts rules from the HPG. To test how the approach performs as the number of states increases, a range query test was performed.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

By far the most time-consuming part in the hybrid approach is the database query time. In most cases, more than 90% of the total time is spent on this part. The time used for the BFS mining is very short. The time spent on initializing the HPG only adds little processing time to the overall processing time, usually 5-10%. Thus, it seems that a possible tuning effort should focus on the database query part. One of the most obvious ways to optimize the database is to create indexes on the key attributes in the click fact schema. Materialized views are another promising opportunity for tuning. We now compare the hybrid approach to the subsession schema on extraction of information for all sessions, session-specific constraints, and click-specific constraints.

All Sessions
It is found that the non-optimized hybrid approach performs poorer than.the subsession schema by a factor of approximately 25. However, the hybrid approach where both the database querying and the BFS mining is optimized takes only 30% longer than the subsession approach. The small performance advantage of the subsession approach comes at a large price: the storage requirements are more than five times higher than for the hybrid approach, due to the need to store subsession of all lengths.

Click-Specific Constraints
The test is performed by extracting information for all sessions containing requests including the string tbp, which is a specific personal homepage on the Web site. The hybrid approach proves to be a factor of 10 faster than the subsession schema approach; even in the non-optimized case, the optimized version is 20 times faster.

Session-Specific Constraints
The session-specific constraint is evaluated using a constraint on the total_session_seconds field from the session_dimension table. The optimized hybrid approach outperforms all the other approaches by an average factor of 3-4. All the other approaches are rather similar in performance. Again, the superior performance of the optimized hybrid approach is accompanied by a storage requirement that is more than five times smaller than the subsession approach.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

0 Pedersen, Thorhauge, & Jespersen

To conclude, our evaluation of the hybrid approach has shown that the optimized hybrid approach is very competitive when compared to the subsession fact approach. Even with a storage requirement that is more than five times smaller than for the subsession fact approach, the optimized hybrid approach performs 3-20 times faster. Only for mining all rules, the performance is a little slower, making the hybrid approach the clear winner.

The Post-Check Enhanced Hybrid Approach
Overview
We now describe the concept of false trails in detail, derive the notion of a true rule, and present the Post Check Enhanced Hybrid (PEHA) approach for achieving true rules. To understand why a rule extracted from an HPG mining process could be indicating a false trail, it is important to note that the HPG model assumes that the productions expanded from a single state X are entirely dependent upon state X and not on any previous states. This assumption can be overcome somewhat by introducing history depth into the single states in the HPG. Then each state in the HPG models a sequence of, for example, three sequential clicks. The HPG model presented above implicitly used a history depth of 1. Note, that a rule extracted from an HPG is given with a certain probability. This probability can be translated into a number of sessions supporting (under the HPG assumption) the trail of the rule, called the expected number of sessions, Esessions, by multiplying this probability with the number of sessions that have visited the first state of the rule. We now want to extend the hybrid approach so that it outputs entirely true rules instead of rules which might be (partly) falsethat is, have an incorrect support count. First, we define an existing rule: • Given a rule R, consisting of a sequence of states R=s1…sn, and a support level Ssessions, indicating the number of sessions which include the sequence of states, R is an existing rule if and only if Ssessions ≥ 1. We now define a false rule to be a rule that is not an existing rulethat is, a rule indicating a sequence of states not traversed by any sessions. We define a true rule as: • Given an existing rule R, consisting of a sequence of states R=s1…sn, with an expected number of users Esessions, and a support level Ssessions, indicating

•

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

the number of sessions which include the sequence of states, R is a true rule if and only if Esessions = Ssessions. With the assumption of limited browsing history in the HPG model, the expected number of sessions for a rule might not be the correct support level of the rule. In extending the hybrid approach, we note that the set of existing rules generated by the HPG model is a superset of the true rules, since a true rule will always be contained in an existing rule. This fact leads us towards an approach where the HPG model is used as a generator of candidate rules, which can then be validated (modified) to become true rules. In other words, the rules extracted from the HPG are checked against the actual log data to achieve the correct support level. In the following, the set of usage sessions are referred to as the true traversals. To go from candidate rules to true rules, we must develop a way of post-checking or validating the candidate rules in order to convert them to true rules. This validation does not alter the page sequences, since the HPG ensures that the sequence is valid. The validation is only an adjustment of the number of sessions having traversed the page sequence, to get the actual support of the page sequence. In essence, we want to adjust Esessions to be equal to Ssessions. Using the hybrid approach, we already have the framework in place to perform this validation of support level. As described above, we have stored all clickstream information in a click fact schema, so the idea is to use this schema to validate a given rule. Validating a rule in our click fact schema requires the execution of queries in which we join multiple instances of the click fact table as the click fact schema stores the individual clicks. We want the check of a candidate rule against the click fact schema to result in the number of sessions containing the given rule. By containing, we mean that a session includes the given sequence of Web pages in the rule at least once. If a session contains a sequence more than once, the support is only incremented with one. This definition of support is adopted elsewhere (Agrawal et al., 1995).

Post-Checking Rules
The conditions to be verified for every production p [s1→s2] in a rule are the following (number_in_session captures at what position in the browsing session a given click occurs): 1. 2. 3. s1 is followed by s2 s2’s number_in_session is 1 greater than s1’s number_in_session s1 and s2 are found within the same session

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

Furthermore, there are some cases in which additional conditions apply which must be checked as well. The α parameter, used when mining a HPG for (candidate) rules, must also be considered when it is set to 0. Mining with α=0 means that all rules mined from the HPG are rules in which the first Web page is also the first page visited in a session. Furthermore, a rule might include the artificial end-state, which is not represented in the click fact schema, in its last production, indicating that the previous state was the last in a session. Adding these two special case conditions to the list of checks performed in the query, we get: 1. 2. if α=0, s1 in p1[s1→s2] must have is_first = 1 if s2 in pn[s1→s2] is the artificial end-state, s1 must have is_last = 1

Note that is_first and is_last is assumed to be implemented as 1 and 0 in the click fact schema, representing true and false respectively. To clarify how a query would look when checking a given rule, a short example is now presented. Consider the rule 1→2→3→E, where the numbers are the schema identifiers for the specific URLs traversed in the rulethat is, the url_key and referer_key fields of the click fact table. The parameter α is set to 0, and the state E is the artificial end-state. Formally, we must check the productions p1[s1→s2] and p2[s2→s3]. The corresponding query used to validate the rule against the click fact schema in the data warehouse is seen below.
SELECT COUNT(DISTINCT(cf.sesson_key)) FROM Clck_fact cf INNER JOIN Clck_fact cf ON (cf.url_key =  AND cf.url_key = cf.referrer_key AND cf.url_key =  	 AND	cf1.is_first	=	1	AND	cf2.number_in_session	=	cf1.number_in_session+1 AND cf.sesson_key = cf.sesson_key) INNER JOIN Clck_fact cf ON (cf.url_key = cf.referrer_key AND cf.url_key =  AND cf.s_last =  AND cf.number_n_sesson = cf.number_n_sesson+ AND cf.sesson_key = cf.sesson_key)

If α>0, the check for is_first is omitted, and for rules not ending with the artificial end-state, the check for is_last is omitted. It is not possible to remove a join from the query when the check for is_first is not needed. Even though checking for the existence of referrer_key and url_key in the same table instance would do the job, condition 2 would be impossible to check. Condition 2 needs to be checked as we could otherwise risk missing pages visited inside session, thus generating wrong results.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

The hybrid framework presents a flexible way to add constraints on rules extracted from the data warehouse. These constraints should also be applied when post checking a rule, since we should only want to count support for the sessions from which the rule was extracted. The use of constraints in post-checking of rules is quite similar to the use of constraints when creating the HPG in the hybrid approach. For sessionspecific constraints, a join between the click fact table and the dimension holding the constraint is added to the query. For click-specific constraints, the temporary table used when building the HPG is re-used by adding a join between the temporary table (which holds session-id’s fulfilling the click-specific constraints) and the click fact table. An example query using constraints in post-checking a rule is given below. Here, extra joins on the date_dimension and the temp_dimension is added to illustrate the use of session- and click-specific constraints, respectively.
SELECT COUNT(DISTINCT(cf.sesson_key)) FROM Clck_fact cf INNER JOIN Clck_fact cf ON (cf.url_key =  AND cf.url_key = cf.referrer_key AND cf.url_key =  	 AND	cf1.is_first	=	1	AND	cf2.number_in_session	=	cf1.number_in_session+1 AND cf.sesson_key = cf.sesson_key) INNER JOIN Clck_fact cf ON (cf.url_key = cf.referrer_key AND cf.url_key =  AND cf.s_last =  AND cf.number_n_sesson = cf.number_n_sesson+ AND cf.sesson_key = cf.sesson_key) INNER JOIN date_dmenson dd ON (cf.date_key = dd.date_key AND dd.year = 00) INNER JOIN Temp_dmenson td ON (cf.sesson_key = td.sesson_key)

Experimental.Evaluation
The evaluation of PEHA is aimed at comparing it with an existing Web usage mining approach based on data warehousing technology, namely the subsession schema. Note that we do not compare PEHA to the original hybrid approach since we are only interested in comparing approaches that always provide rules with correct support. The original hybrid approach will always be faster than PEHA since the post-checking phase is not done, but this comes at the cost of sometimes quite incorrect results (Jespersen, Pedersen, & Thorhauge, 2003). Two datasets are used in the experiments. Experiments are conducted for each dataset, namely the performance using three different ways of constraining the extracted rules (session and click specific, and both). For each constraint type, the threshold used for mining the HPG is varied. By decreasing the support threshold,
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

we experienced an increasing average rule length and rule-set size for both datasets. In order to examine performance using rule-sets with different characteristics, we utilize this property to evaluate the performance for different average lengths of rules. To minimize the effect of distorted results, each experiment is repeated five times and an average is calculated. The sub-tasks of MIMER that are evaluated in this chapter are: 1. 2. 3. 4. Query data warehouse for information used to construct the HPG Construction of the HPG structures in main memory Mining the constructed HPG Post-checking the rules extracted from the HPG

The first Web log (CS) is from the Web site of the Computer Science Department at Aalborg University (http://www.cs.aau.dk). The Web site is primarily an information site for students and staff at the institution, but also contains personal homepages with a wide range of content and homepages for individual classes. The second Web log (Bank) is taken from an educational intranet placed at a major Danish financial institution. The Web site contains various courses which can be followed by the employees of the institution. The CS dataset contains 68,745 sessions, with 232,592 clicks, 58,232 unique pages, and an average session length of 3.38. For Bank, the numbers are 21,661, 395,913, 2,368, and 18.23, respectively. Note the differences between the datasets in the average length of a session and the number of unique pages, indicating a more scattered browsing on the CS Web site as opposed to the more continuous browsing on the Bank Web site. The DBMS used is an industrial system supporting materialized views (for licensing issues, we cannot name the specific DBMS used). The DBMS and MIMER is running on an Athlon XP1800+ with 512 MB of memory and Windows 2000. For most SQL queries used, matching materialized views are implemented in order to optimize the query performance. Likewise, the queries on the subsession schema are also optimized using materialized views. In the experiments, only rules of length less than 10 are post-checked since the materialized views used are dedicated views for each rule length and not designed to promote the post-checking of longer rules. The space used to hold the materialized views used and the subsession approach are approximately 185 MB and 740 MB, respectively. Each Web log is loaded into the data warehouse using a batch load utility. Because of the very time-consuming task of constructing and loading very long subsessions, only subsessions of length up to 5 is generated, which is ok since we only want to compare the performance and not the correctness of rule extraction by the two approaches. Additionally, rules longer than 5 will in practice be rare and have low support, and are thus not that interesting from a practical point of view.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

The results of extracting and post-checking rules through MIMER using the DB dataset show that MIMER and all of the sub-tasks in MIMER perform in approximately constant time, even as the average rule length is increased. On average, 75% of the total time is used to extract data from the DBMS and construct the HPG structure in main memory. The subtask of querying the data warehouse for information needed to construct the HPG does not at present utilize materialized views to optimize access to the data. Constructing the HPG in main memory takes minimal time partly due to the fact that dedicated structures to hold both productions and states in the HPG have been implemented. Mining the constructed HPG for (candidate) rules takes relatively little time, approximately 50-250 ms. The time used to post-check the candidate rules mined in the previous stage is constant as well. We found that the time used to check a rule scales very well with the average rule length, indicating that no matter the length of a candidate rule in the experiment, post-checking only adds a near constant factor to the total time used. Notice that we would not expect such constant behavior to be achieved without the use of materialized views in the DBMS. Our experiments show that both for click- and session-specific constraints, the post-checks run in almost similar time, indicating that no specific type of single constraint is more time consuming. Performing post-check on a rule for a combined click- and session-specific constraint shows a slight increase in the running time of approximately 200 ms as the average rule length is increased by 2, indicating that adding constraints on several dimensions will increase the running time slightly. When comparing MIMER to direct queries on the subsession schema, the result is that queries on the subsession schema are several times faster than MIMER, in this case, using click-specific constraints. The subsession schema thereby proves faster at locating true rules compared to MIMER, primarily because all possible rules (of length ≤ 5) are stored explicitly, and finding rules by frequency of occurrence, utilizing materialized views, run relatively fast. In order to run click-specific constraints, the subsession queries use the LIKE statement of the SQL language to extract information “inside” the sequences stored in the schema. Note however that not all click-specific constraints can be implemented on top of the subsession schema, since some information is too finely grained to be stored in the schema, for example, the time spent on a particular page, also referred to as the dwell-time of a request. The tables and materialized views of the subsession approach consume approximately 4.5 times the storage of MIMER for both CS and Bank. The extra space used in the subsession approach comes from the fact that each sub-sequence of clicks found in an entire session of clicks is explicitly stored. One should notice that certain constrained rules are impossible to find using the Subsession approach, for example, constraining on how long a certain page has been viewed, also referred to as the dwelltime.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

In summary, we have found that the average time of post-checking a candidate rule from an HPG runs in approximately constant time, regardless of the length of the rule when utilizing materialized views. The actual running time of the post-check is also dependent on the number of constraints placed on the HPG, but not on the type of constraint as both click- and session-specific constraints run in similar time. Compared to the direct queries on the subsession schema, MIMER performs several times worse, however the flexibility inherent in PEHA and in MIMER allows for more specialized information to be extracted from MIMER. The difference in space usage is clearly in favor of the PEHA approach. Notice that in our experiments, only subsession up to length 5 was generated, and storing longer subsessions would further increase the overhead in space required by the subsession approach.

Conclusion
Motivated by the need to combine data warehousing/DBMS and data mining technologies for Web log analysis, this chapter presented a number of approaches that combine data warehousing and data mining techniques in order to analyze Web logs. After introducing the well-known click and session DW schemas (Kimball & Merz, 2000), the chapter presented the subsession schema which allows fast queries on sequences of page visits (Andersen et al., 2000). Then, the chapter presented the so-called hybrid technique (Jespersen et al., 2002), which combines DW Web log schemas (Kimball & Merz, 2000) with a data mining technique called Hypertext Probabilistic Grammars (Borges, 2000), hereby providing fast and flexible constraintbased Web log analysis. Finally, the chapter presented the “post-check enhanced” (PEHA) improvement of the hybrid technique. The techniques have their own pros and cons, and are suitable for different settings. If the rather high storage use is not a problem, the subsession schema approach is both fast and easy-to-use for sequence-focused queries. If storage use is a problem or more detailed information is needed in queries, the hybrid approach performs well, albeit at the cost of possibly returning imprecise rule support counts. If absolute precision and correctness is mandatory, the PEHA approach offers validation of hybrid result rules at a reasonable cost. This chapter was the first to provide an integrated presentation of these approaches combining data warehousing and data mining techniques for Web log analysis.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Combnng Data Warehousng and Data Mnng Technques 

References
Agrawal, R., & Srikant, R. (1995). Mining sequential patterns. In Proceedings of ICDE (pp. 6-10). Andersen, J., Giversen, A., Jensen, A.H., Larsen, R.S., Pedersen, T.B., & Skyt, J. (2000). Analyzing clickstreams using subsessions. In Proceedings of DOLAP (pp. 25-32). Extended version available as (Tech. Rep. No. 00-5001), Department of Computer Science, Aalborg University, Denmark. Borges, J. (2000). A data mining model to capture user Web navigation patterns. PhD Thesis, Department of Computer Science, University College London, UK. Borges, J., & Levene, M. (1999). Data mining of user navigation patterns. In Proceedings of WEBKDD (pp. 92-111). Borges, J., & Levene, M. (2000). A fine-grained heuristic to capture Web navigation patterns. SIGKDD Explorations, 2(1), 40-50. Büchner, A.G., Anand, S.S., Mulvenna, M.D., & Hughes, J.G. (1998). Discovering Internet marketing intelligence through Web log mining. SIGMOD Record, 27(4), 54-61. Chamberlin, D.D., Robie, J., & Florescu, D. (2000). Quilt: An {XML} query language for heterogeneous data sources. In Proceedings of WebDB (Informal Proceedings) (pp. 53-62). Cooley, R., Mobasher, B., & Srivastava, J. (1999). Data preparation for mining World Wide Web browsing patterns. Knowledge and Information Systems, 1(1), 5-32. Cooley, R., Srivastava, J., & Mobasher, B. (1997). Web mining: Information and pattern discovery on the World Wide Web. In Proceedings of ICTAI (pp. 558567). Cooley, R., Tan, P., & Srivastava, J. (1999). Websift: The Web site information filter system. In Proceedings of the 1999 KDD Workshop on Web Mining. Han, J. (1997). OLAP mining: Integration of OLAP with data mining. In Proceedings of DS-7 (pp. 3-20). Han, J., & Kamber, M. (2000). Data miningconcepts and techniques. San Francisco: Morgan Kaufmann. Hinneburg, A., Lehner, W., & Habich, D. (2003). COMBI-operator: Database support for data mining applications. In Proceedings of VLDB (pp. 429-439). Jespersen, S., Pedersen, T.B., & Thorhauge, J. (2003). Evaluating the Markov assumption for Web usage mining. In Proceedings of WIDM (pp. 82-89).

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Pedersen, Thorhauge, & Jespersen

Jespersen, S., Thorhauge, J., & Pedersen, T.B. (2002). A hybrid approach to Web usage mining. In Proceedings of DaWaK (pp. 73-82). Extended version available as (Tech. Rep. No. 02-5002), Department of Computer Science, Aalborg University, Denmark. Kimball, R., & Merz, R. (2000). The data Webhouse toolkit. New York: John Wiley & Sons. Levene, M., & Loizou, G. (1999). A probabilistic approach to navigation in hypertext. Information Sciences, 114(1-4), 165-186. Pei, J., Han, J., Mortazavi-asl, B., & Zhu, H. (2000). Mining access patterns efficiently from Web logs. In Proceedings of PAKDD (pp. 396-407). Pei, J., Han, J., Mortazavi-Asl, B., Pinto, H., Chen, Q., Dayal, U., & Hsu, M. (2001). PrefixSpan: Mining sequential patterns by prefix-projected growth. In Proceedings of ICDE (pp. 215-224). Sarawagi, S., Thomas, S., & Agrawal, R. (1998). Integrating mining with relational database systems: Alternatives and implications. In Proceedings of SIGMOD (pp. 343-353). Srikant, R., & Agrawal, R. (1996). Mining sequential patterns: Generalizations and performance improvements. In Proceedings of EDBT (pp. 3-17). Spiliopoulou, M., & Faulstich, L.C. (1998). WUM: A Web Utilization Miner. In Proceedings of WebDB (pp. 109-115). Wu, K.-L., Yu, P.S., & Ballman, A. (1998). A Web usage mining and analysis tool. IBM System Journal, Internet Computing, 37.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 29

Chapter II

Computing Dense Cubes Embedded in Sparse Data
Lixin Fu, The University of North Carolina at Greensboro, USA

Abstract
In high-dimensional data sets, both the number of dimensions and the cardinalities of the dimensions are large and data is often very sparse, that is, most cubes are empty. For such large data sets, it is a well-known challenging problem to compute the aggregation of a measure over arbitrary combinations of dimensions efficiently. However, in real-world applications, users are usually not interested in all the sparse cubes, most of which are empty or contain only one or few tuples. Instead, they focus more on the “big picture” informationthe highly aggregated data, where the “where clauses” of the SQL queries involve only few dimensions. Although the input data set is sparse, this aggregate data is dense. The existing multi-pass, full-cube computation algorithms are prohibitively slow for this type of application involving very large input data sets. We propose a new dynamic data structure called Restricted Sparse Statistics Tree (RSST) and a novel cube evaluation algorithm, which are especially well suited for efficiently computing dense sub-cubes imbedded in high-dimensional sparse data sets.RSST only computes the
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

30 Fu

aggregations of non-empty cube cells where the number of non-star coordinates (i.e., the number of group by attributes) is restricted to be no more than a userspecified threshold. Our innovative algorithms are scalable and I/O efficient. RSST is incrementally maintainable, which makes it suitable for data warehousing and the analysis of streaming data. We have compared our algorithms with top, stateof-the-art cube computation algorithms such as Dwarf and QCT in construction times, query response times, and data compression. Experiments demonstrate the excellent performance and good scalability of our approach.

Introduction
Given n dimensions D1, D2,…, Dn, where domain values of Di are in 0..Ci-1, Ci is the cardinality of Di, and a measure M, the data cube problem is to compute the aggregation of M over any subset Q ⊆ D1 × D2 ×…× Dn. Domain values other than integers are converted into integers. For example, strings and ordinary domains are mapped into integers starting from 0. Real numbers can be discretized into ranges. SUM, COUNT, and MIN/MAX are typical aggregation operators. In this chapter, we mainly focus on COUNT and SUM operators; others can be implemented similarly. Data cube facility is essential in data warehousing and OLAP (Gray et al., 1997). Because of the importance of efficient data cube computation and exploration, numerous significant studies have been performed. However, the state-of-the-art algorithms and systems do not scale well in terms of I/O bottleneck. This problem is important because in the data warehousing and analytical data processing environment, the data sets are often very massive. Almost all current technologies build certain structures (e.g., materialized views, indexes, trees, etc.) before answering user queries. The running times of setting up the structures for such large data sets will be dominated by I/O operations. Current top algorithms either entirely ignore the problem of I/O efficiency or do not handle it well. They require multiple passes for large data sets, which makes the data cube computation prohibitively slow or infeasible. On the other hand, although the input data sets are very large and very sparse (i.e., the vast majority of cubes are empty), the high-level views with few group-bys contain dense datathe typical targets of user navigation. In data cube applications, users (mostly data analyzers and top managers) are usually not interested in the low-level, detailed data, which is the typical focus of daily transactional processing operations. The analytical users are more interested in the highly aggregated, “big picture” information. For example, in a warehouse storing car sales data, a manager may not be so interested in a query like “What is the total sale for golden, Honda Accord
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 31

LX sedans that are sold on day X and store Y?” as in a query for sales grouped by season and manufacturer, for example. Motivated by the idiom of “make common things fast,” we have developed a new method of efficiently evaluating dense, aggregated cubes that are commonly queried so that the number of data passes of original input data sets is minimized. This new I/O efficient strategy will make the analysis for very large data sets feasible. Our major innovations and contributions in this article include the following. We present a new data structure called RSST (Restricted Sparse Statistics Tree), which binds all the views that have few group-by attributes into one compact data structure, thus facilitating the generation, maintenance, and querying of these views. To build RSST, we only need one pass of the input data set without any pre-sorting step. BUC algorithm (Beyer & Ramakrishnan, 1999) computes iceberg queries whose aggregates are above certain threshold (we call it cube aggregation threshold). Harinarayan, Rajaraman, and Ullman (1996) give an algorithm to select a set of views under the constraint of available space. To our knowledge, RSST is the first structure that binds and computes all the views whose number of group-by attributes is no more than a threshold (we call it cube dimensionality threshold). These views occupy the top layers of a view lattice structure. For large sparse datasets, even restricting the dimensionality of the computed cuboids is not sufficient. The views with group by attributes of large cardinalities are less likely queried. For example, if a table contains 1 million records, and has six dimensions D1 to D6 whose cardinalities are 10000, 5000, 1000, 100, 100, and 100 respectively, then on the average the probability that a cuboid of view D1D2D3 contains one or more tuples is 1/50000. So, we further restrict to the views that the product of the cardinalities of the group-by attributes is no more than the product of a threshold (we call it cube cardinality threshold) and the number of records in the data set. Notice that the cube dimensionality threshold and cube cardinality threshold are used for us to focus on the computation of the commonly queried views or cubes. They do not restrict the number of dimensions or the cardinalities of the dimensions of the original input datasets. We give an efficient cube query algorithm. If the cube queried is densethat is, its aggregate is above the cube aggregation thresholdit will be answered immediately by in-memory RSST; otherwise, we need only one I/O to compute the aggregate of the cube. Our query algorithm can answer point queries, iceberg queries, range queries, and partial queries. We have performed comprehensive simulation experiments, paying particular attention to the I/O bottleneck issue. We have implemented three recent top cube computation algorithms such as BUC, Dwarf, and QC Trees, and compared our algorithm with them. The results show that ours is significantly faster.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

32 Fu

Related Work
Conceptually, a data cube can be regarded as a d-dimensional array whose cells store the aggregated measures for the sub-cubes defined by their dimensional coordinates (Gray et al., 1997). There are three approaches for data cube implementation: ROLAP (Relational OLAP), MOLAP (Multidimensional OLAP), and HOLAP (Hybrid OLAP) (Chaudhuri & Dayal, 1997). For example, the algorithm in Agarwal et al. (1996) belongs to the ROLAP camp, while Zhao, Deshpande, and Naughton’s (1997) is in MOLAP. In recent years, considerable work has been done on cube computation. The cubing algorithms can be roughly categorized as full cube vs. partial cube computation, or exact cube vs. approximate cube computation. Both of the above example algorithms are full and exact cubing algorithms, computing all the cube cells, including super cubes, exactly. RSST is a partial and exact cube algorithm. Although stars appear in Gray et al. (1997) to represent super cubes, star pointers are first introduced in a statistics tree structure to actually compute the aggregates of the super cubes (Fu & Hammer, 2000). For dense data sets, packaged arrays are very effective to compute the data cube (Fu, 2004). In Johnson and Shasha (1997), cube trees and cube forests are proposed. Very scant work in the literature addresses the issues of computing the data cube for hierarchical dimensions (e.g., day-month-year for time dimension). A family of materialized ST trees (Hammer & Fu, 2001) have been used to improve the performance for queries with constraints on dimension hierarchies. Markl, Ramsak, and Bayer (1999) give a multidimensional hierarchical clustering scheme (MHC) for a fact table with hierarchical dimensions in a data warehouse. In addition to removing prefix redundancy for dense cubes, recent important work in Sismanis, Deligiannakis, Roussopoulos, and Kotidis (2002) focuses on a new compressed Dwarf structure to remove suffix redundancy for sparse cubes as well. It outperforms Cubetrees in terms of storage space, creation time, query response time, and updates. This algorithm also gives an optimization of clustering data cubes that belong to the same views. However, it requires a pre-sorting of the input records, which needs multiple passes for large data sets. Moreover, each insertion of a record in the sorted file into the structure may incur several additional I/Os in the process of coalescing. This is equivalent to having multiple additional passes for the construction of the dwarf tree. It is not incremental for bulk loading. Another drawback is that updating is complex and somewhat inconvenient. Rather than computing all the data cubes, Beyer and Ramakrishnan (1999) developed the Bottom-Up Cubing (BUC) algorithm for computing only “iceberg cubes.” Similar to some ideas in Apriori (Agrawal & Srikant, 1994) and partitioned-cube (Ross & Srivastava, 1997), BUC is a ROLAP algorithm and may require multiple passes for large data sets. External sorting and accessing large intermediate files slow down BUC-based types of algorithms such as BU-BST, Quotient Cube, and QC-trees that we will discuss in the following paragraph.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 33

In Wang, Feng, Lu, and Yu (2002), the data cube tuples aggregating from the same set of tuples in the input base table are condensed into one physical tuple called base single tuple (BST). BU-BST heuristics is similar to BUC, except that computation quits for BST. Quotient Cube (Lakshmanan, Pei, & Han, 2002) generalizes the idea of BST compression so that all the cubes with the same aggregate value form a class while the drill-down semantics are preserved. More extensive research and complete results over Quotient Cube are given in Lakshmanan, Pei, and Zhao (2003). One of the major contributions of this paper is that it gives a new data structure called QCtrees for storing and searching a quotient cube. However, in the data warehousing environment, both the original table and the temporary class table usually do not fit in memory. The construction of QC-trees requires expensive depth-first searching of the large original tables and sorting temporary class tables. To compute iceberg cubes with complex measures such as AVERAGE, Han, Pei, Dong, and Wang (2001) extend BUC to Top-k BUC and propose the Top-k H-cubing method. Xin, Han, Li, and Wah (2003) compute full or iceberg cubes by integrating the strengths of Top-Down and Bottom-Up approaches. Due to the complexity and long response times, some algorithms give a quick approximation instead of an exact answer. Sampling is often used in estimations (Acharya, Gibbons, & Poosala, 2000; Gibbons & Matias, 1998). Vitter and Wang (1999) use wavelets to estimate aggregates for sparse data. An interesting idea with a relatively low cost is to refine self-tuning histograms by using feedback from a query execution engine (Aboulnaga & Chaudhuri, 1999). Materialized views are commonly used to speed up cube queries. A greedy algorithm over the lattice structure to choose views for materialization is given in Harinarayan et al. (1996). Other work related to the selection, indexing, and maintenance of views is addressed by Gupta and Mumick (1999), Labio, Quass, and Adelberg (1997), Lehner, Sidle, Pirahesh, and Cochrane (2000), Mumick, Quass, and Mumick (1997), and Yan and Larson (1995). In an environment with infrequent updates, indexes such as Bitmap, encoded bitmap (Chan & Ioannidis, 1998), and B+-tree can improve the performance of cube queries. Unlike in the OLTP (online transactional processing), the query patterns are usually unpredictable (i.e., ad-hoc) in analytical environment. Therefore, pre-scheduled view materialization and indexing are not sufficient. In summary, although great progress has been made, the evaluation of data cube for very large data sets is still infeasible because current technologies still need multiple passes. In this chapter we propose new I/O-efficient cube algorithms to address this important issue.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

34 Fu

Restricted Sparse Statistics Trees
Tree Structure
RSSTs are multi-way trees whose internal nodes are used for branching and whose leaves contain aggregates. A node at level h (h = 0, 1, …, n-1) contains (index, pointer) pairs. The indexes represent the domain values of the corresponding dimensions, and the pointers direct query paths to nodes at the next level. An additional special index value called star value, denoted by *, represents the ALL value of dimension h; the corresponding star pointer is used to direct the path to the next level for a cube query that has no constraints for dimension h. A root-to-leaf path represents a cuboid (a cube or a cube cell; we use these terms interchangeably). In RSST, the number of non-star pointers along the paths is restricted to be no more than a pre-specified threshold r. We term these data cuboids that have no more than r non-star coordinates r-dimensional cuboids. If r is small, we say these cuboids are low-dimensional. If r equals n, we are computing the full data cube (i.e., all cuboids). Essentially, we extend the work in Lakshmanan, Ng, Han, and Pang (1999), where constrained frequent set queries with 2-variable constraints are optimized (r = 2). In the example RSST of Figure 1, there are four dimensions, and COUNT operator is implemented. The figure shows the RSST after the first two records (6, 9, 5, 1) and (20, 1, 3, 2) are inserted into an empty tree. We will give details of insertion and loading algorithms in the next subsection. If we set r to be 2, all the root-to-leaf paths have no more than two non-star labels. Along the pointers are the labels of the corresponding domain values. Leaves shown in boxes contain the aggregation values of the cuboids represented by the root-to-leaf paths. Unlike a B-tree, which is an index for only one dimension, RSST deals with multiple dimensions at the same time. The leaves of RSST do not store records or pointers to records. Instead, they store aggregation values only, which saves space since duplicate records or records in the same cuboids are aggregated into the same leaves. In this way, many operations performed on records like sorting/hashing, writing the intermediate data into and reading data from disks, and so forth (Agarwal et al., 1996) are eliminated. In particular, the low-dimensional cuboids tend to contain many duplicates that are aggregated into the same leaves. RSST is a highly condensed data structure. The reason why we focus on low-dimensional cubes is that in the analytical processing environment, users are mainly interested in the views that have few group-by attributes. In other words, the dense cubes representing the “big picture” are most often queried. For example, among the 22 standard queries in the TPC-H benchmark, 20 of them have no more than three group-by attributes. Another two involve constraints at very fine levels, which are best dealt with by ordinary RDBMSs. High-dimensional cubes whose coordinates have no or few stars consume most of the storage space, but are rarely queried in real-world applications. Then,
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 35

why not ignore them and just focus on speeding up the common queries? In the same line, we can also incorporate the concept of density into the tree structure. The cuboids that have non-star dimensions with large cardinalities are more likely sparse. Therefore, we can add another restriction that the product of the cardinalities of the non-star dimensions in the queried cube should be less than certain threshold. The two abovementioned heuristics can be used together. In this way, we focus on the dense, low-dimensional views that are predominantly queried by the users. The tree is unbalanced. All leaves are automatically sorted and naturally indexed by the corresponding root-to-leaf paths. In comparison with complex structures such as Dwarf trees, QC-trees, H-trees, and star-trees, which all have side links and actually become graphs, RSSTs remain simple tree structures, thus rendering simpler implementations and faster runtimes. As a MOLAP data structure, RSST preserves the advantages of ROLAP in the sense that it only stores non-empty data cubes for sparse data. For example, if the sizes of all four dimensions in the previous example are 1,000, then there are 1012 full-dimensional cube cells, most of which are empty. However, in the RSST, we only store non-empty cubes. We eliminate the need for special data compression techniques such as (OffsetInChunk, data pairs) that are widely used in MOLAP systems for sparse data.

Figure 1. An example of RSST
After inserting: 6, 9, 5, 1 20, 1, 3, 2
*

6 1

A

20 * 3 2

*

D
*

C 1
9

1

B

1

*

1 E
1

1
9 *

5

1

*

3

1
* 2

*

5

F
* 3 * 5 1 *

1
* 1

1

*

2

*

1

2

*

1

1

1

1

1

1

1

1

1

1

1

1

2

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

36 Fu

Insertions
The RSST is a dynamic data structure. To generate RSST, we first create a root with an empty list and then repeatedly insert all the input records. Whenever a new record is inserted, starting from the root for the first dimension, we search the entries of the node. If the index is already there, we simply follow its pointer to the next level node. If not, a new node will be created as its child and a new entry of (index, pointer) pair will be added to the node. At the same time we always follow the star pointer to the next level. If the star child is not there, a new child is also created for the star index entry. Figure 2 shows the pseudo-code of a recursive insertion algorithm insert(cur, level, ns, product, tuple, meas). In Figure 1, we give an example of inserting the second record (20, 1, 3, 2). The whole updated parts of inserting the second record are in dashed lines. Starting from the root, since index 20 is not an entry of the root yet, we create a new node A and add an entry (20, Pa) to the root, where pointer Pa points to the new node A (step 2 of Figure 2). At the same time, we also follow the existing star pointer to node B. For the next value 1, it is not in the lists of node A and node B, so new nodes C and E are created, and corresponding entries (1, Pc), (1, Pe) are added to nodes A, B respectively. Because the star child of A is not there, we need to generate node D and add an entry (*, Pd) to A (step 3). Following the existing star pointer of B to node F, node C is a leaf because the path root-A-C has already two non-star labels. A default COUNT value of 1 is its initial aggregate value (step 1). We can continue similarly for two other domain values 3 and 2. Notice that the COUNT value of

Figure 2. Insertion algorithm
Insert a tuple (with a measure “meas”) into the current node "cur" on level "level" recursively ns: #non-stars on the root-to-cur path, product: product of the cardinalities of the dimensions having non-star labels on the path insert(cur, level, ns, product, tuple, meas ) { if (ns reaches cube dimensionality threshold or product is too large) { 1. make it be a leaf and aggregate meas; return;} // base case for leaf nodes if ( domain value of this level is not an entry of cur) { 2. create a new node as a child of cur; add a new entry; 3. if (star child is not there) create a star child of cur & add a new entry; } 4. let n1, n2 be the two lower level nodes following the Pointers labelled by the domain value & star value insert(n1, level+1, ns+1, product*card, tuple, meas); / / card is the cardinality of dimension “level” insert(n2, level+1, ns, product, tuple, meas); return;
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 37

the last leaf increases by one to 2. Continuing the example of Figure 1, two more records (6, 9, 3, 3) and (20, 9, 3, 1) are inserted. The newly created pointers are shown in dashed lines (Figure 3). For other aggregate operators, we simply modify step 1 correspondingly. For example, to implement SUM, we add the measure of the record being inserted to the current aggregate of the leaf. To implement MAX, the max value of these two replaces with the current aggregate of the leaf.

Cutting Sparse Leaves and Generating Runs
Even though the RSST stores only the non-empty data cubes that have a limited number of non-star coordinates, it may still not fit into the available memory for large sparse data sets at some point during the insertion process. Our strategy is to cut off sparse, low-dimensional cubes in the leaves and store them on disks so that they can be retrieved later. In this sense, RSST makes an optimal use of memory space and is used to speed up the most interesting queries. The total number of nodes in the RSST is maintained during the construction and maintenance. Once it reaches a certain threshold (we say, RSST becomes full), a cut phase starts. We cut off sparse leaves whose COUNT values are smaller than a threshold minSup. So, the size of the RSST always fits in available memory size. The initial minSup value can be estimated by the average density of r-dimensional cubesthat is, the total number of records divided by the product of cardinalities of r-non-star dimensions. By default minSup is set to 2. It can increase in later cutting phases.

Figure 3. The RSST after inserting the first four records
After inserting: 6, 9, 5, 1 20, 1, 3, 2 6, 9, 3, 3 20, 9, 3, 1
*

6 1 9

20 * 3 1

*

* 2

1
9

1

2

2

*

1
1 3

1

2
9 *

A
3

1

5

1

*

3

1
*

*

2
*

1
1

5

*

3 *

5 1

C
* 1

*

B

1

3

2

3

*

1

2

3

2

3

*

1

1

2

1

1

2

1

3

1

1

1

3

1

1

2

1

1

4

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

38 Fu

Figure 4. Cutting sparse leaves
Cut all sparse leaves and their empty ancestors in the RSST tree Input: the RSST tree, density threshold minSup Output: the streamlined RSST, runs of sparse cubes stored on disk; Method: 1. Push the root into a stack S; while (stack S in not empty) { // expand up to the leaf 2. while (the top node has unprocessed child & it is not a leaf) push the next unprocessed child on stack // process the leaf 3. if (the aggregate of the leaf < minSup) { 4. peek inside the stack upwards to form the leaf’s path; save this sparse leaf on disk; 5. 6. delete this leaf from RSST and pop off stack; delete the entry of parent 7. } else just pop off stack; // process the internal node 8. W hile ( all the children of the top node are processed) { 9. if (the top node is an empty internal node) 10. delete the node; pop off stack; delete the entry; else just pop off stack} }

Figure 4 shows the pseudo code for cutting. It is an iterative procedure that uses a stack to store nodes on the path being processed. First, the root is pushed into the stack. We then process each of the root-to-leaf paths in the RSST from left to right one by one. In addition to cutting sparse leaves that form runs on disks (lines 4 and 5), the empty internal nodes are also deleted (lines 9 and 10). Once a child is deleted, the corresponding entry should also be deleted (lines 6 and 7). Suppose that the aggregation threshold minSup is set to be 2. Continuing on Figure 3, we expand the first path 6-9 to the first leaf by pushing nodes on the stack. It is dense, therefore, we check the next two paths 6-*-3 and 6-*-5. Since their leaves are sparse, they are cut off from the tree. Other paths can be processed similarly. Empty internal nodes (e.g., nodes A, B, and C in Figure 3) must also be cut and the corresponding entries in their parent nodes should be deleted. All memory space resulting from cutting is reclaimed for later insertions. The sparse leaves being cut in one cutting phase form a run and are stored on disks. For example, the first run formed by the cutting of the sparse leaves in Figure 3 contains 20 cubes: (6, *, 3, *), (6, *, 5, *), (6, *, *, 1) …, (*, *, *, 3). The RSST after cutting off the sparse leaves and empty internal nodes is shown in Figure 5.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 39

Figure 5. The RSST after cutting
After cutting sparse leaves if minSup is set to 2

6

20

*

9

2

*

*

9

*

*

3

*

3

2
* 1

*

2
*

3 *

*

A 2 B

*

1

*

2

2

3

3 C

2

4 D

Construction of RSSTs: Loading
Once the cutting phase is finished, the insertion of records resumes until RSST is full again, at which time a new cutting phase starts. Repeated iterations of inserting and cutting continue until all input data are processed. If the available memory can hold all the cubes, no cutting phase is necessary. The RSST then contains all nonempty, low-dimensional cuboids. If cutting does happen, after all input records are inserted, we need make a final cut so that the RSST only retains low-dimensional dense cubes in memory. In this case, the loading process generates multiple runs (see lines 1-3 of Figure 6). Since the same data cuboids may be scattered across multiple runs due to the different ordering of input records, aggregating them together may qualify them as dense, low-dimensional cubes. In addition, there may be a case where a cube is sparse and cut, but later it becomes dense and is retained in memory. The query result of matching RSST alone will give a wrong answer. To resolve these issues, we merge all the runs into one output run (lines 4-5). We use a modified version of a loser tree (Horowiz, Sahni, & Mehta, 1995) to accomplish this. A loser tree is a complete binary tree used to merge a set of ordered runs into one merged run. The fields of a node in the loser tree are cube coordinates, aggregate value of the cube, and run index, which is used to track which run the output cuboid is from. A couple of issues need special care here. First, if the coordinates of a cube are equal to those of a node in the loser tree during upward travel, the aggregate values of the two cubes are just simply aggregated. Second, when a run is depleted during
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

40 Fu

Figure 6. Construction of RSST
Input: base table Output: RSST tree and a merged run file storing sparse cubes Load () { 1. Open and scan the input file record by record; 2. Insert each record into the RSST tree; 3. if ( available memory cannot hold the tree) { cut all the sparse leaves and all the empty internal nodes; store the sparse leaves in separate runs (on disk files); } 4. 5. 6. 7. } setup a loser tree by reading cubes into the buffers from the run files; merge the runs into the final run through an output buffer; Re-insert the elements in output buffer into the tree; if ( aggr of a cube > minSup or the coordinates matches a path in tree) re-insert into the tree or update the aggregates of the leaves; else write it into the merged run file for later retrieval;

Figure 7. The final RSST after inserting and cutting
6, 9, 5, 1 20, 1, 3, 2 6, 9, 3, 3 20, 9, 3, 1 6, 9, 3, 1 20, 1, 5, 2 6, 9, 5, 1
*

6 1

20 * 3 2

*

*

2
9

2

A
* 1 5 3

B

4

C

2

3

D
9 *

3

2

2

*

*

3
*

2
1

5

*

3

5 1

*

1

*

2

*

1

*

*

1

2

*

2

3

4

2

2

E

F

4

5

2

4

2

2

3 4

2

7

G

H

I

merging, a sentinel cube with infinity coordinates is filled. Notice that the “infinity” value is a large number plus the run index so that the different “infinities” are not erroneously merged. Continuing on Figure 5, suppose three more records (6, 9, 3, 1), (20, 1, 5, 2), (6, 9, 5, 1) are inserted. After cut, the second run contains 13 cubes: (6, *, 3, *), (6, *, 5, *), (20, 1, *, *), …, (*, *, *, 2). The RSST after cut is shown in Figure 7, with dotted parts

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 41

excluded. Now, we merge the first two runs. The first three cubes in the output buffer are (6, *, 3, *), (6, *, 5, *), (6, *, *, 1), whose aggregates of the first two are 2. For each cuboid from output buffer, we first check if its coordinates match a path of RSST. If yes, the aggregate value of the matched leaf is updated. For example, (6, *, *, 1) matches a path of RSST, so its COUNT increases from 2 to 3. If its aggregate value is above the threshold minSup, it becomes a dense cube and should be inserted into the RSST. However, this insertion is just the insertion of a new path. It is different from the insertion in the early insertion algorithm, which also requires it to visit or create the star paths. For example, new path (6, *, 3, *) with COUNT value of 2 must be inserted. If the cube (e.g., (6, *, *, 3)) is neither dense nor in the RSST, it is sparse and stored in the final merged run for later retrieval (lines 6-7). Figure 7 is the final tree. The parts with broken lines are newly inserted. The final merged run contains 11 cubes: (6, *, *, 3), (20, 9, *, *), (20, *, 5, *), (20, *, *, 1), (*, 1, 3, *), (*, 1, 5, *), (*, 9,*, 3), (*, *, 3, 2), (*, *, 3, 3), (*, *, 5, 2), and (*, *, *, 3). The loading process is shown in Figure 8. Notice that we only need one pass of the original input data to insert all the records. The runs to be merged contain only the sparse, low-dimensional cubes which could be of significantly smaller sizes than the input dataset. Furthermore, the merging process can be run as a daemon process. When all the input records are inserted into RSST, queries can be immediately evaluated for quick approximations. The estimated aggregate is between the quick answer from RSST and the quick answer plus the minSup*num_runs.

Figure 8. Prototype subsystem of loading
Input data insert Re-insert rSST cut
buf1 Output buf

In memory Write back

merge
Loser tree buf2 buf3

Run 1 Run 2 Run 3 Merged Run

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

42 Fu

Space Complexity of RSST and the Threshold Values
The size of RSST in terms of total number of nodes is polynomial O(nr) instead of exponential O(2n). The maximal number of nodes Ni at level i are given as follows:
for r = 2, N i = 2i − 2, i > 2 for r = 3, N i = 2 N i −1 − (i − 2)(i − 3), i > 4, N 4 = 8

The actual size of the RSST depends on a number of factors such as the number of dimensions, cardinality and data distribution of each dimension, the number of records that have been inserted, and input data ordering. Although we do not have a mathematical formula to compute the RSST size from these factors, we can control it by applying the thresholds formerly introduced. Through the cube dimensionality threshold r, users can specify the number of group-by attributes in the materialized views. It is typically no more than 4. The cardinality threshold can be initially set according to average density estimated as total-number-of-records/product of non-star cardinalities. These two thresholds influence how fast the tree grows. The cube aggregation threshold minSup determines how extensive the cutting goes. It has an influence on the sizes of the runs and indirectly the total number of runs. By default, it is set to 2. We allow two modes of setting this threshold: fixed mode and automatic mode. In fixed mode, user-given minSup stays the same across all cuts. In automatic mode, when the RSST is close to full after a cut and the runs are small, minSup will increase in some fashion (e.g., doubled each time). The thresholds provide users the flexibility of controlling the behavior of the algorithm according to their needs and a priori knowledge of input data and queries. If we set minSup to 1, r to n, and cardinality threshold to infinity, then the full cube is computed. Notice that regardless of these threshold values, the tree never grows out of available memory because when it nears full, a cutting phase is triggered.

Incremental Maintenance of RSSTs
We have described the insertion and cutting procedures on the RSST. In the analytical applications and data warehousing, the main refreshing operations are insertions and bulk loading. Although deletions and updates (can be regarded as a combination of insertions and deletions) are common in transactional data processing of traditional DBMSs, they are rare in data warehousing and OLAP. To delete a record, we can traverse the RSST by following the pointer corresponding to the domain value and the star pointer at each level as insertions do. When a leaf is reached, the measure value of the record is subtracted from the aggregate value
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 43

of the leaf. If there is no entry for the domain value of the record being deleted at the level of the node, the common prefix composed by the path from the root to that node should be expanded into r-dimensional cubes and save them to match sparse cubes in the merged run. For example, suppose the second record (20, 1, 3, 2) is to be deleted. After traversal, we find that the leaves A-I in Figure 7 are matched and their aggregates decrease by one. The unmatched common prefixes are (*, 1, 3, *) and (*, *, 3, 2), which are then matched with the merged run. If matching, the measure value (the default is one here for COUNT operation) is subtracted from the aggregate values of the matched cubes. So, RSST is incrementally maintainable. In particular, our bulk load and insertion operations share the same process of the RSST construction.

Answering Queries
Point Queries and Iceberg Queries
A point query can be represented by a tuple q (q0, q1,…,qn-1), where qi is in 0..Ci-1 or a special value *. Since we only compute the low-dimensional cuboids, the number of non-star coordinates in the query is restricted to be no more than a given threshold r. It is straightforward to evaluate such a query. Starting from the root of the RSST, we follow the pointers indexed by q0, q1,…, and so on to the next level nodes until a leaf is reached. The aggregate value of that leaf is returned as the query answer. At any node along the path, if no entry can be matched with the query value at the level of the node, query q is then matched with sparse cubes in the merged run. While merging the runs, we have also set up a page table whose entries are the coordinates of the first cubes of the pages. The target page of query q is first found in the page table. The target page is swapped into the memory. The cube is then found in the loaded page using binary search, and its aggregate is returned as the final answer if the cuboid queried is non-empty. If the memory can hold the page table, we only need at most one I/O operation to retrieve a point query. Otherwise, one or more levels of indirection may be necessary. For example, q = (20, *, 3, *) will eventually reach leaf B and the answer is 2. Query (6, *, *, 3) is not in the RSST, but matches a cube in the merged run and the answer is 1. Query (20, *, 7, *) matches neither, so the final answer is 0. For an iceberg query, if its threshold in the HAVING clause of SQL is larger than the cube aggregation threshold minSup, the query can be computed simply from the RSST. Otherwise, checking the sparse cubes in the merged run is necessary. If RSST can hold all the r-dimensional cuboids, both point queries and iceberg queries can be evaluated by the tree in memory without any I/O operations.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

44 Fu

Range Queries and Partial Queries
If a query specifies a range constraint or an arbitrary subset constraint on a dimension, it is a range query or a partial query. Given such a query, we extract the constrained domain values and store them using n vectors v0, v1, …, vn-1, each containing the chosen values for a dimension. If dimension i is absent, vi contains one single star * value. These vectors are called selected values sets (SVSs) of the query. We provide our evaluation algorithm for SVSs. Starting from the root of the SST, our algorithm follows the paths directed by the SVS until it will encounter related leaves. The access paths are determined by v0 × v1×…× vn-1, the cross product of the selected values. If the domain value is not in the index list of a node P, the path stops at node P because there is no dense cube for this path. If the tree holds all cubes, then the other query paths starting from here can be pruned. Otherwise, the aggregation of the visited leaves is returned to users as a quick approximation. The accuracy of estimation depends on the minSup value. If users require an exact answer, the unmatched cubes are checked with the merged run and aggregated. The pseudo code is shown in Figure 9. Suppose a cube query COUNT (*; {1, 9}; {3, 5}; *) is submitted and the RSST shown in Figure 7 and the merged run are used to evaluate the query. We first compute its SVS: v0 = {*}, v1 = {1, 9}, v2 = {3, 5}, and v3 = {*}. Starting from the root, follow the star pointer to the next level node. Then follow the pointer labeled 1 and the traversal stops there. The query result is still 0. The common prefix (*, 1) is expanded to (*, 1, 3, *), (*, 1, 5, *) stored in a vector Q. Check the second value 9 of v2 and follow the pointer labeled 9. Further tracking along the pointers for 3 and 5 leads to fall-off leaves. After adding them up, the result becomes 5. A quick

Figure 9. Answering cube queries with RSST
1 double query(n, level, SVS) { // aggregate all the related fall-off leaves in the subtree rooted at // node n on level “level” as the returned quick approximation; // SVS is the input selected vectors extracted from the input query; 2 sum = 0; 3 if n is a leaf then // base case 4 for each selected value e in v level do 5 if e is in the list of node n then 6 sum =sum+ aggregate of the leaf corresponding to e; 7 return sum; 8 level = level +1; 9 for each selected value e in v level-1 do 10 if e is in the list of node n then 11 sum =sum+query(child of n following e, level, SVS); 12 return sum; 13 }
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 45

approximate query result of 5 is returned. Next, we can refine the result by matching the paths stored in Q with the runs generated previously. The result is updated from 5 to 7 (after aggregating (*, 1, 3, *), (*, 1, 5, *) in the run). For large range and partial queries, vector Q may contain a large number of treeunmatched sparse cubes. Instead of matching each of them individually against the merged run file, we have designed a more efficient new “zigzag” method to accomplish. We use a simple example shown in Figure 10 to illustrate. The columns under column heads Q and pageTable are the vectors storing the paths. We use integers here for explanation purpose only. The values are strictly increasing. Buddies (i.e., the cubes in Q that are in the same page) and their target page numbers are also listed in the right columns in the figure. Starting from the first cube, find its target page index is 0. The right-pointing arrow corresponds to the “zig” step which computes the target page index of this cube (the page before the pointed one). At the “zag” step (left-pointing arrow) are the buddies if any are found. Since 40 is larger than 7, it is in a different page. So, there cube 5 alone is in page 0. The target page of 40 is one before the pointed 54 (i.e., page 2). The next cube in column Q that is greater than or equal to 54 indicates the buddies of 40 (from 40 to the one before the pointed). So cubes 40 and 44 are buddies on page 2. Other steps are similar.

Figure 10. “Zigzag” matching method
Q pageTable 0 5 40 44 54 90 131 7 23 54 101 142 251 54, 90 131 3 4 5 40, 44 0 2 buddies page #

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

46 Fu

Simulation Results
Setup for Simulation
We have implemented our RSST algorithm, BUC, Dwarf, and QC-tree in C++ language. We compare with them because they represent recent top data cube algorithms. We have conducted comprehensive experiments on the construction times and query response times by varying the number of records, the number of dimensions, the cardinalities of the dimensions, and data distributions. Our BUC implementation is based on the paper of Beyer and Ramakrishnan (1999). However, that paper only implements the internal BUC. To investigate its overall behavior including I/O operations, we implement both internal BUC and external BUC, and some other features in the paper such as switching from counting sort to quick sort, and duplicate elimination as well. We have also adapted BUC algorithm to RBUC, the dimensionality-restricted BUC, where only low-dimensional cubes are computed using modified BUC. In Dwarf implementation, we first use the sorting algorithm in Standard Template Library (STL) to sort the original table if it fits in memory. Otherwise, an external merging sorting is used. We use a stack to store the nodes in the current path. All the nodes are written to a disk in the order of closing. During coalescing, we bring back the nodes in the previous subtrees into the memory by referring to their disk addresses. For QCT (QC-tree) implementation, first the temporary classes are computed by applying a recursive DFS (depth-first search) procedure described in Lakshmanan et al. (2003). In our implementation, we regard the star value as the largest, thus deferring the construction of the star subtree to the end. If we first create the nodes in the star subtree, we have to bring into memory the nodes that have drill-down links, which significantly increases the number of I/Os. It is difficult to modify Dwarf and QCT algorithms so as to restrict the dimensionality of queried data cubes. All experiments are conducted on a Dell Precision 330 with 1.7GHZ CPU, 256MB memory, and the Windows 2000 operating system.

Varying Number of Records
In this set of experiments, we fixed the number of dimensions to five, each of size 20. We use data in Zipf distribution of factor 2 for each dimension. The number of records increases from 10,000 to 1,000,000. The cube aggregation threshold minSup and cube dimensionality threshold in RSST are set to 2 and 3 respectively. The runtimes of Figure 11 are the times for constructing the data structures. In QCT, the times of computing the temporary classes (i.e., the QC table) are recorded. The Dwarf construction times already include the pre-sorting times. Our RSST is about
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 47

two times faster than the closest competitor Dwarf in this case. The query evaluation times are much faster than construction times. We measure the query times using total response times of 1,000 random queries. The queries are generated by first randomly choosing three dimensions where random numbers within the domains are selected as queried values. All other coordinates in the queries are star values. RSST is much faster than other algorithms (Figure 12).

Figure 11. Construction times for varying number of records

3000

Runtime (sec.)

2 50 0 2000 15 0 0 10 0 0 50 0 0 R SST RBUC D w arf QC T 10 0 K 55 296 13 7 207 50 0 K 2 76 13 9 9 59 4 10 14 1M 552 2 54 9 118 2 19 4 4

N u m be r o f R e co rds

Figure 12. Query response times for varying number of records
Response times (sec.)
 0     0 R SST RB UC D w ar f QC T 0 0 K 0 .   . .    .  0 0 K 0 .   . .   . M 0 .   . .   . 

Nu m be r o f Re co rds

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

48 Fu

Figure 13. Varying number of dimensions
0 0 0 0

Runtime (sec.)

0000 0000 0000 0 0 0 0 0 R SST RBUC D w ar f QCT  0               0     0    0   0           

Nu m be r of di m e n si on s

Varying Number of Dimensions
In this subsection we investigate the behavior of the algorithms while increasing the number of dimensions. We use data of Zipf distribution (factor is 2), the number of records is 100,000, and the cardinality of each dimension is fixed to 1,000. The number of dimensions increases from 6 to 12. Figure 13 shows the construction times. Clearly, RSST and RBUC are scalable with respect to the number of dimensions, while Dwarf and QCT have steep increases of construction times with respect to the increase of dimensions. For high-dimensionality data, RSST is faster than Dwarf and QCT.

Varying Cardinalities
We use uniform data and set the number of records to be 100,000. The cardinalities of three dimensions are the same, which increase from 20 to 500. The construction times are recorded in Figure 14. RSST is scalable in terms of cardinalities. The construction time of RSST is not sensitive to the changes of dimension orders (Table 1).

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 49

Figure 14. Varying cardinalities using uniform distributed data
Const ruct i on t ime(s)
400 300 200 10 0 0 R SST BUC D w arf QCT

20 2 1. 4 10 3 9 3 .2 70

10 0 2 6 .2 2 2 7.8 14 2 . 6 13 5

50 0 3 2 .8 2 78 348 16 7

C a rdi n a l i ti e s

Table 1. Construction times of varying dimension orders
Cons. times(s) RSST RBUC Dwarf QCT 500, 100, 50, 10 44.4 679 299 453 441 10, 50, 100, 500 52.2 602 825

Conclusion
In this chapter, we present a new data structure called RSST, which stores the cuboids whose number of group by attributes is no more than a certain threshold. In practice, these cuboids, especially the dense ones, are most often queried. Based on RSST, a cubing algorithm has been given. RSST retains the dense cubes, but dynamically cuts off sparse cubes that are rarely or never queried, and stores them on disks for later retrieval if necessary. RSST can also automatically choose the dimension combinations according to cube cardinality threshold. These methods combined can efficiently compute the dense cuboids embedded in large high-dimensional data sets. RSST is incrementally maintainable. Our comprehensive sets of experiments have shown that RSST is I/O efficient and scalable in terms of number of records, number of dimensions, and dimension cardinalities. RSST is insensitive to dimension orders and is especially suitable for skewed data sets.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

50 Fu

References
Aboulnaga, A., & Chaudhuri, S. (1999). Self-tuning histograms: Building histograms without looking at data. In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data (SIGMOD ’99), Philadelphia (pp. 181-192). Acharya, S., Gibbons, P.B., & Poosala, V. (2000). Congressional samples for approximate answering of group-by queries. In Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data (SIGMOD ’00), Dallas, TX (pp. 487-498). Agarwal, S., Agrawal, R., Deshpande, P., Naughton, J., Sarawagi, S., & Ramakrishnan, R. (1996). On the computation of multidimensional aggregates. In Proceedings of the International Conference on Very Large Databases, Mumbai (Bomabi), India (pp. 506-521). Agrawal, R., & Srikant, R. (1994, September 12-15). Fast algorithms for mining association rules in large databases. In C. Zaniolo (Ed.), In Proceedings of 20th International Conference on Very Large Data Bases, Santiago de Chile, Chile (pp. 487-499). San Francisco: Morgan Kaufmann. Beyer, K., & Ramakrishnan, R. (1999). Bottom-up computation of sparse and iceberg CUBEs. In S.B. Davidson & C. Faloutsos (Eds.), In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data (SIGMOD ’99), Philadelphia (pp. 359-370). Beyer, K., & Ramakrishnan, R. (1999). Bottom-up computation of sparse and iceberg CUBEs. In C. Faloutsos (Ed.), In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data (SIGMOD ’99), Philadelphia (pp. 359-370). Chan, C.Y., & Ioannidis, Y.E. (1998). Bitmap index design and evaluation. In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data (SIGMOD ’98), Seattle, WA (pp. 355-366). Chaudhuri, S., & Dayal, U. (1997). An overview of data warehousing and OLAP technology. SIGMOD Record, 26(1), 65-74. Fu, L. (2004). Computation of dense data cubes using packaged arrays. In Proceedings of the International Conference on Internet Computing 2004 (IC’04), Las Vegas, NV (pp. 116-120). Fu, L., & Hammer, J. (2000, November). CUBIST: A new algorithm for improving the performance of ad-hoc OLAP queries. In Proceedings of the ACM 3rd International Workshop on Data Warehousing and OLAP, Washington, DC (pp. 72-79).

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Computing Dense Cubes Embedded in Sparse Data 51

Gibbons, P.B., & Matias, Y. (1998). New sampling-based summary statistics for improving approximate query answers. In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data (SIGMOD ’98), Seattle, WA (pp. 331-342). Gray, J., Chaudhuri, S., Bosworth, A., Layman, A., Reichart, D., Venkatrao, M. et al. (1997). Data cube: A relational aggregation operator generalizing groupby, cross-tab, and sub-totals. Data Mining and Knowledge Discovery, 1(1), 29-53. Gupta, H., & Mumick, I. (1999). Selection of views to materialize under a maintenance cost constraint. In Proceedings of the International Conference on Management of Data, Jerusalem, Israel (pp. 453-470). Hammer, J., & Fu, L. (2001, September). Improving the performance of OLAP queries using families of statistics trees. In Proceedings of the 3rd International Conference on Data Warehousing and Knowledge Discovery (DaWaK ’01), Munich, Germany (pp. 274-283). Han, J., Pei, J., Dong, G., & Wang, K. (2001). Efficient computation of Iceberg cubes with complex measures. ACM SIGMOD Record, In Proceedings of the 2001 ACM SIGMOD International Conference on Management of Data (Vol. 30, pp. 1-12). Harinarayan, V., Rajaraman, A., & Ullman, J.D. (1996). Implementing data cubes efficiently. SIGMOD Record, 25(2), 205-216. Horowiz, E., Sahni, S., & Mehta, D. (1995). Fundamentals of data structures in C++. W.H. Freeman & Company. Johnson, T., & Shasha, D. (1997). Some approaches to index design for cube forests. Bulletin of the Technical Committee on Data Engineering, IEEE Computer Society, 20(1), 27-35. Labio, W., Quass, D., & Adelberg, B. (1997). Physical database design for data warehouses. In Proceedings of the International Conference on Database Engineering, Birmingham, UK (pp. 277-288). Lakshmanan, L.V.S., Ng, R., Han, J., & Pang, A. (1999). Optimization of constrained frequent set queries with 2-variable constraints. In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data, Philadelphia (pp. 157-168). Lakshmanan, L.V.S., Pei, J., & Han, J. (2002). Quotient cube: How to summarize the semantics of a data cube. In Proceedings of 28th International Conference on Very Large Databases (VLDB ’02), Hong Kong, China (pp. 778-789). Lakshmanan, L.V.S., Pei, J., & Zhao, Y. (2003, June 9-12). QC-trees: An efficient summary structure for semantic OLAP. In A. Doan (Ed.), In Proceedings of the 2003 ACM SIGMOD International Conference on Management of Data, San Diego, CA (pp. 64-75). ACM Press.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

52 Fu

Lehner, W., Sidle, R., Pirahesh, H., & Cochrane, R.W. (2000). Maintenance of cube automatic summary tables. In Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data (SIGMOD ’00), Dallas, TX (pp. 512-513). Markl, V., Ramsak, F., & Bayer, R. (1999, August 2-4). Improving OLAP performance by multidimensional hierarchical clustering. In Proceedings of the 1999 International Database Engineering and Applications Symposium (IDEAS ’99), Montreal, Canada. Mumick, I.S., Quass, D., & Mumick, B.S. (1997). Maintenance of data cubes and summary tables in a warehouse. In Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data (SIGMOD ’97), Tucson, AZ (pp. 100-111). Ross, K.A., & Srivastava, D. (1997). Fast computation of sparse datacubes. In Proceedings of the 23rd VLDB Conference (VLDB ’97), Athens, Greece (pp. 116-125). Sismanis, Y., Deligiannakis, A., Roussopoulos, N., & Kotidis, Y. (2002). Dwarf: Shrinking the PetaCube. In Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data (SIGMOD ’02), Madison, WI (pp. 464-475). Vitter, J.S., & Wang, M. (1999). Approximate computation of multidimensional aggregates of sparse data using wavelets. In Proceedings of the 1999 ACM SIGMOD International Conference on Management of Data (SIGMOD ’99), Philadelphia (pp. 193-204). Wang, W., Feng, J., Lu, H., & Yu, J.X. (2002). Condensed cube: An effective approach to reducing data cube size. In Proceedings of 18th IEEE International Conference on Data Engineering (ICDE ’02), San Jose, CA (pp. 155-165). Xin, D., Han, J., Li, X., & Wah, B.W. (2003, September 9-12). Star-cubing: Computing iceberg cubes by top-down and bottom-up integration. In J.C. Freytag, P.C. Lockemann, S. Abiteboul, M.J. Carey, P.G. Selinger, & A. Heuer (Eds.), In Proceedings of 29th International Conference on Very Large Data Bases, Berlin, Germany (pp. 476-487). San Francisco: Morgan Kaufmann. Yan, W.P., & Larson, P. (1995). Eager aggregation and lazy aggregation. In Proceedings of the 8th International Conference on Very Large Databases, Zurich, Switzerland (pp. 345-357). Zhao, Y., Deshpande, P.M., & Naughton, J.F. (1997). An array-based algorithm for simultaneous multidimensional aggregates. SIGMOD Record, 26(2), 159170.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 53

Chapter III

Exploring Similarities Across High-Dimensional Datasets
Karlton Sequeira, Rensselaer Polytechnic Institute, USA Mohammed Zaki, Rensselaer Polytechnic Institute, USA

Abstract
Very often, related data may be collected by a number of sources, which may be unable to share their entire datasets for reasons like confidentiality agreements, dataset size, and so forth. However, these sources may be willing to share a condensed model of their datasets. If some substructures of the condensed models of such datasets, from different sources, are found to be unusually similar, policies successfully applied to one may be successfully applied to the others. In this chapter, we propose a framework for constructing condensed models of datasets and algorithms to find similar substructure in pairs of such models. The algorithms are based on the tensor product. We test our framework on pairs of synthetic datasets and compare our algorithms with an existing one. Finally, we apply it to basketball player statistics for two National Basketball Association (NBA) seasons, and to breast cancer datasets. The results are statistically more interesting than results obtained from independent analysis of the datasets.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

54 Sequeira & Zaki

Introduction
Often, data may be collected by a number of sources. These sources may be geographically far apart. There are a number of disadvantages in transferring the datasets from their source to a central location for processing. These include less reliability, security, higher computational and storage requirements, and so forth. It may be preferable to share condensed models of the datasets. Similarly, for reasons like confidentiality agreements, it may be required to use condensed models of datasets, which obfuscate individual details while conveying structural information about the datasets. Lastly, the datasets may have slightly different dimensionality or transformations like rotations, with respect to each other. This may preclude simply appending the datasets to each other and processing them. If unusually similar substructure can be detected from the condensed models of some of the datasets, then policies successfully applied to one may be successfully applied to the others. For example, two consumer markets (A and B) differing in geography, economy, political orientation, or some other way may have some unusually similar consumer profiles. This may prompt sales managers in B to use successful sales strategies employed by sales managers in A for consumer profiles in which they are unusually similar. Also, profiles which are unusually dissimilar to any of those in the other graph are particularly interesting. The latter is analogous to the problem of finding contrast sets (Bay & Pazzani, 2001). Additionally, determining similarities and dissimilarities between snapshots of a dataset taken over multiple time intervals can help in identifying how the dataset characteristics evolve over time (Ganti, Gehrke, Ramakrishnan, & Loh , 1999). A dataset may be a set of points drawn in possibly different proportions, from a mixture of unknown, multivariate, and perhaps non-parametric distributions. A significant number of the points may be noisy. There may be missing values as well. We currently assume that the dataset may belong to non-identical attribute spaces, which are mixtures of nominal and continuous variables. The datasets may be subject to translational, rotational, and scaling transformations as well. High-dimensional datasets are inherently sparse. It has been shown that under certain reasonable assumptions on the data distribution, the ratio of the distances of the nearest and farthest neighbors to a given target is almost 1 for a variety of distance functions and data distributions (Beyer, Goldstein, Ramakrishnan, & Shaft, 1999). Hence, traditional distance metrics which treat every dimension with equal importance have little meaning. Algorithms using such dissimilarity measures as a building block for application to high-dimensional datasets may produce meaningless results due to this lack of contrast.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 55

In this chapter, we explore similarities across datasets using a two-step solution: 1. Constructing a condensed model of the dataset. This involves finding the components of the model, and relationships between these components. In our case, the components are subspaces. The condensed model is a weighted graph where the vertices correspond to subspaces and the weighted edges to relationships between the subspaces. A condensed model allows: (a) sharing of dataset summaries, (b) noise and outlier removal, and (c) normalization and dataset scaling. Identifying similarities between the condensed models. In our solution, this reduces to finding structurally similar subgraphs in the two models and matching vertices between the structurally similar subgraphs.

2.

In previous work (Sequeira & Zaki, 2004), we have shown algorithms to find components of the model. In this chapter, we make the following contributions: 1. We propose two kinds of similarity measures for subspaces (components). The first kind is projection basedthat is, it uses the similarity of the projections of the subspaces. The other is support basedit uses the number of points shared by the subspaces. We provide algorithms for identifying unusually similar substructure from the condensed models corresponding to pairs of datasets with possibly differing dimensionality. We test our framework with synthetic datasets and apply it to finding similar substructure in models constructed from basketball player statistics and breast cancer datasets. Inferences from the similar substructure are found to be logically meaningful. Further, they reveal information, which remains unknown under independent dataset analysis.

2.

3.

Preliminaries
Consider dataset DA having dA dimensions. If SA,i is the domain of the ith dimension, then SA=SA,1 × SA,2 ×...×SA,d is the high-dimensional space for DA, where DA={xi|i∈[l,m,]xi∈SA}. Similarly, DB={yi|i∈[l,n,]yi∈SB}. If the range SA,i of each dimension is divided into x equi-width intervals, then SA has a grid superimposed over it. Accordingly, we have the following definition: a subspace is a grid-aligned hyper-rectangle [l1, h1]×[l2, h2]×...×[ld, hd], ∀i∈[l, d,] [li, hi]⊆SA,i. Here for a given
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.
A

56 Sequeira & Zaki

interval [li, hi], we have li =(aS A,i)/x and hi =(bsA,i)/x, where a,b are non-negative integers, and a<b≤x. If [li, hi]⊂SA,i, the subspace is said to be constrained in dimension ithat is, the subspace does not span the entire domain of the dimension i, A subspace that is constrained in all the dimensions to a single intervalthat is, b−a=1 is referred to as a grid cell. If our algorithm to find components in a dataset finds |VA| components/subspaces internal to DA, the relationships between these subspaces are expressed by a |VA|×|VA| matrix wA : SA ×SA→ℜ. We also use the following notations for the rest of this chapter: let A=(ai,j)1≤i, j≤m,n and B=(bkl)1≤k,l≤p,q be two matrices. If m=n, Tr[A]=Σi[1,m]ai,i is the trace of A. A T refers to the transpose of A.
m n

|| A || F = (∑i =1∑ j =1 | ai , j |2 )1/2 , is the Frobenius norm of A. ones(m,n) returns a m×n matrix containing all ones. The tensor product of A and B is a mp×nq matrix, and is defined as:
 a1,1 B a1,2 B  a2,n B    A ⊗ B =  a2,1 B a2,2 B  a2,n B  a B a B  a B m ,2 m,n   m ,1

An n×n matrix X is called normal, if it can be written as X=UX DXUTX, where UX is a unitary matrix containing the eigenvectors of X, and DX is a diagonal matrix containing the eigenvalues of X .lX,i denotes the ith eigenvalue, where ∀i[1,n-1], lX,i ≥lX,i+1, and UX,i denotes the eigenvector corresponding tolX,i. If lX,1>lX,2, lX,1 and UX,1are called the dominant eigenvalue and dominant eigenvector respectively. If S=[s1,s2...] where s1,s2... are column vectors, then vec(S) creates a column vector T by stacking its column vectors one below the other, so that vec( S ) = [ s1T s2 ]T . Let VA and VB be the components (subspaces) of datasets DA and DB, respectively. Let P be the function, which takes as argument a mapping f:VA→VB, and returns a permutation matrix (typically, a permutation matrix is a square matrix)that is, a |VA|×|VB| matrix, such that:
1 if f (u ) = v Pf (u , v) =  0 otherwise

(1)

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 57

If f is a one-to-one mapping, then if |VA|≤|VB| (|VA|>|VB|), the rows (columns) of P are orthogonal to each other and PPT=I (PTP=I). As in Van Wyk and Van Wyk (2003), we want f which minimizes the associated error function err, which we define as:
err ( f | wA , wB ) =|| wA − Pf wB PfT || F

(2)

A mapping f from a subset of subspaces corresponding to wA to a subset corresponding to wB is unusually similar, if the probability of finding another mapping f ′ between these subsets, by MonteCarlo sampling as later in this chapter, such that err ( f | wA , wB ) > err ( f ′ | wA , wB ) is very low.

Example
Let DA and DB be two datasets as shown in Table 1, with domains [0,1000) for each dimension. DA(p1,d1) refers to row p1, column d1 of dataset DA. If we discretize the domain of each dimension into 10 intervals (i.e., x=10), then the grid cells surrounding the points in DA, DB yield the datasets D' A , D' B in Table 2. For example, 915 ×  = 9. Thus, p1 is constrained to the DA(p1,d1)=915. Therefore, D' A ( g1 , d1 ) =  last interval in dimension d1that is, [900,1000). We then run a subspace mining algorithm (e.g., SCHISM (Bay & Pazzani, 2001), CLIQUE (Ganti et al., 1999)) on each of the discretized datasets independently and find two sets of subspaces S and S’ corresponding to DA and DB respectively, as shown in Table 3. Here -1 implies that the dimension is unconstrained. S(c1,d2)=5 means that the subspace c1 in the set S of subspaces is constrained to interval 5 in dimension d2 that is, [500,600). Subspaces may be constrained to more than one interval in a dimension. Typically, subspace mining algorithms also partition the dataset based on the subspaces it finds. Let us assume that the subspace mining algorithm assigns p1, p2 to c1, p3, p4, p5 to c2, p6, p7, p8 to c3 and labels p9 as noise. Similarly, it assigns p '1 , p ' 2 , p '3 , p ' 4 to c'1; p '5 , p ' 6 , p ' 7 to c' 2 vand labels p '8 as noise. Given such subspaces and the points assigned to them, we wish to construct condensed models of the datasets, which can be used to discover structurally similar subspaces across the two datasets without having access to the datasets or their schema. For example, in Table 3, if d2 corresponds to d ' 4 and d4 corresponds to d ' 2, then c1 and c'1 are both constrained in the same dimension and to the same intervalthat is, [500,600). Also, c2 and c' 2 are constrained in the same dimensions to similar intervals. Hence, c1 ≈ c'1 and c 2 ≈ c' 2. Thus, we wish to recover the mapping between c1 and c'1, and c2 and c' 2.
1000

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

58 Sequeira & Zaki

Table 1. Original data
DA
p1 p2 p3 p4 p5 p6 p7 p8 p9

d1
915 965 217 758 276 268 239 237 33

d2
561 575 506 512 531 520 514 510 118

d3
866 534 121 357 327 351 369 377 144

d4
657 860 452 423 418 348 301 650 388

d5
661 365 303 289 335 454 451 472 280

DB
p′ 1 p′ 2 p′ 3 p′ 4 p′ 5 p′ 6 p′ 7 p′ 8

d′ 1
889 854 553 779 88 391 574 805

d′ 2
710 189 869 690 453 436 450 60

d′ 3
591 641 449 203 965 193 220 803

d′ 4
564 564 612 598 541 578 588 525

d′ 5
679 666 199 872 324 301 270 152

Table 2. Discretized data
D′ A d1
9 9 2 7 2 2 2 2 3

d2
5 5 5 5 5 5 5 5 1

d3
8 5 1 3 3 3 3 3 1

d4
6 8 4 4 4 3 3 6 3

d5
6 3 3 2 3 4 4 4 2

D′ B
g′ 1 g′ 2 g′ 3 g′ 4 g′ 5 g′ 6 g′ 7 g′ 8

d′ 1
8 8 5 7 8 3 5 8

d′ 2
7 1 8 6 4 4 4 6

d′ 3
5 6 4 2 9 1 2 8

d′ 4
5 5 6 5 5 5 5 5

d′ 5
6 6 1 8 3 3 2 1

g1

g2 g3 g4 g5 g6 g7 g8 g9

Table 3. Two sets of subspaces
S

d1
-1 -1 22

d2
5 5 5

d3
-1 -1 33

d4
-1 44 -1

d5
-1 33 44

c1 c2 c3
S′

d′ 1
-1 -1

d′ 2
-1 44

d′ 3
-1 -1

d′ 4
5 5

d′ 5
-1 22

c′ 1 c′ 2

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 59

Related Work
Our two-step solution to finding unusually similar substructure across datasets involves: 1. Constructing a condensed model of the dataset; this involves two sub-steps: a. b. 2. finding components in the dataset constructing a condensed model from the components

Identifying similarities between the condensed model

Finding Components in the Dataset
We find components in the dataset using a subspace mining algorithm called SCHISM (Bay & Pazzani, 2001), which finds sets of possibly overlapping subspaces, for example, set S from dataset DA in the example above. It partitions the points in the datasets using these subspaces. Note that any other hyper-rectangular subspace mining algorithmfor example, MAFIA (Beyer et al., 1999), CLIQUE (Sequeira & Zaki, 2004), and so forthmay be used to find the subspaces and partition the dataset. Hence, we do not delve into the details of the SCHISM algorithm.

Constructing Condensed Models from the Components
We condense the dataset using a weighted graph, where the vertices correspond to subspaces and the weights on the edges to similarities between the subspaces. While we are unaware of much related work on similarities between subspaces, it is noteworthy that subspaces are also clusters. Accordingly, we review some of the existing similarity measures used for comparing clusterings. Clusterings may be compared based on the number of point pairs, in which the two clusterings C,C′ agree or disagree. Each pair of dataset points is assigned to one of four categories N00, N01, N10, and N11. Pairs of points in N00 are assigned to distinct clusters in both C and C′, those in N01 are assigned to the same cluster in both C and C′, those in N01 are assigned to the same cluster in C but to distinct clusters in C′, and so on. If the dataset has n points, N00+N01+N10+N11=n(n-1)/2. Accordingly there exists the Rand index:
Rand (C , C ′) = N11 + N 00 N11 + N10 + N 01 + N 00

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

60 Sequeira & Zaki

and the Jaccard index

Jaccard (C , C ′) =

N11 N11 + N10 + N 01

to compare the clusterings. Further Meila (2003) proposes the VI (variance of information) metric to compare clusterings:VI (C , C ′) = H (C ) + H (C ′) − 2 I (C , C ′), where n p |C | ′ H (C ) = ∑i =1 − pi log ( pi ), pi = i and where I (C , C ′) = ∑ ∑ p log ( p p ), pi , j = | Ci ∩ C j |, n n with ni being the number of points in Ci, the ith cluster in C. This implies that pi and pi,j are simply the support of clusters Ci and Ci ∩ C j respectively, according to the traditional definition of support in the data mining literature (Bay & Pazzani, 2001).
|C | |C ′| i =1 j =1 i , j i, j i j

Thus, these clustering (dis)similarity measures use (dis)similarity in support overlap to express cluster similarity.

Identifying Similarities Between Condensed Models of Different Datasets
Ganti et al. (1999) compare datasets by comparing their respective models. The datasets share a common schema. A dataset may be typically modeled by a decision tree, a set of clusters, or a set of frequent itemsets. The model consists of a set of pairs. Each pair consists of an “interesting region” in the dataset (called the structural component) and the fraction of the dataset (called the measure component) it accounts for. They then partition the attribute space using hyperplanes, which (as per the type of model chosen) define the leaves, clusters or frequent itemsets, induced by the models of the two datasets. Using a single scan of each dataset, they can compute the fraction of each dataset in each distinct hyperspace, resulting from the superposition of the two models of the datasets. They then compare these fractions, corresponding to different datasets but the same hyperspace, using a “difference” function and combine the resulting “deviation” using an “aggregation” function which returns a measure of the similarity of the datasets. This method does not leverage the structure present in the data and hence is susceptible to translational transformations. Much of the existing work in the database community (Bay & Pazzani, 2001) assumes the datasets have identical schema and that access to both datasets simultaneously is possible. By utilizing the underlying structure in the datasets, we avoid making such assumptions. Li, Ogihara, and Zhu (2002) use a variant of the mutual information between datasets DA and DB, modeled by sets of maximal frequent itemsets (MFIs) FA and FB,
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 61

which is defined as: I ( FA , FB ) = ∑i∈FA , j∈FB | i ∩ j | log (1 + | i ∩ j | ) * min(| i |, | j |). They assume an |i∪ j| |i∪ j| identical schema for two datasets and define the similarity between the datasets as:

approaches in which disjoint pairs of subsets of the attributes are drawn at random from samples of the given datasets. The similarity between the pairs of samples is used to estimate the distribution of similarity between the two datasets. They then generalize their approach to heterogeneous datasets, of which matchings between some of the attributes of the two datasets are known. These matchings are used to identify matchings of at least x attributes of one dataset with those of the other.

I ( FA , FB ) * 2 I ( FA , FA ) + I ( FB , FB ) . To test for significance of similarity, they propose bootstrapping-based

There have been a number of graph matching algorithms, stemming from work in the field of computer vision, regarding applications like image registration, object recognition, and so forth. Many of the past approaches involve matching between labeled or discrete-attributed graphs (Bunke, 1999; Carcassoni, 2002; Kalviainen & Oja, 1990; Van Wyk & Van Wyk, 2003). Like the solutions to many other NP-hard problems, graph matching algorithms may be enumerative (Bunke, 1999; Shapiro & Haralick, 1985) or optimization based (Carcassoni, 2002; Van Wyk & Van Wyk, 2003). Most of these algorithms assume the graphs lie in the same space, which is usually low dimensional (i.e., two or three dimensions). The concept “two vertices are similar, if vertices they are related to are similar” allows recursive definition of inter-vertex similarity. This idea is used explicitly or implicitly by a number of propagation-based algorithms (Melnik, Garcia-Molina, & Rahm, 2002) for a range of applications. The recursive definition causes similarity to flow from one vertex to the other. Blondel, Gajardo, Heymans, Senellart, and Van Dooren (2004) show that given wA and wB (the similarity among the subspaces with the two datasets), |VA|×|VB| similarity matrix S, whose real entry si,j represents the similarity between vertex i of GA and j of GB , can be obtained as the limit of the normalized even iterates of T T S k +1 = wB S k wA + wB S k wA. Note that this model does not assume that wA and wB are symmetric. This algorithm has time complexity of matrix multiplication, which is currently O(=n2.376). We compare our algorithms with Blondel’s algorithm. Gionis, Mannila, and Tsaparas (2005) examine the problem of finding a clustering that agrees as much as possible with a set of given clusterings on a given dataset of objects. They provide an array of algorithms seeking to find either: (i) the clustering that minimizes the aggregate number of disagreements with the given set of clusterings (clustering aggregations), or (ii) a partition of the objects into two groups, such that the sum of aggregate dissimilarities between objects in the same group and aggregate similarities between objects in different groups is minimized (correlation clustering). Here the (dis)similarities between objects are defined using the given clusterings. This differs from our work, in that the same dataset is used to produce each clustering.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

62 Sequeira & Zaki

Constructing a Condensed Model of the Dataset
We represent each dataset DA by a weighted graph GA(VA, EA, wA), where VA is the set of subspaces found by the subspace mining algorithm, EA⊆VA×VA is the set of edges between the subspaces in the dataset, and wA:SA×SA→ℜ is the adjacency matrix/set of weights on the edges of the graph G A , indicating similarity between components/subspaces in the condensed model/graph of GA. Depending on whether we use support or similarity of projections as the basis for comparing subspaces, we prescribe the following subspace similarity measures.

Support-Based Subspace Similarity
Each subspace u∈VA partitions the space SA into a clustering containing two clustersthat is, u and SA\u. Accordingly, if Cu,Cu′ are the clusterings yielded by subspaces u,u′∈VA, we can define wAv(u,u′) using Jaccard (Cu , Cu′ ) and Rand (Cu , Cu′ ). Additionally, we experiment with using the VI measure of Meila (2003): wA (u , u ′) = exp(−VI (Cu , Cu′ )).

Projection-Based Subspace Similarity
Consider the case where the datasets being modeled are sets of points sampled in different proportions with respect to each other from the same mixture of multivariate distributions. Then, correctly matching these distributions using support-based subspace similarity measures is unlikely. Accordingly, we seek similarity measures which use similarity of the projections of the subspaces. We define the similarity between subspace R∈VA and a grid cell Q surrounding a point r∈DA using the Jaccard-coefficient as:
( r ∈ Q, R ∈ V A ) = 1 dA

∑|Q ∪R |
i =1 i i

dA

| Qi ∩ Ri |

(3)

Here, Qi, Ri refer to the set of intervals spanned by subspaces Q, R respectively, in dimension i. If dimension i of R is unconstrained, then |Ri|=x. For example, using our running example,
( p1 ∈ g1 , c1 ) = 1 1 1 1 1 1  + + + +  = 0.28.  dA  1  

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 63

Based on the subspaces found by the subspace mining algorithm, it is possible, for example using nearest neighbors, to assign points in the dataset to subspaces. Using the assignment of points to subspaces, we have devised two similarity measures: AVGSIM and HIST.

AVGSIM
Each subspace may be thought to be more accurately approximated by the points assigned to it. As we know the similarity between the grid cell around each point and every subspace found by the subspace mining algorithm using r() from Equation 3, the similarity between two subspaces u∈VA,u′∈VA can be defined as:

wA (u , u ′) =

∑
r∈u

(r , u ′) |u|

+ r∈u′

∑

(r , u ) | u′ |

(4)

From our running example:
( p1 ∈ g1 , c2 ) = 0.24, ( p2 ∈ g 2 , c2 ) = 0.44, ( p3 ∈ g 3 , c1 ) . = ( p4 ∈ g 4 , c1 ) = ( p5 ∈ g 5 , c1 ) = 0.28

Then, wA (c1 , c2 ) =

0.24 + 0.44 0.28 + 0.28 + 0.28 + = 0.62. 2 3

To ensure that ∀u ∈ VA , wA (u, u ) = 1, we normalize by setting:
wA (u , u ′) = wA (u , u ′) wA (u , u ) × wA (u ′, u ′) .

HIST
Based on the coordinates of points assigned to each subspace in VA, we estimate discrete p.d.f.s. for each dimension for each subspace. If each dimension of the dAdimensional dataset is discretized into x equi-width intervals, then u(i,j) corresponds to the fraction of points assigned to vertex/subspace u, which are discretized to the jth interval in the ith dimension. Using our running example, there are two points p1,p2 assigned to subspace c1. Both of them are discretized to the interval 5 in the 2 dimension d2that is, [500,600). Therefore, c1 (2,5) = = 1. Accordingly, 2
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

64 Sequeira & Zaki

wA (u , u ′) =

1 dA

∑∑sim(u (i, j ), u′(i, j ))
i =1 j =1

dA

(5)

where sim : [0,1] × [0,1] → [0,1] is a similarity function. Note that this wA() requires no normalization if we use the Gaussian or increasing weighted sim() function shown below. Otherwise, normalization is required. We have tested a number of symmetric similarity functions: • • • Dot product: sim (a, b) = a × b Gaussian weighted: sim(a, b) = exp( Increasing weighted: sim(a, b) =
− ( a − b) 2 ) 2 12 s

| a −b| s where s is a user-defined parameter controlling the spread of sim. Note that our similarity measures are both symmetric and independent of the number of points assigned to each subspace. 1+

Identifying Similarities Between Condensed Models
Once we have the internal similarity among the subspaces within each dataset in the form of weighted graphs, to find similarities between the dataset graphs, we test three algorithms. One (OLGA) uses the tensor product, the next (EigenMatch) uses ideas from the first and Blondel’s algorithm, and the last uses MonteCarlo sampling.

OLGA
We combine the graphs GA and GB into a single bipartite graph:
G = (VA ∪ VB , E ⊆ VA × VB , Π ). Π is a |VA|×|VB| matrix of pairwise vertex similarities.

To find Π, we construct the product graph (see function ProductGraph in Figure 1 G′ = (VA × VB , E ′ ⊆ (VA × VB ) × (VA × VB ), wA,B ), where wA, B : E ′ → ℜ is the adjacency matrix, indicating similarity between vertices corresponding to pairs of subspaces from underlying graphs of G′. Let ( A, B) = sim( wA (u , u ′), wB (v, v′)), then:
 ( A, B) w A, B ((u , v), (u ′, v ′)) =   0 if ( A, B) > otherwise

(6)

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 65

where t is a user-specified threshold, used to minimize noise and limit space complexity of the algorithm. As wA (u, u ′), wB (v, v′) depend on GA,GB respectively, the weight of an edge in product graph G′ is high, if the weights on the corresponding edges in the underlying graphs are similar. Thus, we do not explicitly compare dimensions of vertices in the two graphs, thereby making no assumptions on identical schema. Let S = vec (Π ) (as defined above) and l =| VA || VB |length column vector. Using the concept, “two vertices are similar, if vertices they are related to, are similar,” then similarity between u∈VA and v∈VB is a function of all the vertices in VA and VB, and the relationships that u and v have with them, respectively. If Si denotes S at iteration i, we can write this as (with u ′ ∈ VA , v′ ∈ VB):
Si ((u , v)) = Then, Si

∑w

A, B

((u , v), (u ′, v′)) Si −1 ((u ′, v′))

= wA, B ((u , v), :) ⋅ Si −1 = wA, B ⋅ Si −1

where wA, B ((u , v), :) returns the (u,v)th row of wA,B. As shown in Figure 1, we set the initial similaritiesthat is, all entries in S0to 1.0 (line 6). We then iterate using Equation 7 (line 8). We determine convergence by checking to see if the Frobenius norm of the residual at the end of each iteration is less than a user-specified threshold e (line 9). As we are looking for a matching between vertices from GA to GB, we may unstack the vector S and use the resulting |VA|×|VB| matrix as the adjacency matrix of the bipartite graph G (i.e.,∏). Ideally, ∏ is a permutation matrix which minimizes err(f |wA,wB) (Equation 2). Typically however, ∏ is a real matrix. Hence, we need to round ∏ to a permutation matrix. We use the Match function to do the same. Match returns f: VA→VB. There are a number of matching algorithms, for example, stable matching, the Kuhn-Munkres algorithm (Kuhn, 1955), perfectionist egalitarian polygamy (Melnik et al., 2002), and so forth. We can formulate the rounding as finding a matching which maximizes the sum of the weights on the edges of the matching. Finding such a matching (also called an alignment) is called bipartite weighted matching, which has earlier been optimally solved by the Hungarian algorithm (Kuhn, 1955). This algorithm has complexity O(max{| VA |, | VB |}) 3 ). This is equivalent to partitioning G into a number of clusters such that no cluster contains two vertices from the same graph, and the total of the similarity among the vertices within each cluster is maximized. Match, unless otherwise mentioned, refers to the Hungarian algorithm. There are other approximate matching algorithms of lower complexity. We do not take into account the complexity of Match while stating complexity of the algorithms, as it is a parameter. This idea is similar to similarity propagation in Melnik et al. (2002). However, they use directed, labeled graphs.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

66 Sequeira & Zaki

If wA,B is normal, it is diagonalizable. If it has a dominant eigenvalue,
Si = wA, B ⋅ Si −1 || wA, B ⋅ Si −1 || F Then, S ′ = lim Si =
i →∞

wA, B ⋅ S ′ || wA, B ⋅ S ′ || F

(7)

Rearranging, ( wA, B − || wA, B ⋅ S ′ || F ⋅I ) S ′ = 0, where I is the l×l identity matrix. Note, this is the characteristic equation for wA,B. Then, wA,B has a dominant eigenvalue ′ 1 =|| wA, B ⋅ S || F and dominating eigenvector S′. The rate of convergence is determined by the ratio
2 1

(Golub & Van Loan, 1996).

If sim returns the scalar product of its inputs and t=0, then wA, B ((u , v), (u ′, v′)) = w(u , u ′) w(v, v′) and wA, B = wA ⊗ wB, as defined above. If w A,B corresponds to the tensor product, further improvements in the time and space complexity of the algorithm are possible. Accordingly, we have FastOLGA algorithm in Figure 1. It is known (West, 1996) that the set of eigenvalues of the tensor product of two matrices is the set of values in the tensor product of the eigenvalues of these matrices, that is:

Figure 1. Matching two graphs
ProductGraph( G, G A , GB ): 1. ∀(u , v) ∈ (VA × VB ) create vertex (u , v) 2. ∀(u , u ′) ∈ (VA × VA ) 3. ∀(v, v′) ∈ (VB × VB ) 4. add edge ((u , v), (u ′, v′)) using Eq. 6 OLGA( G A , GB , , k ): 5. ProductGraph( G, G A , GB ) 6. S 0 = ones (| VA |, | VB |) 7. for i=1:k wA, B ⋅ Si −1 Si = 8. || wA, B ⋅ Si −1 || F 9. if || Si − Si −1 || F < 10. return Match( S k ) FastOLGA( G A , GB ): 11. Find U A,1 , A,1 , A,2 12. Find U B ,1 , 13. if 14. 15.
/ A,1 =
B ,1

break

,

B ,2 B ,1

A,2

and

/ =

B ,2

S = U A,1 ⊗ U B ,1

return Match( S )

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 67

wA, B = wA ⊗ wB ⇒ 1 ≤ i, j ≤| VA |, | VB |,

wA

,i

wB

,j

is an eigenvalue of wA,B. Hence, the dominant eigenvalue of the tensor product of wA,B (if it exists) is the product of the dominant eigenvalues of the wA and wB. This implies that convergence is achieved if both wA and wB have dominant eigenvalues (line 13). Similarly, the set of eigenvectors of the tensor product of two matrices is the set of values in the tensor product of the eigenvectors of these matrices. This implies that S ′ = U A,1 ⊗ U B ,1, using notation from above for dominant eigenvectors. Finding a maximal matching in the tensor product of the dominant eigenvectors corresponds to projecting the longer eigenvector onto the space of the smaller eigenvector and permuting the dimensions of the former, such that their cosine similarity is maximized (i.e., aligning them). The dominant eigenvector of an n×n matrix can be determined in O(n2) time (lines 11,12) using QR factorization (Golub & Van Loan, 1996), and the tensor product of |VA| and |VB| length vectors is computed in |VA|•|VB| steps (line 14). This allows computation of S′ in O(max(| VA |2 , | VB |2 )) time (i.e., faster than the Blondel algorithm).

EigenMatch
The main result of the OLGA algorithm is that it approximately reduces graph matching to the problem of aligning the dominant eigenvectors of the two graphs to be matched. This raises the question: why not try to align more than just the dominant eigenvectors? Accordingly, we analyze the optimization function err in 2. As Tr[ wwT ] =|| w || 2 , F

min || w
P

A

− PwB P T || 2 F
A

= =

min Tr[(w
P P

− PwB P T )( wA − PwB P T )T ]
T A T + PwB P T PwB P T

min Tr[w w
A P

− wA PwB P T − PwB P T wA ] = || wA || 2 + min || PwB P T || 2 F F − Tr[ wA PwB P T + PwB P T wA ]

As the trace of the product of two square matrices is independent of the order of multiplication, Tr[ wA ( PwB P T )] = Tr[( PwB P T ) wA ]. Also, || wA || 2 , || PwB P T || 2 are terms F F related to the magnitude of the matched subgraphs, while the latter two terms pertain
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

68 Sequeira & Zaki
T to the structure of the matching. Hence the problem reduces to max P Tr[ wA PwB P ]. If wA,wB are normal matrices, then using eigen decomposition,

max Tr[w Pw P
P A B

T

]

= = = =

max Tr[U
P P

A

T T DAU A PU B DBU B P T ] T A T PU B DBU B P T )U A ]

max Tr[(D U
A

max Tr[ D
P W

A

T T (U A PU B ) DB (U B P T U A )] T T ] where W = U A PU B

max Tr[ D WD W
A B

Blondel et al. (2004) use normalized even iterates of S k +1 = wA S k wB to find similarities between normal matrices wA,wB. We adopt this idea, so that Wk +1 = DAWk DB. We drop the normalization as it is a constant for a single iteration. However, instead of an iterative algorithm, we choose a good seed and utilize just one iteratio'—W1 = DAW0 DB . For the seed, we use the FastOLGA algorithm (line 2), which aligns the dominant T eigenvectors. Substituting in W, we get DAW0 DB = U A PU B. Rearranging, we get:
T P = U A DAW0 DBU B , where W0 = FastOLGA ( wA , wB )

(8)

U X DX = [U X ,1 X ,1 U X ,2 X ,2 ]. Thus, each eigenvector of w and w will then be A B weighted by its eigenvalue. Then during rounding of P, the matching algorithm will be fully cognizant of the smaller eigenvalues as well.

Accordingly, we have the algorithm EigenMatch as shown in Figure 2. This algorithm has the same time complexity as eigen decompositionthat is, O(n3) (Golub & Van Loan, 1996).

Figure 2. Matching all eigenvectors
EigenMatch ( G A , GB ):
T T 1. wA = U A DAU A , wB = U B DBU B 2. W0 = FastOLGA( wA , wB )
T 3. P = U A DAW0 DBU B 4. return Match( P )

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 69

Matching Using MonteCarlo Sampling
One way of estimating the unusualness of matchings produced by our algorithms involves generating random matchings and comparing the err value of the best of these, with that produced by our algorithms. Accordingly, if |VA|>=|VB|, we generate a random permutation of the numbers [1,|VA|] and map the first |VA| numbers of this permutation to the vertices numbered [1,|VB|] of GB. Otherwise, we swap the graphs and get the mapping in the same way. We call this MonteCarlo sampling. We repeat this sampling a number of times, evaluate them using the Zscore described further on, and keep the one with the best Zscore. The number of such samples generated is controlled by the time taken to run OLGA. This ensures that OLGA and MonteCarlo sampling have the same amount of time to find the matching.

Experiments
In evaluating the performance of the algorithms, we pay attention to the following measures: • • Execution time Number of matches (#(matches)): It is the number of DB’s matchable components that are correctly matched. A component in DB is matchable if there exists a known, unusually similar component in DA.

•

Zscore: We estimate the distribution of err(f |wA,wB) (Equation 2) by generating a number of matchings using MonteCarlo sampling and computing the err. Using this distribution, the mean and standard deviation can be determined, and the scores corresponding to the mapping found by an algorithm are normalized to get the Zscore. Thus, the Zscore is the number of standard deviations from the mean. Very negative Zscore implies that the corresponding matching is very unlikely to have happened by MonteCarlo sampling, and such a matching is said to have found unusually similar substructure.

Experiments on OLGA, Blondel’s algorithm, and MonteCarlo sampling were carried out on a SUN Sparc 650 MHz machine running on Solaris O/S with 256 MB RAM in C++. Blondel’s algorithm, EigenMatch, FastOLGA, and MonteCarlo sampling were also implemented on a Pentium 2 GHz machine running on Windows XP with 256MB RAM in Matlab.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

70 Sequeira & Zaki

Synthetic Datasets
We use synthetic datasets to test the performance of our algorithms and similarity measures, as dataset and algorithm parameters are varied. By generating the datasets ourselves, we can verify the correctness. Our program for generating synthetic datasets is based on that previously described in Sequeira and Zaki (2004). It has the following set of parameters: 1. 2. 3. 4. 5. 6. 7. Average number of dimensions (d) Average number of points in a dataset (n) Average number of embedded subspaces (k) Average probability that a subspace is constrained in a dimension (c) Average probability that a subspace is constrained in the same dimension as the previous subspace (o) Amount of perturbation (p) Type of transformation

First, 1.5k subspaces are generated one after the other. They are by default multivariate normal, with means in each dimension ( j ∈ [1, d ]), chosen from U[0,1000), where U[l,h) implies a uniform distribution over the interval [l,h). The standard deviation in each dimension ( j ∈ [1, d ]) is by default set to 20. A dimension is constrained with probability c. Two serially generated subspaces are constrained in the same dimension with probability o. Their means are constrained to be within 2 standard deviations of each other, to allow overlapping of subspaces. Unconstrained dimensions have means chosen from U[0,1000). For i ∈ {1,2}, for dataset Di, ni, ki are chosen uniformly from U(5n,1.5n) and U(5k,1.5k) respectively. The first ki subspaces are embedded in Di after perturbing their parameters using a transformation. There are three types of transformations: • • • Noisy:∀j ∈ [1, d ], ( j ) = ( j ) + U (− p, p ) *1000 ( j ) = ( j ) * (1 + U (− p, p )) Translation: ( j ) = ( j ) + i * p *1000 Scaling: ( j ) = ( j ) * (1 + ip/5)

where p is the perturbation parameter. Each embedded subspace accounts for at least 1% of the total number of points. The actual number of points corresponding to a subspace is a function of the imbalance factor:
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 71

a, a =

max min

l

l l

l

where l is the fraction of Di generated using parameters of the lth subspace embedded in Di. Noisy points, which account for 5% of the points in Di, are multivariate uniformthat is, each coordinate is chosen from U[0,1000). In experiments shown below, we assume that the subspace mining algorithm finds the embedded subspaces correctly, so as to isolate the contributions of this chapter. Thus, we test only the graph creation and matching algorithms described in this chapter. We tested the algorithms by matching synthetic datasets having embedded subspaces. As we serially insert subspaces, for every pair of datasets, we ensure that the dataset with the larger number of embedded subspaces includes all subspaces embedded in the other dataset. The datasets have, on average, n=1000 points and d=50 dimensions and k =25 embedded subspaces, except those with k >40 subspaces, which have n=10000 points. Unless otherwise stated,c=o=0.5, p=0.03, a=4.0, we use the noisy transformation and Gaussian weighted sim()function. By default, we try to map a 27-vertex graph to a 34-vertex one using OLGA and the HIST similarity measure. For OLGA, we set t=.925, k=30. We evaluate the algorithms based on the #(matches) and its Zscore, as some parameter in the dataset is varied or the subspace similarity function is varied. Comparison of Similarity Functions We first tested the similarity functions by attempting to match each graph to itself. As expected, we found that while the linear algebra-based algorithms succeed in doing so, the MonteCarlo sampling often does not. We have not shown these results, due to space constraints. In Figures 3, 5, and 7 we compare OLGA’s performance in terms of #(matches) as some parameter, namely, p,o and c, used in generating the embedded subspaces is varied. Note that #(matches) is virtually the same for both measures, except parameter p, where HIST performs better at p>0.05. It also outperforms AVGSIM in terms of Zscore as seen in Figures 4, 6, and 8. This suggests that HIST favors a more global solution as compared to AVGSIM. This occurs because it develops a profile for the entire subspace, including dimensions for which the subspace is not constrained, whereas AVGSIM takes more of a discretized approach. Also, there exist some values of these parameters, for which HIST’s Zscore drops below that of the optimal matching, in spite of having a significantly lower #(matches). This happens for extreme settings of the parameters. It suggests that Zscore and hence err quality measures are best suited to matching datasets having a low amount of perturbation.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

72 Sequeira & Zaki

Figure 3. #(matches) v/s p

In Figures 9 and 10, we compare the effect that different transformations on the dataset have on similarity measures. We notice that AVGSIM is consistently outperformed by HIST in the Zscore category, emphasizing the robustness of the latter. In terms of #(matches), HIST is outperformed for the noisy transformation because again it tries to optimize globally. Thus, in general HIST outperforms AVGSIM.

Comparison of Support-Based Similarity Functions
Early in this chapter, we discussed three functions used to compare clusterings. We then showed how to use them to find support-based subspace similarity. From Figure 11, Jaccard Index outperforms Rand Index and VI Metric.

Comparison of Mapping Algorithms
Firstly, we found experimentally that FastOLGA and Blondel’s algorithm always arrive at the identical matching, suggesting that the similarity transformation found by Blondel’s algorithm is basically the tensor product of the dominant eigenvectors. Note however that our algorithm is theoretically faster than Blondel’s. In view of this result, we show their results combined except for the timing results. In Figures 12, 13, and 14, we compare the performance of OLGA and EigenMatch with that of Blondel’s algorithm (Blondel et al., 2004) and the best matching produced in terms of Zscore by MonteCarlo sampling. Note that for Figure 12, the log scale is used for the y-axis. Although OLGA is the most consistent performer, the best matching produced by MonteCarlo sampling (denoted in the figures as ``best MonCopyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 73

MonteCarlo”) performs well for matching small graphs, as it has a smaller state space to search. In Figure 13, EigenMatch outperforms the others in minimizing the Zscore more often than not. However, EigenMatch is unreliable in terms of #(matches) it produces. It sometimes produces no matches, while for k =75, it perfectly matches 13 vertices. This is because it attempts global optimization in trying to align all the eigenvectors. OLGA, by virtue of using the sim function, prunes the graph and hence tries to find unusually similar matches. Hence, it typically outperforms Blondel’s algorithm.Also note that while Blondel’s algorithm converges faster than the other algorithms, it is provably slower than FastOLGA and produces the same results. We have verified this using our Matlab simulation, but have not shown it in the graph, as efficient simulation of OLGA in Matlab is non-trivial. All the algorithms have running time independent of n and d. Hence, results for these are not shown.

Applications
As stated earlier in the chapter, our framework may be applied to monitoring evolution of datasets over time. Rather than use dataset snapshots, we use the statistics of players from the NBA, averaged annually from two consecutive basketball seasons, namely, 2003-04 (dataset A) and 2004-05 (dataset B). They are accessible at http://sports.yahoo.com/nba/stats/. Another possible application of our framework is the mining of related but schematically differing datasets. We use two datasets pertaining to breast cancer (Beyer et al., 1999) donated to the UCI ML repository at ftp://ftp.ics.uci.edu/pub/machine-learning-databases/breast-cancer-wisconsin, obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. Finally, we apply our methodology on time series microarray datasets from the cell cycle of S. Cerevisiae.

NBA Data Analysis
Datasets A and B contain statistics for 443 and 464 players respectively. Each dataset has 16 columns:number of games played, average minutes played per game, average field goals, 3-pointers and free throws made and attempted, offensive and defensive rebounds, assists, turnovers, steals, blocks, personal fouls, and points per game. In this application, we seek to find groups of players having similar performance across the two seasons. If models of performance for the two seasons yield structurally similar clusters which overlap in their members, then these overlapping members are likely to have very similar performance. Consider the following scenario in which such knowledge may be employed: let players, say E, F, and G, currently belonging to distinct teams P, Q, and R respectively, all wish to leave their current team. If by
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

74 Sequeira & Zaki

Figure 4. Zscore v/s p

Figure 5. #(matches) v/s o

Figure 6. Zscore v/s o

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 75

Figure 7. #(matches) v/s c

Figure 8. Zscore v/s c

Figure 9. #(matches) v/s transformation

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

76 Sequeira & Zaki

Figure 10. Zscore v/s transformation

Figure 11. Clustering Comparison Functions

Figure 12. #(matches) v/s k

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 77

Figure 13. Zscore v/s k

Figure 14. Time v/s k

our clustering models for the two seasons, it is known that players E and F belong to structurally similar clusters, then they show similar performance across the two seasons, prompting management at P and Q to consider “exchanging” them. The statistics of each year yield a set of clusters/subspaces, 22 for dataset A and 15 for dataset B, for which we construct a graph using methods described above. We then structurally matched the two graphs using OLGA. For each pair of matched clusters, we report the intersection set of players. We found clusters as shown in Figure 4, preserved structurally, with respect to the rest of the dataset, across the two years. In basketball, there are primarily three positions at which the players
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

78 Sequeira & Zaki

play: ‘center’, ‘forward’, and ‘guard’. Within these three positions there are further variants, like ‘power forward’, ‘point guard’, and so on. The position at which the NBA players played (i.e., player position) is not a column in our datasets. Examination of the cluster members revealed that the clusters primarily had members having the same player position. For example, in the first cluster, out of six members, fourCurtis Borchardt, Ervin Johnson, Kendrick Perkins, and Stanislav Medvedenkoall play as ‘centers’. Across both datasets, the probabilities of a randomly chosen player being either ‘center’, ‘forward’, or ‘guard’ are approximately the same and are given as p(‘center’)=0.25, p(‘forward’)=0.42, p(‘guard’)=0.33. If the six players were drawn independently with replacement from this distribution, the probability that k of them are ‘centers’ is binomially distributed with parameters n=6 and p=0.25. Accordingly, the p-value of this cluster is bounded by the area of the tail of this distribution, to the right of k=4. Thus, p-value= ∑n  n  p k (1 − p) n−k = ∑5  5 (0.25) k (0.75) 5−k = 0.0562, which may be        
k =4

considered to be statistically significant. As player position was not a part of the dataset, this analysis has provided us with a new insight. Also, it was found that all the players in the clusters do not start the game and come off the bench. As the players in the same cluster, as found by our algorithm, are typically in the same position, exchanging them seems very reasonable. The cluster from dataset A corresponding to the first cluster in Table 4 has 50 players, of which 11, 25, and 14 are ‘centers’, ‘forwards’, and ‘guards’, respectively. These players are alike in that they belong in the lower third in terms of attempts at field goals, 3-pointers, and free throws; middle third for field goals made; and upper third for number of games played. Such a cluster has high entropy with respect to the player position. None of these categories singly yield statistically significant p-values. The same is true for the corresponding cluster from dataset B as well. The corresponding cluster in B has players belonging to the lower third in terms of attempts at field goals, 3-pointers, and free throws; number of field goals and free throws made; number of blocks and personal fouls; and average minutes per game. The six players reported in the table. Thus, structural alignment of the models for the datasets produces higher-entropy clusters with respect to those of the original models, with respect to the hidden variable (i.e., player position).

k 

k =4

k 

Breast Cancer Data Analysis
The first dataset (X) has nine dimensions/columns having integral values between 1 and 10 for clump thickness, uniformity of cell shape and size, marginal adhesion, bare nuclei, and so forth, and 699 samples/rows. There are a few missing values as well. Thirty-five percent of the samples are malignant (M) and the rest are benign.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 79

Table 4. Structurally similar clusters from two NBA seasons
Common Cluster Members Curtis Borchardt, Ervin Johnson, KendrickPerkins, Stanislav Medvedenko, Walter McCarty, Lonny Baxter Calbert Cheaney, Howard Eisley, Kendall Gill, Anfernee Hardaway, Jumaine Jones, Mickael Pietrus, James Posey, Kareem Rush, Theron Smith Jeff Foster, Mark Madsen, Jamal Sampson Brevin Knight, Tyronn Lee, Jeff McInnis, Latrell Sprewell, Maurice Williams 3/3 are ‘centers’ 4/5 are ‘guards’ 0.0156 0.0436 7/9 are ‘forwards’ 0.062 Characteristic 4/6 are ‘centers’ p-value 0.0562

The second dataset (Y) has 30 dimensions/columns corresponding to three statistics (mean, standard error, max) for each of 10 real-valued features (radius, symmetry, area, texture, etc.) of the cell nuclei drawn from 569 samples/rows. Thus, the schema for X and Y is different. In Y, 37.25% are malignant and the rest are benign. Each sample in both X and Y is labeled as either malignant or benign. Our goal is to discover these labels using unsupervised, rather than supervised learning techniques. Using SCHISM (Sequeira & Zaki, 2004), we find 36 clusters in X and 21 in Y. After creating the graphs and matching the clusters structurally, we examine the labels of the samples in matched clusters. Let p ( M , u ∈ VX ), p ( M , v ∈ VY ) denote the probability that a sample drawn uniformly at random from clusters u,v respectively, from graphs corresponding to datasets X,Y respectively, is labeled malignant. Then if our framework finds that Pf(u,v)=1, from Equation 1that is, the cluster u of X is matched to cluster j of Y we found that p(M,u)≈ p(M,v) (i.e., we found a strong correlation between labels of elements of matched clusters). In Table 5, we report the probabilities of p(M,u∈VX) and p(M,v∈VY) ∀Pf(u,v)=1 and p(M,u)≠p(M,v). The first column, interpreted as cluster 0 of the second dataset (Y), has all its elements labeled as malignant, while cluster 31 of the first dataset (X) has three of its five elements (i.e., 3/5=0.6) labeled as malignant. Such findings allow us to search for correlations between the two spaces corresponding to X and Y. Although, the clusters found in both datasets are predominantly malignant, our algorithm correctly matches the benign onesthat is, cluster 2 of Y with cluster 23 of X, and the higher entropy clusters 16 of Y with 25 of X. A few of the clusters matched do not have a high correlation, as we forcibly attempt to match every cluster in Y to some cluster in X. Blondel’s algorithm produces a worse mapping, in that it matches a cluster of

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

80 Sequeira & Zaki

Table 5. Structurally similar clusters from schematically different breast cancer da/ tasets and p( M , u ) = p( M , v). Here v ∈ VY , u ∈ VX , and 2 = (( p ( M , v) − p ( M , u )) 2.
( v,u) p(M,v) p(M,u) d2 (0,31) 1.0 0.6 0.16 (2,23) 0.021 0.027 0.0000036 (4,13) 0.933 1.0 0.0044 (8,3) 0.5 1.0 0.25 (11,24) 0.97 1.0 0.0009 (14,34) 1.0 0.56 0.193 (16,25) 0.833 0.77 0.0044 (17,28) 0.833 1.0 0.027 (19,35) 1.0 0.28 0.50 (20,2) 0.8 1.0 0.04

malignant samples with a cluster of predominantly benign samples. We compare the results from the algorithms by measuring the correlation between the matched clusters using:

corr ( f ) =

v∈VY

∑ exp(−( p(M , v) − p(M , f (v))) )
2 v∈VY

∑ exp(−1)

Accordingly, we find corr ( f OLGA ) = 2.586 and corr ( f BLONDEL ) = 2.0636, where f OLGA and f BLONDEL are the mappings produced by OLGA and Blondel’s algorithm, respectively. Thus, OLGA outperforms Blondel’s algorithm for the breast cancer dataset.

Microarray Data
With a large number of noisy, high-dimensional gene expression datasets becoming available, there is a growing need to integrate information from heterogeneous sources. For example, different clustering algorithms, designed to serve the same purpose, may be run on a dataset, and we may wish to integrate output from the two algorithms. Alternatively, the same algorithm may be run on two datasets differing only slightly in experimental conditions. In the first example, the algorithm provides heterogeneity, while in the latter, it is the experimental conditions. In our specific application, we look at three microarray datasets, called GDS38, GDS39, and GDS124, pertaining to the Saccharomyces cerevisiae (yeast) cell cycle (to access the datasets, visit http://www.ncbi.nlm.nih.gov/projects/geo/gds/gds_browse. cgi). The datasets contain expression values of the different genes of yeast sampled over its cell cycle. The cultures are synchronized by different mechanisms, namely, alpha factor block-release(A), centrifugal elutriation (E), and cdc15 block release
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 81

(C). GDS38 (i.e., A) has 16 samples/columns taken at seven-minute intervals, while GDS39 (i.e., E) has 14 samples/columns taken at 30-minute intervals, and GDS124 (i.e., C) has 25 samples/columns taken from almost three full cell cycles. Datasets A and E have 7,680 genes/rows, while C has 8,832 rows. The entry in the ith row and jth column of the dataset corresponds to the gene expression value for the jth time sample of gene i during the cell cycle of yeast. Microarray datasets are known to be very noisy. Also, these datasets have a large number of missing values as well. It is hypothesized that genes which exhibit similar expression patterns may be coregulatedthat is, having similar regulation mechanisms. Hence, we are looking for subspaces having similar expression patterns. We use SCHISM (Sequeira & Zaki, 2004) to find these subspaces. We use x=3. This discretizes gene expression values into three categories: under-expressed (first interval), normal (second interval), and over-expressed (third interval). Thus, the subspaces correspond to a subset of the genes/rows which are simultaneously either under-expressed or normal or overexpressed for some subset of the time samples/columns. SCHISM returns 13 and 25 subspaces for datasets GDS38 and GDS39 respectively. We then construct the graphs for each dataset and match the underlying subspaces/vertices using OLGA. We examined the genes in the intersection of the matched subspaces to verify the efficacy of our algorithms. We submitted the list of genes in the intersection of the matched subspaces to the SGD Gene Ontology (GO) Term Finder (for details, see http://db.yeastgenome.org/cgi-bin/GO/goTermFinder) tool. This tool searches for significant shared GO terms, or parents of the GO terms, used to describe the genes in the submitted list of genes to help discover what the genes may have in common. A small sample of their results is shown in Table 6. The first row of Table 6 is interpreted as follows: Genes SUM1 and BRE1 are associated with the process of chromatin silencing at telomere. These genes actually belong to a cluster of seven genes, but out of 7,274 genes in yeast, there are 42 involved in this process. Using the right tail of the binomial distribution, GO TermFinder reports the p-value (measure of statistical significance) as 0.00068. Further, they are also associated with gene silencing, and the p-value is 0.00215. SUM1 and BRE1 belong to a subspace of 193 genes when SCHISM is applied to dataset GDS38. This results in a much lower p-value and is hence not reported as statistically significant. This is true for other clusters reported too. Thus, the condensed model technique yields smaller, more statistically interesting clusters, by leveraging information from multiple sources.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

82 Sequeira & Zaki

Table 6. GO-based interpretation of similar substructure
Gene Ontology(GO) Term chromatin silencing at telomere telomeric heterochromatin formation gene, chromatin silencing regulation of metabolism organelle organization and biogenesis ribosome biogenesis ribosome biogenesis and assembly dicarboxylic acid transporter activity recombinase activity cytoskeletal protein binding transcription cofactor activity signal transduction DNA replication DNA repair response to DNA damage stimulus response to endogenous stimulus growth p-value 0.00068 0.00068 0.00215 0.00561 0.00918 4.13e-05 0.0001 0.00059 0.00059 0.00251 0.00885 0.01704 0.00194 0.00402 0.00551 0.00551 0.0078 TEC1, BEM2 BRE1, ADR1, SUM1 BRE1, ADR1, SUM1, SPC110 MAK16, SPB4, CGR1 ... ... TSR2, RLP7, NOP4 SFC1, DIC1 KEM1, RAD52 NUM1, BNR1, ASE1, MLC2 SPT8, SWI6, ARG81 COS111, BEM2 ECO1, DPB2 Genes SUM1, BRE1

Conclusion
From the Zscore values obtained by the algorithms, it is obvious that the algorithms find unusually similar matchings with respect to MonteCarlo sampling. The p-values of the inferences from the application to the NBA datasets confirm this. It is evident that OLGA and EigenMatch succeed in finding similar subspaces based on the structure of the dataset alone, without sharing the datasets. The experiments on the breast cancer data suggest that correlations between clusters in related datasets of differing schema may also be inferred, using our framework. As part of future work, we hope to extend our algorithms to finding common substructure across multiple datasets. Also, currently our similarity measures are best suited to finding similarities between hyperrectangular subspaces. Patterns in datasets may require less restrictive descriptions, for example, coherent patterns in NBA datasets, curves, and so forth. We hope to develop similarity measures for such patterns as well.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 83

Acknowledgments
This work was supported in part by NSF CAREER Award IIS-0092978, DOE Career Award DE-FG02-02ER25538, and NSF grants EIA-0103708 and EMT-0432098. Karlton Sequeira is now at Amazon.com, but the work was done while he was at RPI.

References
Agrawal, R., Gehrke, J., Gunopulos, D., & Raghavan, P. (1998). Automatic subspace clustering of high dimensional data for data mining applications. In Proceedings of the ACM SIGMOD Conference on Management of Data. Bay, S., & Pazzani, M. (2001). Detecting group differences: Mining contrast sets. Data Mining and Knowledge Discovery, 5(3), 213-246. Bennett, K.P., & Mangasarian, O.L. (1992). Robust linear programming discrimination of two linearly inseparable sets. Optimization Methods and Software, 1, 23-34. Beyer, K., Goldstein, J., Ramakrishnan, R., & Shaft, U. (1999). When is nearest neighbors meaningful? In Proceedings of the International Conference on Database Theory. Blondel, V., Gajardo, A., Heymans, M., Senellart, P., & Van Dooren, P. (2004). A measure of similarity between graph vertices: Applications to synonym extraction and Web searching. SIAM Review, 46(4), 647-666. Bunke, H. (1999). Error correcting graph matching: On the influence of the underlying cost function. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(9), 917-922. Carcassoni, M., & Hancock, E. (2002). Alignment using spectral clusters. In Proceedings of the British Machine Vision Conference. Ganti, V., Gehrke, J., Ramakrishnan, R., & Loh, W. (1999). A framework for measuring changes in data characteristics. In Proceedings of the ACM Symposium on Principles of Database Systems. Gionis, A., Mannila, H., & Tsaparas, P. (2005). Clustering aggregation. In Proceedings of the IEEE International Conference on Data Engineering. Nagesh, H., Goil, S., & Choudhary, A. (2001). Adaptive grids for clustering massive data sets. In Proceedings of the SIAM Data Mining Conference. Golub, G., & Van Loan, C. (1996). Matrix computations (3rd ed.). Baltimore: Johns Hopkins University Press.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

84 Sequeira & Zaki

Han, J., & Kamber, M. (2001). Data mining: Concepts and techniques. San Francisco: Morgan Kaufmann. Kalviainen, H., & Oja, E. (1990). Comparisons of attributed graph matching algorithms for computer vision. In Proceedings of the Finnish Artificial Intelligence Symposium. Kuhn, H. (1955). The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2, 83-97. Li, T., Ogihara, M., & Zhu, S. (2002). Similarity testing between heterogeneous datasets. Technical Report UR-CS-TR781, Computer Science Department, University of Rochester, USA. Meila, M. (2003). Comparing clusterings by the variation of information. In Proceedings of the International Conference on Learning Theory. Melnik, S., Garcia-Molina, H., & Rahm, E. (2002). Similarity flooding: A versatile graph-matching algorithm. In Proceedings of the IEEE International Conference on Data Engineering. Neuwald, A., Liu, J., & Lawrence, C. (1995). Gibbs motif sampling: Detection of bacterial outer membrane repeats. Protein Science, 4, 1618-1632. Sequeira, K., & Zaki, M. (2004). SCHISM: A new approach to interesting subspace mining. In Proceedings of the IEEE International Conference on Data Mining. Shapiro, L., & Haralick, M. (1985). A metric for comparing relational descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 7(1), 9094. Van Wyk, B., & Van Wyk, M. (2003). Orthonormal Kronecker product graph matching. Lecture Notes in Computer Science, 2726, 107-117. Berlin: Springer-Verlag. West, D. (1996). Introduction to graph theory. Englewood Cliffs, NJ: PrenticeHall. Wolberg, W.H., Street, W.N., Heisey, D.M., & Mangasarian, O.L. (1995). Computerderived nuclear features distinguish malignant from benign breast cytology. Human Pathology, 26, 792-796.

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Exploring Similarities Across High-Dimensional Datasets 85

Section II Patterns

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Ntouts, Peleks, & Theodords

Chapter.IV

Pattern.Comparison.in. Data.Mining:
A.Survey
Irene Ntouts, Unversty of Praeus, Greece Nkos Peleks, Unversty of Praeus, Greece Yanns Theodords, Unversty of Praeus, Greece

Abstract
Many patterns are available nowadays due to the widespread use of knowledge discovery in databases (KDD), as a result of the overwhelming amount of data. This “flood” of patterns imposes new challenges regarding their management. Pattern comparison, which aims at evaluating how close to each other two patterns are, is one of these challenges resulting in a variety of applications. In this chapter we investigate issues regarding the pattern comparison problem and present an overview of the work performed so far in this domain. Due to heterogeneity of data mining patterns, we focus on the most popular pattern types, namely frequent itemsets and association rules, clusters and clusterings, and decision trees.
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Pattern Comparson n Data Mnng: A Survey 

Introduction
Nowadays a large quantity of raw data is collected from different application domains (business, science, telecommunication, health care systems, etc.). According to Lyman and Varian (2003), “The world produces between 1 and 2 exabytes of unique information per year, which is roughly 250 megabytes for every man, woman, and child on earth”. Due to their quantity and complexity, it is impossible for humans to thoroughly investigate these data collections directly. Knowledge discovery in databases (KDD) and data mining (DM) provide a solution to this problem by generating compact and rich semantics representations of raw data, called patterns (Rizzi et al, 2003). With roots in machine learning, statistics, and pattern recognition, KKD aims at extracting valid, novel, potentially useful, and ultimately understandable patterns from data (Fayyad, Piatetsky-Shapiro, & Smyth, 1996). Several pattern types exist in the literature mainly due to the wide heterogeneity of data and the different techniques for pattern extraction as a result of the different goals that a mining process tries to achieve (i.e., what data characteristics the mining process highlights). Frequent itemsets (and their extension, association rules), clusters (and their grouping, clusterings), and decision trees are among the most well-known pattern types in data mining. Due to the current spreading of DM technology, even the amount of patterns extracted from heterogeneous data sources is large and hard to be managed by humans. Of course, patterns do not raise from the DM field only; signal processing, information retrieval, and mathematics are among the fields that also “yield” patterns. The new reality imposes new challenges and requirements regarding the management of patterns in correspondence to the management of traditional raw data. These requirements have been recognized by both the academic and the industrial parts that try to deal with the problem of efficient and effective pattern management (Catania & Maddalena, 2006), including, among others, modeling, querying, indexing, and visualization issues. Among the several interesting operations on patterns, one of the most important is that of comparisonthat is, evaluating how similar two patterns are. As an application example, consider a supermarket that is interested in discovering changes in its customers’ behavior over the last two months. For the supermarket owner, it is probably more important to discover what has changed over time in its customers’ behavior rather than to preview some more association rules on this topic. This is the case in general: the more familiar an expert becomes with data mining, the more interesting it becomes for her to discover changes rather than already-known patterns. A similar example also stands in the case of a distributed data mining environment, where one might be interested in discovering what differentiates the distributed branches with respect to each other or, in grouping together branches of similar patterns. From the latter, another application of similarity arises, that of exploiting similarity between patterns for meta-pattern managementthat is,
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

 Ntouts, Peleks, & Theodords

applying data mining techniques over patterns instead of raw data (e.g., Xin, Han, Yan, & Cheng, 2005). So far, the importance of defining similarity operators between patterns has been justified. However, this definition is not so straightforward. At first, there are a lot of different pattern types like association rules, frequent itemsets, decision trees, clusters, and so forth; so similarity operators should be defined for each pattern type. Secondly, except for patterns of the same pattern type, an interesting extension would be the comparison between patterns of different pattern types, for example, a cluster with a decision tree (extracted from the same raw data set). Furthermore, an important aspect is that of examining whether similarity between patterns reflects in some degree the similarity between the original raw data. From an efficiency point of view, this is desirable, since the pattern space is usually of lower size and complexity. In the next sections we overview the work performed so far in the area of data mining patterns comparison. Due to the widespread use of the data mining pattern types, we mainly focus on three basic pattern types that have been used extensively in KDD literature, namely frequent itemsets and association rules, clusters and clusterings, and decision trees. The chapter is organized as follows. First, we present the above mentioned basic pattern types in detail. We then overview the work regarding the comparison of frequent itemsets and association rules, and focus on decision trees’ comparison. We present the related work regarding the comparison of clusters and clusterings, along with the general frameworks for the comparison/monitoring of data mining patterns that have appeared in the literature. Finally, we conclude the chapter.

Data.Mining.Patterns
According to Rizzi et al. (2003), patterns can be defined as compact and rich in semantics representations of raw datacompact by means that they summarize in some degree the amount of information contained in the original raw data, and rich in semantics by means that they reveal new knowledge hidden in the huge amount of raw data. A variety of pattern types exists in the literature due to the heterogeneity of the raw data from which patterns are extracted and the different goals that each mining task tries to accomplish. Different pattern types highlight different characteristics of the raw data; for example, frequent itemsets capture the correlations between attribute values, clusters reveal natural groups in the data, whereas decision trees detect characteristics that predict (with respect to a given class attribute) the behavior of future records (Ganti & Ramakrishnan, 2002).
Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Pattern Comparson n Data Mnng: A Survey 

Ganti, Gehrke, and Ramakrishnan (1999) introduced the 2-component property of patterns. The central idea of their work is that a broad class of pattern types (called ‘models’) can be described in terms of a structural component and of a measure component. The structural component identifies “interesting regions,” whereas the measure component summarizes the subset of the data that is mapped to each region. In other words, the structural component describes the pattern space, whereas the measure component quantifies, in some way, how well the pattern space describes the underlying raw data space. The 2-component property of patterns has been extended in Rizzi et al. (2003), where the authors introduced a general model for patterns, including also a source component that describes the data set from which patterns have been extracted and an expression component that describes the relationship between the source data space and the pattern space. We refer to the 2-component property of patterns since, as will be shown from the related work, most of the similarity measures exploit these components. In the following subsections we present three popular data mining pattern types that are relevant to this work, namely frequent itemsets (and their extensions, association rules), clusters (their groupings, clusterings), and decision trees.

Frequent.Itemsets.and.Association.Rules
Frequent itemsets and association rules mining are strongly related to each other by means that frequent itemsets mining is the first step towards association rules mining. In this section we present more detail on both of them. The Frequent Itemset Mining (FIM) problem is a core problem in many data mining tasks, although it was first introduced in the context of market basket analysis. To define the FIM problem, we will follow the work by Agrawal, Imielinski, and Swami (1993): Let I be a set of distinct items and D be a database of transactions where each transaction T contains a set of items T ⊆ I. A set X ⊆ I with |X| = k is called k-itemset or simply itemset. The frequency of X in D equals to the number of transactions in D that contain X, that is, frD(X) = |{T ∈ D: X ⊆ T}|. The percentage of transactions in D that contain X is called support of X in D, that is, suppD(X) = fro(x)/|D|. An itemset X is called frequent if its support is greater than or equal to a user-specified minimum support threshold σ called minSupport, suppD(X) ≥σ. The FIM problem is defined as finding all itemsets X in D that are frequent with respect to a given minSupport threshold σ. Let Fσ(D) be the set of frequent itemsets extracted from D under minSupport threshold σ. The set of frequent itemsets forms the itemset lattice L in which the lattice property holds: an itemset is frequent iff all of its subsets are frequent. The lattice property

Copyright © 2007, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

