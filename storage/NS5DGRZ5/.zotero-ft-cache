Journal of Vision (2007) 7(8):2, 1–9

http://journalofvision.org/7/8/2/

1

Local ﬁgure–ground cues are valid for natural images
Charless C. Fowlkes
David R. Martin
Jitendra Malik
Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA, USA
Computer Science Department, Boston College, Boston, MA, USA
Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA, USA

Figure–ground organization refers to the visual perception that a contour separating two regions belongs to one of the regions. Recent studies have found neural correlates of ﬁgure–ground assignment in V2 as early as 10–25 ms after response onset, providing strong support for the role of local bottom–up processing. How much information about ﬁgure– ground assignment is available from locally computed cues? Using a large collection of natural images, in which neighboring regions were assigned a ﬁgure–ground relation by human observers, we quantiﬁed the extent to which ﬁgural regions locally tend to be smaller, more convex, and lie below ground regions. Our results suggest that these Gestalt cues are ecologically valid, and we quantify their relative power. We have also developed a simple bottom–up computational model of ﬁgure–ground assignment that takes image contours as input. Using parameters ﬁt to natural image statistics, the model is capable of matching human-level performance when scene context limited. Keywords: ﬁgure–ground, ecological statistics, natural scenes, perceptual organization Citation: Fowlkes, C. C., Martin, D. R., & Malik, J. (2007). Local ﬁgure–ground cues are valid for natural images. Journal of Vision, 7(8):2, 1–9, http://journalofvision.org/7/8/2/, doi:10.1167/7.8.2.

Introduction
In the 1920s, the Gestalt psychologists identiﬁed grouping and ﬁgure–ground as two major principles underlying the process of perceptual organization. Grouping describes the way that individual elements of a stimulus come together to form a perceptual whole. Figure–ground refers to the perception that a contour separating two regions “belongs” to one of the two regions. The ﬁgural region takes on shape imparted by the separating contour and appears closer to the viewer, whereas the ground region is seen as extending behind the ﬁgure. Both grouping and ﬁgure–ground are thought to be important in reducing the visual complexity of a scene to that of processing a small number of cohesive, nonaccidental units. Starting with Rubin (1921), who ﬁrst pointed out the signiﬁcance of ﬁgure–ground organization, a long list of factors that affect ﬁgure–ground assignment have been identiﬁed. These include size, surroundedness, orientation, and contrast (Rubin, 1921), as well as symmetry (Bahnsen, 1928), parallelism (Metzger, 1953), convexity (Kanizsa & Gerbino, 1976; Metzger, 1953), meaningfulness (Peterson, 1994), and lower region (Vecera, Vogel, & Woodman, 2002). How might these cues be computed in the brain? It is conceivable that cues such as orientation or contrast could be a function of information present in receptive ﬁelds
doi: 1 0. 11 67 / 7 . 8 . 2

local to a given contour, but other cues like symmetry and parallelism would seem to require long-range lateral interactions. Evidence from electrophysiology (Qiu & von der Heydt, 2005; Zhou, Friedman, & von der Heydt, 2000) suggests the existence of cells in V2 that code for contour ownership within 10–25 ms of the onset of response activity. The early availability of this signal (both in terms of time course and stage in the visual pathway) provides strong support for the role of local bottom–up cues, contrasting with the traditional Gestalt emphasis on global organization. On the other hand, studies on meaningfulness (Peterson, 1994) show that contours tend to associate with the abutting region that has a familiar shape, pointing to the integration of top–down knowledge. Although there is little doubt that both local and global information sources play a role in ﬁgure–ground processing, a key problem is understanding their relative importance. Identifying cues, even in physiological detail, does not provide an explanation as to why they exist or how conﬂicting cues might be fused to yield a cohesive percept. More than 50 years ago, Egon Brunswik (Brunswik & Kamiya, 1953) suggested a solution to these concerns, namely, that the Gestalt cues reﬂect the statistics of the natural world in which the visual system evolved. He proposed that Gestalt cues be validated by studying the statistics of natural scenes and carried this agenda out on a limited scale.
ISSN 1534-7362 * ARVO

Received November 30, 2006; published June 8, 2007

Journal of Vision (2007) 7(8):2, 1–9

Fowlkes, Martin, & Malik

2

Figure 1. Formulating local cues to ﬁgure–ground assignment. Three cues are deﬁned locally inside an analysis window centered at a contour point p. The Size cue describes the relative size of the neighboring regions. LowerRegion compares the relative locations of the center of masses of the two regions. The Convexity cue captures the relative convexity of the two neighboring regions. Convexity is deﬁned as the probability that a line segment connecting two points in a region lies completely within the region. The six panels at the right demonstrate the information captured by each cue at two different scales. The base of each colored line segment along the boundary marks the point on the contour at which the cue was computed and points towards the predicted ground region. The length of the line indicates the relative magnitude of the cue. The cues of size, lower region, and convexity are indicated with red, blue, and green, respectively.

In this article, we pursue such a strategy to understand how well local bottom–up cues predict ﬁgure–ground relations in natural scenes. We study the ecological statistics of size, convexity, and lower-region cues using a large collection of natural images for which true ﬁgure– ground relations have been assigned by human observers. Measuring the frequency with which these features make the correct prediction provides an ecological validation of the corresponding Gestalt cues. We describe a simple computational model that combines these three cues to predict ﬁgure–ground relations. We ﬁnd that the model performs as well as human subjects asked to make similar local judgments. We also highlight the importance of ﬁgure–ground information contained in the image luminance, which lies outside the scope of classical conﬁgural shape cues. Together, these results provide a ﬁrst quantitative measurement of the relative power of local and global cues in ﬁgure–ground assignment.

Methods
Formulating cues to ﬁgure–ground
We formulate the cues of size, lower region, and convexity as functions of the boundary shape between

two regions inside a local analysis window centered at a contour point p. Size(p) is deﬁned as the log ratio of the areas of the two regions. LowerRegion(p) is deﬁned as the cosine of the angle between the line connecting the center of masses of the two regions and the vertical direction given by the camera orientation. In contrast to measuring the angle of the boundary tangent at the center point, LowerRegion(p) incorporates information over the entire analysis window. The convexity of a single region is computed as the fraction of point pairs in a region for which a straight line connecting the two lies completely within the region. Convexity(p) is then given by the log ratio of the two region convexities. In natural scenes, an object may appear at any distance from the viewer. As a result, a window subtending a ﬁxed visual angle may include an entire object at a distance or cover only a small uninformative portion of a nearby object boundary. To provide an intuitive notion of context that is independent of the scale at which an object appears in a scene, we specify the analysis window radius as a percentage of the arc length of the underlying contour on which it is centered. This makes the local cues we measure approximately invariant to an object’s distance from the viewer. Figure 1 provides a graphical description of each cue computation and shows the cue response along the

Journal of Vision (2007) 7(8):2, 1–9

Fowlkes, Martin, & Malik

3

Figure 2. Acquiring ﬁgure–ground labels. Human subjects labeled each contour in an image, indicating to which region it “belongs”. Starting from a segmentation of the original image (left), subjects were presented with a sequence of highlighted contours corresponding to each pair of neighboring regions (center). The subject indicated which of the two regions was the ﬁgural element. The reported ﬁgural region is displayed here with a red tint, ground with a blue tint. Subjects also had the option of attributing a boundary to a change in surface albedo or a discontinuity in the surface normal. Such a boundary, exempliﬁed by the corner between the building and earth, marked in green, was seen as belonging to both segments. Once all the contours had been labeled, the subject was presented with the ﬁnal labeling (right) and given the opportunity to ﬁx any mistakes.

boundary of a test ﬁgure at two different scales of analysis. These local cues are not always in agreement with our global percept. Along the top of the bear’s nose, all three cues correctly predict the bear-shaped segment as ﬁgural, whereas at the bottom of the bear’s leading foot, Size and Convexity correctly indicate the foot as ﬁgural, but LowerRegion gives a contradictory response. At a small scale, Convexity suggests that the space between the bear’s legs is ﬁgural but reverses at a larger scale.

Acquiring ground-truth labels
To understand how often each cue provides the correct prediction, we compiled ground-truth ﬁgural assignments for contours in a collection of 200 images depicting a wide variety of indoor and outdoor scenes containing manmade and natural objects, including humans and other animals. These 200 images were chosen at random from the set of 1,000 hand segmented images in the Berkeley

Segmentation DataSet (Martin, Fowlkes, Tal, & Malik, 2001). A segmentation of each image was selected randomly from the set of ﬁve available color segmentations to provide ground-truth contour locations. The images and annotations are available online (http://www. cs.berkeley.edu/projects/vision/grouping/segbench). Human subjects were asked to indicate, for each pair of abutting segments in an image, which region was ﬁgural and which was ground. Figure 2 shows a typical example of the labeling process where a subject has assigned each contour in turn yielding a complete labeling of the image. Subjects also had the option of indicating that a given contour was the result of a change in albedo or surface normal and hence “belonged” to both neighboring regions. The ﬁgure–ground labeling was carried out by 10 subjects who were naive to the experimental purpose. Each of the 200 segmented images was presented at a resolution of 481 Â 321 pixels on a typical computer monitor. Subjects were not asked to label contours whose length was less than 2% of the image diagonal. There were no time constraints imposed on the labeling task.

Journal of Vision (2007) 7(8):2, 1–9

Fowlkes, Martin, & Malik

4

Each contour in the dataset was labeled by two different human observers. A consistency check of the human labels shows that observers agreed on the ﬁgure–ground/albedo labeling for 83.9% of the contour points sampled. Of the remaining 16.1%, 12.3% involved one subject marking ﬁgure–ground and the other marking albedo, whereas 3.7% had conﬂicting ﬁgure–ground assignments. For use in the following experiments, 10% of the contour points were sampled from the 400 labelings, for a total of 285,000 points. We did not utilize those 40,000 points that were labeled as lying on albedo boundaries. Points that were within two thirds of the largest analysis window radius from a junction between three or more segments or an image border were also excluded, leaving 50,000 points for analysis at all scales. On this restricted set of points, humans agreed on the labeling 96% of the time. Inconsistently labeled points were included in our analysis with the label chosen randomly, establishing an upper bound of 96% on classiﬁcation accuracy.

Results
Figure 3 shows the empirical distribution of cue responses at a single scale (r = 5% contour length) for 50,000 points sampled from the human-labeled boundaries. We plot only distributions for positive values of each cue. Because every boundary point contributes two values of equal magnitude and opposite sign, the distributions of negative values are identical with the roles of ﬁgure and ground reversed. Note that the marginal distribution of contour orientations is not uniform. The greater prevalence of horizontal (LowerRegion = 1) and vertical (LowerRegion = 0) boundaries is consistent with previous results on the statistics of brightness edges in natural images (Switkes, Mayer, & Sloan, 1978). These histograms show that ﬁgural regions in natural scenes tend to be smaller, more convex, and lie below the ground regions. For example, when the sizes of the two regions are the same, Size(p) = log(Area1/Area2) = 0, they are equally likely to be ﬁgure. When one region is larger, Size(p) 9 0, it is more common that the larger region is ground. All three cues uniformly differentiate ﬁgure and ground on average, in agreement with psychophysical demonstrations of the corresponding Gestalt cues (Kanizsa & Gerbino, 1976; Metzger, 1953; Rubin, 1921; Vecera et al., 2002). At 5% contour length, we estimate the mutual information (Cover & Thomas, 1991) between each cue and the true label to be 0.047, 0.075, and 0.018 bits for Size, LowerRegion, and Convexity, respectively. To further gauge the relative power of these three cues, we framed the problem of ﬁgure–ground assignment as a discriminative classiﬁcation task: “With what accuracy can a cue predict the correct ﬁgure–ground labeling?”

Figure 3. The statistics of local ﬁgure–ground cues in natural scenes. Each histogram shows the empirical distributions of Size(p), LowerRegion(p), and Convexity(p) for 50,000 points sampled from human-labeled contours in 200 natural images computed over a window with radius r = 5% contour length.

Journal of Vision (2007) 7(8):2, 1–9

Fowlkes, Martin, & Malik

5

Figure 4. Quantifying the relative power of local ﬁgure–ground cues in natural scenes. The power of individual cues and cue combinations is quantiﬁed by measuring the correct classiﬁcation rate, plotted here as a function of window radius. Multiple cues are combined using logistic regression ﬁt to training data. The error bars show 1 SD measured over held-out data during 10-fold cross-validation. The legend gives the highest classiﬁcation rate achieved for each combination of cues. The analysis window radius is measured relative to the length of the contour being analyzed to make it (approximately) invariant to an object’s distance from the camera.

For individual cues, it is clear from Figure 3 that the optimal strategy is to always report the smaller, more convex, or lower region as ﬁgure. To combine multiple cues, we ﬁt a logistic function, Pðfigurekcð pÞÞ ¼ 1 ; T 1 þ ej" cðpÞ ð1Þ

which takes a linear combination of the cue responses at point p, arranged into vector c(p) along with a constant offset, and applies a sigmoidal nonlinearity. The classiﬁer outputs a value in [0, 1] that is an estimate of the likelihood that a segment is ﬁgural. In the classiﬁcation setting, we declare a segment to be ﬁgure if this likelihood is greater than 0.5. The model parameters " were ﬁt using iteratively reweighted least squares to maximize the training data likelihood (Hastie, Tibshirani, & Friedman, 2001). We also considered models that attempted to

exploit nonlinear interactions between the cues, such as logistic regression with quadratic terms and nonparametric density estimation, but found no signiﬁcant gains in performance over the simple linear model. Figure 4 shows the correct classiﬁcation rate as a function of the analysis window radius for different combinations of cues. Values in the legend give the best classiﬁcation rate achieved for each combination of cues. The performance ﬁgures suggest that all three cues are predictive of ﬁgure–ground, with Size being the most powerful, followed by LowerRegion and Convexity. Combining LowerRegion and the Size cues yields better performance, indicating that independent information is available in each. The addition of Convexity when Size is already in use yields smaller performance gains because these two cues are closely related: A locally smaller region tends to be locally convex. We found that increasing context past 25% contour length did not further improve the model performance. In fact, computing the relative Size, Convexity, and LowerRegion at the level of whole segments (100% context) yielded lower correct classiﬁcation rates of 56.9%, 55.4%, and 59.5%, respectively. One explanation for the worse performance of global Size and Convexity is that natural scenes typically involve many interacting objects and surfaces. Object A may occlude object B, creating a contour whose local convexity cue is consistent with the ﬁgure–ground layout. However, the global convexity of the region composing A may well be affected by its relation to other objects C, D, E, and so forth, in a manner that is largely independent of the ﬁgure–ground relation between A and B. At the most informative window radius, our combined model achieved a 74% correct classiﬁcation rate, falling short of the human labeling consistency (96%). This is likely due to several sources of information absent from our local model that could have been exploited by human subjects viewing a whole image during labeling. First, integration of local noisy measurements along a contour should yield a consistent label for the entire contour. Our feed-forward approach does not assume that grouping of contours has taken place before ﬁgure–ground binding begins. Second, we exclude junctions from our analysis. Junctions embody important information about the depth ordering of regions; however, they are quite difﬁcult to detect locally in natural scenes (McDermott, 2004). Third, human subjects have access to important nonlocal and high-level cues such as symmetry (Bahnsen, 1928), parallelism (Metzger, 1953), and familiarity (Peterson, 1994; Rubin, 1921), which we have not considered here. Lastly, our model only utilizes the shape or conﬁguration of the abutting regions, with no regard to the luminance content associated with each one. This ignores important local photometric evidence such as terminators signaling occlusion (von der Heydt & Peterhans, 1989) and cues to three-dimensional geometry such as texture, shading, and familiarity.

Journal of Vision (2007) 7(8):2, 1–9

Fowlkes, Martin, & Malik

6

Figure 5. Subjects made ﬁgure–ground judgments for local stimuli, like those shown, consisting of a cropped disc depicting either region shape (conﬁguration) or image luminance (conﬁguration + content). In the luminance condition, the two regions on either side of the contour were distinguished by red and blue tints. The color assignments were randomized over trials, but in this ﬁgure, the white/red tinted segments indicate which region was ﬁgural according to the ground-truth labels. Numbers indicate the window radius for each patch as a percentage of the contour length.

Quantifying the role of local luminance cues
To better understand the role of these unmodeled information sources, a second group of subjects was presented with circular patches extracted from the set of labeled images and asked to indicate which of the two neighboring regions they judged to be ﬁgural. Two conditions were used: one in which subjects saw the cropped grayscale image patch and one in which subjects were presented with only the corresponding cropped segment map where each region was ﬁlled in with a constant gray level (see Figure 5). These two conditions deprived the observer of global context or both context and luminance content, respectively. Local patches were displayed through a physical aperture placed in front of a computer monitor. This was done to prevent the aperture from interfering with the perceived shape of the regions being viewed, instead giving the impression that they extended behind the aperture. The aperture size was ﬁxed with respect to the subject (subtending 7 degrees of arc) and image patches scaled to the same presentation size. The subject’s head was stabilized with a chin rest 75 cm from the monitor. Exposure times were not limited, but subjects usually spent 1–2 s per patch. Eight hundred contour points for use in the local patch display experiment were randomly sampled with the same exclusion criteria as used in collecting statistics (described above). RMS contrast for the image patches varied widely from 0.2 to 1.1, depending on the scene. The distribution of contrast over the patches in our dataset was quite consistent with that reported by Mante, Frazor, Bonin, Geisler, and Carandini (2005). Figure–ground was not signiﬁcantly correlated with brightness. The average brightness of the ground segment was greater than that of the ﬁgural segment in 51.5% of the patches sampled. Each subject labeled all 800 patches, 200 at each of four levels of context: r = 2.5%, 5%, 10%, and 20% contour length. Of the eight subjects, four were presented with

image luminance patches; and four, with segment-only displays. None of the subjects presented with luminance patches had previously seen the images used. For the segment-only display, subjects indicated which side was ﬁgural (black or white). In the luminance display, the image patch was overlaid with a red and blue tint to unambiguously specify the contour location. The gray level or tint assignment was randomized over trials. Subjects showed little bias, choosing the white region in 51.7% of the segment-only trials and choosing the blue tinted segment in 56.0% of the luminance trials. Subjects also showed no signiﬁcant bias toward the brighter segment in the luminance display, assigning it as ﬁgure in 50.5% of the trials. The resulting local classiﬁcation performance of human subjects is presented in Figure 6, along with the performance of our local conﬁgural model on this subset of patches. We found that, in combination, LowerRegion, Convexity, and Size cues approach human-level performance when only boundary shape information was available. At 20% contour length, human subjects in the conﬁguration-only condition averaged 69% correct classiﬁcation, whereas the model achieved 68%. Furthermore, labels assigned by human subjects for a given patch agreed quite closely with those of the model. On average, the model prediction matched the subject’s response, both correct and incorrect, for 79% of the 800 patches classiﬁed. For comparison, pairs of human subjects averaged 75% agreement on the patch labels in the conﬁguration-only condition. Tables 1, 2, 3, and 4 in the Appendix document the performance and level of agreement for individual subjects in both conditions. Human subjects did make good use of information contained in the image luminance that was not captured by the conﬁgural cues. At 20% of the object contour length, access to luminance content decreased the number of errors by more than a factor of 2 over the conﬁgurationonly presentation. Performance on luminance patches also

Journal of Vision (2007) 7(8):2, 1–9

Fowlkes, Martin, & Malik

7

Figure 6. Quantifying the importance of context and content. The correct classiﬁcation rate and standard deviation across subjects (n = 4 subjects in each condition) are plotted as a function of context. We also plot the classiﬁcation performance of our computational model (S, L, and C) on the same set of local windows, with whiskers marking 1 SD of the sample proportion. The grid line at 0.96 indicates the level of global labeling consistency in the ground truth ﬁgure–ground assignments.

improved signiﬁcantly (p G 2 Â 10j4 for all subjects) as the window radius increased from 5% to 20% contour length. Figure 5 shows individual image patches for which the difference in human classiﬁcation rate without and with luminance information was particularly large. For each of these patches, more than two subjects responded correctly to the luminance content presentation, but more than two responded incorrectly to the conﬁguration-only presentation. The jump in human performance when luminance content is available can be explained by additional local cues exempliﬁed in the patches shown. These include terminators created by occlusion of background texture (ﬁrst column); three-dimensional shape information available from shadows, shading, and highlights (second column); and recognition of familiar materials or objects based on texture and other internal markings (third and fourth columns, respectively).

Discussion
Taken together, our results provide a quantiﬁcation of the relative amounts of information about ﬁgure–ground

assignment provided by local boundary conﬁguration, local luminance content, and global scene context in natural scenes. In particular, our simple bottom–up model appears to sufﬁciently capture much of the ﬁgure–ground information available from local boundary shape. Surprisingly, the gap that remains between human performance on local conﬁgurations and whole scenes appears to be bridged in large part by exploiting information contained in the local image luminance content rather than global reasoning. Although restricting context prevents the utilization of global conﬁgural cues such as parallelism or symmetry, it seems evident from the patches shown in Figure 5a that “high-level” familiarity or meaningfulness can still function locally alongside generic “low-level” cues such as texture, shading, and terminators. As seen in Figure 1, local ﬁgure–ground assignments along a given contour are by no means consistent. It is interesting to consider how pooling local measurements might improve the classiﬁcation rate by propagating information outwards from zones of high certainty (e.g., Zhaoping, 2005). One difﬁculty is knowing which local estimates to pool because detecting and grouping contour elements are themselves difﬁcult tasks in natural images. A preliminary study (Ren, Fowlkes, & Malik, 2006) suggests that for those contours that can be detected

Journal of Vision (2007) 7(8):2, 1–9

Fowlkes, Martin, & Malik r 2.5% 5% 10% 20% 5 57 60 81 83 6 70 75 84 89 7 67 72 82 87

8 8 70 68 82 88

locally, pooling measurements yields small but noticeable gains in performance (approximately 5%). The study of the statistics of natural stimuli has become an increasingly prominent theme in understanding sensory information processing. For example, natural image statistics provide an elegant explanation of the localized receptive ﬁelds found in primary visual cortex in terms of optimal coding strategies (Atick & Redlich, 1992; Olshausen & Field, 1996; Ruderman, 1994). The ﬁndings described here are more closely related to a smaller body of work, starting with Brunswik and Kamiya (1953), which examines the joint statistics of ground-truth percept and scene measurements pursued in the context of grouping by similarity (Fowlkes, Martin, & Malik, 2003) and contour completion (Elder & Goldberg, 2002; Geisler, Perry, Super, & Gallogly, 2001; Ren & Malik, 2002). Figure–ground organization has a long history in the ﬁeld of psychology, where the focus has largely been on identifying which cues impact perception. Our results provide a novel perspective on these ﬁndings, offering an explanation as to why such cues exist. An organism that exploits size, lower region, or convexity as a cue to infer ﬁgure–ground would have an obvious advantage, more often correctly grasping nearby objects and navigating through gaps rather than colliding with obstacles. Visual theorists (Brunswik & Kamiya, 1953; Gibson, 1979) have sought justiﬁcation for particular cues in the physical and statistical regularities of the “external world”. With the recent availability of large collections of digitized images and the development of statistical learning techniques, such theories are now amenable to direct experimental veriﬁcation.

Table 2. Average correct classiﬁcation rate (%) on the conﬁguration + content patch display for human subjects (5–8) at each window radius. Subject 1 2 3 4 2 75 3 79 80 4 74 69 74 M 80 77 85 74

Table 3. Labeling agreement (%) between human subjects (1– 4) and the local model (M) for conﬁgural patch displays. Subject 5 6 7 8 6 74 7 72 77 8 71 78 76 M 67 65 67 65

Table 4. Labeling agreement (%) between human subjects (5–8) and the local model (M) for conﬁguration + content patch displays.

patches for which both labelers selected the same ﬁgure–ground assignment, regardless of whether or not it was correct.

Appendix A
The following tables document the agreement between the model and individual human subjects on local patches. Tables 1 and 2 show the correct classiﬁcation rates for each level of context with conﬁguration only or conﬁguration + content displays, respectively. Tables 3 and 4 document the level of agreement on labels assigned by subjects and the model. The values indicate the percentage of

Acknowledgments
The authors would like to thank Xiaofeng Ren and the reviewers for valuable comments and discussion. This research was supported by an NSF graduate research fellowship to C.F. Commercial relationships: none. Corresponding author: Charless C. Fowlkes. Email: fowlkes@cs.berkeley.edu. Address: Computer Science Division, Soda Hall, University of California at Berkeley, Berkeley, CA 94720.

r 2.5% 5% 10% 20%

1 65 64 63 72

2 65 64 64 67

3 63 67 72 68

4 60 64 71 72

M 63 67 62 68

References
Atick, J., & Redlich, A. (1992). What does the retina know about natural scenes? Neural Computation, 4, 196–210.

Table 1. Average correct classiﬁcation rate (%) on the conﬁgural patch display for human subjects (1– 4) and the local model (M) at each window radius.

Journal of Vision (2007) 7(8):2, 1–9

Fowlkes, Martin, & Malik

9

¨ Bahnsen, P. (1928). Eine untersuchung uber symmetrie und asymmetrie bei visuellen wahrnehmungen. Zeitschrift fu Psychologie, 108, 129–154. ¨r Brunswik, E., & Kamiya, J. (1953). Ecological cue-validity of “proximity” and of other gestalt factors. American Journal of Psychology, 66, 20–32. [PubMed] Cover, T., & Thomas, J. (1991). Elements of information theory. New York: Wiley. Elder, J. H., & Goldberg, R. M. (2002). Ecological statistics of Gestalt laws for the perceptual organization of contours. Journal of Vision, 2(4):5, 324–353, http://journalofvision.org/2/4/5/, doi:10.1167/2.4.5. [PubMed] [Article] Fowlkes, C., Martin, D., & Malik, J. (2003). Learning afﬁnity functions for image segmentation: Combining patch-based and gradient-based approaches. in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2, 54–61. Geisler, W. S., Perry, J. S., Super, B. J., & Gallogly, D. P. (2001). Edge co-occurrence in natural images predicts contour grouping performance. Vision Research, 41, 711–724. [PubMed] Gibson, J. J. (1979). The ecological approach to visual perception. Boston: Houghton Mifﬂin. Hastie, T., Tibshirani, R., & Friedman, J. (2001). The elements of statistical learning: Data mining, inference, and prediction. New York: Springer-Verlag. Kanizsa, G., & Gerbino, W. (1976). Convexity and symmetry in ﬁgure–ground organization. In M. Henle (Ed.), Vision and artifact (pp. 25–32). New York: Springer. Mante, V., Frazor, R. A., Bonin, V., Geisler, W. S., & Carandini, M. (2005). Independence of luminance and contrast in natural scenes and in the early visual system. Nature Neuroscience, 8, 1690–1697. [PubMed] Martin, D., Fowlkes, C. C., Tal, D., & Malik, J. (2001). A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. Proceedings of the IEEE International Conference on Computer Vision, 2, 416–425. McDermott, J. (2004). Psychophysics with junctions in real images. Perception, 33, 1101–1127. [PubMed] Metzger, F. (1953). Gesetze des Sehens. Frankfurt-amMain: Waldemar Kramer.

Olshausen, B. A., & Field, D. J. (1996). Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381, 607–609. [PubMed] Peterson, M. A. (1994). Object recognition processes can and do operate before ﬁgure–ground organization. Current Directions in Psychological Science, 3, 105–111. Qiu, F. T., & von der Heydt, R. (2005). Figure and ground in the visual cortex: V2 combines stereoscopic cues with gestalt rules. Neuron, 47, 155–166. [PubMed] [Article] Ren, X., Fowlkes, C., & Malik, J. (2006). Figure/Ground assignment in natural images. Proceedings of the European Conference on Computer Vision, 2, 614–627. Ren, X., & Malik, J. (2002). A probabilistic multi-scale model for contour completion based on image statistics. Proceedings of the European Conference on Computer Vision, 1, 312–327. Rubin, E. (1921). Visuell wahrgenommene ﬁguren. Kobenhaven: Glydenalske Boghandel. Ruderman, D. (1994). The statistics of natural images. Network, 5, 517–548. Switkes, E., Mayer, M. J., & Sloan, J. A. (1978). Spatial frequency analysis of the visual environment: Anisotropy and the carpentered environment hypothesis. Vision Research, 18, 1393–1399. [PubMed] Vecera, S. P., Vogel, E. K., & Woodman, G. F. (2002). Lower region: A new cue for ﬁgure<ground assignment. Journal of Experimental Psychology: General, 131, 194–205. [PubMed] von der Heydt, R., & Peterhans, E. (1989). Mechanisms of contour perception in monkey visual cortex: I. Lines of pattern discontinuity. Journal of Neuroscience, 9, 1731–1748. [PubMed] [Article] Zhaoping, L. (2005). Border ownership from intracortical interactions in visual area V2. Neuron, 47, 143–153. [PubMed] [Article] Zhou, H., Friedman, H. S., & von der Heydt, R. (2000). Coding of border ownership in monkey visual cortex. Journal of Neuroscience, 20, 6594–6611. [PubMed] [Article]

