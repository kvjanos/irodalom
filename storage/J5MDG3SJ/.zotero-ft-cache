Effective Databases for Text & Document Management
Shirley A. Becker Northern Arizona University, USA

Publisher of innovative scholarly and professional information technology titles in the cyberage

IRM Press

Hershey • London • Melbourne • Singapore • Beijing

Acquisitions Editor: Senior Managing Editor: Managing Editor: Development Editor: Copy Editor: Typesetter: Cover Design: Printed at:

Mehdi Khosrow-Pour Jan Travers Amanda Appicello Michele Rossi Maria Boyer Jennifer Wetzel Kory Gongloff Integrated Book Technology

Published in the United States of America by IRM Press (an imprint of Idea Group Inc.) 1331 E. Chocolate Avenue, Suite 200 Hershey PA 17033-1117 Tel: 717-533-8845 Fax: 717-533-8661 E-mail: cust@idea-group.com Web site: http://www.irm-press.com and in the United Kingdom by IRM Press (an imprint of Idea Group Inc.) 3 Henrietta Street Covent Garden London WC2E 8LU Tel: 44 20 7240 0856 Fax: 44 20 7379 3313 Web site: http://www.eurospan.co.uk Copyright © 2003 by IRM Press. All rights reserved. No part of this book may be reproduced in any form or by any means, electronic or mechanical, including photocopying, without written permission from the publisher. Library of Congress Cataloging-in-Publication Data Becker, Shirley A., 1956Effective databases for text & document management / Shirley A. Becker. p. cm. Includes bibliographical references and index. ISBN 1-931777-47-0 (softcover) -- ISBN 1-931777-63-2 (e-book) 1. Business--Databases. 2. Database management. I. Title: Effective databases for text and document management. II. Title. HD30.2.B44 2003 005.74--dc21 2002156233

British Cataloguing in Publication Data A Cataloguing in Publication record for this book is available from the British Library.

New Releases from IRM Press
• • • • • • • • • • • • • • • • Multimedia and Interactive Digital TV: Managing the Opportunities Created by Digital Convergence/Margherita Pagani ISBN: 1-931777-38-1; eISBN: 1-931777-54-3 / US$59.95 / © 2003 Virtual Education: Cases in Learning & Teaching Technologies/ Fawzi Albalooshi (Ed.), ISBN: 1-931777-39-X; eISBN: 1-931777-55-1 / US$59.95 / © 2003 Managing IT in Government, Business & Communities/Gerry Gingrich (Ed.) ISBN: 1-931777-40-3; eISBN: 1-931777-56-X / US$59.95 / © 2003 Information Management: Support Systems & Multimedia Technology/ George Ditsa (Ed.), ISBN: 1-931777-41-1; eISBN: 1-931777-57-8 / US$59.95 / © 2003 Managing Globally with Information Technology/Sherif Kamel (Ed.) ISBN: 42-X; eISBN: 1-931777-58-6 / US$59.95 / © 2003 Current Security Management & Ethical Issues of Information Technology/Rasool Azari (Ed.), ISBN: 1-931777-43-8; eISBN: 1-931777-59-4 / US$59.95 / © 2003 UML and the Unified Process/Liliana Favre (Ed.) ISBN: 1-931777-44-6; eISBN: 1-931777-60-8 / US$59.95 / © 2003 Business Strategies for Information Technology Management/Kalle Kangas (Ed.) ISBN: 1-931777-45-4; eISBN: 1-931777-61-6 / US$59.95 / © 2003 Managing E-Commerce and Mobile Computing Technologies/Julie Mariga (Ed.) ISBN: 1-931777-46-2; eISBN: 1-931777-62-4 / US$59.95 / © 2003 Effective Databases for Text & Document Management/Shirley A. Becker (Ed.) ISBN: 1-931777-47-0; eISBN: 1-931777-63-2 / US$59.95 / © 2003 Technologies & Methodologies for Evaluating Information Technology in Business/ Charles K. Davis (Ed.), ISBN: 1-931777-48-9; eISBN: 1-931777-64-0 / US$59.95 / © 2003 ERP & Data Warehousing in Organizations: Issues and Challenges/Gerald Grant (Ed.), ISBN: 1-931777-49-7; eISBN: 1-931777-65-9 / US$59.95 / © 2003 Practicing Software Engineering in the 21st Century/Joan Peckham (Ed.) ISBN: 1-931777-50-0; eISBN: 1-931777-66-7 / US$59.95 / © 2003 Knowledge Management: Current Issues and Challenges/Elayne Coakes (Ed.) ISBN: 1-931777-51-9; eISBN: 1-931777-67-5 / US$59.95 / © 2003 Computing Information Technology: The Human Side/Steven Gordon (Ed.) ISBN: 1-931777-52-7; eISBN: 1-931777-68-3 / US$59.95 / © 2003 Current Issues in IT Education/Tanya McGill (Ed.) ISBN: 1-931777-53-5; eISBN: 1-931777-69-1 / US$59.95 / © 2003

Excellent additions to your institution’s library! Recommend these titles to your Librarian!
To receive a copy of the IRM Press catalog, please contact 1/717-533-8845 ext. 10, fax 1/717-533-8661, or visit the IRM Press Online Bookstore at: [http://www.irm-press.com]! Note: All IRM Press books are also available as ebooks on netlibrary.com as well as other ebook sources. Contact Ms. Carrie Skovrinskie at [cskovrinskie@idea-group.com] to receive a complete list of sources where you can obtain ebook information or IRM Press titles.

Effective Databases for Text & Document Management
Table of Contents
Preface ....................................................................................................................... vii Shirley A. Becker, Northern Arizona University, USA Section I: Information Extraction and Retrieval in Web-Based Systems Chapter I. System of Information Retrieval in XML Documents ........................ 1 Saliha Smadhi, Université de Pau, France Chapter II. Information Extraction from Free-Text Business Documents ................................................................................................................. 12 Witold Abramowicz, The Poznan University of Economics, Poland Jakub Piskorski, German Research Center for Artificial Intelligence in Saarbruecken, Germany Chapter III. Interactive Indexing of Documents with a Multilingual Thesaurus ................................................................................................................. 24 Ulrich Schiel, Universidade Federal de Campina Grande, Brazil Ianna M.S.F. de Sousa, Universidade Federal de Campina Grande, Brazil Chapter IV. Managing Document Taxonomies in Relational Databases ........... 36 Ido Millet, Penn State Erie, USA Chapter V. Building Signature-Trees on Path Signatures in Document Databases ................................................................................................................... 53 Yangjun Chen, University of Winnipeg, Canada Gerald Huck, IPSI Institute, Germany Chapter VI. Keyword-Based Queries Over Web Databases ................................ 74 Altigran S. da Silva, Universidade Federal do Amazonas, Brazil Pável Calado, Universidade Federal de Minas Gerais, Brazil Rodrigo C. Vieira, Universidade Federal de Minas Gerais, Brazil Alberto H.F. Laender, Universidade Federal de Minas Gerais, Brazil Bertheir A. Ribeiro-Neto, Universidade Federal de Minas Gerais, Brazil

Chapter VII. Unifying Access to Heterogeneous Document Databases Through Contextual Metadata ................................................................................. 93 Virpi Lyytikäinen, University of Jyväskylä, Finland Pasi Tiitinen, University of Jyväskylä, Finland Airi Salminen, University of Jyväskylä, Finland Section II: Data Management and Web Technologies Chapter VIII. Database Management Issues in the Web Environment .............. 109 J.F. Aldana Montes, Universidad de Málaga, Spain A.C. Gómez Lora, Universidad de Málaga, Spain N. Moreno Vergara, Universidad de Málaga, Spain M.M. Roldán García, Universidad de Málaga, Spain Chapter IX. Applying JAVA-Triggers for X-Link Management in the Industrial Framework ................................................................................................................ 135 Abraham Alvarez, Laboratoire d’Ingéniere des Systèmes d’Information, INSA de Lyon, France Y. Amghar, Laboratoire d’Ingéniere des Systèmes d’Information, INSA de Lyon, France Section III: Advances in Database and Supporting Technologies Chapter X. Metrics for Data Warehouse Quality ................................................ 156 Manuel Serrano, University of Castilla-La Mancha, Spain Coral Calero, University of Castilla-La Mancha, Spain Mario Piattini, University of Castilla-La Mancha, Spain Chapter XI. Novel Indexing Method of Relations Between Salient Objects ...... 174 R. Chbeir, Laboratoire Electronique Informatique et Image, Université de Bourgogne, France Y. Amghar, Laboratoire d’Ingéniere des Systèmes d’Information, INSA de Lyon, France A. Flory, Laboratoire d’Ingéniere des Systèmes d’Information, INSA de Lyon, France Chapter XII. A Taxonomy for Object-Relational Queries ................................... 183 David Taniar, Monash University, Australia Johanna Wenny Rahayu, La Trobe University, Australia Prakash Gaurav Srivastava, La Trobe University, Australia Chapter XIII. Re-Engineering and Automation of Business Processes: Criteria for Selecting Supporting Tools ................................................................ 221 Aphrodite Tsalgatidou, University of Athens, Greece Mara Nikolaidou, University of Athens, Greece

Chapter XIV. Active Rules and Active Databases: Concepts and Applications . 234 Juan M. Ale, Universidad de Buenos Aires, Argentina Mauricio Minuto Espil, Universidad de Buenos Aires, Argentina

Section IV: Advances in Relational Database Theory, Methods and Practices Chapter XV. On the Computation of Recursion in Relational Databases ......... 263 Yangjun Chen, University of Winnipeg, Canada Chapter XVI. Understanding Functional Dependency ......................................... 278 Robert A. Schultz, Woodbury University, USA Chapter XVII. Dealing with Relationship Cardinality Constraints in Relational Database Design ....................................................................................................... 288 Dolores Cuadra Fernández, Universidad Carlos III de Madrid, Spain Paloma Martínez Fernández, Universidad Carlos III de Madrid, Spain Elena Castro Galán, Universidad Carlos III de Madrid, Spain Chapter XVIII. Repairing and Querying Inconsistent Databases ...................... 318 Gianluigi Greco, Università della Calabria, Italy Sergio Greco, Università della Calabria, Italy Ester Zumpano, Università della Calabria, Italy About the Authors ..................................................................................................... 360 Index .......................................................................................................................... 368

vii

Preface

The focus of this book is effective databases for text and document management inclusive of new and enhanced techniques, methods, theories and practices. The research contained in these chapters is of particular significance to researchers and practitioners alike because of the rapid pace at which the Internet and related technologies are changing our world. Already there is a vast amount of data stored in local databases and Web pages (HTML, DHTML, XML and other markup language documents). In order to take advantage of this wealth of knowledge, we need to develop effective ways of extracting, retrieving and managing the data. In addition, advances in both database and Web technologies require innovative ways of dealing with data in terms of syntactic and semantic representation, integrity, consistency, performance and security. One of the objectives of this book is to disseminate research that is based on existing Web and database technologies for improved information extraction and retrieval capabilities. Another important objective is the compilation of international efforts in database systems, and text and document management in order to share the innovation and research advances being done at a global level. The book is organized into four sections, each of which contains chapters that focus on similar research in the database and Web technology areas. In the section entitled, Information Extraction and Retrieval in Web-Based Systems, Web and database theories, methods and technologies are shown to be efficient at extracting and retrieving information from Web-based documents. In the first chapter, “System of Information Retrieval in XML Documents,” Saliha Smadhi introduces a process for retrieving relevant information from XML documents. Smadhi’s approach supports keyword-based searching, and ranks the retrieval of information based on the similarity with the user’s query. In “Information Extraction from Free-Text Business Documents,” Witold Abramowicz and Jakub Piskorski investigate the applicability of information extraction techniques to free-text documents typically retrieved from Web-based systems. They also demonstrate the indexing potential of lightweight linguistic text processing techniques in order to process large amounts of textual data. In the next chapter, “Interactive Indexing of Documents with a Multilingual Thesaurus,” Ulrich Schiel and Ianna M.S.F. de Sousa present a method for semi-automatic indexing of electronic documents and construction of a multilingual thesaurus. This method can be used for query formulation and information retrieval. Then in the next chapter, “Managing Document Taxonomies in Relational Databases,” Ido Millet ad-

viii

dresses the challenge of applying relational technologies in managing taxonomies used to classify documents, knowledge and websites into topic hierarchies. Millet explains how denormalization of the data model facilitates data retrieval from these topic hierarchies. Millet also describes the use of database triggers to solving data maintenance difficulties once the data model has been denormalized. Yangjun Chen and Gerald Huck, in “Building Signature-Trees on Path Signatures in Document Databases,” introduce PDOM (persistent DOM) to accommodate documents as permanent object sets. They propose a new indexing technique in combination with signature-trees to accelerate the evaluation of path-oriented queries against document object sets and to expedite scanning of signatures stored in a physical file. In the chapter, “Keyword-Based Queries of Web Databases,” Altigran S. da Silva, Pável Calado, Rodrigo C. Vieira, Alberto H.F. Laender and Berthier A. Ribeiro-Neto describe the use of keyword-based querying as a suitable alternative to the use of Web interfaces based on multiple forms. They show how to rank the possible large number of answers returned by a query according to relevant criteria and typically done by Web search engines. Virpi Lyytikäinen, Pasi Tiitinen and Airi Salminen, in “Unifying Access to Heterogeneous Document Databases Through Contextual Metadata,” introduce a method for collecting contextual metadata and representing metadata to users via graphical models. The authors demonstrate their proposed solution by a case study whereby information is retrieved from European, distributed database systems. In the next section entitled, Data Management and Web Technologies, research efforts in data management and Web technologies are discussed. In the first chapter, “Database Management Issues in the Web Environment,” J.F. Aldana Montes, A.C. Gómez Lora, N. Moreno Vergara and M.M. Roldán García address relevant issues in Web technology, including semi-structured data and XML, data integrity, query optimization issues and data integration issues. In the next chapter, “Applying JAVA-Triggers for X-Link Management in the Industrial Framework,” Abraham Alvarez and Y. Amghar provide a generic relationship validation mechanism by combining XLL (X-link and X-pointer) specification for integrity management and Java-triggers as an alert mechanism. The third section is entitled, Advances in Database and Supporting Technologies. This section encompasses research in relational and object databases, and it also presents ongoing research in related technologies. In this section’s first chapter, “Metrics for Data Warehouse Quality,” Manuel Serrano, Coral Calero and Mario Piattini propose a set of metrics that has been formally and empirically validated for assessing the quality of data warehouses. The overall objective of their research is to provide a practical means of assessing alternative data warehouse designs. R. Chbeir, Y. Amghar and A. Flory identify the importance of new management methods in image retrieval in their chapter, “Novel Indexing Method of Relations Between Salient Objects.” The authors propose a novel method for identifying and indexing several types of relations between salient objects. Spatial relations are used to show how the authors’ method can provide high expressive power to relations when compared to traditional methods. In the next chapter, “A Taxonomy for Object-Relational Queries,” David Taniar, Johanna Wenny Rahayu and Prakash Gaurav Srivastava classify object-relational queries into REF, aggregate and inheritance queries. The authors have done this in order to provide an understanding of the full capability of object-relational query language in terms of query processing and optimization. Aphrodite Tsalgatidou and Mara Nikolaidou describe a criteria set for selecting appropriate Business Process Modeling Tools

ix

(BPMTs) and Workflow Management Systems (WFMSs) in “Re-Engineering and Automation of Business Processes: Criteria for Selecting Supporting Tools.” This criteria set provides management and engineering support for selecting a toolset that would allow them to successfully manage the business process transformation. In the last chapter of this section, “Active Rules and Active Databases: Concepts and Applications,” Juan M. Ale and Mauricio Minuto Espil analyze concepts related to active rules and active databases. In particular, they focus on database triggers using the SQL-1999 standard committee’s point of view. They also discuss the interaction between active rules and declarative database constraints from both static and dynamic perspectives. The final section of the book is entitled, Advances in Relational Database Theory, Methods and Practices. This section includes research efforts focused on advancements in relational database theory, methods and practices. In the chapter, “On the Computation of Recursion in Relational Databases,” Yangjun Chen presents an encoding method to support the efficient computation of recursion. A linear time algorithm has also been devised to identify a sequence of reachable trees covering all the edges of a directed acyclic graph. Together, the encoding method and algorithm allow for the computation of recursion. The author proposes that this is especially suitable for a relational database environment. Robert A. Schultz, in the chapter “Understanding Functional Dependency,” examines whether functional dependency in a database system can be considered solely on an extensional basis in terms of patterns of data repetition. He illustrates the mix of both intentional and extensional elements of functional dependency, as found in popular textbook definitions. In the next chapter, “Dealing with Relationship Cardinality Constraints in Relational Database Design,” Dolores Cuadra Fernández, Paloma Martínez Fernández and Elena Castro Galán propose to clarify the meaning of the features of conceptual data models. They describe the disagreements between main conceptual models, the confusion in the use of their constructs and open problems associated with these models. The authors provide solutions in the clarification of the relationship construct and to extend the cardinality constraint concept in ternary relationships. In the final chapter, “Repairing and Querying Inconsistent Databases,” Gianluigi Greco, Sergio Greco and Ester Zumpano discuss the integration of knowledge from multiple data sources and its importance in constructing integrated systems. The authors illustrate techniques for repairing and querying databases that are inconsistent in terms of data integrity constraints. In summary, this book offers a breadth of knowledge in database and Web technologies, primarily as they relate to the extraction retrieval, and management of text documents. The authors have provided insight into theory, methods, technologies and practices that are sure to be of great value to both researchers and practitioners in terms of effective databases for text and document management.

x

Acknowledgment

The editor would like to acknowledge the help of all persons involved in the collation and review process of this book. The authors’ contributions are acknowledged in terms of providing insightful and timely research. Also, many of the authors served as referees for chapters written by other authors. Thanks to all of you who have provided constructive and comprehensive reviews. A note of thanks to Mehdi Khosrow-Pour who saw a need for this book, and to the staff at Idea Group Publishing for their guidance and professional support. Shirley A. Becker Northern Arizona University, USA February 2003

xi

Section I Information Extraction and Retrieval in Web-Based Systems

System of Information Retrieval in XML Documents 1

Chapter I

System of Information Retrieval in XML Documents
Saliha Smadhi Université de Pau, France

ABSTRACT
This chapter introduces the process to retrieve units (or subdocuments) of relevant information from XML documents. For this, we use the Extensible Markup Language (XML) which is considered as a new standard for data representation and exchange on the Web. XML opens opportunities to develop a new generation of Information Retrieval System (IRS) to improve the interrogation process of document bases on the Web. Our work focuses instead on end-users who do not have expertise in the domain (like a majority of the end-users). This approach supports keyword-based searching like classical IRS and integrates structured searching with the search attributes notion. It is based on an indexing method of document tree leafs which authorize a contentoriented retrieval. The retrieval subdocuments are ranked according to their similarity with the user’s query. We use a similarity measure which is a compromise between two measures: exhaustiveness and specificity.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

2 Smadhi

INTRODUCTION
The World Wide Web (WWW) contains large amounts of information available at websites, but it is difficult and complex to retrieve pertinent information. Indeed, a large part of this information is often stored as HyperText Markup Language (HTML) pages that are only viewed through a Web browser. This research is developed in the context of the MEDX project (Lo, 2001) of our team. We use XML as a common structure for storing, indexing and querying a collection of XML documents. Our aim is to propose the suited solutions which allow the end-users not specialized in the domain to search and extract portions of XML documents (called units or subdocuments) which satisfy their queries. The extraction of documents portion can be realized by using XML query languages (XQL, XML-QL) (Robie, 1999; Deutsch, 1999). An important aspect of our approach concerns the indexation which is realized on leaf elements of the document tree and not on the whole document. Keywords are extracted from a domain thesaurus. A thesaurus is a set of descriptors (or concepts) connected by hierarchical relations, equivalence relations or association relations. Indexing process results are stored in a resources global catalog that is exploited by the search processor. This chapter is organized as follows. The next section discusses the problem of relevant information retrieval in the context of XML documents. We then present the model of XML documents indexing, followed by the similarity measure adopted and the retrieval strategy of relevant parts of documents. The chapter goes on to discuss related work, before its conclusion. An implementation of SIRX prototype is currently underway in Python language on Linux Server.

INFORMATION RETRIEVAL AND XML DOCUMENTS
The classical retrieval information involves two principal issues, the representation of documents and queries and the construction of a ranking function of documents. Among Information Retrieval (IR) models, the most-used models are the Boolean Model, Vector Space Model and Probabilist Model. In the Vector Space Model, documents and queries are represented as vectors in the space of index terms. During the retrieval process, the query is also represented as a list of terms or a term vector. This query vector is matched against all document vectors, and a similarity measure between a document and a query is calculated. Documents are ranked according to their values of similarity measure with a query. XML is a subset of the standard SGML. It has a richer structure that is composed mainly of an elements tree that forms the content. XML can represent more useful information on data than HTML. An XML document contains only data as opposed to an HTML file, which tries to mix data and presentation and usually ignores structure. It preserves the structure of the data that it represents, whereas HTML flattens it out. This meta markup language defines its own system of tags representing the structure of a document explicitly. HTML presents information and XML describes information.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

System of Information Retrieval in XML Documents 3

A well-formed XML document doesn’t impose any restrictions on the tags or attribute names. But a document can be accompanied by a Document Type Definition (DTD), which is essentially a grammar for restricting the tags and structure of a document. An XML document satisfying a DTD is considered a valid document. The Document Object Model (DOM) is simply a set of plans or guidelines that enables the user to reconstruct a document right down to the smallest detail. The structure of a document can be transformed with XSLT (1999) and its contents displayed by using the eXtensible Style Language (XSL) language or a programming language (Python, Java, etc.). XSL is a declarative language in which the model refers the data by using patterns. It is limited when one wants to retrieve data with specific criteria, as one can realize that with the query language XQL (or OQL) for relational databases (or object). This extension is proposed by three languages coming from the database community: XML-QL (Florescu, 2000), Lorel (Abiteboul, 1997) and XQL (Robie, 1999) from the Web community.

Requirements for a System of Relevant Information Retrieval for XML Documents
We propose an approach for information retrieval with relevance ranking for XML documents of which the basic functional requirements are: a) to support keyword-based searching and structured searching (by proposing a set of search attributes) by end-users who have no expertise in the domain and of that the structure is then unknown (like a majority of the end-users); b) to retrieve relevant parts of documents (called subdocuments) ranked by their relevancy with the query; and c) to navigate in the whole document. a) b) c) d) In order to satisfy the essential requirements of this approach, we have opted to: use a domain thesaurus; define an efficient model of documents indexing that extends the classic “inverted index” technology by indexing document structure as well as content; integrate search attributes that concern a finite number of sub-structure types, which we like to make searchable; propose an information retrieval engine with ranking of relevant document parts.

Architectural Overview of SIRX
We present an overview of the System of Information Retrieval in XML documents (SIRX) showing its mains components (see Figure 1). The main architectural components of the system are the following: 1) User Interface: This is used to facilitate the interaction between the user and the application. It allows the user to specify his query. It also displays retrieved documents or parts of documents ranked by relevance score. It does not suppose an expertise or a domain knowledge of the end-user. 2) Search Processor: This allows retrieval of contents directly from the Resources Global Catalog on using the various index and keywords expressed in an input query.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

4 Smadhi

Figure 1. The General Architecture of SIRX

XML Documents Base

Indexer Processor

Thesaurus

Resources Global Catalog

Search Processor Query User Interface Figure 1: The General Architecture of SIRX

Viewer Result

3) 4) 5) 6) 7)

XML Documents Base: This stores XML documents well-formed in their original formats. Thesaurus: The domain thesaurus contains the set of descriptors (keywords) which allow the user to index documents of this domain. Indexer Processor: For every XML document, the indexer processor creates indexes by using the thesaurus and the XML documents base. These indexes allow the user to build the Resources Global Catalog. Resources Global Catalog: This is an indexing structure that the search processor uses to find the relevant document parts. It is exploited mainly by the search processor. Viewer: The viewer displays retrieved document parts. The results are recombined (XML + XSL) to show the document to the user in an appropriate manner (into HTML).

THE MODEL OF XML DOCUMENTS INDEXING
In our approach that is based on Vector Space Model, we propose to index the leafs of the document tree (Shin, 1998) and the keywords that correspond to the descriptor terms extracted from the domain thesaurus (Lo, 2000). Indexing process results are structured by using the XML language in meta-data collection which is stored in the Resources Global Catalog (see Figure 2). This catalog is the core of the SIRX system. It encapsulates all semantic content of the XML document’s base and thesaurus.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

System of Information Retrieval in XML Documents 5

Elementary Units and Indexing
In classic information retrieval, the documents are considered as atomic units. The keyword search is based on classic index structures that are inverted files. A classic inverted file contains <keyword, document> pairs, meaning that the word can be found in the document. This classical approach allows the user to retrieve the whole document. It is not necessary to forget that documents can often be quite long and in many cases only a small part of documents may be relevant to the user’s query. It is necessary to be able to retrieve only the part of document that may be relevant to the end-user’s query. To accomplish this objective, we extend the classic inverted file by making the unit structure explicit. The indexing processor extracts terms from the thesaurus and calculates their frequencies in each element at the text level. Every elementary unit is identified in a unique way by an access-path showing his position in the document. The form of this index is <keyword, unit, frequency> where: 1) keyword is a term appearing in the content of element or values of an attribute of this document; 2) unit specifies the access path to element content that contains keyword; the access path is described by using XPath (1999) compliance syntax; 3) frequency is the frequency of the keyword in the specified unit. This indexation method allows a direct access to any elementary unit which appears in the result of the query and regroups results of every document by using XSLT.

Search Attributes
Methods of classical information retrieval propose a function of search from signaletic metadata (author, title, date, etc.) that concerns mostly characteristics related to a whole document. To be able to realize searches on sub-structures of a document, we propose to integrate a search based on the document structure from a finite number of element types, which we like to make searchable from their semantic content. These specific elements are called search attributes. They are indexed like keywords in the Resources Global Catalog. Every search attribute has the following form: <identifier, unit>, where identifier is the name (or tag) of the search attribute under which it will appear to the user, and unit indicates the access path to a elementary unit (type 1) or an another node (type 2) of document that will carry this structural search based on its content. Search attribute names are available at the level of the user’s interface. In the following example, the tag of elementary unit is ‘title,’ and ‘author’ is the name of an attribute of the tag ‘book.’ <info idinfo=“title” path=“//title”/> <info idinfo=“author” path=“//book/@author”/> The query result depends on type of search attribute. If the indexed search attribute is an elementary unit, then the returned result is the node that is the father of this unit. If the indexed search attribute is a node different from elementary unit, then the returned result is this node.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

6 Smadhi

Query Examples
Query 1: title = ’dataweb’. This query returns the following result: all the names of documents of which value of <title> contains ‘dataweb’ text. Query 2: author = ’smadhi’. This query returns the following result: all the sub-structures (at first level) which have for name ‘book’ and for that the attribute ‘author’ contains ‘smadhi’ text.

Resources Global Catalog
The Resources Global Catalog is defined as a generalized index that allows the user to maintain for SIRX, to efficiently support keyword searching and sub-structure searching. It is used by the search processor use to find the relevant documents (or parts of documents). It is represented by an XML document which describes every XML document that is indexed by the indexing processor. This catalog is described in XML according the following DTD: Figure 2. The Catalog DTD <!ELEMENT catalog(doc*)> <!ELEMENT doc(address, search-attributes, keywords)> <!ATTLIST doc iddoc ID #REQUIRED> <!ELEMENT search-attributes(info*)> <!ELEMENT info (#PCDATA)> <!ATTLIST info idinfo ID #REQUIRED) <!ATTLIST info path CDATA #REQUIRED> <!ELEMENT address(#PCDTA)> <!ELEMENT keywords(key*)> <!ELEMENT key (#PCDATA)> <!ATTLIST key idkey ID #REQUIRED> <!ATTLIST key path CDATA #REQUIRED> <!ATTLIST key freq CDATA #REQUIRED> The following example illustrates the structure of this catalog: Figure 3. An Example of Resources Global Catalog <catalog> <doc iddoc=“d1” > <address>c:/SRIX/mcseai.xml</address> <search-attributes> <info idinfo=“title” path=“//title”/> <info idinfo=“author” path=“//book/@author”/> </search-attributes> <keywords> <key idkey =“k1” path=“//dataweb/integration” freq=2>xml </key> <key idkey =“k2” path=“// mapping/@base” freq=1>xml </key>
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

System of Information Retrieval in XML Documents 7

… </keywords> </doc> <doc iddoc=“d2” > <address>c:/SRIX/cari2000.xml</address> <search-attributes> <info idinfo=“title” path=“//title”/> <info idinfo=“author” path=“//book/@author”/> </search-attributes> <keywords> <key idkey =“k25” path=“//architecture/integration” freq=2>web </ key> <key idkey =“k26” path=“// architecture/integration” freq=2>dataweb </key> … </keywords> </doc> …. </catalog>

Keyword Weights
In the Vector Space Model, documents and queries are represented as vector weighted terms (the word term refers to keyword) (Salton, 1988; Yuwono, 1996). In our approach each indexed elementary unit j of document i is represented by a vector as follows:

U ij

( wij1 , wij 2 , ,..., wijk ,...,wijp ) , k 1, 2, ..., p

• • •

nu: number of elementary units j of document i p: number of indexing keywords wijk: weight of the kth term in the jth elementary unit of the ith document We use the classical tf.idf weighting scheme (Salton, 1988) to calculate wijk.

w ijk
• •

i tf jk

idf k

tf ijk: the frequency of the kth term in the jth elementary unit of the ith document idfk: the inverse document frequency of the index term tk. It is computed as a function of the elementary unit frequency by the following formula: idfk = log(tnu/nuk)

• •

tnu: the total number of elementary units in the document base nuk: the number of elementary units which the kth term occurs at least once

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

8 Smadhi

RELEVANT INFORMATION RETRIEVAL
a) SIRX supports two ways to retrieve parts of documents: Querying by Search Attributes: Authorizes a search based on a document structure from a list of search attributes proposed to a user. It allows one to retrieve documents or parts of documents according the type search attributes. This aspect is not detailed in this chapter. Querying by Content with Keywords: Allows retrieval of documents or parts of documents.

b)

In this section we describe the search process of relevant information retrieval that involves two issues: generating query vector, and computing the similarity between vector query and each elementary unit vector. The adopted model of data rests mainly on the use of the catalog in memory central for an exploitation, during the process of interrogation by a set of end-users.

Query Processing
A user’s query is a list of one or more keywords which belong to the thesaurus. When the user inputs a query, the system generates a query vector by using the same indexing method as that of the element unit vector. A query vector Q is as follows: Q =(q1, q 2, …, qk, …, qm) with m p

Query terms qk (j=1…m) are weighted by the idf value where idf is measured by log(tnu/nu k).

Retrieval and Ranking of Relevant XML Information Units
The search process returns the relevant elementary units of an XML document. These information units are ranked according to their similarity coefficients measuring the relevance of elementary units of an XML document to a user’s query. In the Vector Space Model, this similarity is measured by cosine of the angle between the elementary unit vector and query vector. On considering the two vectors Ui and Q in the Euclidean space with scalar product noted <,> and norm noted . , the similarity is (Smadhi, 2001):
m

q j wij Sim(U i , Q)
j 1 m j 1

q2 j

m j 1

2 wij

This measure like others (Salton, 1988; Wang, 1992) is based on the following hypothesis: the more a document looks like the query, the more it is susceptible to be relevant for the user. We question this hypothesis because the query and the document
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

System of Information Retrieval in XML Documents 9

do not play a symmetric role in the search for information (Simonnot & Smail, 1996; Fourel, 1998). It is necessary to note that the user expresses in his query only characteristics of the document which interests him at the given moment. It is necessary to take into account two important criteria: the exhaustiveness of the query in the document and the specificity of the document with regard to the query (Nie, 1988). Now, we show how to spread this measure of similarity to take into account these two criteria. A measure is based on the exhaustiveness if it estimates the degree of inclusion of the query Q in the unit Ui. Conversely, a measure based on the specificity measures the degree of inclusion of Ui elementary unit in the query Q. We propose the two following measures: a) The exhaustiveness measure noted mexh is:

mexh(U i , Q )
b)

cos(U i , Q) U i Q

The specificity measure noted mspec is:

mspec(U i , Q)

cos(U i , Q) Q Ui

These two measures have intuitively a comprehensible geometrical interpretation because mexh(Ui,Q) represents the norm of the vector projection Ui on the vector Q. In the same way, mspec(Ui,Q) represents the norm of vector projection Q on the Ui vector. The similarity measure became:

Sim(U i , Q )

mspec(U i , Q) mexh(U i , Q ))

Experiments Results
The reference collection that we built is not very important. This collection has 200 XML documents which correspond to articles extracted from proceedings of conferences. First estimates seem to us very interesting: the measure of similarity that we proposed allowed us to improve about 20% the pertinence of restored subdocuments. These tests are realized on a Linux Server using a Dell computer with an 800Mhz Intel processor with 512 MB RAM.

RELATED WORK AND CONCLUSION
Many works are done to propose methods of information retrieval in XML documents. Among various approaches (Luk, 2000), the database-oriented approach and information retrieval-oriented approach seem the most used.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

10 Smadhi

In the database-oriented approach, some query languages — such as XIRQ (Fuhr, 2000), XQL and XML-QL — are proposed, but these languages are not suitable for endusers in spite of the integration of a keyword search into an XML query language (Florescu, 2000). Xset (Zhao, 2000) supposes to have knowledge about document structure. If XRS (Shin, 1998) proposes an interesting indexing method at the leaf elements, it still may present an inconvenience with the use of DTD. Our approach proposes, like XRS, an indexing at the leaf elements, and it extends the inverted index with XML path specifications. It also takes into account the structure of the XML document. Moreover we introduce a particular measure of similarity which is a compromise between two measures: exhaustiveness and specificity. This new approach allows users to retrieve parts of XML documents with relevance ranking.

REFERENCES
Abiteboul, S., Quass, D., McHugh, D., Widom, J. and Wiener, J. (1997). The Lorel query language for semi-structured data. Journal of Digital Libraries, 68-88. Deutsch, A., Fernandez, M.F., Florescu, D. and Levy, A. (1999). A query language for XML. WWW8/Computer Networks, 31, 1155-1169. Florescu, D., Manolescu, I. and Kossman, D. (2000). Integrating search into XML query processing. Proceedings of the Ninth International WWW Conference. Fuhr, N. and Grossjohann, K. (2000). XIRQ: An extension of XQL for information retrieval. Proceedings of the ACM SIGIR 2000 Workshop on XML and Information Retrieval. Govert, N., Lalmas, M. and Fuhr, N. (1999). A probabilistic description-oriented approach for categorising Web documents. Proceedings of the Ninth International Conference on Information and Knowledge Management (pp. 475-782) New York: ACM. Hayashi, Y., Tomita, J. and Kikui, G. (2000). Searching text-rich XML documents with relevance ranking. Proceedings of the ACM SIGIR 2000 Workshop on XML and Information Retrieval. Lo, M. and Hocine, A. (2000). Modeling of Dataweb: An approach based on the integration of semantics of data and XML. Proceedings of the Fifth African Conference on the Search in Computing Sciences, Antananarivo, Madagascar. Lo, M., Hocine, A. and Rafinat, P. (2001). A designing model of XML-Dataweb. Proceedings of International Conference on Object Oriented Information Systems (OOIS’2001) (pp. 143-153) Calgary, Alberta, Canada. Luk, R., Chan, A., Dillon, T. and Leong, H.V. (2000). A survey of search engines for XML documents. Proceedings of the ACM SIGIR 2000 Workshop on XML and Information Retrieval. Nie, J. (1988). An outline of a general model for information retrieval systems. Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval (pp. 495-506). Robie, J. (1999). The Design of XQL, 1999. Available online at: http://www.texcel.no/ whitepapers/xql-design.html. Salton, G. and Bukley, D. (1988). Term weighting approaches in automatic text retrieval. Information Processing and Management, 24(5), 513-523.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

System of Information Retrieval in XML Documents 11

Shin, D., Chang, H. and Jin, H. (1998). Bus: An effective indexing and retrieval scheme in structured documents. Proceedings of Digital Libraries’98 (pp. 235-243). Simonnot, B. and Smail, M. (1996). Modèle flexible pour la recherche interactive de documents multimedias. Proceedings of Inforsid’96 (pp. 165-178) Bordeaux. Smadhi, S. (2001). Search and ranking of relevant information in XML documents. Proceedings of IIWAS 2001 (pp. 485-488) Linz, Austria. Wang, Z.W., Wong, S.K. and Yao, Y.Y. (1992). An analysis of Vector Space Models based on computational geometry. Proceedings of the AMC SIGIR International Conference on Research and Development in Information Retrieval (pp. 152-160) Copenhagen, Denmark. Xpath. (1999). Available online at: http://www.w3.org/TR/1999/REC-xpath-19991116. XSLT. (1999). Available online at: http://www.w3.org/TR/1999/REC-xslt-19991116. Yuwono, B. and Lee, D.L. (1996). WISE: A World Wide Web Resource Database System. IEEE TKDE, 8(4), 548-554. Zhao, B.Y. and Joseph, A. (2000). Xset: A Lightweight XML Search Engine for Internet Applications. Available online at: http://www.cs.berkeley.edu/ravenben/xset/html/ xset-saint.pdf.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

12 Abramowicz & Piskorski

Chapter II

Information Extraction from Free-Text Business Documents
Witold Abramowicz The Poznan University of Economics, Poland Jakub Piskorski German Research Center for Artificial Intelligence in Saarbruecken, Germany

ABSTRACT
The objective of this chapter is an investigation of the applicability of information extraction techniques in real-world business applications dealing with textual data since business relevant data is mainly transmitted through free-text documents. In particular, we give an overview of the information extraction task, designing information extraction systems and some examples of existing information extraction systems applied in the financial, insurance and legal domains. Furthermore, we demonstrate the enormous indexing potential of lightweight linguistic text processing techniques applied in information extraction systems and other closely related fields of information technology which concern processing vast amounts of textual data.

INTRODUCTION
Nowadays, knowledge relevant to business of any kind is mainly transmitted through free-text documents: the World Wide Web, newswire feeds, corporate reports, government documents, litigation records, etc. One of the most difficult issues concerning applying search technology for retrieving relevant information from textual data

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Information Extraction from Free-Text Business Documents

13

collections is the process of converting such data into a shape for searching. Information retrieval (IR) systems using conventional indexing techniques applied even to a homogenous collection of text documents fall far from obtaining optimal recall and precision simultaneously. Since structured data is obviously easier to search, an ever-growing need for effective and intelligent techniques for analyzing free-text documents and building expressive representation of their content in the form of structured data can be observed. Recent trends in information technology such as Information Extraction (IE) provide dramatic improvements in converting the overflow of raw textual information into valuable and structured data, which could be further used as input for data mining engines for discovering more complex patterns in textual data collections. The task of IE is to identify a predefined set of concepts in a specific domain, ignoring other irrelevant information, where the domain consists of a corpus of texts together with a clearly specified information need. Due to the specific phenomena and complexity of natural language, this is a non-trivial task. However, recent advances in Natural Language Processing (NLP) concerning new robust, efficient, high coverage shallow processing techniques for analyzing free text have contributed to the size in the deployment of IE techniques in business information systems.

INFORMATION EXTRACTION
Information Extraction Task
The task of IE is to identify instances of a particular pre-specified class of events or relationships and entities in natural language texts, and the extraction of the relevant arguments of the events or relationships (SAIC, 1998). The information to be extracted is pre-specified in user-defined structures called templates (e.g., company information, meetings of important people), each consisting of a number of slots, which must be instantiated by an IE system as it processes the text. The slots are usually filled with: some strings from the text, one of a number of pre-defined values or a reference to other already generated template. One way of thinking about an IE system is in terms of database construction, since an IE system creates a structured representation of selected information drawn from the analyzed text. In recent years IE technology has progressed quite rapidly, from small-scale systems applicable within very limited domains to useful systems which can perform information extraction from a very broad range of texts. IE technology is now coming to the market and is of great significance to finance companies, banks, publishers and governments. For instance, a financial organization would want to know facts about foundations of international joint-ventures happening in a given time span. The process of extracting such information involves locating the names of companies and finding linguistic relations between them and other relevant entities (e.g., locations and temporal expressions). However, in this particular scenario an IE system requires some specific domain knowledge (understanding the fact that ventures generally involve at least two partners and result in the formation of a new company) in order to merge partial information into an adequate template structure. Generally, IE systems rely to some degree on domain knowledge. Further information such as appointment of key personnel

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

14 Abramowicz & Piskorski

or announcement of new investment plans could also be reduced to instantiated templates.

Designing IE Systems
There are two basic approaches to designing IE systems: the Knowledge Engineering Approach and the Learning Approach (Appelt & Israel, 1999). In the knowledge engineering approach, the development of rules for marking and extracting sought-after information is done by a human expert through inspection of the test corpus and his or her own intuition. In the learning approach the rules are learned from an annotated corpus and interaction with the user. Generally, higher performance can be achieved by handcrafted systems, particularly when training data is sparse. However, in a particular scenario automatically trained components of an IE system might show better performance than their handcrafted counterparts. Approaches to building hybrid systems based on both approaches are currently being investigated. IE systems built for different tasks often differ from each other in many ways. Nevertheless, there are core components shared by nearly every IE system, disregarding the underlying design approach. The coarse-grained architecture of a typical IE system is presented in Figure 1. It consists of two main components: text processor and template generation module. The task of the text processor is performing general linguistic analysis in order to extract as much linguistic structure as possible. Due to the problem of ambiguity pervading all levels of natural language processing, this is a non-trivial task. Instead of computing all possible interpretations and grammatical relations in natural language text (so-called deep text processing — DTP), there is an increased tendency towards applying only partial analysis (so-called shallow text processing — STP), which is considerably less time consuming and could be seen as a trade-off between pattern matching and fully fledged linguistic analysis (Piskorski & Skut, 2000). There is no standardized definition of the term shallow text processing. Shallow text processing can be characterized as a process of computing text analysis which is less complete than the output of deep text processing systems. It is usually restricted to identifying non-recursive structures or structures with limited amount of structural recursion, which can be identified with high degree of certainty. In shallow text analysis, language regularities which cause problems are not handled and, instead of computing all possible readings, only underspecified structures are computed. The use of STP instead of DTP may be advantageous since it might be sufficient for the Figure 1. A Coarse-Grained Architecture of an Information Extraction System
IE Core System

Free Texts

Text Processor

Template Generation

Instantiated Templates

Domain Knowledge

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Information Extraction from Free-Text Business Documents

15

extraction and assembly of the relevant information and it requires less knowledge engineering, which means a faster development cycle and fewer development expenses. Most of the STP systems follow the finite-state approach, which guarantees time and space efficiency. The scope of information computed by the text processor may vary depending on the requirements of a particular application. Usually, linguistic analysis performed by the text processor of an IE system includes following steps: • Segmentation of text into a sequence of sentences, each of which is a sequence of lexical items representing words together with their lexical attributes • Recognition of small-scale structures (e.g., abbreviations, core nominal phrases, verb clusters and named entities) • Parsing, which takes as input a sequence of lexical items and small-scale structures and computes the structure of the sentence, the so-called parse tree Depending on the application scenario, it might be desirable for the text processor to perform additional tasks such as: part-of-speech disambiguation, word sense tagging, anaphora resolution or semantic interpretation (e.g., translating the parse tree or parse fragments into a semantic structure or logical form). A benefit of the IE task orientation is that it helps to focus on linguistic phenomena that are most prevalent in a particular domain or particular extraction task. The template generation module merges the linguistic structures computed by the text processor and using domain knowledge (e.g., domain-specific extraction patterns and inference rules) derives domain-specific relations in the form of instantiated templates. In practice, the boundary between text processor and template generation component may be blurred. The input and output of an IE system can be defined precisely, which facilitates the evaluation of different systems and approaches. For the evaluation of IE systems, the precision, recall and f-measures were adopted from the IR research community (e.g., the recall of an IE system is the ratio between the number of correctly filled slots and the total number of slots expected to be filled).

Information Extraction vs. Information Retrieval
IE systems are obviously more difficult and knowledge intensive to build and they are more computationally intensive than IR systems. Generally, IE systems achieve higher precision than IR systems. However, IE and IR techniques can be seen as complementary and can potentially be combined in various ways. For instance, IR could be embedded within IE for pre-processing a huge document collection into a manageable subset to which IE techniques could be applied. On the other side, IE can be used as a subcomponent of an IR system to identify terms for intelligent document indexing (e.g., conceptual indices). Such combinations clearly represent significant improvement in the retrieval of accurate and prompt business information. For example, Mihalcea and Moldovan (2001) introduced an approach for document indexing using named entities, which proved to reduce the number of retrieved documents by a factor of two, while still retrieving relevant documents.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

16 Abramowicz & Piskorski

Message Understanding Conferences
The rapid development of the field of IE has been essentially influenced by the Message Understanding Conferences (MUCs). These conferences were conducted under the auspices of several United States government agencies with the intention of coordinating multiple research groups and government agencies seeking to improve IE and IR technologies (Grishman & Sundheim, 1996). The MUCs defined several generic types of IE tasks. These were intended to be prototypes of IE tasks that arise in real-world applications, and they illustrate the main functional capabilities of current IE systems. The IE tasks defined in MUC competitions focused on extracting information from newswire articles (e.g., concerning terrorist events, international joint venture foundations and management succession). Altogether seven MUC competitions took place (1987-1998), where the participants were given the same training data for the adaptation of their systems to a given scenario. Analogously, the evaluation was performed using the same annotated test data. The generic IE tasks for MUC-7 (1998) were defined as follows: • Named Entity Recognition (NE) requires the identification and classification of named entities such as organizations, persons, locations, product names and temporal expressions. • Template Element Task (TE) requires the filling of small-scale templates for specified classes of entities in the texts, such as organizations, persons, certain artifacts with slots such as name variants, title, description as supplied in the text. • Template Relation Task (TR) requires filling a two-slot template representing a binary relation with pointers to template elements standing in the relation, which were previously identified in the TE task (e.g., an employee relation between a person and a company). • Co-Reference Resolution (CO) requires the identification of expressions in the text that refer to the same object, set or activity (e.g., variant forms of name expressions, definite noun phrases and their antecedents). • Scenario Template (ST) requires filling a template structure with extracted information involving several relations or events of interest, for instance, identification of partners, products, profits and capitalization of joint ventures. State-of-the-art results for IE tasks for English reported in MUC-7 are presented in Figure 2.

IE SYSTEMS IN THE BUSINESS DOMAIN
Early IE Systems
The earliest IE systems were deployed as commercial products already in the late eighties. One of the first attempts to apply IE in the financial field using templates was the ATRANS system (Lytinen & Gershman, 1986), based on simple language processing techniques and script-frames approach for extracting information from telex messages regarding money transfers between banks. JASPER (Andersen, Hayes, Heuttner, Schmandt, Nirenburg & Weinstein, 1992) is an IE system that extracts information from
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Information Extraction from Free-Text Business Documents

17

Figure 2. State-of-the-Art Results Reported in MUC-7

MEASURE\TASK RECALL PRECISION

NE 92 95

CO 56 69

RE 86 87

TR 67 86

ST 42 65

reports on corporate earnings from small sentence fragments using robust NLP methods. SCISOR (Jacobs & Rau, 1990) is an integrated system incorporating IE for extraction of facts related to the company and financial information. These early IE systems had a major shortcoming, namely they were not easily adaptable to new scenarios. On the other side, they demonstrated that relatively simple NLP techniques are sufficient for solving IE tasks narrow in scope and utility.

LOLITA
The LOLITA System (Costantino, Morgan, Collingham & Garigliano, 1997), developed at the University of Durham, was the first general purpose IE system with finegrained classification of predefined templates relevant to the financial domain. Further, it provides a user-friendly interface for defining new templates. LOLITA is based on deep natural language understanding and uses semantic networks. Different applications were built around its core. Among others, LOLITA was used for extracting information from financial news articles which represent an extremely wide domain, including different kinds of news (e.g., financial, economical, political, etc.). The templates have been defined according to the “financial activities” approach and can be used by the financial operators to support their decision-making process and to analyze the effect of news on price behavior. A financial activity is one potentially able to influence the decisions of the players in the market (brokers, investors, analysts, etc.). The system uses three main groups of templates for financial activities: company-related activities — related to the life of the company (e.g., ownership, shares, mergers, privatization, takeovers), company restructuring activities — related to changes in the productive structure of companies (e.g., new product, joint venture, staff change) and general macroeconomics activities — including general macroeconomics news that can affect the prices of the shares quoted in the stock exchange (e.g., interest rate movements, inflation, trade deficit). In the “takeover template” task, as defined in MUC-6, the system achieved precision of 63% and recall of 43%. However, since the system is based on DTP techniques, the performance in terms of speed can be, in particular situations, penalized in comparison to systems based on STP methods. The output of LOLITA was fed to the financial expert system (Costantino, 1999) to process an incoming stream of news from online news providers, companies and other structured numerical market data to produce investment suggestions.

MITA
IE technology has been successfully used recently in the insurance domain. MITA (Metalife’s Intelligent Text Analyzer) was developed in order to improve the insurance
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

18 Abramowicz & Piskorski

underwriting process (Glasgow, Mandell, Binney, Ghemri & Fisher, 1998). Metalife’s life insurance applications contain free-form textual fields (an average of 2.3 textual fields per application) such as: physician reason field — describing a reason a proposed insured last visited a personal physician, family history field — describing insured’s family medical history and major treatments and exams field — which describes any major medical event within the last five years. In order to identify any concepts from such textual fields that might have underwriting significance, the system applies STP techniques and returns a categorization of these concepts for risk assessment by subsequent domain-specific analyzers. For instance, MITA extracts a three-slot template from the family history field, consisting of the concept slot which describes a particular type of information that can be found, value slot for storing the actual word associated with a particular instance of the concept and the class slot representing the semantic class that the value denotes. The MITA system has been tested in a production environment and 89% of the information in the textual field was successfully analyzed. Further, a blind testing was undertaken to determine whether the output of MITA is sufficient to make underwriting decisions equivalent to those produced by an underwriter with access to the full text. Results showed that only up to 7% of the extractions resulted in different underwriting conclusions.

History Assistant
Jackson, Al-Kofahi, Kreilick and Grom (1998) present History Assistant — an information extraction and retrieval system for the juridical domain. It extracts rulings from electronically imported court opinions and retrieves relevant prior cases, and cases affected from a citator database, and links them to the current case. The role of a citator database enriched with such linking information is to track historical relations among cases. Online citators are of great interest to the legal profession because they provide a way of testing whether a case is still good law. History Assistant is based on DTP and uses context-free grammars for computing all possible parses of the sentence. The problem of identifying the prior case is a nontrivial task since citations for prior cases are usually not explicitly visible. History Assistant applies IE for producing structured information blocks, which are used for automatically generating SQL queries to search prior and affected cases in the citator database. Since information obtained by the IE module might be incomplete, additional domain-specific knowledge (e.g., court hierarchy) is used in cases when extracted information does not contain enough data to form a good query. The automatically generated SQL query returns a list of cases, which are then scored using additional criteria. The system achieved a recall of 93.3% in the prior case retrieval task (i.e., in 631 out of the 673 cases, the system found the prior case as a result of an automatically generated query).

Trends
The most recent approaches to IE concentrated on constructing general purpose, highly modular, robust, efficient and domain-adaptive IE systems. FASTUS (Hobbs, Appelt, Bear, Israel, Kameyama, Stickel & Tyson, 1997), built in the Artificial Intelligence Center of SRI International, is a very fast and robust general purpose IE system which
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Information Extraction from Free-Text Business Documents

19

deploys lightweight linguistic techniques. It is conceptually very simple, since it works essentially as a set of cascaded nondeterministic finite-state transducers. FASTUS was one of the best scoring systems in the MUCs and was used by a commercial client for discovering an ontology underlying complex Congressional bills, for ensuring the consistency of laws with the regulations that implement them. Humphreys, Gaizauskas, Azzam, Huyck, Mitchell, Cunningham and Wilks (1998) describe LaSIE-II, a highly flexible and modular IE system, which was an attempt to find a pragmatic middle way in the shallow versus deep analysis debate which characterized the last several MUCs. The result is an eclectic mixture of techniques ranging from finitestate recognition of domain-specific lexical patterns to using restricted context-free grammars for partial parsing. Its highly modular architecture enabled one to gain deeper insight into the strengths and weaknesses of the particular subcomponents and their interaction. Similarly to LaSIE-II, the two top requirements on the design of the IE2 system (Aone, Halverson, Hampton, Ramos-Santacruz & Hampton, 1999), developed at SRA International Inc., were modularity and flexibility. SGML was used to spell out system interface requirements between the sub-modules, which allow an easy replacement of any sub-module in the workflow. The IE2 system achieved the highest score in TE task (recall: 86%, precision 87%), TR task (recall: 67%, precision: 86%) and ST task (recall: 42%, precision: 65%) in the MUC-7 competition. REES (presented in Aone & Santacruz, 2000) was the first attempt to constructing a large-scale event and relation extraction system based on STP methods. It can extract more than 100 types of relations and events related to the area of business, finance and politics, which represents much wider coverage than is typical of IE systems. For 26 types of events related to finance, it achieved an F-measure of 70%.

BEYOND INFORMATION EXTRACTION
The last decade has witnessed great advances and interest in the area of information extraction using simple shallow processing methods. In the very recent period, new trends in information processing, from texts based on lightweight linguistic analysis closely related to IE, have emerged.

Textual Question Answering
Textual Question Answering (Q/A) aims at identifying the answer of a question in large collections of online documents, where the questions are formulated in natural language and the answers are presented in the form of highlighted pieces of text containing the desired information. The current Q/A approaches integrate existing IE and IR technologies. Knowledge extracted from documents may be modeled as a set of entities extracted from the text and relations between them and further used for conceptoriented indexing. Srihari and Li (1999) presented Textract — a Q/A system based on relatively simple IE techniques using NLP methods. This system extracts open-ended domain-independent general-event templates expressing the information like WHO did WHAT (to WHOM) WHEN and WHERE (in predicate-argument structure). Such information may refer to argument structures centering around the verb notions and associ-

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

20 Abramowicz & Piskorski

ated information of location and time. The results are stored in a database and used as a basis for question answering, summarization and intelligent browsing. Textract and other similar systems based on lightweight NLP techniques (Harabagiu, Pasca & Maiorano, 2000) achieved surprisingly good results in the competition of answering factbased questions in Text Retrieval Conference (TREC) (Voorhess, 1999).

Text Classification
The task of Text Classification (TC) is assigning one or more pre-defined categories from a closed set of such categories to each document in a collection. Traditional approaches in the area of TC use word-based techniques for fulfilling this task. Riloff and Lorenzen (1998) presented AutoSlog-TS, an unsupervised system that generates domain-specific extraction patterns, which was used for the automatic construction of a high-precision text categorization system. Autoslog-TS retrieves extraction patterns (with a single slot) representing local linguistic expressions that are slightly more sophisticated than keywords. Such patterns are not simply extracting adjacent words since extracting information depends on identifying local syntactic constructs (verb and its arguments). AutoSlog-TS takes as input only a collection of pre-classified texts associated with a given domain and uses simple STP techniques and simple statistical methods for automatic generation of extraction patterns for text classification. This new approach of integrating STP techniques in TC proved to outperform classification using word-based approaches. Further, similar unsupervised approaches (Yangarber, Grishman, Tapanainen & Huttunen, 2000) using light linguistic analysis were presented for the acquisition of lexico-syntactic patterns (syntactic normalization: transformation of clauses into common predicate-argument structure), and extracting scenario-specific terms and relations between them (Finkelstein-Landau & Morin, 1999), which shows an enormous potential of shallow processing techniques in the field of text mining.

Text Mining
Text mining (TM) combines the disciplines of data mining, information extraction, information retrieval, text categorization, probabilistic modeling, linear algebra, machine learning and computational linguistics to discover valid, implicit, previously unknown and comprehensible knowledge from unstructured textual data. Obviously, there is an overlap between text mining and information extraction, but in text mining the knowledge to be extracted is not necessarily known in advance. Rajman (1997) presents two examples of information that can be automatically extracted from text collections using simple shallow processing methods: probabilistic associations of keywords and prototypical document instances. Association extraction from the keyword sets allows the user to satisfy information needs expressed by queries like “find all associations between a set of companies including Siemens and Microsoft and any person.” Prototypical document instances may be used as representative of classes of repetitive document structures in the collection of texts and constitute good candidates for a partial synthesis of the information content hidden in a textual base. Text mining contributes to the discovery of information for business and also to the future of information services by mining large collections of text (Abramowicz & Zurada, 2001). It will become a central technology to many businesses branches, since companies and enterprises “don’t know what they don’t know” (Tkach, 1999).
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Information Extraction from Free-Text Business Documents

21

Figure 3. Application Potential of Shallow Text Processing
TEXT DOCUMENTS Concept indices, more accurate queries Tokens Clause structure

Building ontologies

Domain-specific patterns

Document Indexing/Retrieval

MULTI-AGENTS
Term association extraction

Shallow Text Processing Core Components
Phrases

Word Stems

Template generation

EXECUTIVE INFORMATION SYSTEMS

Text Mining

Information Extraction Question/Answering
Named Entities Fine-grained concept matching

Semi-structured data

E-COMMERCE
Automatic Database Construction

WORKFLOW MANAGEMENT

DATA-WAREHOUSING
Text Classification

SUMMARY
We have learned that IE technology based on lightweight linguistic analysis has been successfully used in various business applications dealing with processing huge collections of free-text documents. The diagram in Figure 3 reflects the enormous application potential of STP in various fields of information technology discussed in this chapter. STP can be considered as an automated generalized indexing procedure. The degree and amount of structured data an STP component is able to extract plays a crucial role for subsequent high-level processing of extracted data. In this way, STP offers distinct possibilities for increased productivity in workflow management (Abramowicz & Szymanski, 2002), e-commerce and data warehousing (Abramowicz, Kalczynski & Wecel, 2002). Potentially, solving a wide range of business tasks can be substantially improved by using information extraction. Therefore, an increased commercial exploitation of IE technology could be observed (e.g., Cymfony’s InfoXtract — IE engine, http:\\www.cymfony.com). The question of developing a text processing technology base that applies to many problems is still a major challenge of the current research. In particular, future research in this area will focus on multilinguality, cross-document event tracking, automated learning methods to acquire background knowledge, portability, greater ease of use and stronger integration of semantics.

REFERENCES
Abramowicz, W. & Szymanski, J. (2002). Workflow technology supporting information filtering from the Internet. Proceedings of IRMA 2002, Seattle, WA, USA. Abramowicz, W. & Zurada, J. (2001). Knowledge Discovery for Business Information Systems. Boston, MA: Kluwer Academic Publishers.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

22 Abramowicz & Piskorski

Abramowicz, W., Kalczynski, P. & Wecel, K. (2002). Filtering the Web to Feed Data Warehouses. London: Springer. Andersen, P.M., Hayes, P.J., Heuttner, A.K., Schmandt, L.M., Nirenburg, I.B. & Weinstein, S.P. (1992). Automatic extraction of facts from press releases to generate news stories. Proceedings of the 3rd Conference on Applied Natural Language Processing, Trento, Italy, 170-177. Aone, C. & Ramos-Santacruz, M. (2000). RESS: A large-scale relation and event extraction system. Proceedings of ANLP 2000, Seattle, WA, USA. Aone, C., Halverson, L., Hampton, T., Ramos-Santacruz, M. & Hampton, T. (1999). SRA: Description of the IE2 System Used for MUC-7. Morgan Kaufmann. Appelt, D. & Israel, D. (1999). An introduction to information extraction technology. Tutorial prepared for the IJCAI 1999 Conference. Chinchor, N.A. (1998). Overview of MUC7/MET-2. Proceedings of the Seventh Message Understanding Conference (MUC7). Costantino, M. (1999). IE-Expert: Integrating natural language processing and expert system techniques for real-time equity derivatives trading. Journal of Computational Intelligence in Finance, 7(2), 34-52. Costantino, M., Morgan, R.G., Collingham R.J. & Garigliano, R. (1997). Natural language processing and information extraction: Qualitative analysis of financial news articles. Proceedings of the Conference on Computational Intelligence for Financial Engineering 1997, New York. Finkelstein-Landau, M. & Morin, E. (1999). Extracting semantic relationships between terms: Supervised vs. unsupervised methods. Proceedings of International Workshop on Ontological Engineering on the Global Information Infrastructure, Dagstuhl Castle, Germany, May, 71-80. Glasgow, B., Mandell, A., Binney, D., Ghemri, L. & Fisher, D. (1998). MITA: An information-extraction approach to the analysis of free-form text in life insurance applications. AI Magazine, 19(1), 59-71. Grishman, R. & Sundheim, B. (1996). Message Understanding Conference–6: A brief history. Proceedings of the 16th International Conference on Computational Linguistics (COLING), Kopenhagen, Denmark, 466-471. Harabagiu, S., Pasca, M. & Maiorano, S. (2000). Experiments with open-domain textual question answering. Proceedings of the COLING-2000 Conference. Hobbs, J., Appelt, D., Bear, J., Israel, D., Kameyama, M., Stickel, M. & Tyson, M. (1997). FASTUS—A cascaded finite-state transducer for extracting information from natural language text. Chapter 13 in Roche, E. & Schabes, Y. (1997). Finite-State Language Processing. Cambridge, MA: MIT Press. Humphreys, K., Gaizauskas, R., Azzam, S., Huyck, C., Mitchell, B., Cunningham, H. & Wilks, Y. (1998). University of Sheffield: Description of the LaSIE-II system as used for MUC-7. Proceedings of the Seventh Message Understanding Conference (MUC-7). Jackson, P., Al-Kofahi, K., Kreilick, C. & Grom, B. (1998). Information extraction from case law and retrieval of prior cases by partial parsing and query generation. Proceedings of the ACM 7th International Conference on Information and Knowledge Management, Washington, DC, USA, 60-67. Jacobs, P. & Rau, L. (1990). SCISOR: Extracting information from online news. Communications of the ACM, 33(11), 88-97.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Information Extraction from Free-Text Business Documents

23

Lytinen, S. & Gershman, A. (1986). ATRANS: Automatic processing of money transfer messages. Proceedings of the 5th National Conference of the American Association for Artificial Intelligence. IEEE Computer Society Press (1993), 93-99. Mihalcea, R. & Moldovan, D. (2001). Document indexing using named entities. Studies in Informatics and Control Journal, 10(1). Piskorski, J. & Skut, W. (2000). Intelligent information extraction. Proceedings of Business Information Systems 2000, Poznan, Poland. Rajman, M. (1997). Text mining, knowledge extraction from unstructured textual data. Proceedings of EUROSTAT Conference, Frankfurt, Germany. Riloff, E. & Lorenzen, J. (1998). Extraction-based text categorization: Generating domainspecific role relationships automatically. In Strzalkowski, T. (Ed.), Natural Language Information Retrieval. Kluwer Academic Publishers. SAIC. (1998). Seventh Message Understanding Conference (MUC-7). Available online at: http://www.muc.saic.com. Srihari, R. & Li, W. (1999). Information extraction-supported question answering. Proceedings of the Eighth Text Retrieval Conference (TREC-8). Tkach, D. (1999). The pillars of knowledge management. Knowledge Management, 2(3), 47. Voorhess, E. & Tice, D. (1999). The TREC-8 Question Answering Track Evaluation. Gaithersburg, MD: National Institute of Standards and Technology. Yangarber, R., Grishman, R., Tapanainen, P. & Huttunen, S. (2000). Unsupervised discovery of scenario-level patterns for information extraction. Proceedings of the Conference on Applied Natural Language Processing ANLP-NAACL 2000, Seattle, WA, USA, May.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

24 Schiel & de Sousa

Chapter III

Interactive Indexing of Documents with a Multilingual Thesaurus
Ulrich Schiel Universidade Federal de Campina Grande, Brazil Ianna M.S.F. de Sousa Universidade Federal de Campina Grande, Brazil

ABSTRACT
With the growing significance of digital libraries and the Internet, more and more electronic texts become accessible to a wide and geographically disperse public. This requires adequate tools to facilitate indexing, storage and retrieval of documents written in different languages. We present a method for semi-automatic indexing of electronic documents and construction of a multilingual thesaurus, which can be used for query formulation and information retrieval. We use special dictionaries and user interaction in order to solve ambiguities and find adequate canonical terms in the language and an adequate abstract language-independent term. The abstract thesaurus is updated incrementally by new indexed documents and is used to search for documents using adequate terms.

INTRODUCTION
The growing relevance of digital libraries is generally recognized (Haddouti, 1997). A digital library typically contains hundreds or thousands of documents. It is also

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Interactive Indexing of Documents with a Multilingual Thesaurus 25

recognized that, even though English is the dominant language, documents in other languages are of great significance and, moreover, users want to retrieve documents in several languages associated to a topic, stated in their own language (Haddouti, 1997; Go02). This is especially true in regions such as the European Community or Asia. Therefore a multilingual environment is needed to attend user requests to digital libraries. The multilingual communication between users and the library can be realized in two ways: • The user query is translated to the several languages of existing documents and submitted to the library. • The documents are indexed and the extracted terms are converted to a languageneutral thesaurus (called multilingual thesaurus). The same occurs with the query, and the correspondence between query terms and documents is obtained via the neutral thesaurus. The first solution is the most widely used in the Cross-Language Information Retrieval (CLIR) community (Go02; Ogden & Davis, 2000; Oard, 1999). It applies also to other information retrieval environments, such as the World Wide Web. For digital libraries, with thousands of documents, indexing of incoming documents and a good association structure between index terms and documents can become crucial for efficient document retrieval. In order to get an extensive and precise retrieval of textual information, a correct and consistent analysis of incoming documents is necessary. The most broadly used technique for this analysis is indexing. An index file becomes an intermediate representation between a query and the document base. One of the most popular structures for complex indexes is a semantic net of lexical terms of a language, called thesaurus. The nodes are single or composed terms, and the links are pre-defined semantic relationships between these terms, such as synonyms, hyponyms and metonyms. Despite that the importance of multilingual thesauri has been recognized (Go02), nearly all research effort in Cross-Lingual Information Retrieval has been done on the query side and not on the indexing of incoming documents (Ogden & Davis, 2000; Oard, 1999; Haddouti, 1997). Indexing in a multilingual environment can be divided in three steps: 1. language-dependent canonical term extraction (including stop-word elimination, stemming, word-sense disambiguation); 2. language-neutral term finding; and 3. update of the term-document association lattice. Bruandet (1989) has developed an automatic indexing technique for electronic documents, which was extended by Gammoudi (1993) to optimal thesaurus generation for a given set of documents. The nodes of the thesaurus are bipartite rectangles where the left side contains a set of terms and the right side the set of documents indexed by the terms. Higher rectangles in the thesaurus contain broader term sets and fewer documents. One extension to this technique is the algorithm of Pinto (1997), which permits an incremental addition of index terms of new incoming documents, updating the thesaurus.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

26 Schiel & de Sousa

We show in this chapter how this extended version of Gammoudi’s technique can be used in an environment with multilingual documents and queries whose language need not be the same as that of the searched documents. The main idea is to use monolingual dictionaries in order to, with the user’s help, eliminate ambiguities, and associate to each unambiguous term an abstract, language-independent term. The terms of a query are also converted to abstract terms in order to find the corresponding documents. Next we introduce the mathematical background needed to understand the technique, whereas the following section introduces our multilingual rectangular thesaurus. Then, the chapter shows the procedure of term extraction from documents, finding the abstract concept and the term-document association and inclusion of the new rectangle in the existing rectangular thesaurus. We then show the query and retrieval environment and, finally, discuss some related work and conclude the chapter.

RECTANGULAR THESAURUS: BASIC CONCEPTS
The main structure used for the indexing of documents is the binary relation. A binary relation can be decomposed in a minimal set of optimal rectangles by the method of Rectangular Decomposition of a Binary Relation (Gammoudi, 1993; Belkhiter, Bourhfir, Gammoudi, Jaoua, Le Thanh & Reguig, 1994). The extraction of rectangles from a finite binary relation has been extensively studied in the context of Lattice Theory and has proven to be very useful in many computer science applications. A rectangle of a binary relation R is a pair of sets (A, B) such that A B R. More precisely:

Definition 1: Rectangle
Let R be a binary relation defined from E to F. A rectangle of R is a pair of sets (A,B) such that A E, B F and A x B R. A is the domain of the rectangle where B is the codomain. A rectangle (A,B) of a relation R is maximal if, for each rectangle (A’,B’): AxB A’ x B’ R A = A’ e B= B’.

A binary relation can always be represented by sets of rectangles, but this representation is not unique. In order to gain storage space, the following coefficient is important for the selection of an optimal set of rectangles representing the relation.

Definition 2: Gain in Storage Space
The gain in storage space of a rectangle RE=(A,B) is given by: g(RE)= [Card(A) x Card (B)] - [Card (A) + Card(B)] where Card(A) is the cardinality of the set A.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Interactive Indexing of Documents with a Multilingual Thesaurus 27

The gain becomes significant if Card(A) > 2 and Card(B) > 2, then g(RE) > 0 and grows with Card(A) and Card(B). On the other hand, there is no gain (g(GE)<0) if Card(A) =1 or Card(B) =1. The notion of gain is very important because it allows us to save memory space, in particular when we need to store a large volume of terms.

Definition 3: Optimal Rectangle
A maximal rectangle containing an element (x, y) of a relation R is called optimal if it produces a maximal gain with respect to other maximal rectangles containing (x, y). Figure 1(a) presents an example of a relation R, and Figures 1(b), 1(c) and 1(d) represent three maximal rectangles containing the element (y,3). The corresponding gains are 1, 0 e -1. Therefore, the optimal rectangle containing (y,3) of R is the rectangle of Figure 1(b).

Definition 4: Rectangular Graph
Let “ follows: ” be a relation defined over a set of rectangles of a binary relation R, as

(A1,B1 ) e (A2, B2 ) two rectangles of R: (A1,B 1 ) (A 2, B2 ) A1 A2 and B2 B1. We call (R, Note that “ ) a Rectangular Graph. ” defines a partial order over the set of rectangles.

Definition 5: Lattice
A partially ordered set (R, <) is called a lattice if each subset X upper bound and a maximal lower bound. R has a minimal

Proposition 1:
“ Let R E G be a binary relation. The set of optimal rectangles of R, ordered by ” (R, ), is a lattice with a lower bound ( , G) and an upper bound (E, ).

Semantic Relations. Semantic relations are the basis for the transformation of a rectangular graph in a rectangular thesaurus. They connect the nodes of a rectangular Figure 1. Finding the Optimal Rectangle
(a) Rectangle R
x y z 1 2 3 4 5
x y 1 2 3 y z 3 5

(b) RE1(y, 3)

(c) RE2(y, 3)

(d) RE3(y, 3)
1 2 y 3 4 5

g( RE1(y, 3) ) = 1

g( RE2(y, 3) ) = 0

g( RE3(y, 3) ) = --1

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

28 Schiel & de Sousa

thesaurus in order to find more generic, more specific or other related rectangles. There are three kinds of semantic relations in a rectangular thesaurus: hierarchical relations (generic and specific), equivalence relations (synonyms and pseudo-synonyms) and neighborhood relations. The hierarchical relations are based on the following definition:

Definition 6: Hierarchic Relation
Let RE i = (Ai, Bi ) and Re = (Aj, B j ) be two optimal rectangles of a relation R. RE i is a generic of REj if: (Ai, Bi ) (Aj, B j )

If the domains are terms and the co-domains are document identifiers, then Ai indexes more documents than A j. The rectangles of interest in this chapter will all be of this format, i.e., relating terms to documents. Each term in a rectangle is the representative of an equivalence relation of synonyms. The (representative) terms in the rectangle are called pseudo-synonyms. Note that two terms are pseudo-synonyms if they index the same set of documents. Non-hierarchic relations between rectangles occur when two rectangles have some information in common, but none of them is a generic of the other (Gammoudi, 1993).

Definition 8: Non-Hierarchic Relation
Let REi = (Ai, Bi ) and REj = (Aj, Bj ) be two optimal rectangles of R. REi is a neighbor RE j if and only if the following conditions hold: Ai Aj or Bi Bj and

(Ai, Bi ) << (Aj, B j ) and (A j, B j) << (Ai, B i ) Figure 2 shows two neighbor rectangles. The central structure, associating index terms to documents of a digital library, is a language-independent thesaurus of bipartite rectangles, containing the relation between sets of index terms and of sets of documents. Each term in a rectangle is the representative of an equivalence relation of synonyms. The terms in the rectangle are called pseudo-synonyms. Note that two terms are pseudo-synonyms if they index the same set of documents. For a lattice of rectangles with the hierarchic relation (A1, B1) (A2, B2), defined above, we consider a simplified representation, in order to save space. We eliminate the Figure 2. Neighbor Rectangles

information document

information retrieval Intersection = {information}

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Interactive Indexing of Documents with a Multilingual Thesaurus 29

Figure 3. Example Lattice of Rectangles — Full and Simplified Version
R8 System Information Development D1 R8 D1

R4

System Development

D1 D4

R4 Development

D4

R5

System Information

D1 D2

R5

Information

D2

R2 System

D1 D2 D3, D4

R2 System D3

repetition of terms in the hierarchy. If (A1, B1) (A 2, B 2), the rectangle (A2, B2) is represented by (A2-A1 , B1-B2), without loss of information content. Figure 3 illustrates a lattice of four rectangles, disrespecting language independence at this moment, in its full and its simplified versions. Figure 4 shows a relation between synonyms and pseudo-synonyms. The terms Information, Retrieval and Document are pseudo-synonyms representing: fact and data, search and query, and report, formulary and article, respectively. Finally we define a Rectangular Thesaurus as a graph where the nodes are optimal rectangles and the edges are the semantic relations defined above. With the hierarchic relation <<, the graph forms a lattice.

MULTILINGUAL RECTANGULAR THESAURUS
The idea of creating a language-independent conceptual thesaurus has been defined by Sosoaga (1991). The textual structure of specific languages is mapped to a conceptual thesaurus (see Figure 5). One problem of this mapping is the elimination of multiple senses of terms. For instance, the term ‘word’ has 10 distinct meanings in the WordNet (Princeton University, n.d.) system. In general, each meaning occurs in a different context of using the word. Figure 4. Example of Synonyms and Pseudo-Synonyms
fact data
information

search query

recuperation

document

report formulary article

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

30 Schiel & de Sousa

Figure 5. Multilingual Thesaurus
Lexical structure of language 1 Conceptual Thesaurus Lexical structure of language 2

We extend the definition of multilingual thesaurus of Sosoaga to include a notion of contexts that permits the elimination of ambiguities. A multilingual thesaurus is a classification system defined as: MTh = (V, n, r; L 1, ..., Ln, C1, ..., Cm , t 1, ..., tn*m) composed of a unique set of abstract concepts (V), a set of lexicons {L1,…, Ln}, a set of contexts {C 1, ..., Ck} a set of functions tk: Li Cj V which associates to each term of the lexicon in a given context a unique abstract term. The hierarchic and non-hierarchic relationships are given by n (narrower term) and r (related term). Therefore, both n and r are subsets of V V. A rectangular multilingual thesaurus is a rectangular thesaurus as defined in the previous section, where the terms at the left part of each rectangle are elements of the abstract concepts (V) of a multilingual thesaurus, and the right side are identifiers of documents. Note that the way to obtain the rectangular multilingual thesaurus of a set of documents is: (1) indexing each document, (2) defining the correct meaning of each term and (3) finding the abstract concept applying the corresponding function t. In order to construct rectangles with representative terms, we can decompose each function t in two parts, t0 and t 1, with t(x, c) = t 1(t 0(x, c)). The function t 0 is responsible for the selection of the canonical representative for the synonyms in a given context, and t 1 is an injective function that determines the abstract term associated to the original term in a given language.

CONSTRUCTION AND MAINTENANCE OF A RECTANGULAR MULTILINGUAL THESAURUS
In a digital library each incoming document (which can be a full document, an abstract or an index card) must be indexed and integrated in the library. Instead of indexing documents one by one, we can consider many incoming documents to be inserted in the library.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Interactive Indexing of Documents with a Multilingual Thesaurus 31

Figure 6: Semi-Automatic Indexing
DOCUMENT

Indexing

RECTANGLE

Updating

THESAURUS

DICTIONARY

NEW WORDS

context classification

WordNet

librarian

The construction of a Rectangular Multilingual Thesaurus is completed in three steps: 1. term extraction from one or more documents and determination of the abstract concept, using a monolingual dictionary (semi-automatic indexing); 2. generation of one or more optimal rectangles; 3. optimal insertion of the new rectangles in the existing abstract thesaurus. Figure 6 shows the main modules of the system called SIM-System for Indexing of Multilingual Documents.

Indexing, Disambiguation and Abstraction
The construction of a rectangular thesaurus referencing a set of electronic documents in natural language begins with relevant term extraction contained in the document. Our semi-automatic method allows the user to eliminate ambiguities interactively. In the current version of the system, the format of an incoming document can be pure text (txt), Microsoft Word documents (doc) or html. Other formats must be converted to pure text. The first step consists of words selection, stopword elimination and, for significant words, finding of the abstract term. As shown in Figure 7, two dictionaries are used for this step. First, the dictionary of term variations contains all lexical variations of words in a language and determines its normal form. Compound words, such as ‘Data Base’ or ‘Operating System’ must be considered as single terms, since in other languages they are written as single words (e.g., ‘Datenbank’ and ‘Betriebssystem’ in German) and should be associated to a single abstract term code. These are identified by a look-ahead step when the dictionary identifies a candidate compound word. Having found the normal form of a term, the main dictionary is then used to find the abstract language-independent term, depending on the context. In the main dictionary, the column “Representative” and the list of “Related Terms” will be used in the construction of the thesaurus in a given language for query formulation (see “Information Retrieval” section).

Updating the Rectangular Thesaurus
Each rectangle obtained in the previous step relates a set of terms to a set of documents. If we are processing a single document, one rectangle is generated with the

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

32 Schiel & de Sousa

Figure 7. Dictionaries
Term Data Base Database Compound Term 1

Term Data Base Data Data Date

Category noun noun noun noun

Context C. Science C.Science Ling. History

Concept 10125 10230

Represent. Database Information Data Age

Related

0 0 0

significant terms indexing that document. We must now insert the new rectangles in the existing abstract rectangular thesaurus. Figure 8 shows the rectangular thesaurus for a document base, where the abstract terms of the domains of the rectangles has been exchanged by its representatives in English. Since it is in the simplified form, term redundancy has been eliminated. Note that in a rectangular thesaurus, we can identify several cardinality levels, due to the cardinality of the rectangle domain. This level goes from 0 to n, where n is the total number terms of the thesaurus. Each new rectangle must be placed in the corresponding level. In the example, the second level has cardinality 5 and the third level has cardinality 12, since seven terms have been added to the level below. The following algorithm, from Pinto (1997), provides the insertion of a new rectangle in an existing rectangular thesaurus. We consider the thesaurus in its original definition, without simplification. The simplification is straightforward. Figure 8: Document Thesaurus
C

Structured

11

Distributed

2, 11

Relational, client-server

11, 12

Object-oriented, library, programming, Database, Software, development, inheritance

2, 11, 12

Tool, object, class, model, concept, system

2, 11, 12, 14

D

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Interactive Indexing of Documents with a Multilingual Thesaurus 33

1.

2. 3. 4.

Check if the cardinality level of the new rectangle exists in the thesaurus 1.1. If it does not exist, create the new level for the rectangle 1.2. else 1.2.1. If the domain of the new rectangle coincides with an existing rectangle, then add the new document to the co-domain of the existing rectangle else insert the new rectangle in the level If a new rectangle has been created, establish the hierarchic connections, searching for the higher level rectangles containing the new terms New terms not occurring in the supremum, are inserted there If the descendants of the new rectangle are empty, connect it to the infimum

INFORMATION RETRIEVAL
The purpose of an information retrieval system is to return to the user a set of documents that match the keywords expressed in a query. In our system we present an interface, using the user’s preferred language, representing the thesaurus of concepts occurring in the document database. This thesaurus includes synonyms, related terms and hierarchic relations. Using pre-existing terms obtained from the documents in the library helps users to formulate a query in an adequate terminology, reducing significantly natural language interpretation problems. The dictionary-like form of the interface guarantees fast access to the documents searched for. As it was reported by LYCOS, typical user queries are only two or three words long. Figure 9 shows the prototype’s interface (Sodré, 1998) with terms defined in Portuguese. In a rectangular thesaurus, the retrieval process consists of finding a rectangle Ri = Ci x Di, such that Ci is a minimal domain containing the set of terms from the query Q. If Ci Q the user can receive feedback from the system concerning other terms which Figure 9. Query Interface

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

34 Schiel & de Sousa

index the retrieved documents. This fact is identified as a Galois connection in Gammoudi (1993). Note that we can obtain several rectangles matching the query. On the other hand, the user can eliminate some terms from the query in order to obtain more documents. As can be seen in the figure, the prototype allows one to choose a language and, as he/she is selecting the terms, the system lists the corresponding documents. The prototype has been implemented in the Delphi Programming Environment and, in its first release, recognizes Word (.doc), html and text (.txt) documents.

RELATED WORK
The model HiMeD (Ribeiro-Neto, Laender & Lima, 2001; Lima, Laender & RibeiroNeto, 1998) deals with the indexing and retrieval of documents. It is specialized on medical documents and the indexing step is completely automatic. Since the domain is restricted, the occurrence of polysemic terms is not as frequent as for general digital libraries. As with language-neutral ontology of medical terms, they use the medical vocabulary MeSH (NLM, 2000) combined with a generic dictionary. Gilarranz, Gonzalo and Verdejo (1997) proposed an approach of indexing documents using the information stored in the EuroWordNet database. From this database they take the language-neutral InterLingual Index. For the association of queries to documents, they use the well-known vectorial approach. The MULINEX project (Erbach, Newmann & Uszkoreit, 1997) is a European Union effort to develop tools to allow cross-language text retrieval for the WWW, conceptbased indexing, navigation tools and facilities for multilingual WWW sites. The project considers several alternatives for document treatment, such as translation of the documents, translation of index terms and queries, relevance feedback with translation.

CONCLUSION
Most work on thesaurus construction uses automatic indexing of a given set of documents (Bruandet, 1989; Gammoudi, 1993) and, in the case of a multilingual framework, uses machine translation techniques applied on the query (Yang, Carbonell, Brown & Frederking, 1998). In Pinto (1997) an incremental version of the approach on automatic generation of rectangular thesauri has been developed. The main contribution of this chapter is to integrate the incremental approach with a dictionary-based multilingual indexing and information retrieval, including an interactive ambiguity resolution. This approach eliminates problems of automatic indexing, linguistic variations of a single concept and restrictions of monolingual systems. Furthermore the problem of terms composed of more than one word has been solved with a look-ahead algorithm for candidate words found in the dictionary. It is clear that the interaction with the user is very time consuming. But, it seems to be a good trade-off between the precision of manual thesaurus construction and the efficiency of automatic systems. With an ‘apply to all’ option, one can avoid repetition of the same conflict resolution. Lexical databases, such as WordNet and the forthcoming EuroWordNet, can be useful to offer a semantic richer query interface using the hierarchic relations between terms. These hierarchies must also be included in the abstract conceptual thesaurus.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Interactive Indexing of Documents with a Multilingual Thesaurus 35

REFERENCES
Belkhiter, N., Bourhfir, C., Gammoudi, M.M., Jaoua, A., Le Thanh, N. & Reguig, M. (1994). Décomposition rectangulaire optimale d’une relation binaire: Aplication aux bases de données documentaires. INFOR, 32(1), 33-54. Bruandet, M.-F. (1989). Outline of a knowledge-base model for an intelligent information retrieval system. Information Processing & Management, 25(1), 89-115. Erbach, G., Neumann, G. & Uszkoreit, H. (1997). MULINEX multilingual indexing, navigation and editing extensions for the World Wide Web. Proceedings of the Third DELOS Workshop—Cross-Language Information Retrieval and Proceedings, Zurich, Switzerland. Gammoudi, M.M. (1993). Méthode de Décomposition Rectangulaire d’une Relation Binaire: Une Base Formelle et Uniforme pour la Génération Automatique des Thesaurus et la Recherche Documentaire. Thése de Doctorat, Université de Nice - Sophia Antipolis Ecole Doctorale des Sciences pour L’Ingenieur. Gilarranz, J., Gonzalo, J. & Verdejo, F. (1997). An approach to conceptual text retrieval using the EuroWordNet Multilingual Semantic Database. Working Notes of the AAAI Symposium on Cross Language Text and Speech Retrieval. Haddouti, H. (1997). Survey: Multilingual text retrieval and access. Working Notes of the AAAI Symposium on Cross Language Text and Speech Retrieval. Lima, L.R.S., Laender, A.F. & Ribeiro-Neto, B. (1998). A hierarchical approach to the automatic categorization of medical documents. Proceedings of the 7th International Conference on Information Knowledge Management, 132-138. NLM. (2000). Tree Structures & Alphabetic List—12th Edition. National Library of Medicine. Oard, D. (1999). Global access to multilingual information. Keynote Address at the Fourth International Workshop on Information Retrieval with Asian Languages-IRAL99, Taipei, Taiwan. Ogden, W. & Davis, M. (2000). Improving cross-language text retrieval with human interaction. Proceedings of the 33rd Hawaii International Conference on System Sciences, Maui, Hawaii, USA. Pinto, W.S. (1997). Sistema de Recuperação de Informação com Navegação Através de Pseudo Thesaurus. Master’s Thesis, Universidade Federal do Maranhão. Princeton University. (n.d.). WordNet—A Lexical Database for the English Language. Available online at: http://www.cogsci.princeton.edu/~wn/. Ribeiro-Neto, B., Laender, A.F. & Lima, R.S. (2001). An experimental study in automatically categorizing medical documents. Journal of the American Society for Information Science and Technology, 391-401. Sodré, I.M. (1998). SISMULT—Sistema de Indexação Semiautomática Multilíngüe. Master’s Thesis, Universidade Federal da Paraíba/COPIN, Campina Grande. Sosoaga, C.L. (1991). Multilingual access to documentary databases. In Lichnerowicz, A. (Ed.), Proceedings of the Conference on Intelligent Text and Image Handling (RIAO91), Amsterdam, April, 774-778. Yang, Y., Carbonell, J., Brown, R. & Frederking, R. (1998). Translingual information retrieval: Learning from bilingual corpora. Artificial Intelligence, 103, 323-345.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

36 Millet

Chapter IV

Managing Document Taxonomies in Relational Databases
Ido Millet Penn State Erie, USA

ABSTRACT
This chapter addresses the challenge of applying relational database technologies to manage taxonomies, which are commonly used to classify documents, knowledge and websites into a hierarchy of topics. It first describes how denormalizing the data model can facilitate data retrieval from such topic hierarchies. It then shows how the typical data maintenance difficulties associated with denormalized data models can be solved using database triggers.

INTRODUCTION
The need to maintain classification and retrieval mechanisms that rely on concept hierarchies is as old as language itself. Familiar examples include the Dewey decimal classification system used in libraries and the system for classifying life forms developed in the 1700s by Carolus Linnaeus. A more recent example is Yahoo’s subject taxonomy. Information technology has led to an explosive growth in digital documents, records, multi-media files and websites. To facilitate end-user access to these resources, topic hierarchies are frequently maintained to allow intuitive navigation and searching for resources related to specific categories. This chapter deals with the challenges of using relational database technology to maintain and facilitate queries against such topic hierarchies.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Managing Document Taxonomies in Relational Databases

37

In a chapter written for another book (Millet, 2001), I discuss the generic issue of managing hierarchies in relational databases. This chapter focuses on applying these techniques to the specific needs of managing document, website and knowledge management taxonomies. For example, this domain is complicated by the typical need to classify a single document or website under multiple topics. Relational databases and the current SQL standard are poorly suited to retrieval of hierarchical data. After demonstrating the problem, this chapter describes how two approaches to data denormalization can facilitate hierarchical data retrieval. Both approaches solve the problem of data retrieval, but as expected, come at the cost of difficult and potentially inconsistent data updates. This chapter then describes how we can address these update-related shortcomings via database triggers. Using a combination of denormalized data structure and triggers, we can have the best of both worlds: easy data retrieval and simple, consistent data updates.

THE CHALLENGE
To demonstrate the data retrieval difficulties associated with topic hierarchies, consider a document database where each document is classified into a hierarchy of topics shown in Figure 1. First, let us discuss how the topic hierarchy itself is stored in a relational database. Since each subtopic has at most one parent topic, we can implement this hierarchy via a recursive relationship. This means that each topic record maintains a foreign key pointing to the topic record above it. Figure 2 shows a data model for this situation. Note that the classify table allows us to assign a single document to multiple topics. To demonstrate the difficulty of hierarchical data retrieval against the normalized data model in Figure 2, consider the following requests: • Show a list of all Topics (at all levels) under Topic 1 • Show a list of all Documents (at all levels) under Topic 1 • Show how many Documents (at all levels) are classified under each Topic at Level 1 of the hierarchy Figure 1. A Topic Hierarchy
Taxonomy

Topic 1

Topic 2

Topic 3

Topic 1.1

Topic 1.2

Topic 3.1

Topic 3.2

Topic 1.2.1

Topic 1.2.2

Topic 1.2.2.1

Topic 1.2.2.2

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

38 Millet

Figure 2. A Normalized Data Model with a Topic Hierarchy

Topic
Topic_ID Name Level Topic_Above

Classify
Topic_ID Doc_ID

Document
Doc_ID Name Date_Created Owner

Using SQL, we can easily join each topic to all the documents associated with it via the classify records. However, we cannot easily identify the documents indirectly associated with a topic via its subtopics (at all levels). This is because for each topic we know only its immediate parent topic. This difficulty in locating parent or child nodes at any given level is at the heart of the problem.

SQL-BASED SOLUTIONS
While hierarchies pose a significant challenge, complex SQL can solve surprisingly tough problems. For example, the SQL statement in Listing 1 will return all topics under (and including) Topic 1. Listing 1. Hierarchy Retrieval with UNION Statements SELECT topic_id, name FROM TOPIC WHERE topic_id = 1 UNION ALL SELECT topic_id, name FROM TOPIC WHERE topic_above = 1 UNION ALL SELECT topic_id, name FROM TOPIC WHERE topic_above IN (SELECT topic_id FROM TOPIC WHERE topic_above = 1) UNION ALL SELECT topic_id, name FROM TOPIC WHERE topic_above IN (SELECT topic_id FROM TOPIC WHERE topic_above IN (SELECT topic_id FROM TOPIC WHERE topic_above = 1)) ; The result set, shown in Table 1, can then be established as a view and joined with the classify and document tables to answer more complex queries. The main limitation of this approach is the complexity of the required SQL statement, the dependence on a known top node and the need to extend the statement to the maximum number of possible levels in the hierarchy. The SQL:1999 standard (ANSI/ISO/IEC 9075-2-1999) removes the need to know how many levels the hierarchy may have by supporting recursive queries. For example, the request to show how many documents belong to each main topic, including all subtopics below it, can be handled using the SQL:1999 query shown in Listing 2. This query starts by creating a table expression (TOPIC_PATHS) populated with all main topic records as parents of themselves and appends (UNION) records for all paths of length one from these nodes to the topics directly below them. The RECURSIVE
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Managing Document Taxonomies in Relational Databases

39

Table 1. Result Set Produced by the UNION Statement

Topic 1 1.1 1.2 1.1.1 1.1.2 1.2.1 1.2.2

Topic_Below Topic 1 Topic 1.1 Topic 1.2 Topic 1.1.1 Topic 1.1.2 Topic 1.2.1

Topic 1.2.2 1.2.2.1 Topic 1.2.2.1 1.2.2.2 Topic 1.2.2.2
option continues the process to build all indirect paths from each topic to all its descendants. Listing 2. Recursive Hierarchy Retrieval Using SQL:1999 WITH RECURSIVE TOPIC_PATHS (topic_above, topic_below, level) AS (SELECT topic_id, topic_id, level FROM TOPIC UNION ALL SELECT TOPIC_PATHS.topic_above, TOPIC.topic_id, TOPIC_PATHS.level FROM TOPIC_PATHS, TOPIC WHERE TOPIC_PATHS.topic_below = TOPIC.topic_above) SELECT TOPIC_PATHS.topic_above, DistinctCount (CLASSIFY.Doc_ID) FROM TOPIC_PATHS, CLASSIFY WHERE TOPIC_PATHS.topic_below = CLASSIFY.Topic_ID AND TOPIC_PATHS.level = 1 GROUP BY TOPIC_PATHS.topic_above; The query then joins the end points (Topic_Below) of all paths in the TOPIC_PATHS result set to the documents assigned to these topics. By limiting the start points of these paths to main topics (topics at Level 1) and grouping the end result by those topics, we get the requested information. It is important to note that since each document may be assigned to multiple topics, we must guard against counting the same document multiple times. For example, if one of the documents has been assigned to both Topic 1.1 and Topic 1.2.2, we probably want to include it only once in the total count of documents classified under Topic 1. This is achieved by using DistinctCount(Classify.Doc_ID) rather than Count(Classify.Doc_ID). Again, relying on such complex SQL is beyond the reach of many IT professionals and most end-user reporting tools. This issue can be addressed by implementing the complex portion of these SQL statements as database views. However, another limitation
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

40 Millet

that cannot be addressed via views is that running such queries or views can be too slow in reporting applications with large hierarchies and frequent queries. Celko (2000) reports on a technique leading to significant improvements in query speeds by storing the hierarchy data not as parent-child relationships but as “nested sets” using a somewhat complex numbering scheme. However, this chapter focuses on another approach that can achieve very significant query performance gains while maintaining intuitive data storage and SQL syntax. The following section describes two data denormalization approaches that can support high performance data retrieval against topic hierarchies with simple and intuitive SQL queries. The first approach relies on a Path Table capturing all ancestordescendant relations in the topic hierarchy. The second approach relies on maintaining the complete ancestry information in columns within each topic record. Though they simplify and accelerate data retrieval, both approaches carry the burden of redundant data and potential update anomalies. These limitations are addressed later in the chapter.

THE PATH TABLE APPROACH
The Path Table approach uses a “navigation bridge table” (Kimball et al., 1998) with records enumerating all paths starting from each node to all nodes in the branch above it, including itself. As demonstrated by Table 2, Topic 1.1.1 would require four records in the Path Table reflecting the paths up to itself, Topic 1.1, Topic 1 and Topic 0 (the top node of the hierarchy). These are just four of the 37 records required to capture all paths for the sample hierarchy in Figure 1. To demonstrate how the Path Table can simplify data retrieval, consider the same challenge of showing how many documents belong to each main topic, including all subtopics below it. By joining the tables as shown in Figure 3, we can easily select all documents that belong under each Level-1 topic. Since the Path Table includes a zerolength path between each topic and itself, documents that belong directly to Level-1 topics would be included in the result set. The relatively simple SQL statement in Listing 3 will return the requested information. Other requests for information can use the same approach or variations such as connecting to the Topic table via the Topic_ID column in the Path Table or adding path length and terminal node information to the Path Table (Kimbal et al., 1998).

Table 2. A Path Table for the Sample Hierarchy

Topic_ID Topic_Above 1.1.1 1.1.1 1.1.1 1.1.1 1.1.1 1.1 1 0

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Managing Document Taxonomies in Relational Databases

41

Figure 3. A Path Table Connects Each Classification with all its Parents

Topic
Topic_ID Name Level Topic_Above

PATH
topic_id topic_above

Classify
Topic_ID Doc_ID

Document
Doc_ID Name Date_Created Owner

Listing 3. Retrieval via a Path Table SELECT TOPIC.name, DistinctCount(CLASSIFY.Doc_ID) FROM (TOPIC INNER JOIN PATH ON TOPIC.topic_id = PATH.topic_above) INNER JOIN CLASSIFY ON PATH.topic_id = CLASSIFY.topic_id WHERE TOPIC.level = 1 GROUP BY TOPIC.name ; One limitation of the Path Table approach is that the number of records in the Path Table can grow quite large for deep hierarchies. The following section describes another approach that avoids that problem.

THE DENORMALIZED TOPIC TABLE APPROACH
The denormalized topic table approach maintains information about all higher-level topics within each topic record. As demonstrated by Figure 4, if we assume that our topic hierarchy will not exceed six levels, then we need six more columns to maintain this information. Each node will be indicated as its own parent at its own level. For example, Topic 1.1.1 in Figure 1 would have ‘0’ (the topic_id for the top node in the hierarchy) as Figure 4. Using a Denormalized Topic Table

Topic2
Topic_ID Name Level Topic_Above Topic_Level_1 Topic_Level_2 Topic_Level_3 Topic_Level_4 Topic_Level_5 Topic_Level_6

Topic
Topic_ID Name Level Topic_Above Topic_Level_1 Topic_Level_2 Topic_Level_3 Topic_Level_4 Topic_Level_5 Topic_Level_6

Classify
Topic_ID Doc_ID

Document
Doc_ID Name Date_Created Owner

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

42 Millet

its Topic_Level_1, ‘1’ as its Topic_Level_2, ‘1.1’ as its Topic_Level_2, ‘1.1.1’ (itself) as its Topic_Level_4, and null values for Topic_Level_5 and Topic_Level_6. To demonstrate how the denormalized topic table can simplify data retrieval, consider again the challenge of showing how many documents belong to each main topic, including all subtopics below it. By joining the tables as shown in Figure 4, we can easily select and group all topics according to Topic_Level_2. Documents classified directly under that topic would be included in the result set because each topic is its own parent at its own level. Listing 4 shows that using this approach, a simple SQL statement can generate the requested information. Listing 4. Retrieval via a Denormalized Topic Table SELECT TOPIC2.Name, DistinctCount(CLASSIFY.Doc_ID) FROM (TOPIC2 INNER JOIN TOPIC ON TOPIC2.topic_id = TOPIC.Topic_Level_2) INNER JOIN CLASSIFY ON PATH.Topic_Level_2 = CLASSIFY.topic_id GROUP BY topic.Topic_Level_2; Note that in order to return the topic name, we resorted to adding an aliased (Topic2) copy of the topic table to the SQL statement.

COMPARING THE TWO APPROACHES
As demonstrated above, both the Path Table and the Denormalized Topic Table approaches facilitate data retrieval against topic hierarchies. However, both approaches achieve this at the cost of maintaining redundant data. The redundancy is caused by storing explicit path information from each node to all its ancestor nodes instead of just to its direct parent. While the Path approach is more flexible, the Denormalized Topic Table approach is simpler and easier to maintain. The Path Table approach is more flexible since it does not impose a limit on the number of levels in the hierarchy. In contrast, the Denormalized Topic Table approach limits the number of levels in the hierarchy to the number of Topic_Level_N columns. However, in most application areas one can guard against exceeding the number of levels limitation by assigning several Topic_Level_N columns beyond the maximum expected for the application. For example, if our current taxonomy has five levels, we can design the topic table with eight Topic_Level_N columns. The chances of exceeding this safety margin are slim. While the Path Table is more flexible, it requires slightly more complex SQL due to the addition of one more physical table and its associated joins. Another disadvantage of this approach is that for deep hierarchies, the size of the Path Table can grow quite large, degrading query performance. The most important criterion for comparing the two approaches is probably the ease of maintaining the redundant data required by both methods. The following section discusses this issue.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Managing Document Taxonomies in Relational Databases

43

MAINTENANCE OF DENORMALIZED HIERARCHY DATA
Asking users to maintain the redundant information required by either approach can be a career-limiting move. Users would resist the additional burden, and the chance of incomplete or inconsistent updates would pose significant data quality risk. This section suggests methods that can simplify and automate the maintenance of redundant hierarchy data. The proposed approach and the sample SQL code provide a concrete example of an “incremental evaluation system” for handling recursive SQL queries as advanced by previous literature (Libkin & Wong, 1997; Dong et al., 1999). A key observation is that the redundant information maintained for each topic is a simple extension of the redundant information already maintained for its parent topic. Consider a situation where Topic 1.2.2.2 in Figure 5 is moved from under Topic 1.2.2 to under Topic 3.2. Let us review the procedural logic required for updating the records in the Path Table. We can start by deleting all path records where Topic 1.2.2.2 is a subtopic (1.2.2.2 1.2.2, 1.2.2.2 1.2, 1.2.2.2 1, 1.2.2.2 0), except the path record of the topic to itself (1.2.2.2 1.2.2.2). We can then reconstruct the path records going up from Topic 1.2.2.2 by copying all path records going up from the new parent topic (3.2 3.2, 3.2 3, 3.2 0), and inserting those records to the Path Table after replacing the Topic_Below column in these records with the Topic_ID of the newly attached topic (1.2.2.2). After this procedure, Topic 1.2.2.2 would have the following path records (1.2.2.2 1.2.2.2 , 1.2.2.2 3.2, 1.2.2.2 3, 1.2.2.2 0), reflecting the correct state of the updated hierarchy, as shown in Figure 5. While the shortcomings of “intelligent” primary keys are well known, we must recognize that topic taxonomies (such as the Dewey decimal classification system used in libraries) frequently use “intelligent” primary keys reflecting the position of each topic within the taxonomy. In cases where Topic_ID is indeed an “intelligent” rather than a “surrogate” key, we would obviously need to change the Topic_ID to reflect changes in location of topic nodes. In our example, we would neeed to change the primary key from Figure 5. Moving Topic 10 from Parent 6 to Parent 4
Taxonomy

Topic 1

Topic 2

Topic 3

Topic 1.1

Topic 1.2

Topic 3.1

Topic 3.2

Topic 1.2.1

Topic 1.2.2

Topic 1.2.2.2

Topic 1.2.2.1

Topic 1.2.2.2

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

44 Millet

1.2.2.2 to 3.2.1. Using referential integrity to cascade such primary key updates to the foreign keys in the Path Table would take care of propagating such a change to all affected records. Similar and even simpler logic applies to the same situation when using a denormalized topic table. In that case, the Topic_Level_N columns from the new parent node are first copied to the updated topic record (Topic 1.2.2.2). We then simply add the topic as its own parent at its own level. Because the denormalized topic table allows for simpler maintenance logic, the remainder of this chapter uses that approach as the assumed data structure. Using this update logic, users would update the structure of the hierarchy by specifying only the Topic_Above information for each node. The level of the topic can always be established and updated automatically as the level of the parent node plus one. When inserting or updating a topic record to have a null Topic_Above, we can establish the node level as 1. In such cases the node is at the top of the hierarchy and is its own parent at Level 1. All other Topic_Level_N columns for such a top node would be automatically set to null. The main remaining challenge is to handle situations where a whole branch (a topic with subtopics) is moved in the hierarchy. Consider, for example, moving Topic 1.2.2 in Figure 5 from under Topic 1.2 to under Topic 1.1. The procedural logic above would update the redundant data for Topic 1.2.2, but we now need to update the information for all its descendants. The most elegant solution is to recursively extend the procedural logic by applying it to all descendant nodes, as if their Topic_Above column was updated as well. There is one more threat to our hierarchy data integrity that must be addressed by our procedural logic. When the user specifies Topic_Above information, we must guard against loops and self-references. In other words, the Topic_Above node cannot be a descendant of the current topic nor can it be the topic node itself. For example, given the hierarchy in Figure 5, the user should be blocked from changing the Topic Above of node 1.2 to 1.2.1 or 1.2. This test can be implemented by checking that none of the Topic_Level_N columns of the Topic_Above node is the current topic.

APPLYING THE PROCEDURE THROUGH FRONT-END LOGIC
This procedural logic can be implemented as a front-end function that gets called in application screens each time a user changes the Topic_Above column. The function accepts as arguments a Topic_ID and its new Topic_Above. After completing the logic for that topic, the function would use embedded SQL to identify the descendants of the topic and call itself recursively against all these descendants. One limitation of using such front-end logic to maintain the redundant hierarchy data is that it would require multiple communications between the client and the server. A much more important limitation is that we are dependent on uniform and full compliance by all client applications. The integrity of the hierarchy data can be compromised if some screens or client applications neglect to call or implement the front-end logic appropriately.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Managing Document Taxonomies in Relational Databases

45

APPLYING THE PROCEDURE THROUGH DATABASE TRIGGERS
Moving the procedural logic from front-end functions to back-end triggers removes the threat of multiple points of failure and achieves better performance. We need to implement an Update and Insert triggers on the Topic_Above column of the topic table. Assuming referential integrity takes care of blocking attempts to delete a topic record if a Topic_Above foreign key is pointing to it from another topic record, we do not need a Delete trigger. Appendix A and Appendix B provide commented implementations of the Update and Insert triggers. These particular versions are designed for the Sybase Adaptive Server Anywhere DBMS. Since triggers are not supported uniformly by all DBMSs, the implementations may differ across DBMSs. An example of DBMS-specific consideration in the implementation of these triggers is the issue of calling the trigger recursively by re-setting the Topic_Above code of the descendant nodes. Adaptive Server Anywhere would not fire an “After Update” trigger if the value in the column has not changed. Hence the “Before Update” declaration of the update trigger.

CONCLUSION
This chapter reviewed the data retrieval and data maintenance problems posed by topic hierarchies such as the ones used to classify and search for documents, websites and knowledge areas. Beyond the generic issues imposed by any hierarchy domains, there are two special needs imposed by such taxonomies. First, the need to support classifying the same document or website under multiple topic nodes leads to different SQL, but more importantly requires care in avoiding biased aggregate results due to double counting. Since topic taxonomies frequently utilize intelligent rather than surrogate primary keys, updates to the location of topic nodes require updates to the primary keys as well. In most situations a good solution to fast and simple data retrieval against topic hierarchies is to maintain redundant information. The approach of denormalizing the topic table may be preferred over maintaining a separate Path Table because the Path Table can grow too large and requires more complex data maintenance logic. The limitation of denormalized and redundant hierarchy information is that updates to the hierarchy require special processing logic in order to avoid update anomalies. This chapter describes techniques for selectively refreshing the hierarchy data by exploiting the redundant information already maintained for the specified parent topic as topic records are inserted or updated. The process can then be extended recursively for lower level nodes. If the necessary trigger options are available for the DBMS in use, it is recommended that the processing logic for maintaining the redundant hierarchy information be implemented as triggers. This removes the burden of hierarchy maintenance from client applications. It also ensures that client applications cannot circumvent the hierarchy maintenance logic.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

46 Millet

REFERENCES
ANSI/ISO/IEC 9075-2-1999. ANSI’s Electronic Standards Store. Available online at: http://webstore.ansi.org. Celko, J. (2000). Joe Celko’s SQL for Smarties: Advanced SQL Programming. San Francisco, CA: Morgan Kaufmann. Dong, G., Libkin, L., Su, J. & Wong, L. (1999). Maintaining the transitive closure of graphs in SQL. International Journal of Information Technology, 5, 46-78. Kimball, R., Reeves, L., Ross, M. & Thornthwaite, W. (1998). The Data Warehouse Lifecycle Toolkit: Expert Methods for Designing, Developing, and Deploying Data Warehouses. New York: Wiley Computer Publishing. Libkin, L. & Wong, L. (1997). Incremental recomputation of recursive queries with nested sets and aggregate functions. Database Programming Languages. Springer, 222238. Millet, I. (2001). Accommodating hierarchies in relational databases. In Becker, S. (Ed.), Developing Complex Database Systems: Practices, Techniques, and Technologies. Hershey, PA: Idea Group Publishing.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Managing Document Taxonomies in Relational Databases

47

APPENDIX A
Update Trigger for Topic Hierarchy Maintenance
This trigger completely takes over the task of maintaining: topic_level: • If topic_above is Null then topic_level = 1 • Otherwise, topic_level = topic_level of parent + 1 2. Topic_Level_n: • If topic_above is Null then Topic_Level_1 is the topic itself and Parents_Level_n at all levels below are Null • Otherwise, the Parents at all levels are the parents of the topic above and the topic is its own parent at its own level. This trigger fires just before the Topic_Above column in the topic table gets updated. In order to cascade the updates down the hierarchy branch, this trigger also updates (resets) the Topic_Above of all the topic’s children to their old values. This causes the trigger to fire in those children, and thus a recursive cascade of the update logic ripples down the hierarchy. 1.

==================================================================
CREATE trigger Topic_Hierarchy before UPDATE of TOPIC_ABOVE order 3 on “DBA”.TOPIC referencing old as old_topic new as new_topic for each row BEGIN declare v_parent_level integer; declare v_p1 integer; declare v_p2 integer; declare v_p3 integer; declare v_p4 integer; declare v_p5 integer; declare v_p6 integer; declare err_illegal_parent exception for sqlstate value ‘99999’; declare err_parent_is_child exception for sqlstate value ‘99998’; // check that this is not a top node IF(new_topic.topic_above is not null) THEN BEGIN SELECT topic.topic_level, Topic_Level_1, Topic_Level_2, Topic_Level_3, Topic_Level_4, Topic_Level_5, Topic_Level_6 INTO v_parent_level, v_p1, v_p2, v_p3, v_p4, v_p5, v_p6 FROM “dba”.topic WHERE topic.Topic_ID = new_topic.topic_above;
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

48 Millet

IF new_topic.Topic_ID = v_p1 or new_topic.Topic_ID = v_p2 or new_topic.Topic_ID = v_p3 or new_topic.Topic_ID = v_p4 or new_topic.Topic_ID = v_p5 or new_topic.Topic_ID = v_p6 THEN // call the exception handling specified in the EXCEPTION block signal err_parent_is_child END IF; IF v_parent_level > 5 THEN // call the exception handling specified in the EXCEPTION block signal err_illegal_parent END IF; UPDATE “dba”.topic SET topic.topic_level = v_parent_level + 1, topic.Topic_Level_1 = v_p1, topic.Topic_Level_2 = v_p2, topic.Topic_Level_3 = v_p3, topic.Topic_Level_4 = v_p4, topic.Topic_Level_5 = v_p5, topic.Topic_Level_6 = v_p6 WHERE topic.Topic_ID = new_topic.Topic_ID; // We must use UPDATE rather than just set the values in new_topic // because this is BEFORE update and we need the recursive // children to have access to the updated property. CASE v_parent_level when 1 THEN UPDATE “dba”.topic SET topic.Topic_Level_2 = new_topic.Topic_ID WHERE Topic_ID = old_topic.Topic_ID when 2 THEN UPDATE “dba”.topic SET topic.Topic_Level_3 = new_topic.Topic_ID WHERE Topic_ID = old_topic.Topic_ID when 3 THEN UPDATE “dba”.topic SET topic.Topic_Level_4 = new_topic.Topic_ID WHERE Topic_ID = old_topic.Topic_ID when 4 THEN UPDATE “dba”.topic SET topic.Topic_Level_5 = new_topic.Topic_ID WHERE Topic_ID = old_topic.Topic_ID

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Managing Document Taxonomies in Relational Databases

49

when 5 THEN UPDATE “dba”.topic SET topic.Topic_Level_6 = new_topic.Topic_ID WHERE Topic_ID = old_topic.Topic_ID else signal err_illegal_parent END CASE; // Refresh topic_above of all children to cause recursion UPDATE “dba”.topic SET topic.topic_above = new_topic.Topic_ID WHERE topic.topic_above = new_topic.Topic_ID; EXCEPTION When err_illegal_parent THEN message ‘Parent Level Is Too Low (below 5)’; // signal to the outside world to abort signal err_illegal_parent when err_parent_is_child THEN message ‘This topic cannot be a child of its own child!’; // signal to the outside world to abort signal err_parent_is_child when others THEN // for other exceptions not explicitly handled in the Exception block, // simply pass them up to the procedure that caused the Trigger resignal END END IF; // NULL PARENT (top node handling) IF (new_topic.topic_above is null) THEN // For top node, set level to 1, parent at level 1 to itself and others to Null UPDATE “dba”.topic SET topic.topic_level = 1, topic.Topic_Level_1 = new_topic.Topic_ID, topic.Topic_Level_2 = null, topic.Topic_Level_3 = null, topic.Topic_Level_4 = null, topic.Topic_Level_5 = null, topic.Topic_Level_6 = null WHERE topic.Topic_ID = new_topic.Topic_ID;

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

50 Millet

// Refresh topic_above of all children to cause recursion. UPDATE “dba”.topic SET topic.topic_above = new_topic.Topic_ID WHERE topic.topic_above = new_topic.Topic_ID ; END IF; END

APPENDIX B
Insert Trigger for Topic Hierarchy Maintenance
This trigger is very similar to the Update trigger, except that there is no need for recursive calls since a newly inserted topic doesn’t have descendant nodes. The other change is that instead of attempting to UPDATE the denormalized information, we just SET new values in the columns of the record that is about to be inserted.

==================================================================
Create trigger Insert_Into_Hierarchy before insert order 4 on “DBA”.TOPIC referencing new as new_topic for each row begin declare v_parent_level integer; declare v_p1 integer; declare v_p2 integer; declare v_p3 integer; declare v_p4 integer; declare v_p5 integer; declare v_p6 integer; declare err_illegal_parent exception for sqlstate value ‘99999’; declare err_parent_is_child exception for sqlstate value ‘99998’; // Topic is not a Top Node if(new_topic.topic_above is not null) then begin select topic.topic_level,Topic_Level_1,Topic_Level_2, Topic_Level_3,Topic_Level_4,Topic_Level_5,Topic_Level_6 into v_parent_level, v_p1,v_p2,v_p3,v_p4,v_p5,v_p6 from “dba”.topic where topic.Topic_ID=new_topic.topic_above; IF new_topic.Topic_ID=v_p1 or new_topic.Topic_ID=v_p2 or new_topic.Topic_ID=v_p3 or new_topic.Topic_ID=v_p4 or new_topic.Topic_ID=v_p5 or new_topic.Topic_ID=v_p6 then
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Managing Document Taxonomies in Relational Databases

51

// call the exception handling specified in the EXCEPTION block signal err_parent_is_child end if; IF v_parent_level>5 then // call the exception handling specified in the EXCEPTION block signal err_illegal_parent end if; set new_topic.topic_level=v_parent_level+1; set new_topic.Topic_Level_1=v_p1; set new_topic.Topic_Level_2=v_p2; set new_topic.Topic_Level_3=v_p3; set new_topic.Topic_Level_4=v_p4; set new_topic.Topic_Level_5=v_p5; set new_topic.Topic_Level_6=v_p6; case v_parent_level when 1 then set new_topic.Topic_Level_2=new_topic.Topic_ID when 2 then set new_topic.Topic_Level_3=new_topic.Topic_ID when 3 then set new_topic.Topic_Level_4=new_topic.Topic_ID when 4 then set new_topic.Topic_Level_5=new_topic.Topic_ID when 5 then set new_topic.Topic_Level_6=new_topic.Topic_ID else signal err_illegal_parent end case exception when err_illegal_parent then message ‘Parent Level Is Too Low (below 5)’; signal err_illegal_parent when err_parent_is_child then message ‘This topic cannot be a child of its own child!’; signal err_parent_is_child when others then resignal end end if; IF(new_topic.topic_above is null) then // this is a top node: set Level to 1, parent at level 1 to itself and others to Null. set new_topic.topic_level=1; set new_topic.Topic_Level_1=new_topic.Topic_ID; set new_topic.Topic_Level_2=null;
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

52 Millet

set new_topic.Topic_Level_3=null; set new_topic.Topic_Level_4=null; set new_topic.Topic_Level_5=null; set new_topic.Topic_Level_6=null end if END

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 53

Chapter V

Building Signature-Trees on Path Signatures in Document Databases
Yangjun Chen* University of Winnipeg, Canada Gerald Huck IPSI Institute, Germany

ABSTRACT
Java is a prevailing implementation platform for XML-based systems. Several highquality in-memory implementations for the standardized XML-DOM API are available. However, persistency support has not been addressed. In this chapter, we discuss this problem and introduce PDOM (persistent DOM) to accommodate documents as permanent object sets. In addition, we propose a new indexing technique: path signatures to speed up the evaluation of path-oriented queries against document object sets, which is further enhanced by combining the technique of signature-trees with it to expedite scanning of signatures stored in a physical file.

INTRODUCTION
With the rapid advance of the Internet, management of structured documents such as XML documents has become more and more important (Suciu & Vossen, 2000; World Wide Web, 1998a; Marchiori, 1998). As a subset of SGML, XML is recommended by the W3C (World Wide Web Consortium) as a document description metalanguage to
* The author is supported by NSERC 239074-01 (242523) (Natural Science and Engineering Council of Canada)
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

54 Chen & Huck

exchange and manipulate data and documents on the WWW. It has been used to code various types of data in a wide range of application domains, including a Chemical Markup Language for exchanging data about molecules and the Open Financial Exchange for swapping financial data between banks, and between banks and customers (Bosak, 1997). Also, a growing number of legacy systems are adapted to output data in the form of XML documents. In this chapter, we introduce a storage method for documents called PDOM (persistent DOM), implemented as a lightweight, transparent persistency memory layer, which does not require the burdensome design of a fixed schema. In addition, we propose a new indexing technique: path signatures to speed up the evaluation of path-oriented queries against document object sets, which are organized into a tree structure called a signature-tree. In this way, the scanning of a signature file is reduced to a binary tree search, which can be performed efficiently. To show the advantage of our method, the time complexity of searching a signature-tree is analyzed and the permanent storage of signature-trees is discussed in great detail.

BACKGROUND
The Document Object Model (DOM) is a platform- and language-neutral interface for XML. It provides a standard set of objects for representing XML data: a standard model of how these objects can be combined and a standard interface for accessing and manipulating them (Pixley 2000). There are half a dozen DOM implementations available for Java from several vendors such as IBM, Sun Microsystems and Oracle, but all these implementations are designed to work in main memory only. In recent years, efforts have been made to find an effective way to generate XML structures that are able to describe XML semantics in underlying relational databases (Chen & Huck, 2001; Florescu & Kossmann, 1999; Shanmugasundaram et al., 1999; Shanmugasundaram & Shekita, 2000; Yosjikawa et al., 2001). However, due to the substantial difference between the nested element structures of XML and the flat relational data, much redundancy is introduced, i.e., the XML data is either flattened into tuples containing many redundant elements, or has many disconnected elements. Therefore, it is significant to explore a way to accommodate XML documents, which is different from the relational theory. In addition, a variety of XML query languages have been proposed to provide a clue to manipulate XML documents (Abiteboul et al., 1996; Chamberlin et al., 2001; Christophides et al., 2000; Deutsch et al., 1989; Robie et al., 1998; Robie, Chamberlin & Florescu, 2000). Although the languages differ according to expressiveness, underlying formalism and data model, they share a common feature: path-oriented queries. Thus, finding efficient methods to do path matching is very important to evaluation of queries against huge volumes of XML documents.

SYSTEM ARCHITECTURE
The system architecture can be pictorially depicted as shown in Figure 1, which consists of three layers: persistent object manager, standard DOM API and specific PDOM API, and application support.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 55

1.

2.

3.

Persistent Object Manager — The PDOM mediates between in-memory DOM object hierarchies and their physical representation in binary random access files. The central component is the persistent object manager. It controls the life cycle of objects, serializes multi-threaded method invocations and synchronizes objects with their file representation. In addition, it contains two sub-components: a cache to improve performance and a commit control to mark recovery points in case of system crashes. These two components can be controlled by users through tuning parameters. Standard DOM API and Specific PDOM API — The standard DOM API methods for object hierarchy manipulation are transparently mapped to physical file operations (read, write and update). The system aims at hiding the storage layer from an application programmer’s view to the greatest possible extent. Thus, for most applications, it is sufficient to use only standard DOM methods. The only exception is document creation, which is deliberately left application-specific by the W3C DOM standard. The specific PDOM API allows an application to be aware of the PDOM to tune system parameters for the persistent object manager as well as its subsystems: cache and commit control. The specific API is mainly for the finegrained control of the PDOM, not intended for the casual programmers. Rather, it is the place to experiment with ideas and proof concepts. Application Support — This layer is composed of a set of functions which can be called by an application to read, write, update or retrieve a document. In addition, for a programmer with deep knowledge on PDOM, some functions are available to create a document, to commit an update operation and to compact a PDOM file, in which documents are stored as object hierarchies.

In the database (or PDOM pool), the DOM object hierarchies are stored as binary files while the index structures/path signatures are organized as a pat-tree.

Figure 1. Logical Architecture of the System
Application support create, read, write, update, commit, compact document retrieval Standarc PDOM API

Specific PDOM API Commit control Cache

Document manipulation

Persistent object manager

Binary files

Path signatures

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

56 Chen & Huck

Figure 2. Binary File for Documents
pointer to dictionary length length pointer to node page index

…

e1 e2

1 2

……

a1 a3

1, a2 2 3 ……

……
node page index

node-page1

node-pagen

data dictionary

STORAGE OF DOCUMENTS AS BINARY FILES
The format of the PDOM binary files used to accommodate the object hierarchies is depicted in Figure 2. It is organized in node pages, each containing 128 serialized DOM objects. In PDOM, each object (node) corresponds to a document identifier, an element name, an element value, a “Comment” or a “Processing Instruction.” The attributes of an element are stored with the corresponding element name. These object (node) types are equivalent to the node types in XSL (World Wide Web Consortium, 1998b) data model. Thus, a page does not have a fixed length in bytes, but a fixed number of objects it holds. At the beginning of the file, there are two pointers. The first points to a dictionary containing two mappings, by which each element name ei and attribute aj are mapped to a different number, respectively; the numerical values are used for compact storage. The second points to the node page index (NPI). The NPI holds an array of pointers to the start of each node page. Each object is serialized as follows: 1. A type flag indicating the DOM-type: document identifier, element name, element value, “Comment” or “Processing Instruction.” 2. The object content may be an integer representing an element name, a PCDATA (more or less comparable to a string) or a string (WTF-8 encoded) representing a “Comment” or a “Processing Instruction.” 3. A parent-element identifier (if available). 4. A set of attribute-value pairs, where each attribute name is represented by an integer, which can be used to find the corresponding attribute name in the associated data dictionary. The attribute value is a WTF-8 encoded string. 5. The number of sub-elements of an element and its sub-element identifiers. This serialization approach is self-describing, i.e., depending on the type flag, the serialization structure and the length of the remaining segments can be determined. The mapping between object identifiers in memory (OID) and their physical file location is given by the following equation: OID = PI * 128 + i, where PI is the index of the containing node page in the NPI and i is the object index within that page. Obviously, this address does not refer directly to any byte offset in the file

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 57

Figure 3. A Simple Document and its Storage
(a)
byte num ber <le tte r file c ode= ”9302” > <da te >Ja nuary 27, 1993< /dat e> <gr e eti ng>&sa lute ; Je an Luc ,</ gre et ing> <body> <pa ra >H ow are you doi ng?< /par a > <pa ra >Isn’t it < e mph>a bout tim e< /e mph> you vi sit? </pa ra> < /body> < closi ng>S ee you soon,</ closi ng> < sig>G e nise </si g> < /le tte r> 0: 4: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 22: 23: ... 500: 501: 508: ... 557 ... 565 500 565 0 0 1 1 2 0 0 1 0 “ 9302” 5 2 ... 7 le tte r da te ... 1 ... 8 pointe r t o the da ta dict ionar y pointe r t o the node page inde x first pa ge number node t ype “ docume nt” numbe r of chi ldre n inte ger re pr ese nting t he c hild’s i d node t ype “ ele me nt nam e” inte ger re pr ese nting “ le tte r” pare nt ID of this node numbe r of at tri butes inte ger re pr ese nting “ fi lec ode” at tri but e va lue numbe r of chi ldre n ID of a c hild (“ date ” e le me nt) ... the foll ow ing i s the da ta dic tiona ry numbe r of el eme nt na mes an e le ment name “l ett er” an e le ment name “da te ” ... numbe r of at tri bute na me s ...

(b)

or page (which may change over time). Because of this, it can be used as unique, immutable object identifier within a single document. In the case of multiple documents, we associate each OID with a docID, to which it belongs. Example 1 helps for illustration.

Example 1
In Figure 3(a), we show a simple XML document. It will be stored in a binary file as shown in Figure 3(b). From Figure 3(b), we can see that the first four bytes are used to store a pointer to the dictionary, in which an element name or an attribute name is mapped to an integer. (For example, the element name “letter” is mapped to “0,” “date” is mapped to “1” and so on.) The second four bytes are a pointer to the node page index, which contains only one entry (four bytes) for this example, pointing to the beginning of the unique node page stored in this file. In this node page, each object (node) begins at a byte which shows the object type. In our implementation, five object types are considered. They are “document,” “text” (used for an element value), “3,” “4,” respectively. The physical identifier of an object is implicitly implemented as the sequence number of the object appearing within a node page. For example, the physical identifier of the object with the type “document” is “0,” the physical identifier of the object for “letter” is “1” and so on. The logic object identifier is calculated using the above simple equation when a node page is loaded into the main memory. Finally, we pay attention to the data dictionary structure. In the first line of the data dictionary, the number of the element names is stored, followed by the sequence of element names. Then, each element name is considered to be mapped implicitly to its sequence number in which it appears. The same method applies to the mapping for attribute names. Beside the binary files for storing documents, another main data structure of the PDOM is the file for path signatures used to optimize the query evaluation. To speed up the scanning of the signatures, we organize them into a pat-tree, which reduces the time

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

58 Chen & Huck

complexity by an order of magnitude or more. We discuss this technique in the next section in detail.

PATH-ORIENTED LANGUAGE AND PATH SIGNATURES
Now we discuss our indexing technique. To this end, we first outline the pathoriented query language in the following section, which is necessary for the subsequent discussion. Then, we will describe the concept of path signatures, discuss the combination of path signatures and pat-trees, as well as the corresponding algorithm implementation in great detail.

Path-Oriented Language
Several path-oriented languages such as XQL (Robie et al., 1998) and XML-QL (Deutsch et al., 1998) have been proposed to manipulate tree-like structures as well as attributes and cross-references of XML documents. XQL is a natural extension to the XSL pattern syntax, providing a concise, understandable notation for pointing to specific elements and for searching nodes with particular characteristics. On the other hand, XML-QL has operations specific to data manipulation such as joins and supports transformations of XML data. XML-QL offers tree-browsing and tree-transformation operators to extract parts of documents to build new documents. XQL separates transformation operation from the query language. To make a transformation, an XQL query is performed first, then the results of the XQL query are fed into XSL (World Wide Web Consortium, 1998b) to conduct transformation. An XQL query is represented by a line command which connects element types using path operators (‘/’ or ‘//’). ‘/’ is the child operator which selects from immediate child nodes. ‘//’ is the descendant operator which selects from arbitrary descendant nodes. In addition, the symbol ‘@’ precedes attribute names. By using these notations, all paths of tree representation can be expressed by element types, attributes, ‘/’ and ‘@’. Exactly, a simple path can be described by the following Backus-Naur Form: <simplepath>::=<PathOP><SimplePathUnit>|<PathOp><SimplePathUnit>‘@’<AttName> <PathOp> ::= ‘/’ | ‘//’ <SimplePathUnit>::=<ElementType> | <ElementType><PathOp><SimplePathUnit> The following is a simple path-oriented query: /letter//body [para $contains$’visit’], where /letter//body is a path and [para $contains$’visited’] is a predicate, enquiring whether element “para” contains a word ‘visited.’

Signature and Path Signature
To speed up the evaluation of the path-oriented queries, we store all the different paths in a separate file and associate each path with a set of pointers to the positions of
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 59

Figure 4. Illustration for Path File
path file: letter/data letter/greeting letter/body/para letter/closing letter/sig binary file:

… …

the binary file for the documents, where the element value can be reached along the path (see Figure 4 for illustration). This method can be improved greatly by associating each path with a so-called path signature used to locate a path quickly. In addition, all the path signatures can be organized into a pat-tree, leading to a further improvement of performance. Signature files are based on the inexact filter. They provide a quick test, which discards many of the nonqualifying values. But the qualifying values definitely pass the test, although some values which actually do not satisfy the search requirement may also pass it accidentally. Such values are called “false hits” or “false drops.” The signature of a value is a hash-coded bit string of length k with m bit set to one, stored in the “signature file” (Faloutsos, 1985, 1992). The signature of an element containing some values is formed by superimposing the signatures of these values. The following figure depicts the signature generation and comparison process of an element containing three values, say “SGML,” “database” and “information.” When a query arrives, the element signatures (stored in a signature file) are scanned and many nonqualifying elements are discarded. The rest are either checked (so that the “false drops” are discarded) or they are returned to the user as they are. Concretely, a query specifying certain values to be searched for will be transformed into a query signature sq in the same way as for the elements stored in the database. The query signature is then compared to every element signature in the signature file. Three possible outcomes of the comparison are exemplified in Figure 3: 1. the element matches the query; that is, for every bit set to 1 in sq, the corresponding bit in the element signature s is also set (i.e., s sq = sq) and the element really contains the query word; Figure 5. Signature Generation and Comparison
t ext: ... SGML ... databases ... i nformation ... representat ive word s ignat ure: SGML databas e i nformation object si gnature (OS) 010 000 100 110 100 010 010 100 010 100 011 000 110 110 111 110 queries: SGML XML informatik query signatures : 010 000 100 110 011 000 100 100 110 100 100 000 matchin results: match with OS no match wi th OS false drop

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

60 Chen & Huck

2. 3.

the element doesn’t match the query (i.e., s s q sq); and the signature comparison indicates a match but the element in fact does not match the search criteria (false drop). In order to eliminate false drops, the elements must be examined after the element signature signifies a successful match.

The purpose of using a signature file is to screen out most of the nonqualifying elements. A signature failing to match the query signature guarantees that the corresponding element can be ignored. Therefore, unnecessary element accesses are prevented. Signature files have a much lower storage overhead and a simple file structure than inverted indexes. The above filtering idea can be used to support the path-oriented queries by establishing path signatures in a similar way. First, we define the concept of tag trees. Definition 1 (tag trees): Let d denote a document. A tag tree for d, denoted Td, is a tree, where there is a node for each tag appearing in d and an edge (node a, node b) if node b represents a direct sub-element of node a. Based on the concept of tag trees, we can define path signatures as follows. Definition 2 (path signature): Let root n1 ... nm be a path in a tag tree. Let s root, s i (i = 1, ..., m) be the signatures for root and ni (i = 1, ..., m), respectively. The path signature of nm is defined to be Psm = sroot s1 ... sm.

Example 1
Consider the tree for the document shown in Figure 3(a). Removing all the leave nodes from it (a leaf always represents the text of an element), we will obtain the tag tree for the document shown in Figure 3(a). If the signatures assigned to ‘letter,’ ‘body’ and ‘pare’ are s letter = 011 001 000 101, s body = 001 000 101 110 and spara = 010 001 011 100, respectively, then the path signature for ‘para’ is Pspara = sletter sbody spara = 011001111111. According to the concept of the path signatures, we can evaluate a path-oriented query as follows: 1. Assign each element name appearing in the path of the query a signature using the same hash function as for those stored in the path signature file. 2. Superimpose all these signatures to form a path signature of the query. 3. Scan the path signature file to find the matching signatures. 4. For each matching signature, check the associated path. If the path really matches, the corresponding page of the binary file will be accessed to check whether the query predicate is satisfied. i) Compared to the path file, the path signature file has the following advantages: If the paths (instead of the path signatures) are stored in a separate file, the path matching is more time-consuming than the path signatures. In the worst-case, O(n) time is required for a path matching, where n represents the length of the path (or the number of element names involved in a path). Assume that the average length of element names is w and each letter is stored as a bit string of length l. The time complexity of a path matching is then O(w l n). But for a path signature matching,

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 61

ii)

only O(F) time is required, where F is the length of a path signature. In the terms of Christodoulakis and Faloutsos (1984), F is on the order of O(m n/ln2), where m represents the number of 1s in a path signature (bit string). (Here, we regard each path as a “block” (Christodoulakis & Faloutsos, 1984), which is a set of words whose signatures will be superimposed together. Thus, the size of a block is the length of a path.) In general, w l m/ln2. Therefore, some time can be saved using the path signatures instead of the paths themselves. We can organize all the path signatures into a pat-tree. In this way, the scanning of the path signatures can be expedited tremendously.

SIGNATURE-TREES ON PATH SIGNATURES
If a path signature file is large, the amount of time elapsed for scanning it becomes significant. Especially, the binary searching technique cannot be used to speed-up the searching of such a file since path signatures work only as an inexact filter. As a counter example, consider the following simple binary tree, which is constructed for a path signature file containing only three signatures (see Figure 6). Assume that s = 000010010100 is a signature to be searched. Since s1 > s, the search will go left to s2. But s2 does not match s. Then, the binary search will return a ‘nil’ to indicate that s cannot be found. However, in terms of the definition of the inexact matching, s 3 matches s. For this reason, we try another tree structure, the so-called signature index over path signatures, and change its search strategy in such a way that the behavior of signatures can be modeled. In the following, we first describe how to build a signature-tree. Then, we discuss how to establish an index for path signatures using signature-trees. Finally, we discuss how to search a signature-tree.

Definition of Signature-Trees
A signature-tree works for a signature file is just like a trie (Knuth, 1973; Morrison, 1968) for a text. But in a signature-tree, each path is a signature identifier which is not a continuous piece of bits, which is quite different from a trie in which each path corresponds to a continuous piece of bits. Consider a signature si of length m. We denote it as si = si[1] si[2] ... s i[m], where each s i[j] {0, 1} (j = 1, ..., m). We also use s i(j 1, ..., j h) to denote a sequence of pairs w.r.t. s i: (j1, s i[j1])(j2, si[j2]) ... (jh, si[j h]), where 1 jk £ m for k {1, ..., h}. Definition 3 (signature identifier): Let S = s 1.s 2 ... .sn denote a signature file. Consider s i (1 i n). If there exists a sequence: j 1, ..., j h such that for any k i (1 k n) we Figure 6. A Counter Example
s1 s2 010 000 100 110 010 100 011 000 s3 100 010 010 100 path signature to be searched s 000010010100

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

62 Chen & Huck

have si(j1, ..., jh) s k(j1, ..., j h), then we say si(j1, ..., jh) identifies the signature s i or say s i(j 1, ..., j h) is an identifier of s i w.r.t. S. For example, in Figure 6(a), s6(1, 7, 4, 5) = (1, 0)(7, 1)(4, 1)(5, 1) is an identifier of s6 since for any i 6 we have s i(1, 7, 4, 5) s6(1, 7, 4, 5). (For instance, s1(1, 7, 4, 5) = (1, 0)(7, 0)(4, 0)(5, 0) s6(1, 7, 4, 5), s 2(1, 7, 4, 5) = (1, 1)(7, 0)(4, 0)(5, 1) s6(1, 7, 4, 5) and so on. Similarly, s1(1, 7) = (1, 0)(7, 0) is an identifier for s 1 since for any i 1 we have s i(1, 7) s1(1, 7).) In the following, we’ll see that in a signature-tree each path corresponds to a signature identifier. Definition 4 (signature-tree): A signature-tree for a signature file S = s 1.s 2 ... .sn, where s i s j for i j and |s k| = m for k = 1, ..., n, is a binary tree T such that: 1. For each internal node of T, the left edge leaving it is always labeled with 0 and the right edge is always labeled with 1. 2. T has n leaves labeled 1, 2, ..., n, used as pointers to n different positions of s1, s 2 ... and s n in S. 3. Each internal node is associated with a number which tells how many bits to skip when searching. 4. Let i1, ..., i h be the numbers associated with the nodes on a path from the root to a leaf labeled i (then, this leaf node is a pointer to the ith signature in S). Let p1, ..., ph be the sequence of labels of edges on this path. Then, (j 1, p1) ... (jh, ph) makes up a signature identifier for s i, s i(j1, ..., j h).

Example 2
In Figure 7(b), we show a signature-tree for the signature file shown in Figure 6(a). In this signature-tree, each edge is labeled with 0 or 1 and each leaf node is a pointer to a signature in the signature file. In addition, each internal node is marked with an integer (which is not necessarily positive) used to calculate how many bits to skip when searching. Consider the path going through the nodes marked 1, 7 and 4. If this path is searched for locating some signature s, then three bits of s: s[1], s[7] and s[4] will be checked at that moment. If s[4] = 1, the search will go to the right child of the node marked “4.” This child node is marked with 5 and then the 5th bit of s: s[5] will be checked. Figure 7. A Path Signature File and its Signature Tree
(a)
0 0 1. 0 4 . 7 1 0 8 4 1 5 0 2. 1 6 .

(b)
1 1 0 7 1 8. 4 1 3 .

s1. s2. s3. s4. s5. s6. s7. s8.

011 001 000 101 111 011 001 111 111 101 010 111 011 001 101 111 011 101 110 101 011 111 110 101 011 001 111 111 111 011 111 111

1 0 7 .

5 .

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 63

See the path consisting of the dashed edges in Figure 7(b), which corresponds to the identifier of s 6: s6(1, 7, 4, 5) = (1, 0)(7, 1)(4, 1)(5, 1). Similarly, the identifier of s3 is s3(1, 4) = (1, 1)(4, 1) (see the path consisting of thick edges). In the next subsection, we discuss how to construct a signature-tree for a signature file.

Construction of Signature-Trees
Below we give an algorithm to construct a signature-tree for a signature file, which needs only O(N) time, where N represents the number of signatures in the signature file. At the very beginning, the tree contains an initial node: a node containing a pointer to the first signature. Then, we take the next signature to be inserted into the tree. Let s be the next signature we wish to enter. We traverse the tree from the root. Let v be the node encountered and assume that v is an internal node with sk(v) = i. Then, s[i] will be checked. If s[i] = 0, we go left. Otherwise, we go right. If v is a leaf node, we compare s with the signature s 0 pointed by v. s cannot be the same as v since in S there is no signature which is identical to anyone else. But several bits of s can be determined, which agree with s 0. Assume that the first k bits of s agree with s 0; but s differs from s 0 in the (k + 1)th position, where s has the digit b and s 0 has 1 - b. We construct a new node u with sk(u) = k + 1 and replace v with u. (Note that v will not be removed. By “replace,” we mean that the position of v in the tree is occupied by u. v will become one of u’s children.) If b = 1, we make v and the pointer to s be the left and right children of u, respectively. If b = 0, we make v and the pointer to s be respectively the right and left children of u. The following is the formal description of the algorithm. Algorithm sig-tree-generation(file) begin construct a root node r with sk(r) = 1; /*where r corresponds to the first signature s 1 in the signature file*/ for j = 2 to n do call insert(s j); end Procedure insert(s) begin stack root; while stack not empty do 1 {v pop(stack); 2 if v is not a leaf then 3 {i sk(v); 4 if s[i] = 1 then {let a be the right child of v; push(stack, a);} 5 else {let a be the left child of v; push(stack, a);} 6 } 7 else (*v is a leaf.*) 8 {compare s with the signature s 0 pointed by p(v); 9 assume that the first k bit of s agree with s 0;
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

64 Chen & Huck

10 11 12 13 14 } end

but s differs from s 0 in the (k + 1)th position; w v; replace v with a new node u with sk(u) = k + 1; if s[k + 1] = 1 then make s and w be respectively the right and left children of u else make s and w be the right and left children of u, respectively;}

In the procedure insert, stack is a stack structure used to control the tree traversal. We trace the above algorithm against the signature file shown in Figure 8. In the following, we prove the correctness of the algorithm sig-tree-generation. To this end, it should be specified that each path from the root to a leaf node in a signaturetree corresponds to a signature identifier. We have the following proposition: Proposition 1: Let T be a signature tree for a signature file S. Let P = v1.e1 ... vg-1.eg-1.vg be a path in T from the root to a leaf node for some signature s in S, i.e., p(vg) = s. Denote ji = sk(vi) (i = 1, ..., g - 1). Then, s(j1, j2, ..., j g-1) = (j1, b(e1)) ...(jg-1, b(eg-1)) constitutes an identifier for s. Proof. Let S = s1.s 2 ... .s n be a signature file and T a signature tree for it. Let P = v 1e1 ... vge v be a path from the root to a leaf node for s i in T. Assume that there exists 1 g-1 g another signature st such that s t(j1, j 2, ..., jg-1) = si(j1, j 2, ..., jg-1), where ji = sk(vi) (i = 1, ..., g - 1). Without loss of generality, assume that t > i. Then, at the moment when s t is inserted into T, two new nodes v and v’ will be inserted as shown in Figure 9(a) or (b) (see lines 10-15 of the procedure insert). Here, v’ is a pointer to s t and v is associated with a number indicating the position where p(vt) and p(v’) differs. It shows that the path for si should be v1.e1 ... vg-1.e.ve’.vg or v1.e1 ... vg-1.e.ve”.vg, which contradicts the assumption. Therefore, there is not any other signature s t with s t(j1, j2, ..., j n-1) = (j 1, b(e1)) ...(j n-1, b(en-1)). So s i(j 1, j2, ..., jn-1) is an identifier of s i. The analysis of the time complexity of the algorithm is relatively simple. From the procedure insert, we see that there is only one loop to insert all signatures of a signature Figure 8. Sample Trace of Signature Tree Generation
insert s 1
1. 1. 2. 1. 2. 1 4 4 4 2. 5. 3. 1. 4 5.

insert s 2

1

insert s 3

1 4 3. 1 4 4 8 5 7. 5 6. 2. 3.

insert s4
7 1. 4.

1

insert s 5
4 2. 3.

insert s6
7

1 4

insert s7
7

insert s 8
7 1. 8 4. 7. 5 4

1 4 7 5 6. 2. 8. 3.

7 1.

4 5

2.

3. 1.

6. 4.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 65

Figure 9. Inserting a Node ‘v’ into ‘T’
v 1 v 1

e e’
v g v

vg1

vg1

e e’
v ’ v

e’’
v ’

e’’
v g

file into a tree. At each step within the loop, only one path is searched, which needs at most O(m) time. (m represents the length of a signature.) Thus, we have the following proposition: Proposition 2: The time complexity of the algorithm sig-tree-generation is bounded by O(N), where N represents the number of signatures in a signature file. Proof. See the above analysis.

Searching of Signature-Trees
Now we discuss how to search a signature-tree to model the behavior of a signature file as a filter. Let s q be a query signature. The i-th position of s q is denoted as sq(i). During the traversal of a signature-tree, the inexact matching is defined as follows: i) Let v be the node encountered and s q (i) be the position to be checked. ii) If s q (i) = 1, we move to the right child of v. iii) If s q (i) = 0, both the right and left child of v will be visited. In fact, this definition corresponds to the signature matching criterion. To implement this inexact matching strategy, we search the signature-tree in a depth-first manner and maintain a stack structure stackp to control the tree traversal.

Algorithm Signature-Tree-Search
1. 2. 3. 4. 5. 6. 7. input: a query signature sq; output: set of signatures which survive the checking; S . Push the root of the signature-tree into stackp. If stackp is not empty, v pop(stack p); else return(S). If v is not a leaf node, i sk(v). If sq (i) = 0, push cr and cl into stack p; (where cr and cl are v’s right and left child, respectively) otherwise, push only cr into stackp. Compare sq with the signature pointed by p(v). /*p(v) - pointer to the block signature*/ If s q matches, S S {p(v)}. Go to (3).

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

66 Chen & Huck

Figure 10. Signature Tree Search
0 0 1 . 7 1 0 8 0 4 . 1 7 . 0 5 . 4 1 5 0 2 . 1 6 . 1 1 4 0 7 1 8 . 1 3 .

The following example helps to illustrate the main idea of the algorithm.

Example 3
Consider the signature file and the signature-tree shown in Figure 7(a) once again. Assume sq = 000 100 100 000. Then, only part of the signature-tree (marked with thick edges in Figure 10) will be searched. On reaching a leaf node, the signature pointed by the leaf node will be checked against sq. Obviously, this process is much more efficient than a sequential searching since only three signatures need to be checked while a signature file scanning will check eight signatures. For a balanced signature-tree, the height of the tree is bounded by O(log 2N), where N is the number of the leaf nodes. Then, the cost of searching a balanced signature-tree will be O( log 2N) on average, where represents the number of paths traversed, which is equal to the number of signatures checked. Let t represent the number of bits which are set in sq and checked during the search. Then, = O(N/2t). It is because each bit set to 1 will prohibit half of a subtree from being visited if it is checked during the search. Compared to the time complexity of the signature file scanning O(N), it is a major benefit. We will discuss this issue in the next section in more detail.

Time Complexity
In this section, we compare the costs of signature file scanning and signature-tree searching. First, we show that a signature-tree is balanced on the average. Based on this, we analyze the cost of a signature-tree search.

Analysis of Signature-Trees
Let T n be a family of signature-trees built from n signatures. Each signature is considered as a random bit string containing 0s and 1s. We assume that the probability of appearances of 0 and 1 in a string is equal to p and q = 1 - p, respectively. The occurrence of these two values in a bit string is independent of each other. To study the average length of paths from the root to a leaf, we check the external path length L n - the sum of the lengths of all paths from the root to all leaf nodes of a signature-tree in Tn. Note that in a signature-tree, the n signatures are split randomly into
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 67

the left subtree and the right subtree of the root. Let X denote the number of signatures in the left subtree. Then, for X = k, we have the following recurrence:

Ln =

n Lk

Ln k , for k for k

0, n 0, k n

undefined,

where Lk and Ln-k represent the external path length in the left and right subtrees of the root, respectively. Note that a signature-tree is never degenerate (i.e., k = 0 or k = n). So one-way branching on internal nodes never happens. The above formula is a little bit different from the formula established for the external path length of a binary tree: Bn = n + Bk + Bn-k, for all k = 0, 1, 2, ..., n,

where Bk represents the sum of the lengths of all paths from the root to all leaf nodes of a binary tree having k leaf nodes. According to Knuth (1973), the expectation of Bn is: EB0 = EB1 = 0,
n

EBn =

k 0

n k n p q k

k

Bk

Bn

k

,

n > 1.

When p = q = 0.5, we have:
n

EBn =

1
k 2

k

n k k 1 21

k

.

For large n the following holds:

EBn = nlog 2n + n[

L

+

1 + 2

1

(log2n)] -

1 L+ 2

2

( log2n),

where L = loge2, = 0.577... is the Euler constant, 1(x) and 2(x) are two periodic functions with small amplitude and mean zero (see Knuth, 1973, for a detailed discussion). In a similar way to Knuth (1973), we can obtain the following formulae: EL 0 = EL1 = 0,
n

EL n = n(1 - p - q ) +
n n

k 0

n k n p q k

k

Bk

Bn

k

,,

n > 1.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

68 Chen & Huck

When p = q = 0.5, we have:
n

ELn =

1
k 2

k

n k 21 k = EBn - n + dn,1, k 1 21 k

where n,1 represents the Kronecker delta function (Riordan, 1968) which is 1 if n = 1, 0 otherwise. From the above analysis, we can see that for large n we have the following: ELn = O(nlog2n). This shows that the average value of the external path length is asymptotically equal to nlog 2n, which implies that a signature-tree is normally balanced.

Time for Searching a Signature-Tree
As shown in Example 4, using a balanced signature-tree, the cost of scanning a signature file can be reduced from O(N) to O(N/2t), where t represents the number of some bits which are set in s q and occasionally checked during the search. If t = 1, only half of the signatures will be checked. If t = 2, one-quarter of the signatures will be checked, and so on. For a balanced signature-tree, the average height of the tree is O(log2N). During a search, if half of the s q’s bits checked are set to 1. Then, t = O(log2N)/2. Accordingly, the cost of a signature file scanning can be reduced to O(N/2(logN)/2). If one-third of the sq’s bits checked are set to 1, the cost of a signature file scanning can be reduced to O(N/ 2(logN)/3). Table 1 shows the calculation of this cost for different signature file sizes. Figure 11 is the pictorial illustration of Table 1. This shows that the searching of signature-trees outperforms the searching of signature files significantly.

SIGNATURE-TREE MAINTANENCE
In this section, we address how to maintain a signature-tree. First, we discuss the case that a signature-tree can entirely fit in main memory. Then, we discuss the case that a signature-tree cannot entirely fit in main memory.

Table 1. Cost Calculation
N N/2
(logN)/2

2000 44.68 159.36

4000 63.36 251.92

6000 77.76 330.10

8000 89.44 399.98

10000 100.00 463.90

12000 109.56 524.13

N/2(logN)/3

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 69

Figure 11. Time Complexity of Signature File Scanning and Signature Tree Searching
number of signature comparisons

12000 10000 8000 6000 4000 2000 0 2000 4000 6000 8000 10000 12000

signature file scanning

signature tree searching with t = O(log 2N)/3 signature tree searching with t = O(log2N )/2

N

Maintenance of Internal Signature-Trees
An internal signature-tree refers to a tree that can fit entirely in main memory. In this case, insertion and deletion of a signature into a tree can be done quite easily as discussed below. When a signature s is added to a signature file, the corresponding signature-tree can be changed by simply running the algorithm insert() once with s as the input. When a signature is removed from the signature file, we need to reconstruct the corresponding signature-tree as follows: i) Let z, u, v and w be the nodes as shown in Figure 12(a) and assume that the v is a pointer to the signature to be removed. ii) Remove u and v. Set the left pointer of z to w. (If u is the right child of z, set the right pointer of z to w.) The resulting signature-tree is as shown in Figure 12(b). From the above analysis, we see that the maintenance of an internal signature-tree is an easy task.

Maintenance of External Signature-Trees
In a database, files are normally very large. Therefore, we have to consider the situation where a signature-tree cannot fit entirely in main memory. We call such a tree an external signature-tree (or an external structure for the signature-tree). In this case, a signature-tree is stored in a series of pages organized into a tree structure as shown in Figure 13, in which each node corresponds to a page containing a binary tree. Figure 12. Illustration for Deletion of a Signature

z u v w w z

(a)

(b)

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

70 Chen & Huck

Figure 13. A Sample External Signature Tree
pages

Formally, an external structure ET for a signature-tree T is defined as follows. (To avoid any confusion, we will, in the following, refer to the nodes in ET as the page nodes while the nodes in T as the binary nodes or simply the nodes.) 1. Each internal page node n of ET is of the form: bn(rn, an1, ..., ), where bn represents a subtree of T, r n is its root and an1, ..., are its leaf nodes. Each internal node u of bn is of the form: <v(u), l(u), r(u)>, where v(u), l(u) and r(u) are the value, left link and right link of u, respectively. Each leaf node of bn is of the form: <v(), lp(), rp()>, where v() represents the value of , and lp() and rp() are two pointers to two pages containing the left and right subtrees of, respectively. 2. Let m be a child page node of n. Then, m is of the form: bm(r m, am1, ..., ), where bm represents a binary tree, r m is its root and am1, ..., are its leaf nodes. If m is an internal page node, am1, ..., will have the same structure as an1, ..., described in (1). If m is a leaf node, each = p(s), the position of some signature s in the signature file. 3. The size |b| of the binary tree b (the number of nodes in b) within an internal page node of ET satisfies: |b| 2k, 4. where k is an integer. The root page of ET contains at least a binary node and the left and right links associated with it.

If 2k-1 |b| 2k holds for each node in ET, it is said to be balanced; otherwise, it is unbalanced. However, according to the earlier analysis, an external signature-tree is normally balanced, i.e., 2k-1 |b| 2k holds for almost every page node in ET. As with a B+-tree, insertion and deletion of page nodes begin always from a leaf node. To maintain the tree balance, internal page nodes may split or merge during the process. In the following, we discuss these issues in great detail.

Insertion of Binary Nodes
Let s be a signature newly inserted into a signature file S. Accordingly, a node as will be inserted into the signature-tree T for S as a leaf node. In effect, it will be inserted into a leaf page node m of the external structure ET of T. It can be done by taking the binary tree within that page into main memory and then inserting the node into the tree. If for the binary tree b in m we have |b| > 2k, the following node-splitting will be conducted.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 71

1.

2.

Let bm(rm, am1, ..., ) be the binary tree within m. Let rm1 and rm2 be the left and right child node of r m, respectively. Assume that bm1(rm1, am1, ..., ) (i j < im) is the subtree rooted at r m1 and bm2(r m1, , ..., ) is rooted at rm2. We allocate a new page m’ and put bm2(r m1, , ..., ) into m’. Afterwards, promote r m into the parent page node n of m and remove bm2(r m1, , ..., ) from m. If the size of the binary tree within n becomes larger than 2k, split n as above. The node-splitting repeats along the path bottom-up until no splitting is needed.

Deletion of Binary Nodes
When a node is removed from a signature-tree, it is always removed from the leaf level as discussed in the above subsection. Let a be a leaf node to be removed from a signature-tree T. In effect, it will be removed from a leaf page node m of the external structure ET for T. Let b be the binary tree within m. If the size of b becomes smaller than 2k-1, we may merge it with its left or right sibling as follows. 1. Let m’ be the left (right) sibling of m. Let bm(r m, am1, ..., ) and bm’(r m’, am’1, ..., ) be two binary trees in m and m’, respectively. If the size of bm’ is smaller than 2k-1, move bm’ into m and afterwards eliminate m’. Let n be the parent page node of m and r be the parent node of rm and rm’. Move r into m and afterwards remove r from n. 2. If the size of the binary tree within n becomes smaller than 2k-1, merge it with its left or right sibling if possible. This process repeats along the path bottom-up until the root of ET is reached or no merging operation can be done. Note that it is not possible to redistribute the binary trees of m and any of its left and right siblings due to the properties of a signature-tree, which may leave an external signature-tree unbalanced. According to our analysis, however, it is not a normal case. Finally, we point out that for an application where the signature files are not frequently changed, the internal page nodes of an ET can be implemented as a heap structure. In this way, a lot of space can be saved.

CONCLUSION
In this chapter, a document management system is introduced. First, the system architecture and the document storage strategy have been discussed. Then, a new indexing technique, path signature, has been proposed to speed up the evaluation of the path-oriented queries. On the one hand, path signatures can be used as a filter to get away non-relevant elements. On the other hand, the technique of signature-trees can be utilized to establish index over them, which make us find relevant signatures quickly. As shown in the analysis of time complexity, high performance can be achieved using this technique.

REFERENCES
Abiteboul, S., Quass, D., McHugh, J., Widom, J. & Wiener, J. (1996). The Lorel Query Language for semi-structured data. Journal of Digital Libraries, 1(1).

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

72 Chen & Huck

Bosak, J. (1997, March). Java, and the future of the Web. Available online at: http:// sunsite.unc.edu/pub/sun-info/standards/xml/why/xmlapps.html. Chamberlin, D., Clark, J., Florescu, D., Robie, J., Simeon, J. & Stefanescu, M. (2001). Xquery 1.0: An XML Query Language. Technical Report, World Wide Web Consortium, Working Draft 07. Chen, Y. & Huck, G. (2001). On the evaluation of path-oriented queries in document databases. Lecture Notes in Computer Science, 2113, 953-962. Christodoulakis, S. & Faloutsos, C. (1984). Design consideration for a message file server. IEEE Transactions on Software Engineering, 10(2), 201-210. Christophides, V., Cluet, S. & Simeon, J. (2000). On wrapping query languages and efficient XML integration. Proceedings of the ACM SIGMOD Conference on Management of Data, 141-152. Deutsch, A., Fernandez, Florescu, D., Levy, A. & Suciu, D. (1988, August). XML-QL: A Query Language for XML. Available online at: http://www.w3.org/TR/NOTE-xmlql/. Faloutsos, C. (1985). Access methods for text. ACM Computing Surveys, 17(1), 49-74. Faloutsos, C. (1992). Signature files. In Frakes, W.B. & Baeza-Yates, R. (Eds.), Information Retrieval: Data Structures & Algorithms. Englewood Cliffs, NJ: Prentice Hall, 44-65. Florescu, D. & Kossman, D. (1999). Storing and querying XML data using an RDBMS. IEEE Data Engineering Bulletin, 22(3). Huck, G., Macherius, I. & Fankhauser, P. (1999). PDOM: Lightweight persistency support for the Document Object Model. Proceedings of the OOPSLA’99 Workshop: Java and Databases: Persistence Options, November. Knuth, D.E. (1973). The Art of Computer Programming: Sorting and Searching. London: Addison-Wesley. Marchiori, M. (1998). The Query Languages Workshop (QL’98). Available online at: http://www.w3.org/TandS/QL/QL98. Morrison, D.R. (1968). PATRICIA—Practical Algorithm To Retrieve Information Coded in Alphanumeric. Journal of Association for Computing Machinery, 15(4), 514534. Pixley, T. (2000). Document Object Model (DOM) Level 2 Events Specification Version 1.0. W3C Recommendation. Riordan, J. (1968). Combinatorial Identities. New York: John Wiley & Sons. Robie, J., Chamberlin, D. & Florescu, D. (2000). Quilt: An XML query language for heterogeneous data sources. Proceedings of the International Workshop on the Web and Databases. Robie, J., Lapp, J. & Schach, D. (1998). XML Query Language (XQL). Proceedings of W3C QL’98—The Query Languages Workshop. Shanmugasundaram, J., Shekita, R., Carey, M.J., Lindsay, B.G., Pirahesh, H. & Reinwald, B. (2000). Efficiently publishing relational data as XML documents. Proceedings of the International Conference on Very Large Data Bases (VLDB’00), 65-76. Shanmugasundaram, J., Tufte, K., Zhang. C., He,. D.J., DeWitt, J. & Naughton, J.F. (1999). Relational databases for querying XML documents: Limitations and opportunities. Proceedings of the International Conference on Very Large Data Bases (VLDB’99), 302-314.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Building Signature-Trees on Path Signatures in Document Databases 73

Suciu, D. & Vossen, G. (2000). Proceedings of the Third International Workshop on the Web and Databases (WebDB 2000), LNCS. Springer-Verlag. World Wide Web Consortium. (1998a, February). Extensible Markup Language (XML) 1.0. Available online at: http//www.w3.org/TR/ 1998/REC-xml/19980210. World Wide Web Consortium. (1998b, December). Extensible Style Language (XML) Working Draft. Available online at: http/ /www.w3.org/TR/1998/WD-xsl-19981216. World Wide Web Consortium. (1998c). Document Object Model (DOM) Level 1. Available online at: http://www.w3.org/TR/ REC-DOM-Level-1/. Yoshikawa, M., Amagasa, T., Shimura, T. & Uemura, S. (2001). Xrel: A path-based approach to storage and retrieval of XML documents using relational databases. ACM Transactions on Internet Technology, 1(1).

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

74 Silva, Calado, Vieira, Laender & Ribeiro-Neto

Chapter VI

Keyword-Based Queries Over Web Databases
Altigran S. da Silva Universidade Federal do Amazonas, Brazil Pável Calado Universidade Federal de Minas Gerais, Brazil Rodrigo C. Vieira Universidade Federal de Minas Gerais, Brazil Alberto H.F. Laender Universidade Federal de Minas Gerais, Brazil Bertheir A. Ribeiro-Neto Universidade Federal de Minas Gerais, Brazil

ABSTRACT
In this chapter, we propose an approach to using keywords (as in a Web search engine) for querying databases over the Web. The approach is based on a Bayesian network model and provides a suitable alternative to the use of interfaces based on multiple forms with several fields. Two major steps are involved when querying a Web database using this approach. First, structured (database-like) queries are derived from a query composed only of the keywords specified by the user. Next, the structured queries are submitted to a Web database, and the retrieved results are presented to the user as ranked answers. To demonstrate the feasibilityof the approach, a simple prototype Web search system based on the approach is presented. Experimental results obtained with this system indicate that the approach allows for accurately structuring the user queries and retrieving appropriate answers with minimum intervention from the user.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Keyword-Based Queries Over Web Databases 75

INTRODUCTION
Online information services, such as online stores and digital libraries, have become widespread on the Web nowadays. Such services allow a great number of users to access a large volume of data stored in local databases, also called Web databases. Web users, however, are usually non-specialized and their interests vary greatly. Thus, two important problems are posed to designers of interfaces for Web databases: simplicity and uniformity. Interfaces for accessing Web databases are expected to be simple, since they are intended for laymen users. In addition, if an online service is to provide access to different types of information (i.e., many distinct databases), its interface should be as uniform as possible. Otherwise, users will be required to learn how to use a different interface for each distinct database. The most common solution for implementing online services that access Web databases is the use of customized forms, navigation menus and similar browsing mechanisms. Although useful in some cases, this approach has some important shortcomings. Websites that provide access to multiple databases, such as Amazon.com (http://www.amazon.com), MySimon (http://www.mysimon.com) or Travelocity (http:/ /www.travelocity.com), include dozens of different forms, one for each type of product, where each form might be composed of a large number of fields. From the point of view of a Web user, this type of interface might seem rather complex. From the point of view of a Web developer, it increases the development time and maintenance costs. Another common inconvenience of query interfaces for Web databases is the fact that the answer set is frequently too large. In a traditional database system, appropriate tools and query languages are available to restrict the search results. In a Web search engine, document ranking (Baeza-Yates & Ribeiro-Neto, 1999) is used to deal with this problem. In Web database interfaces, however, such a method is usually not available. In this chapter, we describe the use of keyword-based querying (as in a Web search engine) with Web databases and argue that this approach provides a suitable alternative to the use of interfaces based on multiple forms with several fields. Additionally, we show how to use a relevance criteria to rank a possibly large set of answers retrieved by a keyword-based query, as done in Web search engines. Our approach uses a Bayesian network (Ribeiro-Neto & Muntz, 1996) to model and derive structured (database-like) queries from a query composed only of the keywords specified by the user. The structured queries are then submitted to a Web database and the retrieved results are presented to the user as ranked answers. This means that the user needs just to fill in a single search box to formulate a query. Our approach is thus able to provide online services with: (1) an interface that is simple and intuitive to Web users, and (2) the possibility of querying several heterogeneous databases using a single interface. To demonstrate the feasibility of our approach, a simple prototype Web search system was implemented. Results obtained using this prototype on databases of three distinct domains (Calado, Silva, Vieira, Laender & Ribeiro-Neto, 2002) indicate that our approach allows accurately structuring the user queries and retrieving appropriate answers with minimum intervention from the user. The remainder of this chapter is organized as follows. First, we briefly review the traditional paradigm employed for querying Web databases. We then discuss the use of

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

76 Silva, Calado, Vieira, Laender & Ribeiro-Neto

keywords for formulating queries, and the following section presents our framework for allowing keyword-based queries over Web databases. Finally, we present our conclusions and discuss future work.

TRADITIONAL QUERY PARADIGMS FOR WEB DATABASES
One of the important benefits introduced by the Web was the possibility of making data of general interest, stored in local databases, universally available in a uniform manner. Before the Web, specific interfaces and data access protocols had to be used. In some cases, standard data access protocols (e.g., ODBC) were (and continue to be) used. However, such protocols have limited applications. For instance, they require the use of software layers not always available to the casual user. Thus, the Web (and its HTTP protocol) has emerged as a practical and flexible way of sharing database contents. The access to Web databases is usually done through mechanisms that allow a Web server to contact a DBMS, submit queries to it, receive the results and further process them to fulfill a user request. Figure 1 illustrates the execution of a query submitted to a database over the Web. In this figure, we distinguish two major steps. In Step 1, the Web server receives a query from a client (usually a browser) and, in Step 2 the Web server interacts with the DBMS (in practice, these two steps can be further divided into smaller ones but for our explanatory purposes here, they suffice). There are several options for implementing the interaction between the Web server and the DBMS (Step 2). However, since discussing these options is out of the scope of this chapter, we refer the interested user to more appropriate references on this topic such as Ehmayer, Kappel and Reich (1997) and Labrinidis and Roussopoulos (2000). For processing Step 1, the Web server receives a query encoded according to some standard as input. This query is produced by the Web client as the result of an interaction with a user. By far, the most common solution for implementing Step 1 is by means of customized HTML forms, navigation menus and similar browsing mechanisms. By navigating through menus, the users implicitly select one among several “template” Figure 1. Execution of a Query Submitted to a Database over the Web

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Keyword-Based Queries Over Web Databases 77

Figure 2. Examples of Typical Forms for Querying Web Databases

(a)

(b)

(c)

(d)

queries encoded in the HTML pages (or similar documents). By filling in form fields, the users supply the arguments needed for the query parameters. Figure 2(a) illustrates a typical form for querying Web databases. It contains several fields, but just a few need be actually filled in to produce a valid query. For instance, at the bottom of the form there is no obligation of filling in any of the airline fields. In Figure 2(b), we show another typical type of form, in this case for searching music products on the Amazon.com website. Notice that there are also several distinct types of fields to be filled (text box, check box, check buttons, etc.). Further, there is one such form for each product type by Amazon.com (e.g., books, movies, etc.). For instance, Figure 2(c) shows the advanced search form for books. Having a multitude of distinct forms represents a problem not only for the users, who have to deal with the peculiarities of each of them, but also for the developers, who have to design and implement a specific type of form for each product type. A third example of a form for querying Web databases is presented in Figure 2(d). This form features a single field, where a user can provide a value for a single-parameter
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

78 Silva, Calado, Vieira, Laender & Ribeiro-Neto

query. In cases like this, the actual parameter also has to be selected using a menu that appears as a companion to the single field. Thus, a user can only search for a book by its title or by the name of its author, but not by both. Similarly, the value provided in the single field can be used exclusively as the title of a CD, a name of an artist and so on. From these example cases, we can say that, despite its apparent simplicity, the solution of using multi-field forms for building queries to Web databases has important shortcomings, particularly for websites that provide access to multiple databases.

KEYWORD-BASED QUERIES
The use of keywords for query formulation is a quite common resource in information retrieval systems, like Web search engines (Ribeiro-Neto & Muntz, 1996; Silva, Ribeiro-Neto, Calado, Moura & Ziviani, 2000). In the context of structured and Web databases, however, only more recently have some proposals for the use of keywordbased queries appeared (Agrawal, Chaudhuri & Das, 2002; Dar, Entin, Geva & Palmon, 1998; Florescu, Kossmann & Manolescu, 2000; Ma, 2002). Agrawal et al. (2002) introduce a system that allows querying databases through keywords. Their work, however, focuses on relational databases and does not provide any ranking for the approximate answers. The DataSpot system, described in Dar et al. (1998), proposes the use of “plain language” queries and navigations to explore a hyperbase, built for publishing contents of a database on the Web. The work described in Florescu, Kossmann and Manolescu (2000) proposes the extension of XML-QL, a wellknown query language for XML, with keyword-based searching capabilities. Our solution is distinct since it adopts a much simpler query language. Although this can make our approach less expressive, it also makes it more accessible to regular Web users. In addition, we propose to rank the answers, since spurious data will necessarily be present. In Ma (2002), a strategy is proposed that allows users to search a set of databases and determine the ones that might contain the data of interest. Different from what we propose, this strategy is based on controlled vocabularies, which are used to match the provided keywords with human-provided characterizations of the databases. The vector space model has been widely used as a solution to the problem of ranking query results in text databases (Baeza-Yates & Ribeiro-Neto, 1999). In Cohen (1999), for instance, the vector model is used to determine the similarity between objects in a database composed of relations whose attribute values are free text. For ranking structured objects returned as answers to queries over a database, the work described in Chaudhuri and Gravano (1999) combines application-oriented scoring functions. To exemplify, in a query over a real-state database, scoring functions that evaluate the values of the attributes price and numberofbedrooms may be used for determining the ranking of the answers. An extension of such an idea is proposed in Bruno, Gravano and Marian (2002) for the case of Web-accessible databases. In this case, the scoring functions may be based on evidence coming from distinct Web databases. For instance, while one of these Web databases supplies values for evaluating the rating attribute, another database may supply values for evaluating the price attribute. For determining the final ranking, the evaluations of the individual attributes are combined. In Golman, Shivakumar, Venkatasubramanian and Garcia-Molina (1998), object ranking is based on a measure of the proximity between the objects (seen as nodes) in
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Keyword-Based Queries Over Web Databases 79

the database (seen as a graph). In a movie database, for instance, the movies somehow related to “Cage” and “Travolta” can be retrieved by ranking the movie nodes (objects) that present the smallest distance from the actor or director nodes (objects) whose values contain the strings “Cage” or “Travolta.” This approach led to a search system similar to ours in functionality, since it also allows keyword-based queries, but one that is based on a rather distinct approach.

A FRAMEWORK FOR QUERYING WEB DATABASES USING KEYWORDS
We now present an overview of our approach to query Web databases by using simple keyword-based queries. To simplify the discussion, we start with some basic definitions. In our work, query structuring and the ranking of query results are performed using vectorial techniques modeled in a Bayesian belief network, similar to the one described in Ribeiro-Neto and Muntz (1996). Bayesian networks were first used to model information retrieval problems by Turtle and Croft (1991), and as demonstrated in Ribeiro-Neto, Silva and Muntz (2000), they can be used to represent any of the classic models in information retrieval. Bayesian network models are especially useful when we need to determine the relevance of the answers in view of much independent evidence (Baeza-Yates & RibeiroNeto, 1999). In our case, Bayesian networks are used to compute: (a) the likeliness that a structured query assembled by the system represents the user information need and (b) the relevance of the retrieved answers with regard to the user information need.

Definitions
Consider a database D accessible through a Web query interface (for instance, an HTML form). We define this database as a collection of objects: D = {o1, o2, , on}, n 1.

Each object oi is a set of attribute-value pairs:

oi

A1 , v1i ,

, Aki , vkii , k i

1

where each Aj is an attribute and each vji is a value belonging to the domain of Aj . We note that the attributes do not need, necessarily, to be the same for all objects. For some attributes, instead of a single value, we may have a list of values. For instance, in an object representing a movie, the attribute actor might be a list of names. To represent this, we allow a same attribute to appear several times. Thus, if the attribute Aj of an object oi has n different values, we can represent object oi as:

oi

, A j , v1 , A j , v 2 ,

A j , vn ,

.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

80 Silva, Calado, Vieira, Laender & Ribeiro-Neto

We define a database schema SD as the set of all attributes that compose any of the stored objects. Thus, the schema of a database D is defined as:
SD A j A j is an attribute of some object oi D .

We define an unstructured query U as a set of keywords (or terms) t k, as follows:

U

t1 , t 2 ,

, tk .

A structured query Q is defined as a set of ordered pairs:
Q A1 , v1q , , Am , vmq , m 1

where each Aj is an attribute and each vjq is a value belonging to the domain of Aj. This simplified definition of a database allows us to ignore the details of how its structure is represented. Also, we can regard the database simply as a data repository available through some high-level user interface, ignoring whether it is composed of a single relational table, of a set of relational tables, of a set of XML documents or any other. Throughout the text, we informally use the term application domain of a database to refer to the common knowledge semantic domain associated with the objects in the database. For instance, a database with the application domain Book stores objects with information on books. The database to be queried by the user is called the target database. As an example, consider a database D with the application domain Book. An object oi in D could be:

oi

Title, " I, Robot" , Author, " Isaac Asimov" .

An example of an unstructured query is:

K

" Asimov" , " robot"

An example of a corresponding structured query is:
Q Author, " Isaac Asimov" .

Overview
Querying Web databases using keyword-based queries involves four main steps: (1) specifying an unstructured query as input, (2) building a set of candidate structured queries associated with the unstructured query given as input, (3) selecting one or more candidate structured queries as the “best” ones and (4) processing the results of these selected queries. Figure 3 illustrates the architecture of a search system using this approach.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Keyword-Based Queries Over Web Databases 81

Figure 3. Search System Architecture for Querying Web Databases Using Unstructured Queries

Step 1 consists simply of receiving the unstructured query from the user as a set of keywords. This can be easily accomplished using a simple search box interface, as shown in Figure 4, in which the queries are specified as sets of keywords. In Step 2, for a given unstructured query K = {t 1,t 2,...tn}, we need to determine a set Pk of candidate structured queries. Let D be a database with a schema SD. A simple way to determine Pk is to build all possible combinations of all query terms ti K and all attributes Aj SD. Clearly, in the worst case, this operation would be of combinatorial complexity. In practice, however, it is possible to discard many combinations of terms and attributes (i.e., many attribute value pairs) in advance. For instance, if there are little or no occurrences of the word “Hamlet” in the Author attribute, we can discard all possible queries that make such an assignment. Once we have determined a set Pk={Q1,...,Qn} of candidate structured queries, we proceed to Step 3 that consists of selecting which queries are most likely to match the user information need. For instance, the unstructured query K = {“asimov”,”robot”} might lead to the structured query Q 1 = {<Author, “Asimov”>,<Title, “robot”>}. More precisely, the query structuring process consists of determining the database attributes which are most likely to correspond to each term in K. In Figure 4 some structured queries for the keyword-based query “little prince” are shown. The best candidate structured query is the one that maximizes the likelihood that its component attribute-value pairs correspond to the user query intention. To determine such best candidate query, we need to rank the candidate (structured) queries with regard to the input query, as we later discuss. In Step 4, we take the top-ranked candidate structured queries in PK and process them. For instance, we can pick the best ranked query and process it without user interference. Alternatively, we can present the top-ranked queries to the user and let him choose one of them for processing. In both cases, a structured query Q is submitted to the target database D and a set of objects RDQ is retrieved. The objects are then ranked according to the probability of satisfying the user information need, as we later discuss. Figure 5 illustrates ranked results produced by the unstructured query “Little Prince.” Ranking the retrieved objects is especially important when the query can be sent to more than one target database and the results merged together. For instance, in an application domain like Computer Science Bibliography, the same unstructured query can be used to search the ACM and IEEE digital libraries. Notice that this is accomplished without the need to define a unifying common database schema.

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

82 Silva, Calado, Vieira, Laender & Ribeiro-Neto

Figure 4. Candidate-Structured Queries for the Keyword-Based Query “Little Prince”

To rank the candidate queries in Step 3 (from set Pk) and the retrieved objects in Step 4 (from set RDQ), we adopt the framework of Bayesian belief networks (Pearl, 1988). The following section gives a brief introduction to the use of Bayesian networks in information retrieval problems. We then present a Bayesian network model for ranking candidate queries, followed by a Bayesian network model for ranking the retrieved objects.

Bayesian Networks
Bayesian networks were introduced in the context of information retrieval by Turtle and Croft (1991). Later, Ribeiro-Neto and Muntz (1996) refined this model and used it to combine information from past queries. Both versions take an epistemological view (as opposed to a frequentist view) of the information retrieval problem, interpreting probabilities as degrees of belief devoid of experimentation. For this reason, they are also called belief networks. Figure 5. Results for the Keyword-Based Query “Little Prince”

Last

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Keyword-Based Queries Over Web Databases 83

Figure 6. Belief Network for a Query Composed of the Keywords K1 and K2
Q

Query side

k

K1

K2

Ki

Ki

Root nodes

D1

Dj

DN

Document side

Bayesian networks provide a graphical formalism for explicitly representing independencies among the variables of a joint probability distribution. The probability distribution is represented through a directed acyclic graph, whose nodes represent the random variables of the distribution. Thus, two random variables, X and Y, are represented in a Bayesian network as two nodes in a directed graph. An edge directed from Y to X represents the influence of the node Y, the parent node, on the node X, the child node. The intensity of the influence of the variable Y on the variable X is quantified by the conditional probability P(x|y), for every possible set of values (x,y). According to the model proposed by Ribeiro-Neto and Muntz (1996), queries, documents and keywords are seen as events. These events are not independent, since, for instance, the occurrence of a term will influence the occurrence of a document. Using a Bayesian network, we can model these events and their interdependencies. Considering the fact that both documents and queries are composed of keywords, we can model the document retrieval problem using the network in Figure 6. In this network, each node Dj models a document, the node Q models the user query, and the Ki nodes model the terms in the collection. The vector k is used to refer to any of the possible states of the Ki root nodes. The similarity between a document Dj and the query Q can be interpreted as the probability of document Dj occurring given that query Q has occurred. Thus, using Bayes’ law and the rule of total probabilities, we compute the similarity P(dj|q) as:

P( d j | q)
k

P(d j | k ) P(q | k ) P(k )

where =1/P(q) is a normalizing constant. This is the generic expression for the ranking of a document Dj with regard to a query Q, in the belief network model. To represent any traditional information retrieval model using the network in Figure 6, we need only to define the probabilities P(dj|k), P(q|k) and P(k) appropriately. Using a similar approach, we can build Bayesian network models that allow us to both select the structured query most likely to correspond to the users’ needs and to rank the final query results. In the following sections we present these two models.
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

84 Silva, Calado, Vieira, Laender & Ribeiro-Neto

Figure 7. Bayesian Network Model for Query Structuring
Q1 Q 2 and Q 3 and Q4

Q 11

Q 21

Q 31

Q 22

Q 32

Q 42

a1

a 11

a 12

a13

a 1k

1

a 21

a 22

a 23

a 2k

a2
2

A 11

A 12

A 13

A 1n

1

A 21

A 22

A 2233

A 2k

2

A1

or
A2

or

O

and

Finding the Best Queries
The query structuring process associates structure to a given query K, consisting only of keywords. Several structured queries are generated for a same unstructured query. For this reason, we need a consistent way of ranking the queries according to their similarity to the database, in other words, their similarity to the application domain in question. The ranking of structured queries is accomplished through the use of the Bayesian network model shown in Figure 7. Although the network can be easily expanded to model any database schema, for simplicity, here we show only two attributes, A1 and A2. The network consists of a set of nodes, each representing a piece of information from the problem to be solved. To each node in the network is associated a binary random variable. This variable takes the value 1 to indicate that the corresponding information will be accounted for in the ranking computation. In this case, we say that the information was observed. To simplify the notation, we define Ti as the set of all terms in the database that compose the values in the domain of attribute Ai. In this case, each value is considered as a string and the terms are the words in the string. In the network of Figure 7, the database is represented by the node O. Each node Ai represents an attribute in the database. Each node aij represents a term in Ti. Each node Qi represents a structured query to be ranked. Each node Qij represents the portion of the structured query Qi that corresponds to the attribute Aj. Vectors a1 and a 2 represent a possible state of the variables associated to the nodes a1i and a2i, respectively. This network can be used to model, for instance, a database on books, with attributes A1 = Title and A2 = Author. In this case, node A11 represents the title of a stored book, like “I, Robot,” where the term “I” is represented by node a11 and the term “Robot” is represented by node a12. In a similar fashion, node Q2 represents the structured query Q2 = {<Title, “I, Robot”>, <Author, “Asimov”>}, where Q21 is the part referring to the Title attribute, Q22 the part referring to the Author attribute. Node a22 is the term “Asimov.”
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Keyword-Based Queries Over Web Databases 85

The similarity of a structured query Qi with the database O can be seen as the probability of observing Qi, given that the database O was observed, P(Qi|O). Examining the network in Figure 7, we can derive the following equation:

P(Qi | O)
a1 , a 2

P(Qi | a1 , a 2 ) P (O | a1 , a 2 ) P( a1 , a 2 ) P(Qi1 | a1 ) P(Qi 2 | a 2 )
a1 ,a2

P( A1 | a1 ) P( A2 | a 2 ) P( a1 ) P( a 2 )
where is a normalizing constant (see Pearl, 1988, and Ribeiro-Neto & Muntz, 1996, for details). The following equation is the general equation for ranking a structured query. The conditional probabilities can now be defined according to the values stored in the database. We start with the probability of observing the part of the query Qi assigned to an attribute Aj, given that a set of terms (indicated by aj) was observed:

P(Qij | a j )

1 if k , g k ( a j ) 1 iff t jk ocurs in Qij 0 otherwise

where gk( a j ) gives the value of the k-th variable of the vector a j and tjk is the k-th term in Tj. This equation guarantees that the only states considered are those in which the only active terms are those that compose the query Qi. The probability of observing the attribute Ai, given the terms indicated by ai , is defined as a disjunction of all probabilities for each possible value of Ai, i.e.:

P( Ai | a i ) 1

(1 P( Aij | a i ))
1 j ni

where ni is the number of values in the database for attribute Ai. If we consider that P(Aij| a j ) measures the similarity between the value Aij and the terms indicated by ai , then the equation above tells us that, if the terms in ai fit exactly one of the values Aij, the final probability is 1. If not, the final probability will accumulate all the partial similarities between the terms in ai and the values Aij. To define the probability of observing a value Aij given a set of terms indicated by
ai , P(Aij| ai ), we use the cosine measure, as defined for the vector space model in

information retrieval systems (Salton & McGill, 1983). The value Aij for attribute Ai is seen as a vector of |T i| terms. To each term t k in Aij, we assign a weight wik that reflects the importance of the term tk for attribute Ai, in the database, i.e.:
Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

86 Silva, Calado, Vieira, Laender & Ribeiro-Neto

wik

log(1 f ki ) log(1 ni ) 0

if tk occurs in Aij otherwise

where f ki is the number of occurrences of term tk in the values of the attribute Ai, and ni is the total number of values for attribute Ai, in the database. Notice that we do not consider the traditional idf normalization (Salton & McGill, 1983), since this would penalize the most frequent terms, which are the most important ones in our approach. The probability of observing Aij is, therefore, defined as the cosine of the angle between vector Aij and vector ai , i.e.:

P( Aij | a i ) cos( Aij , a i )

t k Ti

wik g k (a i )
t k Ti 2 wik

.

Finally, since we have no a priori preference for any set of terms, the following equation defines the probability of vector ai as a constant:

P (a i )

1 . 2|Ti |

In sum, the probability P(Qi|O) is the conjunction of the similarities between the attribute values in the database and the values assigned to the respective attributes in the structured query. This is translated by the equation:

P(Qi | O)

1

(1 cos( A1 j , a1 ))
1 j n1

1

(1 cos( A2 j , a 2 ))
1 j n2

where a1 and a 2 are the states where only the query terms referring to attributes A1 and A2, respectively, are active; n1 and n2 are the total number of values for attributes A1 and A2 in the database; and accounts for the constants and P( ai ). We can now rank all the structured queries by computing P(Qi|O) for each of them. The user can then select one query for processing among the top-ranked ones, or the system can simply process the highest ranked query.

Finding the Best Answers
Once a structured query is selected, it is submitted to the target database. The set of objects retrieved can then be ranked according to the probability of satisfying the user

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

Keyword-Based Queries Over Web Databases 87

Figure 8. Bayesian Network Model for Result Ranking
And Q And

QA1

QA2

a1

a 11

a 12

a 13

a 1k

1

a 21

a 22

a 23

a 2k

2

a2

A 11

A 12

A 13

A 1n Or

A 21

A 22

A 2k

L 21

L 22

L 23

L2n

O 1 Avg

O 2 Avg O 3

Avg

On

Avg

information need. To this effect, the structured query and the retrieved objects are modeled using the Bayesian network model shown in Figure 8. Again, although the network can be easily expanded to model any database schema, here we show only two attributes, A1 and A2. To simplify the notation, we define T i as the set of all terms that compose the values in the domain of the attribute Ai. When ranking the returned objects, special attention needs to be paid to value lists, i.e., attribute values that form a list. To exemplify how this type of attribute is treated, in the network we represent attribute A2 as a value list attribute. In the network in Figure 8, node Q represents the structured query, submitted for processing. Each node QA i represents the part of the structured query Q relative to the attribute Ai. Each node aij represents a term in T i. Each node Aij represents the value of the attribute Ai in object Oj. Since attribute A2 is a list attribute, we add one more level to the network. Each node L 2j represents the list of values assigned to the attribute A2 in object Oj. Finally, each node Oj represents a returned object. As before, with each node in the network is associated a random variable that takes the value 1 to indicate that information regarding the respective node was observed. Vectors a1 and a 2 represent a possible state of the variables associated with nodes a1i and a2i, respectively. An object Oj is ranked according to the probability of satisfying the structured query Q, i.e., the probability of observing Oj, given that Q was observed, P(Oj|Q). Examining the network in Figure 8, we have that:

P(O j | Q)
a1 , a 2

P(Q | a 1 , a 2 ) P(O j | a 1 , a 2 ) P( a 1 , a 2 ) P(QA1 | a1 ) P (QA2 | a 2 )

a1 , a 2

Copyright © 2003, Idea Group Inc. Copying or distributing in print or electronic forms without written permission of Idea Group Inc. is prohibited.

