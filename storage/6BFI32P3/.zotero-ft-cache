Mathematical Methods for Knowledge Discovery and Data Mining
Giovanni Felici Consiglio Nazionale delle Ricerche, Rome, Italy Carlo Vercellis Politecnico di Milano, Italy

InformatIon scIence reference
Hershey • New York

Acquisitions Editor: Development Editor: Senior Managing Editor: Managing Editor: Copy Editor: Typesetter: Cover Design: Printed at:

Kristin Klinger Kristin Roth Jennifer Neidig Sara Reed Angela Thor Jamie Snavely Lisa Tosheff Yurchak Printing Inc.

Published in the United States of America by Information Science Reference (an imprint of IGI Global) 701 E. Chocolate Avenue, Suite 200 Hershey PA 17033 Tel: 717-533-8845 Fax: 717-533-8661 E-mail: cust@igi-global.com Web site: http://www.igi-global.com/reference and in the United Kingdom by Information Science Reference (an imprint of IGI Global) 3 Henrietta Street Covent Garden London WC2E 8LU Tel: 44 20 7240 0856 Fax: 44 20 7379 0609 Web site: http://www.eurospanonline.com Copyright © 2008 by IGI Global. All rights reserved. No part of this publication may be reproduced, stored or distributed in any form or by any means, electronic or mechanical, including photocopying, without written permission from the publisher. Product or company names used in this set are for identification purposes only. Inclusion of the names of the products or companies does not indicate a claim of ownership by IGI Global of the trademark or registered trademark. Library of Congress Cataloging-in-Publication Data Felici, Giovanni. Mathematical methods for knowledge discovery and data mining / Giovanni Felici & Carlo Vercellis, editors. p. cm. Summary: “This book focuses on the mathematical models and methods that support most data mining applications and solution techniques, covering such topics as association rules; Bayesian methods; data visualization; kernel methods; neural networks; text, speech, and image recognition; an invaluable resource for scholars and practitioners in the fields of biomedicine, engineering, finance, manufacturing, marketing, performance measurement, and telecommunications”--Provided by publisher. Includes bibliographical references and index. ISBN 978-1-59904-528-3 (hardcover) -- ISBN 978-1-59904-530-6 (ebook) 1. Data mining. 2. Data mining--Mathematical models. 3. Knowledge acquisition (Expert systems) I. Felici, Giovanni. II. Vercellis, Carlo. III. Title. QA76.9.D343F46 2007 006.3’12--dc22 2007022228 British Cataloguing in Publication Data A Cataloguing in Publication record for this book is available from the British Library. All work contributed to this book set is new, previously-unpublished material. The views expressed in this book are those of the authors, but not necessarily of the publisher.

Table of Contents

Foreword ............................................................................................................................................. xii Preface ................................................................................................................................................ xiv Acknowledgment ................................................................................................................................ xx

Chapter I Discretization of Rational Data / Jonathan Mugan and Klaus Truemper .............................................. 1 Chapter II Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem / Massimo Liquori and Andrea Scozzari ................................................................. 24 Chapter III Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations / Xenia Naidenova ........................................................................................... 41 Chapter IV The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification / Giovanni Felici and Valerio Gatta ........................................................... 65 Chapter V Support Vector Machines for Business Applications / Brian C. Lovell and Christian J. Walder......... 82 Chapter VI Kernel Width Selection for SVM Classification: A Meta-Learning Approach / Shawkat Ali and Kate A. Smith ........................................................................................................... 101 Chapter VII Protein Folding Classification Through Multicategory Discrete SVM / Carlotta Orsenigo and Carlo Vercellis ............................................................................................... 116

Chapter VIII Hierarchical Profiling, Scoring, and Applications in Bioinformatics / Li Liao .................................. 130 Chapter IX Hierarchical Clustering Using Evolutionary Algorithms / Monica Chiş ............................................ 146 Chapter X Exploratory Time Series Data Mining by Genetic Clustering / T. Warren Liao ................................. 157 Chapter XI Development of Control Signatures with a Hybrid Data Mining and Genetic Algorithm / Alex Burns, Shital Shah, and Andrew Kusiak ..................................................................................... 179 Chapter XII Bayesian Belief Networks for Data Cleaning / Enrico Fagiuoli, Sara Omerino, and Fabio Stella .................................................................................................................................. 204 Chapter XIII A Comparison of Revision Schemes for Cleaning Labeling Noise /Chuck P. Lam and David G. Stork ............................................................................................................................. 220 Chapter XIV Improving Web Clickstream Analysis: Markov Chains Models and Genmax Algorithms / Paolo Baldini and Paolo Giudici........................................................................................................ 233 Chapter XV Advanced Data Mining and Visualization Techniques with Probabilistic Principal Surfaces: Applications to Astronomy and Genetics / Antonino Staiano, Lara De Vinco, Giuseppe Longo, and Roberto Tagliaferri .......................................................................................... 244 Chapter XVI Spatial Navigation Assistance System for Large Virtual Environments: The Data Mining Approach / Mehmed Kantardzic, Pedram Sadeghian, and Walaa M. Sheta .......... 265 Chapter XVII Using Grids for Distributed Knowledge Discovery / Antonio Congiusta, Domenico Talia, and Paolo Trunfio ............................................................................................................................... 284 Chapter XVIII Fuzzy Miner: Extracting Fuzzy Rules from Numerical Patterns / Nikos Pelekis, Babis Theodoulidis, Ioannis Kopanakis, and Yannis Theodoridis .............................. 299

Chapter XIX Routing Attribute Data Mining Based on Rough Set Theory / Yanbing Liu, Menghao Wang, and Jong Tang ..................................................................................................................................... 322

Compilation of References .............................................................................................................. 338 About the Contributors ................................................................................................................... 361 Index ................................................................................................................................................... 368

Detailed Table of Contents

Foreword ............................................................................................................................................. xii Preface ................................................................................................................................................ xiv Acknowledgment ................................................................................................................................ xx

Chapter I Discretization of Rational Data / Jonathan Mugan and Klaus Truemper .............................................. 1 Frequently, one wants to extend the use of a classification method that, in principle, requires records with True/False values, so that records with rational numbers can be processed. In such cases, the rational numbers must first be replaced by True/False values before the method may be applied. In other cases, a classification method, in principle, can process records with rational numbers directly, but replacement by True/False values improves the performance of the method. The replacement process is usually called discretization or binarization. This chapter describes a recursive discretization process called Cutpoint. The key step of Cutpoint detects points where classification patterns change abruptly. The chapter includes computational results where Cutpoint is compared with entropy-based methods that, to date, have been found to be the best discretization schemes. The results indicate that Cutpoint is preferred by certain classification schemes, while entropy-based methods are better for other classification methods. Thus, one may view Cutpoint to be an additional discretization tool that one may want to consider. Chapter II Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem / Massimo Liquori and Andrea Scozzari ................................................................................................. 24 Traditional classification approaches consider a dataset formed by an archive of observations classified as positive or negative according to a binary classification rule. In this chapter, we consider the financial timing decision problem, which is the problem of deciding the time when it is profitable for the investor to buy shares or to sell shares or to wait in the stock exchange market. The decision is based on classifying a dataset of observations, represented by a vector containing the values of some financial numerical attributes, according to a ternary classification rule. We propose a new technique based on partially defined

vector Boolean functions. We test our technique on different time series of the Mibtel stock exchange market in Italy, and we show that it provides a high classification accuracy, as well as wide applicability for other classification problems where a classification in three or more classes is needed. Chapter III Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations / Xenia Naidenova ............................................................................................ 41 The purpose of this chapter is to demonstrate the possibility of transforming a large class of machinelearning algorithms into commonsense reasoning processes based on using well-known deduction and induction logical rules. The concept of a good classification (diagnostic) test for a given set of positive examples lies in the basis of our approach to the machine-learning problems. The task of inferring all good diagnostic tests is formulated as searching the best approximations of a given classification (a partitioning) on a given set of examples. The lattice theory is used as a mathematical language for constructing good classification tests. The algorithms of good tests inference are decomposed into subtasks and operations that are in accordance with main human commonsense reasoning rules. Chapter IV The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification / Giovanni Felici and Valerio Gatta ........................................................... 65 The analysis of quality of services is an important issue for the planning and the management of many businesses. The ability to address the demands and the relevant needs of the customers of a given service is crucial to determine its success in a competitive environment. Many quantitative tools in the areas of statistics and mathematical modeling have been designed and applied to serve this purpose. Here we consider an application of a well-established statistical technique, the stated preference models (SP), to identify, from a sample of customers, significant weights to attribute to different aspects of the service provided; such aspects may additively compose an overall satisfaction index. In addition, such a weighting system is applied to a larger set of customers, and a comparison is made between the overall satisfaction identified by the SP index and the overall satisfaction directly declared by the customers. Such comparison is performed by two rule-based classification systems, decision trees and the logic data miner Lsquare. The results of these two tools help in identifying the differences between the two measurements from the structural point of view, and provide an improved interpretation of the results. The application considered is related to the customers of a large Italian airport. Chapter V Support Vector Machines for Business Applications / Brian C. Lovell and Christian J. Walder......... 82 This chapter discusses the use of support vector machines (SVM) for business applications. It provides a brief historical background on inductive learning and pattern recognition, and then an intuitive motivation for SVM methods. The method is compared to other approaches, and the tools and background theory required to successfully apply SVM to business applications are introduced. The authors hope that the chapter will help practitioners to understand when the SVM should be the method of choice, as well as how to achieve good results in minimal time.

Chapter VI Kernel Width Selection for SVM Classification: A Meta-Learning / Shawkat Ali and Kate A. Smith ........................................................................................................... 101 The most critical component of kernel-based learning algorithms is the choice of an appropriate kernel and its optimal parameters. In this chapter, we propose a rule-based metalearning approach for automatic radial basis function (rbf) kernel, and its parameter selection for support vector machine (SVM) classification. First, the best parameter selection is considered on the basis of prior information of the data, with the help of maximum likelihood (ML) method and Nelder-Mead (N-M) simplex method. Then the new rule-based metalearning approach is constructed and tested on different sizes of 112 datasets with binary class, as well as multiclass classification problems. We observe that our rule-based methodology provides significant improvement of computational time, as well as accuracy in some specific cases. Chapter VII Protein Folding Classification Through Multicategory Discrete SVM / Carlotta Orsenigo and Carlo Vercellis ............................................................................................... 116 In the context of biolife science, predicting the folding structure of a protein plays an important role for investigating its function and discovering new drugs. Protein folding recognition can be naturally cast in the form of a multicategory classification problem, which appears challenging due to the high number of folds classes. Thus, in the last decade, several supervised learning methods have been applied in order to discriminate between proteins characterized by different folds. Recently, discrete support vector machines have been introduced as an effective alternative to traditional support vector machines. Discrete SVM have been shown to outperform other competing classification techniques both on binary and multicategory benchmark datasets. In this chapter, we adopt discrete SVM for protein folding classification. Computational tests performed on benchmark datasets empirically support the effectiveness of discrete SVM, which are able to achieve the highest prediction accuracy.

Chapter VIII Hierarchical Profiling, Scoring, and Applications in Bioinformatics / Li Liao .................................. 130 Recently, clustering and classification methods have seen many applications in bioinformatics. Some are simply straightforward applications of existing techniques, but most have been adapted to cope with peculiar features of the biological data. Many biological data take a form of vectors, whose components correspond to attributes characterizing the biological entities being studied. Comparing these vectors, a.k.a. profiles, is a crucial step for most clustering and classification methods. We review the recent developments related to hierarchical profiling where the attributes are not independent, but rather are correlated in a hierarchy. Hierarchical profiling arises in a wide range of bioinformatics problems, including protein homology detection, protein family classification, and metabolic pathway clustering. We discuss in detail several clustering and classification methods where hierarchical correlations are tackled with effective and efficient ways, by incorporation of domain specific knowledge. Relations to other statistical learning methods and more potential applications are also discussed.

Chapter IX Hierarchical Clustering Using Evolutionary Algorithms / Monica Chiş ............................................ 146 Clustering is an important technique used in discovering some inherent structure present in data. The purpose of cluster analysis is to partition a given data set into a number of groups such that objects in a particular cluster are more similar to each other than objects in different clusters. Hierarchical clustering refers to the formation of a recursive clustering of the data points: a partition into many clusters, each of which is itself hierarchically clustered. Hierarchical structures solve many problems in a large area of interests. In this chapter, a new evolutionary algorithm for detecting the hierarchical structure of an input data set is proposed. The method could be very useful in economy, market segmentation, management, biology taxonomy, and other domains. A new linear representation of the cluster structure within the data set is proposed. An evolutionary algorithm evolves a population of clustering hierarchies. Proposed algorithm uses mutation and crossover as (search) variation operators. The final goal is to present a data clustering representation to quickly find a hierarchical clustering structure. Chapter X Exploratory Time Series Data Mining by Genetic Clustering / T. Warren Liao ................................. 157 In this chapter, we present genetic-algorithm (GA)-based methods developed for clustering univariate time series with equal or unequal length as an exploratory step of data mining. These methods basically implement the k-medoids algorithm. Each chromosome encodes, in binary, the data objects serving as the k-medoids. To compare their performance, both fixed-parameter and adaptive GAs were used. We first employed the synthetic control chart data set to investigate the performance of three fitness functions, two distance measures, and other GA parameters such as population size, crossover rate, and mutation rate. Two more sets of time series with or without known number of clusters were also experimented: one is the cylinder-bell-funnel data and the other is the novel battle simulation data. The clustering results are presented and discussed. Chapter XI Development of Control Signatures with a Hybrid Data Mining and Genetic Algorithm Approach / Alex Burns, Shital Shah, and Andrew Kusiak ................................................. 179 This chapter presents a hybrid approach that integrates a genetic algorithm (GA) and data mining to produce control signatures. The control signatures define the best parameter intervals leading to a desired outcome. This hybrid method integrates multiple rule sets generated by a data-mining algorithm with the fitness function of a GA. The solutions of the GA represent intersections among rules providing tight parameter bounds. The integration of intuitive rules provides an explanation for each generated control setting, and it provides insights into the decision-making process. The ability to analyze parameter trends and the feasible solutions generated by the GA with respect to the outcomes is another benefit of the proposed hybrid method. The presented approach for deriving control signatures is applicable to various domains, such as energy, medical protocols, manufacturing, airline operations, customer service, and so on. Control signatures were developed and tested for control of a power-plant boiler. These signatures

discovered insightful relationships among parameters. The results and benefits of the proposed method for the power-plant boiler are discussed in the chapter. Chapter XII Bayesian Belief Networks for Data Cleaning / Enrico Fagiuoli, Sara Omerino, and Fabio Stella .................................................................................................................................. 204 The importance of data cleaning and data quality is becoming increasingly clear, as evidenced by the surge in software, tools, consulting companies, and seminars addressing data quality issues. In this contribution, the authors present and describe how Bayesian computational techniques can be exploited for data-cleaning purposes to the extent of reducing the time to clean and understand the data. The proposed approach relies on the computational device named Bayesian belief network, which is a general statistical model that allows the efficient description and treatment of joint probability distributions. This work describes the conceptual framework that maps the Bayesian belief network computational device to some of the most difficult tasks in data cleaning, namely imputing missing values, completing truncated datasets, and outliers detection. The proposed framework is described and supported by a set of numerical experiments performed by exploiting the Bayesian belief network programming suite named HUGIN. Chapter XIII A Comparison of Revision Schemes for Cleaning Labeling Noise / Chuck P. Lam and David G. Stork ...................................................................................................... 220 Data quality is an important factor in building effective classifiers. One way to improve data quality is by cleaning labeling noise. Label cleaning can be divided into two stages. The first stage identifies samples with suspicious labels. The second stage processes the suspicious samples using some revision scheme. This chapter examines three such revision schemes: (1) removal of the suspicious samples, (2) automatic replacement of the suspicious labels to what the machine believes to be correct, and (3) escalation of the suspicious samples to a human supervisor for relabeling. Experimental and theoretical analyses show that only escalation is effective when the original labeling noise is very large or very small. Furthermore, for a wide range of situations, removal is better than automatic replacement. Chapter XIV Improving Web Clickstream Analysis: Markov Chains Models and Genmax Algorithms / Paolo Baldini and Paolo Giudici........................................................................................................ 233 Every time a user links up to a Web site, the server keeps track of all the transactions accomplished, in a log file. What is captured is the “click flow” (clickstream) of the mouse and the keys used by the user during the navigation inside the site. Usually every click of the mouse corresponds to the viewing of a Web page. The objective of this chapter is to show how Web clickstream data can be used to understand the most likely paths of navigation in a Web site, with the aim of predicting, possibly online, which pages will be seen, having seen a specific path of other pages before. Such analysis can be very useful to understand, for instance, what is the probability of seeing a page of interest (such as the buying page in an e-commerce site) coming from another page. Or what is the probability of entering (or exiting)

the Web site from any particular page. From a methodological viewpoint, we present two main research contributions. On one hand we show how to improve the efficiency of the Apriori algorithm; on the other hand we show how Markov chain models can be usefully developed and implemented for Web usage mining. In both cases we compare the results obtained with classical association rules algorithms and models. Chapter XV Advanced Data Mining and Visualization Techniques with Probabilistic Principal Surfaces: Applications to Astronomy and Genetics / Antonino Staiano, Lara De Vinco, Giuseppe Longo, and Roberto Tagliaferri .......................................................................................... 244 Probabilistic principal surfaces (PPS) is a nonlinear latent variable model with very powerful visualization and classification capabilities that seem to be able to overcome most of the shortcomings of other neural tools. PPS builds a probability density function of a given set of patterns lying in a high-dimensional space that can be expressed in terms of a fixed number of latent variables lying in a latent Q-dimensional space. Usually, the Q-space is either two- or three-dimensional and thus, the density function can be used to visualize the data within it. The case in which Q = 3 allows to project the patterns on a spherical manifold, which turns out to be optimal when dealing with sparse data. PPS may also be arranged in ensembles to tackle complex classification tasks. As template cases, we discuss the application of PPS to two real- world data sets from astronomy and genetics. Chapter XVI Spatial Navigation Assistance System for Large Virtual Environments: The Data Mining Approach / Mehmed Kantardzic, Pedram Sadeghian, and Walaa M. Sheta .......... 265 Advances in computing techniques, as well as the reduction in the cost of technology have made possible the viability and spread of large virtual environments. However, efficient navigation within these environments remains problematic for novice users. Novice users often report being lost, disorientated, and lacking the spatial knowledge to make appropriate decisions concerning navigation tasks. In this chapter, we propose the frequent wayfinding-sequence (FWS) methodology to mine the sequences representing the routes taken by experienced users of a virtual environment in order to derive informative navigation models. The models are used to build a navigation assistance interface. We conducted several experiments using our methodology in simulated virtual environments. The results indicate that our approach is efficient in extracting and formalizing recommend routes of travel from the navigation data of previous users of large virtual environments. Chapter XVII Using Grids for Distributed Knowledge Discovery / Antonio Congiusta, Domenico Talia, and Paolo Trunfio ............................................................................................................................... 284 Knowledge discovery is a compute- and data-intensive process that allows for finding patterns, trends, and models in large datasets. The grid can be effectively exploited for deploying knowledge discovery applications because of the high performance it can offer, and its distributed infrastructure. For effective use of grids in knowledge discovery, the development of middleware is critical to support data

management, data transfer, data mining, and knowledge representation. To such purpose, we designed the knowledge grid, a high-level environment providing for grid-based knowledge discovery tools and services. Such services allow users to create and manage complex knowledge discovery applications, composed as workflows, that integrate data sources and data-mining tools provided as distributed grid services. This chapter describes the knowledge grid architecture, and describes how its components can be used to design and implement distributed knowledge discovery applications. Then, the chapter describes how the knowledge grid services can be made accessible using the open grid services architecture (OGSA) model. Chapter XVIII Fuzzy Miner: Extracting Fuzzy Rules from Numerical Patterns / Nikos Pelekis, Babis Theodoulidis, Ioannis Kopanakis, and Yannis Theodoridis...................................................... 299 We study the problem of classification as this is presented in the context of data mining. Among the various approaches that are investigated, we focus on the use of fuzzy logic for pattern classification, due to its close relation to human thinking. More specifically, this chapter presents a heuristic fuzzy method for the classification of numerical data, followed by the design and the implementation of its corresponding tool (fuzzy miner). The initial idea comes from the fact that fuzzy systems are universal approximators of any real continuous function. Such an approximation method coming from the domain of fuzzy control is appropriately adjusted into pattern classification, and an « adaptive » procedure is proposed for deriving highly accurate linguistic if-then rules. Extensive simulation tests are performed to demonstrate the performance of fuzzy miner, while a comparison with a neuro-fuzzy classifier of the area is taking place in order to contradict the methodologies and the corresponding outcomes. Finally, new research directions in the context of fuzzy miner are identified, and ideas for its improvement are formulated. Chapter XIX Routing Attribute Data Mining Based on Rough Set Theory / Yanbing Liu, Menghao Wang, and Jong Tang ..................................................................................................................................... 322 QOSPF (Quality of Service Open Shortest Path First) based on QoS routing has been recognized as a missing piece in the evolution of QoS-based services in the Internet. Data mining has emerged as a tool for data analysis, discovery of new information, and autonomous decision making. This chapter focuses on routing algorithms and their applications for computing QoS routes in OSPF protocol. The proposed approach is based on a data-mining approach using rough set theory, for which the attribute-value system about links of networks is created from network topology. Rough set theory offers a knowledge discovery approach to extracting routing-decisions from attribute set. The extracted rules can then be used to select significant routing-attributes and make routing-selections in routers. A case study is conducted to demonstrate that rough set theory is effective in finding the most significant attribute set. It is shown that

the algorithm based on data mining and rough set offers a promising approach to the attribute-selection problem in Internet routing.

Compilation of References .............................................................................................................. 338 About the Contributors ................................................................................................................... 361 Index ................................................................................................................................................... 368

xiv

Foreword

The importance of knowledge discovery and data mining is evident by the great plethora of books and papers dedicated to this subject. Such methods are finding applications in almost any area of human endeavor. This includes applications in engineering, science, business, medicine, humanities, just to name a few. At the same time, however, there is a great confusion about the development and application of such methods. The main reason for this situation is that many, if not most, of the books examine issues on data mining in a narrow manner. Very few books study issues from the mathematical/algorithmic and also the applications point of view simultaneously. Even fewer books present a comprehensive view of all the critical issues involved with the development and application of such methods to many real-life domains. The present book, edited by two world-renowned scholars, Drs. Giovanni Felici and Carlo Vercellis, is a bright example of the most valuable books in this fast emerging field. The emphasis of this book on the mathematical aspects of knowledge discovery and data-mining methods makes the presentations scientifically sound and easy to understand in depth. The 19 chapters of this book have been written by a number of distinguished scholars, from all over the world, who discuss the most critical subjects in this area. The book starts by discussing an important first step for any application of such methods, that is, how to discretize the data. This step is essential as many methods use binary data, while real-life applications may be associated with nonbinary data. If the analyst is not careful at this step, then it is possible to end up with too many nonrelevant variables that generate computational problems associated with highly dimensional data. A related step is that of cleaning the data before they are used to extract the pertinent models. As before, if this step is not done properly, the validity of the final results may be in jeopardy. Another interesting topic discussed in this book is the development of sophisticated visualization techniques, which are presented in relation with many diverse domains, ranging from astronomy to genetics. The successful application of the proposed visualization techniques to these two highly demanding application areas witnesses the high potential of these methods for a wide spectrum of applications. The high volumes of log data, produced by recording the way people surf the Web, provides an exciting opportunity, amid with interesting algorithmic challenges, for knowledge discovery and data-mining methods. This fascinated topic is also discussed here. Another very interesting subject discussed is how to mine data that come from virtual environments that involve some kind of spatial navigation. Such studies involve the analysis of sequences of routes of actions. A highly promising direction of research seems to be based on the development of methods that attempt to combine characteristics of various approaches. Such methods are known as hybrids, and an interesting development of a new hybrid approach and its applications are presented as well. No book in this area would be complete without the discussion of logic-based methods that offer some unique algorithmic and application advantages. The relevant discussions are done by some of the most knowledgeable world-renowned scholars on this subject; logic methods are also applied to the analysis of financial data. The potential of using grids for distributed approaches and also parallelism is

xv

explained too. Another prominent topic is the use of clustering approaches, which are discussed by developing some specialized evolutionary approaches. Clustering is also used in one of the most promising application areas for the future, the analysis of time series of data. Applications can be found in many domains as one realizes that systems or phenomena of interest usually generate data over time. In such settings, data from one point of time are somehow related to the data of the next point of time. Again, this very fascinating problem is discussed in great depth, and in an easy-to-understand manner by a distinguished expert in this fast-growing field. An extensive treatment of the classification technique, known as support vector machines, is provided by three chapters of the book; altogether with a complete treatment of the main theory of this method and of the related kernel function theory, the new extension of discrete support vector machines is described and applied to bioinformatics. This type of data is also the topic of other applications described in the book. The picture is completed with the description of data-mining approaches for service quality measurement, of fuzzy set and rough set theory applied in different contexts, and of other industrial applications of data mining. It is quite clear that this book is very valuable to all practitioners and researchers working on different fields but unified by the need to analyze their voluminous and complex data. Therefore, it is strongly recommended to anyone who has an interest in data mining. Furthermore, it is hoped that others will follow the example of this book and present more studies that combine algorithmic developments and applications in the way this edited book by Drs. Felici and Vercellis does so successfully. Evangelos Triantaphyllou, PhD Professor Department of Computer Science Louisiana State University Baton Rouge, LA 70803 USA

xvi

Preface

The idea of this book was conceived in June 2004, when a small group of researchers in the data-mining field gathered on the shores of the lake of Como, in Italy, to attend a focused conference–MML, Mathematical Methods for Learning 2004—having the objective of fostering the interaction among scholars from different countries and with different scientific backgrounds, sharing their research interests in data mining and knowledge discovery. As one of the side effects of that meeting, the conference organizers took on the exciting task of editing high quality scientific publications, where the main contributions presented at the MML conference could find an appropriate place, one next to the other, as they fruitfully did within the conference sessions. Some of the papers presented in Como, sharing a focus on mathematical optimization methods for data mining, found their place in a special issue of the international journal Computer Optimization and Applications (COAP, 38(1), 2007). Another large group of papers constituted the most appropriate building blocks for an edited book that would span a vast area of data-mining methods and applications, showing, on one hand, the relevance of mathematical methods and algorithms aimed at extracting knowledge from data, and on the other hand, how wide the application domains of data mining are. Shortly later, such project found interest and support by IGI Global, a dynamic publisher very active in promoting research-oriented publications in technological and advanced fields of knowledge. We eventually managed to finalize all the chapters, and moreover, enriched the book with additional research work that, although not presented at the MML conference, appear to have a strong relevance within the scope of the book. Most of the chapters have evolved since they were presented in 2004, and authors had the opportunity to update their work with additional results until the beginning of 2007.

The Motivations of Data Mining
The interest in data mining of researchers and practitioners with different backgrounds has increased steadily year after year. This growth is due to several reasons. First, data mining plays today a fundamental role in analyzing and understanding the vast amount of information collected by business, government, and scientific applications. The ability to analyze large bodies of data and extract from them relevant knowledge has become a valuable service for most organizations that operate in the highly globalized and competitive business arena. The technical skills required to operate and put to use data-mining techniques are now appreciated, and often required, by the business intelligence units of financial institutions, government agencies, telecommunication companies, service providers, retailers, and distribution operators. A second reason is to be found in the excellent and constantly improving quality of the methods and tools that are being developed in this field. Advanced mathematical models, state-of-the-art algorithmic techniques, and efficient data management systems, combined with a decreasing cost of computational power and computer memory, are now able to support data analysts with methodologies and tools that were not available a few years ago. Furthermore, such instruments are often available at low cost and with easy-to-use interfaces, integrated into well-established data management systems. A third reason that is not to be overlooked is connected with the role that data-mining methods are playing in providing support to basic research in many scientific areas. To mention an example, biology and genetics are currently enjoying the results of the application of advanced mining techniques that allow discovery of valuable facts in complex data gathered from experiments in vitro.

xvii

Finally, we wish to mention the impulse to methodological research that has been given in many areas by the open problems posed by data-mining applications. The learning and classification problems coming from real-life problems have been exploited through many mathematical theories under different formalizations, and theoretical results of unusual relevance have been reached in optimization theory, computer science, and statistics, also thanks to the many new and stimulating problems.

Data Mining as a Practical Science
Data mining is located at the crossing of different disciplines. Its roots are to be found in the data analysis techniques that were originally the main object of the study of statistics. The fundamental ideas at the basis of estimation theory, classification, clustering, sampling theory, are indeed still one of the major ingredients of data mining. But other methods and techniques have been added to the toolbox of the data analyst, extending the limits of the classical parametric statistics with more complex models, reaching their maturity with the actual state of knowledge on decision trees, neural networks, support vector machines, just to mention a few. In addition, the need to organize and manage large bodies of data has required the deployment of computer science techniques for database management, query optimization, optimal coding of algorithms, and other tasks devoted to the storing of information in the memory of computers and to the efficient execution of algorithms. A common trademark of the modern approaches is the formalization of estimation and classification problems arising in data mining as mathematical optimization problems, and the use of consistent algorithmic techniques to determine optimal solutions for these problems. Such methodological framework has been strongly supported by applied mathematics and operations research (OR), a scientific discipline characterized by a deep integration of mathematical theory and practical problems. A significant evidence of the role of OR in data mining is the contribution that nonlinear and integer optimization methods have given to the solution of the error minimization functions that need to be optimized to train neural networks and support vector machines. Analogously, integer programming and combinatorial optimization have been largely used to solve problems arising in the identification of synthetic rule-based classification models and in the selection of optimal subsets of features in large datasets. Despite its strong methodological characterization, data mining cannot be successfully applied without a deep understanding of the semantic of each specific problem, which often requires the customization of existing methods or the development of ad hoc techniques, partially based on already existing algorithms. To some extent, the real challenge that the data mining practitioner has to face is the selection, among many different methods and approaches, of the one that best serves the scope of the task considered, often assessing a compromise between the complexity of the chosen model and its generalization capability.

The Contribution of this Edited Book
This book aims to provide a rich collection of current research on a broad array of topics in data mining, ranging from recent theoretical advancements in the field to relevant applications in diverse domains. Future directions and trends in data mining are also identified in most chapters. Therefore, this volume should be an excellent guide to researchers, practitioners, and students. Its audience is represented by the research community; business executives and consultants; and senior students in the fields of data mining, information and knowledge creation, optimization, statistics, and computer science.

A Guided Tour of the Chapters
The book is composed of 19 chapters. Each one is authored by a different group of scientists, treats one of the many different theoretical or practical aspects of data mining, and is self contained with respect to the treated subject.

xviii

The first four chapters deal, to different degrees, with data-mining problems in logic setting, where the main purpose is to extract rules in logic format from the available data. In particular, Chapter I is written by Johnathan Mugan and Klaus Truemper, and describes a sophisticated and complete technique to transform a set of data represented in various formats by means of an extended set of logic variables. Such task, often referred to as discretization, or binarization, is a key step in the application of logic-based classification methods to data that is described by rational or nominal variables. The chapter extends the notion of rational variables with the definition of set variables, for example, variables that are represented by their membership functions to one or more sets. The method described is characterized by the fact that the set of logic variables extracted is compact, but strongly aimed at the task of classifying, with high precision, the available data with respect to a given binary target variable. The algorithm that implements the ideas described in the chapter has been implemented and integrated into the logic data mining software Lsquare, made available by the authors as open source. Chapter II is written by Massimo Liguori and Andrea Scozzari. Here the subject is the use of another wellknown logic data mining technique, the logical analysis of data (LAD), originally developed at the University of Rutgers by the research team led by Peter Hammer. The authors propose an interesting use of this method to treat logic classification where the target variable is of ternary nature (i.e., it can assume one of three possible values). Even more interesting is the application for which the method has been developed: the financial timing decision problem, namely the problem of deciding when to buy and when to sell a given stock to maximize the profit of the trading operations. The results presented in this chapter testify how logic methods can give a significant contribution in a field where classical statistics has always played the main role. Chapter III, authored by Xenia Naidenova, brings to the readers’ attention several interesting theoretical aspects of logic deduction and induction that find relevant application in the construction of machine-learning algorithms. The chapter treats extensively the many details connected with this topic, and enlightens many results with simple examples. The author adopts the lattice theory as the basic mathematical tool, and succeeds in proposing a sound integration of inductive and deductive reasoning for learning implicative logic rules. The results described are the basis for the implementation of an algorithm that efficiently infers good maximally redundant tests. In Chapter IV, Giovanni Felici and Valerio Gatta describe a study where the results of a stated preference model for measuring quality of service is combined with logic-based data mining to gain deeper insight in the system of preferences expressed by the customers of a large airport. The data-mining methods considered are decision trees and the logic miner Lsquare. The results are presented in the form of a set of rules that enables one to understand the similarities and the differences in two different methods to compute a quality of service index. The topics of the following three chapters evolve around the concept of support vector machines (SVM), a mathematical method for classification and regression emerged in the last decade from statistical learning theory, which quickly attained remarkable results in many applications. SVM are based on optimization methods, particularly in the field of nonlinear programming, and are a vivid example of the contributions that can be given to data mining by state-of-the-art theoretical research in mathematical optimization. In Chapter V, Brian C. Lovell and Christian J. Walder provide a rich overview of SVM in the context of data mining for business applications. They describe, with high clarity, the basic steps in SVM theory, and then integrate the chapter with several practical considerations on the use of this class of methods, comparing it with other learning approaches in the context of real-life applications. An important role in SVM is played by kernel functions, which provide an implicit transformation of the representation of the original space of data into a high dimensional space of features. By means of such transformations, SVM can efficiently determine linear transformations in the feature space that correspond to nonlinear separations into the original space. The identification of the right kernel function is the topic of Chapter VI, written by Shawkat Ali and Kate A. Smith, where they describe the application of a metalearning approach to optimally estimate the parameters that identify the kernel function before SVM is applied. The chapter highlights clearly the role of parameter

xix

estimation in the use of learning models, and discusses how the estimation procedure should be able to adapt to the specific dataset under analysis. The experimental analysis provides tests on both binary and multicategory classification problems. An interesting evolution of SVM is represented by discrete support vector machines, proposed in the last few years by Carlotta Orsenigo and Carlo Vercellis, authors of Chapter VII. According to statistical learning theory, discrete SVM directly face the minimization of the misclassification rate, within the risk functional, instead of replacing it with the misclassification distance as traditional SVM. The problem is then modeled as a mixedinteger programming problem. The method, already successful in other applications, is extended and applied here to protein folding, a very challenging task in multicategory classification. The experiments performed by the authors on benchmark datasets show that the proposed method achieves the highest accuracy in comparison to other techniques. The use of data-mining methods to extract knowledge from large databases in genetic and biomedical applications is increasing at a fast pace, and Chapter VIII, written by Li Liao, deals with this topic. Often the data in this context is based on vectors of extremely large dimensions, and specific techniques must be deployed to obtain successful results. Li Liao tackles several of the specific problems related with handling biomedical data, in particular those related with data described by attributes that are correlated with each other and are organized in a hierarchical structure. Clustering and classification methods that exploit the hierarchies in data are considered and compared with statistical learning methods. Chapters nine and ten both deal with clustering, a fundamental problem in nonsupervised learning. In Chapter IX, Monica Chiş discusses hierarchical clustering, where the clusters are obtained by recursively separating the data into groups of similar objects. The methods investigated belong to the family of genetic algorithms, where an initial population of chromosomes, corresponding to potential clusters, is evolved at each iteration, generating new chromosomes with the objective of minimizing a fitness function. The genetic operators adopted here are standard mutation and crossover. Evolving on these concepts, T. Warren Liao presents, in Chapter X, a method based on genetic algorithms to cluster univariate time series. The study of time series is indeed a very central topic in data analysis, and is often overlooked in standard data-mining applications, where the main attention is addressed to multivariate data. Time series, on the other hand, present several complex aspects linked to autocorrelation and lag parameters, and surely can benefit by the use of the new methods developed in the area of data mining. Using the method of the k-medoids, the author compares the performances of three fitness functions, two distance measures, and other parameters that characterize the genetic algorithms considered. The chapter presents several experiments on data derived from cylinder-bell-funnel data and battle simulation data. Chapter XI is a sound example of how advanced data-mining techniques can provide relevant information in production systems. Alex Burns, Shital Shah, and Andrew Kusiak describe the implementation of a method that integrates genetic algorithms and data mining. The results of a rule-based data-mining algorithm are evaluated and scored using a fitness function, and the related methods made available in the context of genetic algorithms. Here again we find a strong connection between data analysis and optimization techniques, and we see how certain decision problems can be successfully solved building ad hoc procedures, where methodologies and techniques from different backgrounds are deployed. The authors describe an application of the method to a power-plant boiler and highlight the contribution given to the production process. Chapter XII is written by Enrico Fagiuoli, Sara Omerino, and Fabio Stella. It is an interesting work that shows how complex models derived from classical statistical techniques can play an important role in the data treatment process. The chapter describes the use of Bayesian belief networks to perform data cleaning, a relevant problem in most data-mining applications where the information available is obtained with noisy, incomplete, or error-prone procedures. Here Bayesian belief networks are used to instantiate missing values of incomplete records, to complete truncated datasets, and to detect outliers. The effectiveness of the approach is supported by numerical experiments. Chapter XIII deals with a similar topic, data cleaning. Here, Chuck P. Lam and David G. Stork describe, in a complete and accurate way, the problem of labeling noise, requiring the identification and treatment of records

xx

when some of the labels attached to the records are different from the correct value due to some source of noise present in the data collection process. The chapter depicts the two main problems in cleaning labeling noise: the identification of noise and the consequent revision scheme, through removal, replacement, or escalation to human supervision. In particular, the authors examine the k-nearest neighbor method to solve the identification problem, while they use probabilistic arguments to evaluate the alternative revision schemes. The public domain UCI repository is used as a source of datasets where the proposed methods are tested. In Chapter XIV another tool originated in the statistical and stochastic processes environment is used to solve a relevant mining problem, clickstream analysis, which is attracting a growing attention. The problem is generated by the need to investigate the log files produced when users visit a Web site. These log files report the sequence of steps in navigation (clicks) made by the Web users. These applications can be useful for designing Web sites and for related business-oriented analysis. The authors of the chapter, Paolo Baldini and Paolo Giudici, propose to use Markov chain models to investigate the structure of the most likely navigation path in a Web site, with the objective of predicting the next step made by a Web user, based on the previous ones. Antonino Staiano, Lara De Vinco, Giuseppe Longo, and Roberto Tagliaferro are the authors of Chapter XV. Here the topic is the visualization of complex and multidimensional data for exploration and classification purposes. The method used is based on probabilistic principal surfaces: by means of a density function in the original space, data are projected into a reduced space defined by a set of latent variables. A special case arises when the number of latent variables is equal to three and the projected space is a spherical manifold, particularly indicated to represent sparse data. Besides visualization, such reduced spaces can be used to apply classification algorithms for efficiently determining surfaces that separate groups of data belonging to different classes. Applications of the method for data in the astronomy and genetics domains are discussed. Chapter XVI presents an unconventional application of data-mining techniques that assists spatial navigation in virtual environments. In this setting, users are able to navigate in a three-dimensional virtual space to accomplish a number of tasks. Such navigation may be difficult or inefficient for nonexperienced users, and the application discusses the use of data-mining techniques to extract knowledge from the navigation patterns of expert users, and create good navigation models. Such process is put into action by a navigation interface that implements a frequent wayfinding-sequence method. The authors, Mehmed Kantardzic, Pedram Sadeghian, and Walaa M. Sheta, have run experiments in simulated virtual environments, extensively discussed in the chapter. Data-mining algorithms can be very demanding from the point of view of computational requirements, such as speed and memory, especially when large datasets are analyzed. One possible solution to deal with the computational burden is the use of parallel and distributed computing. Such an issue is the topic of Chapter XVII, written by Antonio Congiusta, Domenico Talia, and Paolo Trunfio, on the use of grid computing for distributed data mining. An integrated architecture that can properly host all the steps of the data analysis process (data management, data transfer, data mining, knowledge representation) has been designed and is presented in the chapter. The components of this data-mining-oriented middleware, termed knowledge grid, are described, explaining how these services can be accessed using the standard open grid architecture model. The last two chapters of the book are devoted to two knowledge extraction methods that have received large attention in the scientific community. They extend the limits of standard machine learning theory, and can be used to build data-mining applications able to deal with unconventional applications, and to provide information in an original format. Chapter XVIII is about the use of fuzzy logic. Here Nikos Pelekis, Babis Theodoulidis, Ioannis Kopanakis, and Yannis Theodoridis cover the design of a classification heuristic scheme based on fuzzy methods. The performances of the method are analyzed by means of extensive simulated experiments. The topic of Chapter XIX is the method of rough sets. This method presents several noticeable features that originally characterize the rules extracted from the data. The interest of this chapter, written by Yanbing Liu, Menghao Wang, and Jong Tang, is also due to the application of the method to analyze and evaluate network topologies in routing problems. The application of data mining techniques in network problems associated with telecommunication problems is novel, and is likely to represent a relevant object of research in the future.

xxi

Acknowledgment

The editors want to express their gratitude to all those colleagues who supported, in many different ways, the pleasant effort of editing this book. First and foremost, the 30 authors that contributed to the writing of the book’s chapters; they are the actual creators of the qualified research that is described in the book, and they must be especially thanked for the patience exhibited in the editing process. Among them, special thanks go to Dr. Carlotta Orsenigo, who supported, with her careful and patient touch, many of the steps that led this volume to publication. Speaking of patience, we need not to overlook the contribution of the IGI Globals’s publishing editor, Kristin Roth, altogether with all the very efficient staff at IGI Global. A final word of thanks is warmly dedicated to all those who supported our editing work without even knowing it: our families and our friends, who were on our side in the hard and the exciting moments. Giovanni Felici Carlo Vercellis Editors

xxii



Discretization of Rational Data
Jonathan Mugan University of Texas at Austin, USA Klaus Truemper University of Texas at Dallas, USA

Chapter I

AbstrAct
Frequently, one wants to extend the use of a classification method that, in principle, requires records with True/False values, so that records with rational numbers can be processed. In such cases, the rational numbers must first be replaced by True/False values before the method may be applied. In other cases, a classification method in principle can process records with rational numbers directly, but replacement by True/False values improves the performance of the method. The replacement process is usually called discretization or binarization. This chapter describes a recursive discretization process called Cutpoint. The key step of Cutpoint detects points where classification patterns change abruptly. The chapter includes computational results, where Cutpoint is compared with entropy-based methods that, to date, have been found to be the best discretization schemes. The results indicate that Cutpoint is preferred by certain classification schemes, while entropy-based methods are better for other classification methods. Thus, one may view Cutpoint to be an additional discretization tool that one may want to consider.

IntroductIon
One often desires to apply classification methods that, in principle, require records with True/False values to records that, besides True/False values, contain rational numbers. For ease of reference, we call rational number entries rational data

and refer to True/False entries as logic data. In such situations, a discretization process must first convert the rational data to logic data. Discretization is also desirable in another setting. Here, a classification method in principle can process records with rational numbers directly, but its performance is improved when the rational data are first converted to logic data.

Copyright © 2008, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.

Discretization of Rational Data

This chapter describes a method called Cutpoint for the discretization task, and compares its effectiveness with that of entropy-based methods, which presently are considered to be the best discretization schemes. Define nominal data to be elements or subsets of a given finite set. In Bartnikowski et al. (Bartnikowski, Granberry, Mugan, & Truemper, 2004), an earlier version of Cutpoint is described and used for the transformation of some cases of nominal data to logic data. Specifically, the nominal data are first converted to rational data, which are then transformed to logic data by Cutpoint. We focus here on the following case. We are given records of two training classes, A and B, that have been randomly selected from two populations A and B, respectively. We want to derive a classification scheme from the records of A and B. Later, that scheme is to be applied to records of A − A and B − B. For the purpose of a simplified discussion in this section, we assume for the moment that the records have no missing entries. That restriction is removed in the next section.

Abrupt Pattern changes and cutpoint
Generally, the discretization may be accomplished by the following, well-known approach. One defines, for a given attribute, k ≥ 1 breakpoints and encodes each rational number of the attribute by k True/False values, where the jth value is True if the rational number is greater than the jth breakpoint, and is False otherwise. The selection of the k breakpoints requires care if the records of A − A and B − B are to be classified with good accuracy. A number of techniques for the selection of the breakpoints have been proposed, and later in this section, we provide a review of those methods. Suffice it to say here that the most effective methods to date are based on the notion of entropy. In these methods, the breakpoints are so selected that the rational numbers of a given

attribute can be most compactly classified by a decision tree as coming from A or B. In contrast, Cutpoint is based on a different goal. Recall that the records of the sets A and B are presumed to be random samples of the populations A and B. Taking a different viewpoint, we may view each record of A − A and B − B to be a random variation of some record of A or B, respectively. The goal is then to select the breakpoints so that these random variations largely leave the True/False values induced by the breakpoints unchanged. Cutpoint aims for the stated goal by selecting breakpoints, called markers, that correspond to certain abrupt changes in classification patterns, as follows. First, for a given attribute, the rational numbers are sorted. Second, each value is labeled as A or B, depending on whether the value comes from a record of A or B, respectively. For the sake of a simplified discussion, we ignore, for the moment, the case where a rational number occurs in both a record of A and a record of B. Third, each entry with label A (resp. B) is assigned a class value of 1 (resp. 0). Fourth, Gaussian convolution is applied to the sequence of class values, and the midpoint between two adjacent entries, where the smoothed class values change by the largest amount, is declared to be a marker. For example, if the original sorted sequence, with class membership in parentheses, is ..., 10.5(A), 11.7(A), 15.0(A), 16.7(A), 19.5(B), 15.2(B), 24.1(B), 30.8(B),..., then the sequence of class values is ..., 1, 1, 1, 1, 0, 0, 0, 0,.... Note the abrupt transition of the subsequence of 1s to the subsequence of 0s. When a Gaussian convolution with small standard deviation σ is performed on the sequence of class values, a sequence of smoothed values results, which exhibits a relatively large change at the point where the original sequence changes from 1s to 0s. If this is the largest change for the entire sequence of smoothed class values, then the original entries 16.7(A) and 19.5(B), which correspond to that change, produce a marker with value (16.7 + 19.5)/2 = 18.1.



Discretization of Rational Data

Evidently, a large change of the smoothed class values corresponds in the original sorted sequence of entries to a subsequence of rational numbers, mostly from A, followed by a subsequence of numbers, mostly from B, or vice versa. We call such a situation an abrupt pattern change. Thus, markers correspond to abrupt pattern changes. We differentiate between two types of abrupt pattern changes. We assume, reasonably, that an abrupt change, produced by all records of the populations A and B, signals an important change of behavior and thus should be used to define a True/False value. The records of the subsets A and B may exhibit portions of such pattern changes. We say that these pattern changes of the records of A and B are of the first kind. The records of A and B may also have additional abrupt pattern changes that do not correspond to abrupt pattern changes in the records of the populations A and B. This is particularly so if A and B are comparatively small subsets of the populations A and B, as is typically the case. We say that the latter pattern changes are of the second kind. There is another way to view the two kinds of pattern changes. Suppose we replace records r  of A ∪ B by records r of (A − A) ∪ (B − B),  respectively, where r is similar to r. Then abrupt pattern changes of the first (resp. second) kind produced by the records r likely (resp. unlikely) are abrupt pattern changes produced by the  records r . There is a third interpretation. Suppose we extract from the sorted sequence of numerical values just the A and B labels. For example, the above sequence ..., 10.5(A), 11.7(A), 15.0(A), 16.7(A), 19.5(B), 15.2(B), 24.1(B), 30.8(B),... becomes ..., A, A, A, A, B, B, B, B,.... We call this a label sequence. Then for an abrupt pattern change of the first (resp. second) kind, the random  substitution of records r by records r is unlikely (resp. likely) to change the label sequence. Cutpoint relies on the third interpretation in an attempt to distinguish between the two kinds of pattern changes, as follows. The method estimates the probability that A or B is selected

in label sequences of abrupt pattern changes of the second kind, by assuming p = |A|/(|A| + |B|) (resp. q = |B|/(|A|+|B|)) to be the probability for the label A (resp. B) to occur. Then the standard deviation of the Gaussian convolution process is so selected that the following is assured. Suppose there is at least one abrupt pattern change that, according to the probabilities p and q, has low probability and thus is estimated to be of the first kind. Then the largest change of the smoothed class values and the associated marker tends to correspond to one such abrupt pattern change. Informally, one may say that the standard deviation σ is so selected that marker positions corresponding to abrupt pattern changes of the first kind are favored. Cutpoint has been added to the version of the Lsquare method of Truemper (2004), which is based on prior versions of Felici and Truemper (2002) and Felici et al. (Felici, Sun, & Truemper, 2004). Lsquare computes DNF (disjunctive normal form) logic formulas from logic training data. Cutpoint initially determines one marker for each attribute of the original data, as described previously. Let the transformation of A and B via these markers produce sets A' and B'. If A' and B' cannot be separated by logic formulas, then Cutpoint recursively determines additional markers. The Cutpoint/Lsquare combination is so designed that it does not require user specification of parameters or rules, except for a limit on the maximum number of markers for any attribute. In the tests described later, that maximum was fixed to 6.

computational results
To date, we have used Cutpoint in conjunction with Lsquare in a variety of projects such as credit rating, video image analysis, and word sense disambiguation. In each case, Cutpoint has proved to be effective and reliable. We also have compared the performance of Cutpoint with that of two entropy-based methods that differ by the subdivision selection and termination criterion. In one of the methods,



Discretization of Rational Data

the criterion is the clash condition of Cutpoint introduced later. We refer to this method as Entropy CC. For the other method, the criterion is the minimum description length (MDL) principle (Dougherty, Kohavi, & Sahami, 1995). Accordingly, we refer to that method as Entropy MDL. For the comparison, we applied Cutpoint and the two entropy-based methods to a number of data sets, and processed the resulting logic data by four classification algorithms. The latter schemes were so chosen that classification by decision trees, naive Bayes methods, support vector machines, and learning logic methods were represented. Note that we do not claim that each of the selected classification methods should use discretization as a preprocessing step. But if one decides to use such preprocessing, then the results indicate the following. Entropy MDL is preferred for decision tree methods and support vector machines, while Cutpoint is preferred for learning logic methods and naive Bayes methods. The performance of Entropy CC lies somewhere between that of Cutpoint and Entropy MDL, and thus is always dominated either by Cutpoint or Entropy MDL. These general conclusions are based on average performance results. For specific data sets, preference can be quite different. Thus, if one needs highest possible accuracy for a given situation, one should try all three schemes and select the one performing best.

Entropy-Based Approaches
The concept of entropy, as used in information theory, measures the purity of an arbitrary collection of examples (Mitchell, 1997). Suppose we have two classes of data, labeled N and P. Let n be the number of N instances, and define p to be the number of P instances. An estimate of the probability that class P occurs in the set is p/(p + n), while an estimate of the probability that class N occurs is n/(p + n). Entropy is then estimated as:

entropy( p, n)
=− p p n n log 2 − log 2 p+n p+n p+n p+n

(1)

Another value, called gain, indicates the value of separating the data records on a particular attribute. Let V be an attribute with two possible values. Define p1 (resp. n1) to be the number of P (resp. N) records that contain one of the two values. Similarly, let p2 (resp. n2) be the number of P (resp. N) records that contain the second value. Then:

gain = entropy( p, n)
−[ p1 + n1 p + n2 entropy( p1 , n1 ) + 2 entropy( p2 , n2 )] p+n p+n (2)

Optimized Classification
These results apply only to the situation where costs of obtaining attribute data for records need not be considered. When such costs are present, Cutpoint, Entropy CC and Entropy MDL must be modified. For Cutpoint, the needed adjustments are discussed toward the end of the chapter. In the remainder of this section, we review prior work on discretization. For a comprehensive survey and a computational comparison of techniques, see Liu et al. (Liu, Hussain, Tan, & Dash, 2002).

In generating decision trees, for example, the attribute with the highest gain value is used to split the tree at each level. The simplest approach to discretization is as follows. Assume that each record has a rational attribute, V . The records are first sorted according to V , yielding rational values v1, v2,..., vk. For each pair of values vi and vi+1, the average of the two is a potential marker to separate the P records from the N records. For each possible marker, the associated gain is computed. The highest gain indicates the best marker that separates the two classes of data (Quinlan, 1986). The method has been further developed to separate rational data



Discretization of Rational Data

into more than just two classes. In Fayyad and Irani (1992, 1993), a recursive heuristic for that task is described. The multiinterval technique first chooses a marker giving minimal entropy. It then recursively uses the minimum description length (MDL) principle to determine whether additional markers should be introduced. Another concept, called minimum splits, is introduced in Wang and Goh (1997). Minimum splits minimize the overall impurity of the separated intervals with respect to a predefined threshold. Although, theoretically, any impurity measurement could be used, entropy is commonly chosen. Since many minimum splits can be candidates, the optimal split is discovered by searching the minimum splits’ space. The candidate split with the smallest product of entropy and number of intervals is elected to be the optimal split. Entropy-based methods compete well with other data transformation techniques. In Dougherty et al. (1995), it is shown not only that discretization prior to execution of naive Bayes decision algorithms can significantly increase learning performance, but also that recursive minimal entropy partitioning performs best when compared with other discretization methods such as equal width interval binning and Holte’s 1R algorithm (Holte, 1993). More comparisons involving entropy-based methods can be found in Kohavi and Sahami (1996), which demonstrates situations in which entropy-based methods using the MDL principle slightly outperform errorminimization methods. The error-minimization methods used in the comparison can be found in Maass (1994) and Auer et al. (Auer, Holte, & Maass, 1995). For information regarding the performance of entropy-based methods for learning classification rules, see An and Cercone (1999).

The basic method is introduced in Srikant and Agrawal (1996). Major problems are low speed and bloating of the produced rule set. To offset long execution times, the number of intervals must be reduced. Uninteresting excess rules may be pruned using an interest measure. Data clustering has been used (Miller & Yang, 1997) to generate more meaningful rules. Yet another approach to merging related intervals is used in the contrast set miner (Bay & Pazzani, 1999). The use of one such machine, called STUCCO, is illustrated in Bay (2000).

Other Approaches
Bayes’ Law has also been utilized for discretization. Wu (1996) demonstrates one such method. In it, curves are constructed based upon the Bayesian probability of a particular attribute’s value in the data set. Markers are placed where leading curves differ on two sides. A number of investigations have focused on simultaneous analysis of attributes during the transformation process. Dougherty et al. (1995) coin the term dynamic to refer to methods that conduct a search through the space of possible k values for all features simultaneously. For an example method, see Gama et al. (Gama, Torgo, & Soares, 1998). Relatedly, publications tend to use the term multivariate with different interpretations. Kwedlo and Krętowski (1999) refer to a multivariate analysis as one that simultaneously searches for threshold values for continuousvalued attributes. They use such an analysis with an evolutionary algorithm geared for decision rule induction. Bay (2000), however, declares that a multivariate test of differences takes as input instances drawn from two probability distributions and determines if the distributions are equivalent. This analysis maintains the integrity of any hidden patterns in the data. Boros et al. (Boros, Hammer, Ibaraki, & Kogan, 1997) explores several optimization approaches for the selection of breakpoints. In each case, all

bottom-up Methods
Bottom-up methods initially partition the data set then recombine similar adjacent partitions.



Discretization of Rational Data

attributes of the records of the training sets A and B are considered simultaneously. For example, minimization of the total number of breakpoints is considered. The reference provides polynomial solution algorithms for some of the optimization problems and establishes other problems to be NP-hard. Boros et al. (Boros, Hammer, Ibaraki, Kogan, Mayoraz, & Muchnik, 2000) describes a discretization method that is integrated into the so-called logic analysis of data (LAD) method. In that setting, the discretization requires solution of a potentially large set-covering problem. A heuristic method is employed to solve that problem approximately.

defInItIons
We need a few definitions for the discussion of Cutpoint.

unknown Values
At times, the records of A and B may be incomplete. Following Truemper (2004), we consider two values that indicate entries to be unknown. They are Absent and Unavailable. The value Absent means that the value is unknown but could be obtained, while Unavailable means that the value cannot be obtained. Of course, there are in-between cases. For example, a diagnostic value could be obtained in principle but is not determined, since the required test would endanger the life of the patient. Here, we force such in-between cases to be classified as Absent or Unavailable. For the cited diagnostic case, the choice Unavailable would be appropriate. Another way to view Absent and Unavailable is as follows. Absent means that the value is unknown, and that this fact is, in some sense, independent from the case represented by the given record. On the other hand, Unavailable tells that the reason why the value is not known is directly connected with the case of the record.

Thus, Unavailable implicitly is information about the case of the record, while Absent is not. This way of differentiating between Absent and Unavailable implies how irrelevant values are handled. That is, if a value is declared to be irrelevant or inapplicable, then this fact is directly connected with the case of the record, and thus is encoded by the value Unavailable. In prior work, the treatment of unknown values typically does not depend on whether the unknown value could be obtained. For example, the average value of the attribute is often used for missing values (Mitchell, 1997). As another example, database methods such as SQL use NULL to represent unknown entries (Ramakrishnan & Gehrke, 2003). In applications, we have found the distinction between Absent and Unavailable to be useful. For example, a physician may declare that it is unnecessary that a certain diagnostic value be obtained. In that case, we call the value irrelevant and encode it by assigning the value Unavailable. Conversely, if a diagnostic value is deemed potentially useful but is not yet attained, we assign the value Absent. It is convenient that we expand the definition of logic data and rational data so that Absent and Unavailable are allowed. Thus, logic data have each entry equal to True, False, Absent, or Unavailable, while rational data have each entry equal to a rational number, Absent, or Unavailable.

Records
A record contains any mixture of logic data and rational data. There are two sets A and B of records. Each record of the sets has the same number of entries. For each fixed j, the jth entries of all records are of the same data type. We want to transform records of A and B to records containing just logic data, with the objective that logic formulas, determined by any appropriate method, can classify the records correctly as coming from A or B.



Discretization of Rational Data

Populations
Typically, the sets A and B come from populations A and B, respectively, and we want the transformations and logic formulas derived from A and B to classify the remaining records of A − A and B − B with high accuracy.

As an aside, prior rules on the treatment of unknown values effectively treat them as Absent. For example, the stated evaluation of DNF formulas for Absent values is consistent with the evaluation of logic formulas of SQL for NULL values (Ramakrishnan & Gehrke, 2003).

clash condition DNF Formulas
A literal is the occurrence of a possibly negated variable in a logic formula. A disjunctive normal form (DNF) formula is a disjunction of conjunctions of literals. For example, (x1∧ ¬x2) ∨(x2 ∧ x3)∨(x1 ∧ ¬x3) is a DNF formula. The evaluation of DNF formulas requires the following adjustments when the values Absent and Unavailable occur. Let D be the DNF formula D = D1 ∨D2 ∨· · · ∨Dk, where the Dj are the DNF clauses. For example, we may have Dj = x ∧ y ∧ ¬z, where x, y, and ¬z are the literals of logic variables x, y, and z. The DNF clause Dj evaluates to True if the variable of each literal has been assigned a True/ False value so that the literal evaluates to True. For example, Dj = x ∧ y ∧¬z evaluates to True if x = y =True and z =False. The clause Dj evaluates to False if, for at least one variable occurring in Dj, the variable has a True/False value so that the corresponding literal evaluates to False, or if the variable has the value Unavailable. For example, x = False or x = Unavailable cause Dj = x ∧y ∧¬z to evaluate to False. If one of these cases does not apply, then Dj has the value Undecided. Thus, the Undecided case occurs if the following three conditions hold: (1) Each variable of Dj has True, False, or Absent as values; (2) there is at least one Absent case; and (3) all literals for the True/False cases evaluate to True. For example, Dj = x ∧ y ∧¬z evaluates to Undecided if x = Absent, y = True, and z = False. The DNF formula D = D1 ∧ D2 ∧· · · ∧ Dk evaluates   to True if at least one Dj has value True, to False if all Dj have value False, and to Undecided otherwise. Thus in the Undecided case, each Dj has value False or Undecided, and there is at least one Undecided case. Suppose we desire classification by DNF formulas. Specifically, we want two DNF formulas of which one evaluates to True on the records derived from A and to False on the records derived from B, while the second formula achieves the opposite True/False values. We call these formulas separating. Note that the outcome Undecided is not allowed. That value may occur, however, when a DNF formula evaluates records of (A −A) ∪ (B−B). Effectively, a formula then votes for membership in A or B, or declares the case to be open. We associate with the vote for A and B a numerical value of 1 or −1, resp., and assign to the Undecided case the value 0. This rule is useful when sets of formulas are applied, since then the vote total expresses the strength of belief that a record is in A or B. There is a simple necessary and sufficient condition for the existence of the separating formulas. We call it the clash condition. For the description of the condition, we assume for the moment that the records of A and B contain just logic data. We say that an A record and a B record clash if the A record has a True/False entry for which the corresponding entry of the B record has the opposite True/False value or Unavailable, and if the B record has a True/ False entry for which the corresponding entry of the A record has the opposite True/False value or Unavailable. For example, let each record of A ∪ B have three entries x1, x2, and x3, and suppose that an A record is (x1 = True, x2 = Unavailable, x3 = False) and that a B record is (x1 = False, x2 = True, x3 = False). Then the entry x1 = True of the A record differs from x1 = False of the B record, and thus the two records clash. On the other hand, take



Discretization of Rational Data

the same A record, but let the B record be (x1 = True, x2 = Unavailable, x3 = Unavailable). Then there is no True/False value in the B record for which the A record has the opposite True/False value or Unavailable, and thus the two records do not clash. Define the clash condition to be satisfied by sets A and B containing only logic data if every record of A clashes with every record of B. The following theorem links the existence of separating DNF formulas and the clash condition. We omit the straightforward proof. Theorem 1. Let sets A and B contain just logic data. Then two separating DNF formulas exist if and only if the clash condition is satisfied. Let sets A and B of records be given. Define J to be the set of indices j for which the jth entries of the given records contain rational data. Cutpoint recursively defines markers for the jth entries of the j ∈ J, where in each pass one  marker is defined. It is convenient that we divide the description of the marker selection into two parts, which make up the subsequent two sections. The next section covers the selection of the initial marker for an arbitrary j ∈ J, and the  following section deals with the case where an additional marker is to be found.

with zi as jth entry is in A (resp. B). If zi is not unique, let H be the set of indices h for which zh = zi. Note that i ∈ H. Let HA (resp. HB) be the subset of the h ∈ H for which zh is the jth entry of a record in set A (resp. B). If h ∈ HA (resp. h ∈ HB), we say that zh produces a local class value equal to 1 (resp. 0). The class value vi is then the average of the local class values for the zh with h ∈ H. Thus, vi = [1 · |HA| + 0 · |HB|]/|H| or, compactly,
vi = | H A | / | H |

(3)

InItIAl MArker
Let j ∈ J. We denote the rational numbers in jth position, sorted in increasing order, by z1 ≤ z2 ≤ · · · ≤ zN. For the moment, we ignore all Absent and Unavailable values that may occur in the jth position.

The formula also covers the case of unique zi, since then H = {i} and either HA = {i} or HA = ∅ depending on whether the record with zi as jth entry is in A or B, respectively. For example, suppose z1 = 2, z2 = 5, and z5 = 10 occur in records of set A, and z3 = 7 and z4 = 10 occur in records of set B. Since z1 and z2 are unique and occur in records of set A, we have v1 = v2 = 1. Similarly, uniqueness of z3 and occurrence in a B record produce v3 = 0. The values z4 and z5 are equal and exactly one of them, z5, occurs in a record of set A. Thus for both z4 and z5, we have H = {4, 5} and HA = {5}, and by (3), v4 = v5 = |HA|/|H| = 0.5. Recall that a marker corresponds to an abrupt change of classification pattern. In terms of class values, a marker is a value c where many if not all zi close to c and satisfying zi < c have high class values, while most if not all zi close to c and satisfying zi > c have low class values, or vice versa. We identify markers following a smoothing of the class values by Gaussian convolution, a much used tool. For example, it is employed in computer vision for the detection of edges in digitized images; see Forsyth and Ponce (2003).

class Values smoothed class Values
We associate with each zi a class value vi that depends on whether zi is equal to any other zh, and whether zi is in a record of set A or B. Specifically, if zi is unique and thus not equal to any other zh, then vi is 1 (resp. 0) if the record Gaussian convolution uses the normal distribution with mean equal to 0 for smoothing of data. For completeness, we include the relevant formulas.



Discretization of Rational Data

For mean 0 and standard deviation σ ≥ 0, the probability density function of the normal distribution is:
2 2 1 (4) e − y /(2 σ ) , 0 < y < ∞ σ 2π In our case, we always choose σ to be a positive integer. We cover the selection in a moment. For any integer g and the selected σ, let bg denote the probability that the random variable defined by f(y) falls into the open interval (g−0.5, g+0.5). Since g is the midpoint of the open unit interval (g − 0.5, g + 0.5), we have:

unusable values vi for 1 ≤ i ≤ 2σ and N −2σ +1 ≤ i ≤ N. As a consequence, we ignore these values and declare the remaining vi values usable.

f ( y) =

Selection of Standard Deviation
We select the standard deviation σ via an analysis of classification patterns. Suppose we produce sequences made up of the letters A and B. We construct a given sequence by randomly selecting one letter at a time, choosing the letter A with probability p and the letter B with probability q = 1 − p. In the construction of a sequence, we begin with the sequence AB. For given k ≥ 1 and l ≥ 1, we adjoin k − 1 As in front of AB and l − 1 Bs behind AB. At this point, we have k As followed by l Bs. Finally, we add a B in front and an A at the end. What is the probability that such a sequence S is constructed from AB when we randomly select letters and add them first in front and then at the end, until a sequence of the described form is achieved? Since the initial sequence AB is given, the probability is:

bg = ∫

g + 0.5

g − 0.5

f ( y )dy ≅ f ( g )

(5)

The smoothing process uses the bg values to derive, from the class values vi, smoothed values vi by the formula:
vi =

g =−∞

∑b

∞

g

⋅ vi + g , 1 ≤ i ≤ N

(6)

The formula relies on the convention that each vi+g without defined value, that is, with i + g < 1 or i + g > N, is declared to be 0. For the values of σ of interest and for |g| ≥ 2 σ + 1, the bg are sufficiently small that they can be ignored. That fact and the relation bg = b-g, for all g, allow us to simplify (6) for each actual computation to:
vi = b0 ⋅ vi + ∑ b g ⋅ (vi + g + vi − g ), 1 ≤ i ≤ N g =1 (7)
2σ

P[ S ] = p k q l

(8)

For m ≥ 1, consider the event Em where the previously mentioned process constructs any S for which k ≥ m or l ≥ m. We add up the appropriate probabilities of (8) to get the probability αm that Em occurs. Using the fact that the sum of the probabilities of all possible cases is 1, that is,

k ≥1, l ≥1

∑

p k ql = 1

(9)

we compute αm as shown in equation (10).

The assumption of vi = 0 outside the known values v1, v2,..., vN results in biased or, rather, Equation 10.
αm = = =
k ≥ m , l ≥1

∑

p k ql +
k ≥1, l ≥1

p m −1 p
m −1

∑

k ≥1, l ≥ m

∑

p k ql −
k ≥1, l ≥1

p k q l + q m −1 − ( p q)
m −1

∑

k ≥m, l ≥m

∑

p k ql
k ≥1, l ≥1

p k q l − ( p q ) m −1

∑

p k ql

+q

m −1



Discretization of Rational Data

Define the length of S to be the number of As and Bs minus 2, which is k + l. Effectively, we do not count the initial B of S and the final A of S. The expected length L of S is:
L = = = =
k ≥1,l ≥1

∑

(k + l ) p k q l
k l k l l ≥1 k ≥1 l ≥1

∑ k p ∑ q + ∑ p ∑l q
k ≥1

p q p q ⋅ + ⋅ 2 (1 − p ) 1 − q 1 − p (1 − q ) 2 1 pq

(11) Suppose we have a sequence T of N randomly selected As and Bs. What is the expected number of the above sequences S occurring in T? For our purposes, a sufficiently precise estimate is:

N / L = N pq

(12)

Of the expected number of sequences S occurring in T, the fraction of sequences that qualify for being sequences of event Em is approximately equal to αm. Thus, a reasonable estimate of the expected number of sequences of Em occurring in T, which we denote by K(N,m), is:
K ( N , m) = ( N / L )α m = N p q[ p m −1 + q m −1 − ( p q ) m −1 ]

occur. We use this fact as follows. We select a value m* ≥ 1 so that K(N, m*) is as close to 1 as possible. Then we choose the standard deviation σ so that the sequence S with about m* As or Bs that we can expect to occur does not produce a marker if there is a sequence S' with length significantly larger than m*. By these arguments, the latter sequence S' is unlikely to have been produced by randomness, and thus is likely due to a particular behavior of the values of the attribute under consideration. In terms of the discussion in the introduction, we estimate that we have an abrupt pattern change of the first kind. We achieve the desired effect by selecting σ = m*. Indeed, that choice produces significant probabilities bg for g, m < g ≤ 2m, and these probabilities tend to smooth out the classification values vi associated with the As and Bs of all randomly produced sequences S. When N is not large, certain boundary effects should be addressed. We describe the adjustment and then justify it. Instead of demanding that K(N, m*) is close to 1, we ignore the first and last m* As and Bs of the sequence T, and require that K(N − 2 m*, m*) defined from:

K ( N − 2m, m) = ( N − 2m) p q[ p m −1 + q m −1 − ( p q ) m −1 ] (14)
is as close to 1 as possible. We motivate the adjustment as follows. When Gaussian convolution is performed with σ = m*, the first smoothed class value is computed using the values vi of the 4σ + 1 As and Bs at the beginning of T. Denote that subsequence by T'. If the central 2σ + 1 As and Bs of T' contain an S of some Em with m ≤ m*, then the class values of the As and Bs of any such S tend to be smoothed out. Thus, S is unlikely to result in a marker if a subsequence S' with length greater than m* exists. We establish the probability p for (14) and compute m* as follows. We take p to be the fraction of the number of training records of class A divided by the total number of training records, and we find m* by dichotomous search.

(13)

Each S occurring in T is a potential case for a marker that corresponds to the point where k As transition to l Bs. We do not want markers to result from sequences S that likely have been produced by randomness. We try to avoid such choices as follows. Suppose that, for some m ≥ 1, K(N,m) is approximately equal to 1. This implies that, on average, there is one sequence S of Em. Since αm of (10) decreases geometrically as m increases, such a sequence S of Em typically has not much more than m As or Bs, and any sequence S' with larger number of As or Bs is very unlikely to

0

Discretization of Rational Data

We note that, due to the symmetry of the formula K(N − 2m, m), the choice of m* implicitly also considers subsequences in which the roles of A and B are reversed. Table 1 shows σ as a function of N, for σ ≤ 10 and p = q = 0.5. There is an exceptional case where the selected σ must be reduced. As we argue shortly—see the discussion following (16)—we do not consider a marker between zi and zi−1 if vi = vi−1. Thus, no marker can be placed if no i satisfies 2σ + 2 ≤ i ≤ N − 2σ and vi ≠ vi−1. If that case occurs, several corrective actions are possible. We have found that reduction of σ to 1 is a good choice. If for the reduced σ there still is no index i satisfying 2σ + 2 ≤ i ≤ N − 2σ and vi ≠ vi−1, then we declare that no intervals should be created for the jth entry; as a consequence, we delete the jth entry from all records of A and B. Otherwise, we proceed with the reduced σ = 1. For example, if N = 37, σ = 6, and v13 = 1, v14 = v15 = · · · v30 = 0, v31 = 1, then no i satisfies 2σ + 2 = 14 ≤ i ≤ N − 2 σ = 25 and vi ≠ vi−1. Thus, σ should be reduced to 1. For that value, both i = 14 and i = 31, and possibly other values of i, satisfy 2σ + 2 = 4 ≤ i ≤ N − 2σ = 35 and vi ≠ vi−1. Thus, σ = 1 should be used. On the other hand, let N = 37 and σ = 6 as before, but suppose v1 = 1, v2 = v3 = · · · = v35 = 0, v36 = 1, v37 = 0. For σ = 6, no i satisfies 2σ + 2 = 14 ≤ i ≤ N − 2σ = 25. Reduction of σ to 1 produces the same negative conclusion. Thus, no intervals should be created for the jth entries, and we delete these entries from all records of A and B. Table 1. σ as function of N for σ ≤ 10 and p = q = 0.5
N 3–7 8–11 12–15 16–31 32–54 55–99 100–186 187–359 360–702 703–1387 σ 1 2 3 4 5 6 7 8 9 10

Definition of Marker
Suppose we have selected σ, as described previously, and have computed the smoothed class values vi. As we move along the sequence of usable values vi, the absolute difference δi between adjacent vi −1 and vi,
δi = | vi − vi −1 |

(15)

measures the abruptness with which class values change. We call δi a difference value. The largest such value, say δ *, produces a marker c between
zi* −1 and zi*. That is, c = ( zi* −1 + zi* ) / 2
i

(16)

The selection rule for c requires a small adjustment due to a quirk that may be introduced by the convolution process. It is possible that, for the selected c, the corresponding original class values vi* −1 and vi* are equal. In case all zi are distinct, both values zi* −1 and zi* separated by c come either from A records or from B records. If several zi are equal, more complex interpretations are possible. However, all of them reflect unattractive cases. To rule out all such situations, we restrict the selection of the difference values δ * by considering δi values only if vi ≠ vi−1. Thus, i
δi* = max{δi | vi , vi −1 ∈U , vi ≠ vi −1}
i

(17)

where U is the set of usable values. If the maximum is attained by several i*, we pick one closest to N/2, breaking any secondary tie by a random choice. For example, if σ = 6 and N = 60, then the vi with index i satisfying 2σ+ 1 = 13 ≤ i ≤ N − 2σ = 48 are usable. Suppose these values are v13 = 0.3214, v14 = 0.3594, v15 = 0.4042, v16 = 0.4439, v17 = 0.4760, v18 = 0.4986,..., v45 = 0.4740, v46 = 0.4410, v47 = 0.4007, and v48 = 0.3612. For these values, formula (15) produces δ14 = 0.0380, δ15 = 0.0448, δ16 = 0.0397, δ17 = 0.0321, δ18 = 0.0226,..., δ46 = 0.0330, δ47 = 0.0403, and δ48 =



Discretization of Rational Data

0.0395. Suppose the largest δi for which vi ≠ vi−1, is unique and is δ15 = 0.0448. Thus, i* = 15. If zi*= z15 = 7 and zi* −1 = z14 = 5, the marker c is defined by c = (zi* −1+zi*)/2 = (5 + 7)/2 = 6. The next scheme summarizes the computation producing the initial marker c. The scheme also outputs the standard deviation σ of the convolution process since that information is needed later in another application of the algorithm.

5. (Select marker.) For i = 2σ +2, 2σ + 3,..., N − 2σ, let δi = | vi − vi −1 |. Select i* so that * δi* = maxi{δi | vi ≠ vi−1}. If i is not unique, select an i* closest to N / 2 and break any secondary tie by random choice. Define the marker c by c = ( zi* −1 + zi* ) / 2. Output the marker c, the standard deviation σ, and the difference value δ *.
i

AddItIonAl MArker AlgorIthM InItIAl MArker
Input: Rational numbers z1 ≤ z2 ≤ · · · ≤ zN of the jth attribute of the records of A and B. Output: Either: Marker c for the jth attribute, standard deviation σ of the convolution process, and the difference value δi* associated with the marker. Or: “Marker cannot be determined.” Procedure: 1. (Check if N is too small or if σ = 1 cannot produce a marker.) If N ≤ 6 or if, for σ = 1, there is no index i satisfying 2σ + 2 ≤ i ≤ N − 2σ and vi ≠ vi−1, then output “Marker cannot be determined,” and stop. (In that case, one should delete the jth entries from all records of A and B.) 2. (Compute class values.) For i = 1, 2,..., i N, define Hi = {h | zh = zi}, H A = {h ∈ H | zh is taken from an A record}, and i compute the class value vi = | H A |/|Hi|. 3. (Define p, q, and σ.) Define p = |A|/(|A| + |B|) and q = 1 − p. Let m* be the value m ≥ 1 for which K(N − 2m, m) is closest to 1. Let σ = m*. If there is no index i satisfying 2σ + 2 ≤ i ≤ N − 2σ and vi ≠ vi−1, lower σ to 1. 4. (Compute smoothed class values.) For i = 1, 2,..., N, use the class values vi, the standard deviation σ, and the bg values of (5) to compute the smoothed class 2σ values vi = b0 ⋅ vi + ∑ i =1 b g (vi + g + vi − g ). This section covers how one additional marker is selected, assuming that a certain collection of markers is already at hand. The procedure is invoked if the sets A' and B', derived from the sets A and B via the markers obtained so far, do not satisfy the clash condition and thus cannot be fully separated.

Critical Interval
The markers on hand define intervals of the rational line for each index j ∈ J, and these  markers produce a transformation of A and B to A' and B'. Define such an interval to be critical if a properly chosen subdivision can lead to a transformation of A and B to, say, A" and B" such that A" and B" have more clashing pairs of records than A' and B'. Clearly, each critical interval is associated with a particular attribute j ∈ J, and all critical intervals are readily determined via the nonclashing pairs of records of A' and B'. We omit the obvious process. For each critical interval, we compute an additional marker using a method virtually identical to Algorithm INITIAL MARKER. Specifically, the input sets A and B of the algorithm are now the subsets A ⊆ A and B ⊆ Bof records for which the values of the associated attribute j ∈ J falls  into the critical interval. The algorithm either outputs a marker together with the associated standard deviation σ and the difference value δi*, or it declares that a marker cannot be found. In the latter case, we do not delete any attribute values from A and B, but



Discretization of Rational Data

instead record that the interval cannot be refined, and thus exclude it from further consideration. When all critical intervals have been processed, two cases are possible. Either we have at least one additional marker, or no additional markers could be determined. In the latter case, the transformation process outputs A', B', the markers on hand, and the warning message “A and B cannot be fully separated,” and then stops. If at least one additional marker has been determined, we select one of them and proceed recursively as described previously. The selection of the marker is based on a measure that considers the attractiveness of pattern change at the point of the marker, and on the number of nonclashing pairs of records of A' and B' that determine the interval to be critical. The latter number is called the relevance count. We first discuss the attractiveness of the pattern change.

Proof: Since δi ≥ 0, the claim is trivial if b0 − 2bn+1 ≤ 0. Hence, we suppose that b0 > 2bn+1. Using the formula (7) for vi in the definition of δi of (15), we have, (see equation (18)). Consider δi produced by the last A and first B of the label sequence. Due to the k ≥ n + 1 As (resp. l ≥ n + 1 Bs) in the label subsequence, we have vi −1 = vi−2 = · · · = vi−n−1 = 1 (resp. vi = vi+1 = · · · = vi+n = 0). We use these class values in (18) and simplify to get:
δi = | bn +1 − b0 +

g = n +1

∑ (b

∞

g

− b g +1 )(vi + g − vi − g −1 ) |

(19) Since b0 > 2b n+1 and, for all g ≥ 0, bg ≥ 2b g+1, the right hand side of (19) is minimum if, for all g ≥ n + 1, we have vi+g = 1 and vi−g−1 = 0. For that case, δi becomes δi = |2bn+1 − b0| = b0 − 2bn+1 = ε. If n is sufficiently large, then the label subsequence of Theorem 2 is quite unlikely to be a random occurrence. Thus, if the label subsequence does occur, we estimate that it corresponds to an abrupt pattern change of the first kind. Indeed, as was discussed previously, as n grows beyond σ, this conclusion tends to become valid. For example, n = 1.5σ  is large enough for the   desired conclusion, and we choose this value of n to compute the lower bound ε. Thus,

Attractiveness of Pattern Change
The attractiveness of a pattern change is based on a lower bound ε on the difference values δi of (15) for certain label subsequences. Each such subsequence has, for some n ≥ 1 yet to be specified, k ≥ n + 1 As followed by l ≥ n + 1 Bs, and δi is the difference value produced by the last A and the first B of the sequence. We establish a lower bound ε for δi. Theorem 2. Let a label sequence be given for which the original rational numbers zi are all distinct. For some n ≥ 1, let a label subsequence have k ≥ n + 1 As followed by l ≥ n + 1 Bs. Then ε = b0 − 2b n+1 is a lower bound for δi of (15).

ε = b0 − 2b 1.5σ +1  

(20)

Equation 18.
δi = | [b0 vi + ∑ b g (vi + g + vi − g )] − [b0 vi −1 + ∑ b g (vi + g −1 + vi − g −1 )] |
g =1 g =1 ∞ ∞

= | ∑ (b g − b g +1 )(vi + g − vi − g −1 ) |
g =0

∞



Discretization of Rational Data

Let c be a marker, and define δi* to be the change of smoothed class values corresponding to the marker c. To measure how likely the marker c corresponds to a pattern change of the first kind, we compare δ * with ε. Specifically, if the ratio: i

AlgorIthM AddItIonAl MArker
Input: List of critical intervals. Output: Either: “No critical interval can be refined.” or: Additional marker for one critical interval. Procedure: 1. For each critical interval, do Algorithm INITIAL MARKER where the input sets are the subsets A ⊆ A and B ⊆ B of records for which the value of the associated attribute j ∈ J falls into the critical interval. If the algorithm declares that no marker can be determined, remove the interval from the list of candidates. 2. If the list of critical intervals is empty, output “No critical interval can be refined,” and stop. 3. For each critical interval, use the value δi* and σ determined in Step 1 and the relevance count R to compute the potential γ = Rδ */ (b0 − 2b 1.5σ +1 ).   i 4. Select the critical interval with maximum potential. In case of a tie, favor the interval with larger number of zi values, and break any secondary tie randomly. Using i* of the associated δ *, output the i marker p = (zi* −1+ zi*)/2 for the selected interval, and stop.

δi* / ε = δi* /(b0 − 2b 1.5σ +1 )  

(21)

is near or above 1, then we estimate that we likely have a pattern change of the first kind. Thus, the ratio δ */ε measures the attractiveness i of the marker. We say that the marker c has attractiveness δ */ε.
i

Selection of Marker
For each critical interval for which we have determined an additional marker, define the potential of the marker to be the product of the relevance count of the interval and the attractiveness of the marker. Letting γ and R denote the potential and relevance count, respectively, we have, for each marker, the potential γ as:
γ = Rδi* / ε = Rδi* /(b0 − 2b 1.5σ +1 )  

(22)

We select the marker with highest potential, add that marker to the list of markers on hand, and proceed recursively as described earlier. For example, suppose we have two critical intervals. For the first interval, we have σ = 6, N = 58, δ * = 0.037, and R = 12. For σ = 6, we have i ε = (b0 − 2b 1.5σ +1 ) = 0.035, and the potential is γ   = Rδ */ε = 12(0.037/0.035) = 12.7. If the second i critical interval has a smaller potential, then we refine the first interval. Suppose that for the first interval we have zi* = 17 and zi* −1 = 14. Then the new marker is p = (zi* −1 + zi*)/2 = (14+17)/2 = 15.5. We summarize the selection process.

cutPoInt AlgorIthM
With algorithms INITIAL MARKER and ADDITIONAL MARKER at hand, we are ready to describe the entire algorithm of Cutpoint. We begin the scheme as follows. For each j ∈ J, we carry out algorithm INITIAL MARKER and thus get either a marker, say cj, or conclude that a marker cannot be obtained. In the latter case, the attribute j is deleted from all records of A and B.



Discretization of Rational Data

For the reduced sets, which we again denote by A and B, we select a j ∈ J whose marker has the largest associated difference value δ *. We apply i the transformation implied by that single marker and thus obtain sets A' and B'. We test if A' and B' satisfy the clash condition. If this is not so, that is, if at least one record of A' and one record of B' do not clash, then we compute one additional marker with algorithm ADDITIONAL MARKER, update the sets A' and B' accordingly, and proceed recursively. That is, we test if the sets A' and B' satisfy the clash condition, and so on. The process stops either when A' and B' satisfy the clash condition, or when an additional marker cannot be determined. In the implementation of the method, we also stop introducing additional markers for a given j ∈ J when the number of markers reaches a specified maximum. In the tests described later, that limit was set to six. As an option, one may also stop the refinement process if the introduction of an additional

marker does not reduce the total number of nonclashing pairs of records. Indeed, since that marker had the highest potential among the possible choices and did not reduce the number of nonclashing pairs, one might conjecture that additional markers may not be desirable in any interval, and thus may terminate the refinement process. When the recursive process terminates, some attributes j ∈ J may not have received any marker. Of course, one such marker was determined by algorithm INITIAL MARKER in the initial part of Cutpoint, and we now assign that marker. Thus, each j ∈ J now has at least one marker. At this point, we have the desired collection of markers. We use them for one final update of the sets A' and B'. We output the collection of markers and the associated sets A' and B'. If these sets do not satisfy the clash condition, we also output the warning message “A and B cannot be fully separated.”

Table 2. Accuracy of C4.5 on Testing Data
Dataset heart australian hepatitis horse-colic boston housing wisconsin breast crx haberman ionosphere pima spectf Cutpoint 78.90 85.22 78.71 84.53 80.63 94.98 85.07 69.92 88.31 75.13 74.11 Entropy CC 77.98 86.23 75.48 83.46 77.06 93.69 86.23 70.91 90.88 75.26 79.77 Entropy MDL 77.31 85.94 76.78 88.62 83.59 95.41 86.09 69.26 90.88 73.56 77.86

Overall

81.41

81.54

82.30



Discretization of Rational Data

coMPutAtIonAl results
Cutpoint has been used so far in a variety of projects including credit rating, video image analysis, and word sense disambiguation. In each of the numerous cases, Cutpoint has proved to be effective and reliable. We also have compared Cutpoint with the two entropy-based methods Entropy CC and Entropy MDL, described in the introduction. For the comparison, we selected the following data sets of the UC Irvine repository of machinelearning databases: cleveland heart, australian, hepatitis, horse-colic, boston housing, wisconsin breast, crx, haberman, ionosphere, pima, and spectf. Boston housing was run using the attribute median housing value with a cutoff point of $21,000. For both the heart and the crx databases, some attributes were treated as nominal. In a 5-fold cross-validation approach, we used Cutpoint, Entropy CC, and Entropy MDL for

discretization, and finally applied four classification schemes. The latter methods were chosen so that classification by decision trees, naive Bayes methods, support vector machines, and learning logic formulas were represented. For the first three cases we chose the version J4.8 of C4.5, the naive Bayes method, and the SMO support vector machine implemented by Witten and Frank (2000). In each case, we used the default parameters for the runs, except that we carried out 5-fold crossvalidation instead of 10-fold cross-validation. For the fourth method, we selected the Lsquare version of Truemper (2004). We emphasize that we do not claim that discretization is needed or even desired for the first three classification methods. We do say that, if one contemplates a discretization preprocessing step followed by application of a classification method of one of the four types, then one may want to consider the results shown in Table 2.

Table 3. Accuracy of Naive Bayes on Testing Data
Dataset heart australian hepatitis horse-colic boston-housing wisconsin-breast crx haberman ionosphere pima spectf Cutpoint 82.87 85.07 83.23 82.63 80.03 96.98 85.07 69.92 90.88 76.18 77.48 Entropy CC 81.89 85.80 82.58 82.63 79.63 96.55 86.09 69.92 90.88 73.83 73.00 Entropy MDL 83.87 85.36 81.29 82.34 78.64 97.13 85.80 67.94 88.89 75.26 72.60

Overall

82.76

82.07

81.74



Discretization of Rational Data

Prediction Accuracy for Testing Data
For a compact representation and ease of comparison, we have grouped the results for each of the four classification methods. Tables 2–5 show the accuracy established by the 5-fold cross-validation process. The headings contain the term “Testing Data,” which may seem superfluous. The term has been added to differentiate the results from a second evaluation, to be discussed shortly. Note that the performance data should only be used for a comparison of Cutpoint, Entropy CC, and Entropy MDL, and not for evaluation of the four classification methods. The reason is that each classification method almost certainly is not the best method of that type. For example, there are better commercial decision-tree methods than C4.5, there are numerous ongoing developments concerning naive Bayes methods and support vector machines, and full implementation of

Lsquare, as conceived at present, has not yet been accomplished. From the results, we conclude the following. Based on the average performance, Cutpoint is the preferred approach for naive Bayes and Lsquare, while Entropy MDL is best for C4.5 and SMO. Furthermore, Entropy CC is dominated by Cutpoint and Entropy MDL. When one examines the performance for individual data sets, then the preference is not clearcut. For example, for naive Bayes and Lsquare, Cutpoint is best for 7 of the 11 data sets. In the case of C4.5 and SMO, Entropy MDL is best for 5 of the 11 data sets. Also, there are several cases where Entropy CC is better than Cutpoint and Entropy MDL. Thus, if one needs highest possible accuracy for a given situation, and if sufficient time and data are available to estimate accuracy, then one should try all three discretization schemes and select the one performing best.

Table 4. Accuracy of SMO on Testing Data
Dataset heart australian hepatitis horse-colic boston housing wisconsin breast crx haberman ionosphere pima spectf Cutpoint 82.89 85.80 81.29 85.37 79.07 95.69 85.36 70.25 88.88 74.86 76.77 Entropy CC 82.94 85.94 80.00 83.99 80.65 95.69 85.94 70.91 91.46 74.59 79.73 Entropy MDL 80.92 85.22 77.42 86.97 80.83 95.40 85.80 74.84 90.02 75.90 80.84

Overall

82.38

82.89

83.10



Discretization of Rational Data

Table 5. Accuracy of Lsqcc on Testing Data
Dataset heart australian hepatitis horse-colic boston housing wisconsin breast crx haberman ionosphere pima spectf Cutpoint 83.25 86.67 80.65 85.62 83.80 95.83 85.51 66.65 90.90 73.16 82.37 Entropy CC 78.25 85.65 80.00 87.00 83.99 95.69 84.78 68.29 91.46 76.16 79.35 Entropy MDL 81.22 83.77 76.77 88.61 82.60 95.83 84.20 __* 92.88 72.12 80.12

Overall

83.13

82.78

82.40

* Entropy MDL failed to produce separable sets. For the computation of average performance, the value 68.29 for Entropy CC was used.

Table 6. Accuracy of C4.5 on Training Data
Dataset heart australian hepatitis horse-colic boston housing wisconsin breast crx haberman ionosphere pima spectf Cutpoint 87.05 90.80 88.87 89.07 88.78 96.71 89.82 75.98 94.59 81.32 92.51 Entropy CC 88.70 90.73 89.68 89.27 89.82 97.03 90.07 77.04 96.08 82.36 92.88 Entropy MDL 88.21 89.06 90.48 90.90 90.32 96.89 88.98 75.00 95.65 81.12 92.50

Overall

88.68

89.42

89.01



Discretization of Rational Data

Table 7. Accuracy of Naive Bayes on Training Data
Dataset heart australian hepatitis horse-colic boston housing wisconsin breast crx haberman ionosphere pima spectf Cutpoint 85.48 87.79 87.74 83.83 81.87 97.28 87.18 79.00 92.73 78.97 82.21 Entropy CC 85.56 87.90 88.55 84.03 82.71 97.46 87.93 79.57 93.02 80.73 82.57 Entropy MDL 84.57 86.74 88.23 84.51 81.67 97.50 87.32 74.67 92.17 77.83 81.64

Overall

85.83

86.37

85.17

Table 8. Accuracy of SMO on Training Data
Dataset heart australian hepatitis horse-colic boston housing wisconsin breast crx haberman ionosphere pima spectf Cutpoint 88.86 89.24 92.90 90.69 93.03 97.71 89.06 77.37 97.29 80.99 95.60 Entropy CC 88.04 90.11 94.19 90.83 94.76 98.07 89.56 78.84 98.29 83.10 94.94 Entropy MDL 87.63 85.87 91.77 90.83 91.05 97.60 88.48 74.84 99.64 78.26 93.16

Overall

90.25

90.98

89.01



Discretization of Rational Data

Prediction Accuracy for Training data
In some applications, one not only desires high accuracy when records of A − A and B − B are to be classified, but also wants perfect or nearperfect accuracy for the training sets A and B. For example, if a diagnostic system for certain diseases is trained on sets A and B, then a physician may demand that, at a minimum, the system handles all training cases correctly. As a second example, consider the situation where the training data are obtained via a simulation process, and where the classification method is to produce compact rules that replace a complex decision mechanism of the simulation process. The user of the classification rules then may demand that all training data produced from simulation runs must be correctly classified. One may evaluate the performance of Cutpoint,

Entropy CC, and Entropy MDL and of the four classification methods on training data as follows. In each case of the 5-fold crossvalidation process, 80% of the given data sets A and B are used for training, while 20% are used for testing. One applies the classification rules derived from the training sets to these very same sets, and thus obtains for each pair of sets A and B five accuracy estimates. The average of these five figures is an estimate of the accuracy of the classification method for training data. Tables 6–9 contain these results in the format of Tables 2–5. A first conclusion from Tables 6–9 is that three of the four methods fail to achieve perfect or near-perfect accuracy for the training data. The exception is Lsquare. Indeed, according to Table 9, the average accuracy for the Cutpoint/Lsquare combination is 98.97%. That figure includes the comparatively poor accuracy of 88.6% of the haberman data set that, with only three attributes

Table 9. Accuracy of Lsqcc on Training Data
Dataset heart australian hepatitis horse-colic boston housing wisconsin breast crx haberman ionosphere pima spectf Cutpoint 100.00 99.89 99.68 99.66 100.00 100.00 99.86 89.62 100.00 100.00 100.00 Entropy CC 100.00 99.78 99.68 99.73 99.85 100.00 99.82 85.54 100.00 99.77 100.00 Entropy MDL 99.34 96.60 99.19 98.91 96.25 99.57 98.77 *__ 100.00 83.73 98.88

Overall

98.97

98.56

96.07

* Entropy MDL failed to produce separable sets. For the computation of average performance, the value 85.54 for Entropy CC was used.

0

Discretization of Rational Data

and 306 records defies full separation. When that set is excluded from consideration, the average accuracy of Cutpoint/Lsquare is 99.91%. For the other three methods, the highest accuracy figures are as follows: 89.42% for C4.5, 86.37% for naive Bayes, and 90.98% for SMO. Curiously, each of these numbers is produced with discretization by Entropy CC. When one removes the results for the haberman data set and selects the highest accuracy among the three discretization methods, one gets the following figures: 90.66% for C4.5, 87.05% for naive Bayes, and 92.19% for SMO. One may argue, quite properly so, that classification methods such as C4.5, naive Bayes, and SMO are designed to give highest accuracy for testing data, and that they are not optimized for accuracy on training data. Thus, it would be interesting to see how the methods may be modified so that simultaneously perfect or near-perfect accuracy for training data and high accuracy for testing data are achieved. Some condition needs to be imposed on the construction to rule out trivial modifications such as making the training data part of the output of the classification method, and enlarging the classification rule so that one first checks if the record is part of the training set. For example, one may impose the restriction that the size of the encoding of the entire classification rule must be logarithmic in the size of the encoding of the training sets. The additional requirement of perfect or nearperfect accuracy for training data likely influences the choice of the discretization method. Thus, for a revised classification method, it is quite possible that Cutpoint or Entropy CC lead to better results than Entropy MDL.

Optimized Classification
The results implicitly assume that costs for obtaining attribute values of records need not be considered. When such costs are important, the entire classification approach must be reconsidered. For the sake of discussion, we

assume the setting described in Truemper (2004), which is as follows. Tests T1, T2,..., Tm are available to obtain attribute values. In particular, when test Tj is performed, then a specified subset of attribute values is obtained. Each test Tj carries a certain cost. In the general case, a test may produce rational, nominal, or logic data, except that the value Absent is not possible. On the other hand, the value Unavailable is allowed. A test produces that value if it has been determined that the attribute cannot or should not be obtained. Classification of a record into one of the populations A and B is done as follows. Initially, some entries are given, and the remaining entries are equal to Absent. The classification method recursively decides if it should declare the record to be in A or B, or if it should carry out one of the tests to get additional entries. In the first case, the methods stops with the declaration that the record is classified into A or B. In the second case, the method selects a test, requests that the test be carried out, adds the new values to the record, and invokes recursion. The goal is classification with an accuracy that is above a given lower bound, and that, subject to that condition, involves minimum or closeto-minimum total cost of tests. Depending on the setting, one may also impose the additional condition of perfect or near-perfect accuracy on the training data. We call any scheme that carries out this recursive process and that achieves the desired goal an optimized classification process. At this time, it is largely open how the various classification methods in existence should be modified so that they can carry out optimized classification. For Lsquare, the solution via socalled optimized formulas is given in Truemper (2004). We omit details here, but cover the adjustment needed for Cutpoint. Instead of enforcing at least one marker per attribute, we now require at least k markers, where k is a small positive integer. Then Lsquare is applied with the so-called optimized formula option. Typically, 1 ≤ k ≤ 6 is appropriate. The specific choice may be obtained by trying several values and selecting



Discretization of Rational Data

one so that the classification of testing data is sufficiently accurate and can be done at low total test costs. If such a trial-and-error process is not possible, k = 2 or k = 3 is likely to work well. Regardless of the choice of k, the method always achieves perfect accuracy on the training data, assuming that Cutpoint produces fully separable sets of logic data.

Auer, P., Holte, R.C., & Maass, W. (1995). Theory and applications of agnostic PAC-learning with small decision trees. In Proceedings of the Eighth European Conference on Machine Learning (pp. 21-29). Bartnikowski, S., Granberry, M., Mugan, J., & Truemper, K. (2006). Transformation of rational and set data to logic data. In E. Triantaphyllou & G. Felici (Eds.), Data mining and knowledge discovery approaches based on rule induction techniques. Berlin: Springer-Verlag. Bay, S.D. (2000). Multivariate discretization of continuous variables for set mining. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 315-319). Bay, S.D., & Pazzani, M.J. (1999). Detecting change in categorical data: Mining contrast sets. In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 302-306). Boros, E., Hammer, P.L., Ibaraki, T., & Kogan, A. (1997). A logical analysis of numerical data. Mathematical Programming, 79, 163-190. Boros, E., Hammer, P.L., Ibaraki, T., Kogan, A., Mayoraz, E., & Muchnik, I. (2000). An implementation of logical analysis of data. IEEE Transactions on Knowledge and Data Engineering,12, 292-306. Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and unsupervised discretization of continuous features. In Machine Learning: Proceedings of the Twelfth International Conference (pp. 194-202). Fayyad, U.M. & Irani, K.B. (1992). On the Handling of Continuous-Valued Attributes in Decision Tree Generation. Machine Learning 8, 87-102. Fayyad, U.M., & Irani, K.B. (1993). Multi-interval discretization of continuous-valued attributes for classification learning. In Proceedings of the Thir-

conclusIon
The chapter introduces the Cutpoint method for discretization of rational data and compares the scheme with two entropy-based methods called Entropy CC and Entropy MDL. According to tests, Cutpoint seems best when the classification is done by naive Bayes methods or learning logic methods, while Entropy MDL appears to be best for decision tree methods and support vector machines. The performance differences are fairly small so that, for specific cases, one may want to apply each of the three methods and select the one giving best results. The chapter also discusses the choice of discretization methods when perfect or near-perfect accuracy is desired for training data, or when costs of obtaining data are to be considered in so-called optimized classification processes.

AcknowledgMent
This research was supported in part by the Technical SupportWorking Group (TSWG) under contract N41756-03-C-4045.

references
An, A., & Cercone, N. (1999). Discretization of continuous attributes for learning classification rules. In Proceedings of the Third Pacific-Asia Conference on Methodologies for Knowledge Discovery and Data Mining (pp. 509-514).



Discretization of Rational Data

teenth International Joint Conference on Artificial Intelligence (pp. 1022-1027). Felici, G., Sun, F., & Truemper, K. (2004). Learning logic formulas and related error distributions. In E. Triantaphyllou & G. Felici (Eds.), Data mining and knowledge discovery approaches based on rule induction techniques. Berlin: Springer-Verlag. Felici, G., & Truemper, K. (2002). A MINSAT approach for learning in logic domains. INFORMS Journal on Computing, 14, 20-36. Forsyth, D.A., & Ponce, J. (2003). Computer vision: A modern approach. Englewood Cliffs, NJ: Prentice Hall. Gama, J., Torgo, L., & Soares, C. (1998). Dynamic discretization of continuous attributes. In Proceedings of the Sixth Ibero-American Conference on Artificial Intelligence (pp. 160-169). Holte, R.C. (1993). Very simple classification rules perform well on most commonly used datasets. Machine Learning, 11, 63-91. Kohavi, R., & Sahami, M. (1996). Error-based and entropy-based discretization of continuous features. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (pp. 114-119). Kwedlo, W., & Krętowski, M. (1999). An evolutionary algorithm using multivariate discretization for decision rule induction. In Proceedings of the European Conference on Principles of Data Mining and Knowledge Discovery (pp. 392-397). Liu, H., Hussain, F., Tan, C.L., & Dash, M. (2002). Discretization: An enabling technique. Data Mining and Knowledge Discovery, 6, 393-423.

Maass, W. (1994). Efficient agnostic PAC-learning with simple hypotheses. In Proceedings of the Seventh Annual ACM Conference on Computerized Learning Theory (pp. 67-75). Miller, R.J., & Yang, Y. (1997). Association rules over interval data. In Proceedings of the ACM SIGMOD International Conference on Management of Data (pp. 452-461). Mitchell, T. (1997). Machine learning. Boston, MA: McGraw Hill. Quinlan, J.R. (1986). Induction of decision trees. Machine Learning, 1, 81-106. Ramakrishnan, R., & Gehrke, J. (2003). Database management systems (3rd ed.). New York: McGraw-Hill. Srikant, R., & Agrawal, R. (1996). Mining quantitative association rules in large relational tables. In Proceedings of the ACM SIGMOD International Conference on Management of Data (pp. 1-12). Truemper, K. (2004). Design of logic-based intelligent systems. New York: Wiley. Wang, K., & Goh, H.C. (1997). Minimum splits based discretization for continuous features. In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (pp. 942-951). Witten, I.H., & Frank, E. (2000). Data mining. San Diego: Academic Press. Wu, X. (1996). A Bayesian discretizer for realvalued attributes. The Computer Journal, 39, 688-691.



24

Application to the Financial Timing Decision Problem
Massimo Liquori Università di Roma “La Sapienza”, Italy Andrea Scozzari Università di Roma “La Sapienza”, Italy

Vector DNF for Datasets Classifications:

Chapter II

AbstrAct
Traditional classification approaches consider a dataset formed by an archive of observations classified as positive or negative according to a binary classification rule. In this chapter, we consider the financial timing decision problem, which is the problem of deciding the time when it is profitable for the investor to buy shares or to sell shares or to wait in the stock exchange market. The decision is based on classifying a dataset of observations, represented by a vector containing the values of some financial numerical attributes, according to a ternary classification rule. We propose a new technique based on partially defined vector Boolean functions. We test our technique on different time series of the Mibtel stock exchange market in Italy, and we show that it provides a high classification accuracy, as well as wide applicability for other classification problems where a classification in three or more classes is needed.

IntroductIon
In the area of knowledge-based expert systems, the aim is to detect structural information from

large datasets in order to extract salient features for identifying differences that separate one set of data from another. Classification methods developed in the literature try to classify the given observations

Copyright © 2008, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

and, in addition, to classify new observations in a way consistent with past classifications. Such structural information can provide powerful means for the solution of a variety of problems, including classification, automated knowledge acquisition for expert systems, development of pattern-based decision support systems, detection of inconsistencies in databases, feature selection, medical diagnosis, marketing, and numerous aspects of etiology. Several approaches coming from different fields have been proposed in the literature to tackle the classification problem. One of the best-known methods is support vector machines (SVM) that has proved highly successful in a number of classification studies. Although the subject traces its origin to the seminal work of Vapnik and Lerner (1963), it is only now receiving a growing attention. In the simplest case, given a set of observations classified into two classes, the aim is to construct a function to discriminate between classes. This can be done via a mathematical programming approach. A linear programming-based approach, stemming from the multisurface method of Mangasarian (1965, 1968), has been used for a breast cancer diagnosis system (Mangasarian, Setiono, & Wolberg, 1990; Mangasarian,, Street, & Wolberg, 1995; Wolberg, & Mangasarian, 1990). Another approach is the quadratic programming method based on Vapnik’s Statistical Learning Theory (Cortes & Vapnik, 1995; Vapnik1995). See Burges (1998) for a tutorial on classification via SVMs. Bredensteiner and Bennett (1999) show how the linear programming and quadratic programming methods can be combined to yield two new approaches for the multiclass problem. Other mathematical programming techniques, based on the minimization of some function measuring the classification error (Freed & Glover, 1986; Glover, 1990; Kamath, Karmarkar, Ramakrishnan, & Resende, 1992; Triantaphyllou, Allen, Soyster, & Kumara, 1994), have been used in classification problems. A MINSAT approach for learning logic relationship that correctly classify a given

dataset has been recently proposed in Felici and Truemper (2000). Decision trees are another popular technique for classification. The main reason behind their popularity seems to be their relative advantage in terms of interpretability. There are several efficient and simple implementations of decision trees (Quinlan, 1993). In a recent work, Street (2004) presents an algorithm based on nonlinear programming for multicategory decision trees. Unfortunately, one of the limitations of most decision trees is that they are known to be unstable, especially when dealing with large data sets (Fu, Golden, Lele, Raghavan, & Wasil, 2003). In the literature, there are several papers that provide heuristics and metaheuristics for the problem of finding an optimal decision tree, which is known to be an NP-complete problem (Fu, et al., 2003; Niimi & Tazaki, 2000). Naive Bayes method is another simple but effective classifier (Jefferys & Berger, 1992; Yeung, 1993). The attributes, observed in the training set, are assumed to be conditionally independent, given the value of the class attribute. In order to derive a good classification rule, and considering the independence assumption made, the marginal probabilities of each attribute must be estimated. In Lin (2002), it is shown that the asymptotic target of support vector machines is some interesting classification functions that are directly related to the Bayes rule. Actually, the independence assumption is unrealistic, thus, Bayesian networks have been introduced that explicitly model dependencies between attributes (Pernkopf, 2005). Thus, given a set of observations, the problem is to find a network that best matches the training set. The search for the best network is based on a scoring function that evaluates each network with respect to the training data (Heckerman, Geiger, & Chickering, 1995; Lam & Bacchus,1994). Several classification problems can also be formulated as an artificial neural network problem. An artificial neural network (ANN) can be thought of as a mathematical paradigm that models the bio-

25

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

logical neural system. The theory and the design of ANNs has significantly developed during the last 20 years. The increasing interest for ANNs is mainly due to their ability to learn both from supervised and unsupervised datasets. ANNs are very well suited for solving large classification problems (Archer & Wang, 1993; Bishop, 2004; Boulle, Chandramohan, & Weller, 2001; Coakley & Brown, 2000). There exist several architectures for dealing with real applications, and different algorithms and methods are used for training a neural network. For instance, the backpropagation algorithm (Archer & Wang, 1993; Lawrence & Giles, 2000; Ooyen & Nienhuis, 1992) and the multisurface method of Mangasarian are often used for the training phase. In this chapter, we propose a new classification technique, based on combinatorics, optimization, and partially defined vector Boolean functions, which is suitable when the archive of observations is, in particular, classified in more than two classes. Our method relates to the logical analysis of data (LAD), that is, a classification method proposed in (Hammer, 1986). LAD was first applied to the classification of binary datasets when a binary classification rule was adopted (Boros, Hammer, Ibaraki, Kogan, Mayoraz, & Muchnick, 2000). This method assumes that the archive of observations can be naturally represented by a partially defined Boolean function. The goal of LAD is to obtain an extension of the partially defined Boolean function, that is, a completely defined Boolean function that represents the classification of all the vectors in the sample space. The central concepts used by LAD are those of prime implicants, which are special logical conjunctions of literals imposed on the values of the attributes in the dataset. The aim is to generate a set of prime implicants for finding a suitable minimal disjunctive normal form (DNF) (Crama, Hammer, & Ibaraki, 1988) representation of a Boolean function that allows both to describe the archive, and to correctly classify all known and most new observations. Such a minimal DNF

provides an extension of the partially defined Boolean function. A straightforward extension of the LAD technique when an archive S is classified, for instance, into three classes, S1, S2, and S3, can be obtained by first finding a minimal DNF that separates the class S1 from S2 ∩ S3, and then by referring to a second minimal DNF for classifying a new observation either in S2 or in S3. The disadvantage of this approach is that the two DNF so obtained may not capture all the relationships between the attributes and those salient features that separate one set from another. We apply our classification technique to the financial timing decision problem, which is the problem of deciding, at each time period, if it is profitable for the investor to buy shares, or to sell shares, or to wait in the stock exchange market. The decision is based on finding a ternary classification of a dataset of observations of the stock exchange course. Each observation in the dataset is represented by a vector containing the values of some financial numerical attributes (Murphy, 1997) like, for instance, the relative strength index (RSI), the rate of change (ROC), the stochastic oscillators (SO), and so forth. More precisely, we classify as positive those observations that refer to periods when it is profitable for the investor to buy shares, negative when it is profitable to sell shares, and null when it is a waiting period. In this chapter, we develop an approach that produces a vector disjunctive normal form (VDNF) representation that allows one to classify a new observation directly in one of the sets S1, S2, or S3. In fact, our archive S can be represented by a partially defined vector Boolean function y: S→{0,1}2 in such a way that for a given observation s∈S, we have s∈S1, s∈S2, and s∈S3 if and only if y(s)=(1,1), y(s)=(0,0) and y(s)=(1,0) or y(s)=(0,1), respectively. We then proceed to find a simple VDNF representation consistent to this classification, that is, to find a simple extension of y. Here, the simple requirement means that we want to find a VDNF as short as possible. The VDNF embeds the structural information of

26

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

the dataset so that we can correctly classify new observations and find out indications about the interpretation of the financial phenomenon under study. Both exact and heuristic procedures are used for generating a simple VDNF. Our technique has a wide applicability for classification problems where a classification in three or more classes is needed. We provide the effectiveness of our method by means of numerical experiments. We use three time series of the Mibtel stock exchange market in Italy over a period of 1 year. Each time series reports the open, close, high, and low daily prices. We generate the attributes of the financial timing decision problem by computing four technical oscillators (Murphy, 1997): the Relative Strength Index, the Difference on Average based on 2 days moving average, the Rate of Change, and the Difference on Average based on a period of 3 days. By means of graphical and numerical analysis, we divide the set of observations into three classes: S1, the set of the daily oscillators values suggesting to buy shares (positive observations); S2, the set of values suggesting to sell shares (negative observations); and S3, the set of values indicating a waiting period (null observations). We show that our method provides a high classification accuracy. We also compare our technique with some standard classification methods. In particular, we compare it with support vector machines, decision trees, naive Bayes, neural networks, and linear and quadratic discriminant analysis. The remainder of the chapter is organized as follows: Section 2 provides some notation and the main results of the chapter, Section 3 presents an example and describes the application considered in this chapter. Section 4 reports some experimental results along with the comparison analysis with other classification methods, and finally, in Section 5, conclusions and further research issues are discussed. In the Appendix, we provide a short description of the financial attributes considered in our application.

notAtIon And mAIn results
The input information to be classified is represented by an archive S of financial observations. We assume that each observation is represented by a d-dimensional vector containing the values of some financial numerical attributes (technical oscillators), where d is the number of attributes considered. We propose to have an initial classification of S into three classes, and we denote by S1 the positive class, by S2 the negative class, and by S3 the null class. In financial problems, these classes refer to periods when it is profitable to buy shares, or to sell shares, or to wait in the stock exchange market. That is, an observation that refers to a particular day, and that is classified, for instance, in S1, indicates that it is profitable to buy shares on that day because the next day the same operation will not be as profitable as the day before, that is, there could be an increase in the stocks’ prices. Hence, each daily observation forecasts what will happen in the stock exchange market. Our classification technique is based on a binary representation of the attributes. In fact, a binarization procedure, consisting of the transformation of numerical (real valued) data to binary (0,1) ones, can always be implemented. This procedure can be performed by referring to a single cut-point method, or to an interval cut-point method (see (Boros, Hammer, Ibaraki, & Kogan, 1997; Boros, et al., 2000) for details). The cut points will be chosen in a way that allows one to distinguish between positive, negative, and null observations. A set of cut-points is consistent if in the resulting binarized archive S1 ∩ S2=∅, S1 ∩ S3=∅, and S2 ∩ S3=∅. Hence, our binarized archive S can be represented by a partially defined vector Boolean function (pdVBf) y: S→ {0,1}2 in such a way that for a given observation s∈S, we have s∈S1, s∈S2, and s∈S3 if and only if y(s)=(1,1), y(s)=(0,0), and y(s)=(1,0) or y(s)=(0,1), respectively. When we introduce the largest set of

27

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

cut-points in the binarization process, the resulting binarized archive S is called Master pdVBf.

ean function y in the class of all vector Boolean functions. Fact. A partially defined vector Boolean function y(S1, S2, S3) has an extension in the class of all vector Boolean functions if and only if (S1 ∩ S2) = ∅, (S1 ∩ S3) = ∅, and (S2 ∩ S3) = ∅. Other classes of Boolean functions are of interest. For instance, given a scalar Boolean function f : {0,1}n → {0,1}, f is positive if x ≤ y always implies f(x) ≤ f(y) (Boros et al., 1998). In the case of a vector Boolean function f:{0,1}n → {0,1}m, f is positive if x ≤ y implies f(x) ≤ f(y), that is, fi(x) ≤ f i(y), for all i=1,...,m. Therefore, a positive vector Boolean function f has all the scalar Boolean functions fi, i=1,...,m, positive. As in Boros, et al. (1998), we give necessary and sufficient conditions for the existence of an extension f of a pdVBf y(S1, S2, S3) in the class of positive vector Boolean functions: Theorem 1. A pdVBf y(S1, S2, S3), has an extension f in the class of positive vector Boolean functions if and only if for all x ∈ S1, y ∈ S2, and z ∈ S3, we have x h y, x h z, and z h y. Proof: Necessity. Suppose that there exists a pair x ≤ y with x ∈ S1 and y ∈ S2 (or x ≤ z with z ∈ S3 or z ≤ y), then f(x) ≤ f(y), by positivity of f, contradicting f(x)=(1,1) and f(y)=(0,0). Sufficiency. Define the following vector Boolean function f: S1(f) = {c ∈ Bn | c ≥ x for some x ∈ S1} S2(f) = {b ∈ Bn | b ≤ y for some y ∈ S2} S3(f) = Bn - S1(f) - S2(f). The function f is positive. Moreover, f is a positive extension of y. From the positivity of f, it follows immediately that S1 ⊆ S1(f), and S2 ⊆ S2(f). In order to complete the proof, it only remains to show that (S3 ∩ (S1(f) ∩ S2(f))) = ∅. Indeed, if there

Partially Defined Vector Boolean Functions and Their Extension
A vector Boolean function is a mapping f:{0,1}n → {0,1}m where x∈{0,1}n is a Boolean vector. In the sequel we will consider mostly the case m=2. The extension to the general case is straightforward, but it requires heavier notation. Following the notation introduced previously, we denote by S1(f) the set of all Boolean vectors x such that f(x)=(1,1), S2(f) the set of all Boolean vectors x such that f(x)=(0,0), and S3(f) the set of all Boolean vectors x such that f(x)=(1,0), or f(x)=(0,1). A partially defined vector Boolean function (pdVBf) y is defined by a triple of sets (S1, S2, S3) such that S1 denotes a set of positive examples, S2 denotes a set of negative examples, and S3 denotes a set of null examples, that is, y is defined on S1∪S2∪S3 and y(x)=(1,1) if x∈S1, y(x)=(0,0) if x∈S2, and y(x)=(1,0) or (0,1) if x∈S3. We call a function f an extension of the pdVBf y(S1, S2, S3) if S1 ∈ S1(f), S2 ∈ S2(f), and S3 ∈ S3(f). Referring to our archive of observations y(S1, S2, S3), our goal is the determination of an extension f that agrees with the classification in the archive S and that represents the classification of all the vectors in the sample space. Referring to the theory of the Boolean functions, given a partially defined scalar Boolean function j: S ⊆{0,1}n → {0,1}, necessary and sufficient conditions for the existence of an extension f : {0,1}n → {0,1} in different subclasses of scalar Boolean functions are provided in Boros, et al. (Boros, Ibaraki, & Mikino, 1998). We will denote by f a scalar Boolean function and by f a vector Boolean function. A simple extension of the arguments in Boros, et al. (1998) provides the following necessary and sufficient condition for the existence of an extension f of a partially defined vector Bool-

28

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

exists z ∈ (S1(f) ∩ S3), then we have z ∈ S3, and there exists x ∈ S1 such that x ≤ z, contradicting the assumption x h z. A similar argument shows that S3 ∩ S2(f) = ∅. Hence, S3 ⊆ S3(f), and this completes the proof. In this chapter, given our archive of observations, represented as a pdVBf, we consider the problem of finding an extension f as short as possible in the class of all vector Boolean functions. We first need to introduce some classical definitions from Boolean algebra. The Boolean variables x1, x2,..., xn and their complements x’1, x’2,..., x’n are called literals. A term T is a conjunction of literals such that at most, one of xi and x’ i appears for each variable. A minterm is a maximum length conjunction term, that is, it contains all variables normal (uncomplemented) or complemented (Schneeweiss, 1989). A disjunction of conjunctions T defines a disjunctive normal form (DNF). In the case of scalar Boolean functions, a DNF defines a function, and it is well-known that every scalar Boolean function f can be represented as a DNF (Schneeweiss, 1989); however, such a representation may not be unique. The goal is to find the minimal DNF representation of a scalar Boolean function f. To make the minimality requirement more precise, we need to introduce some further definitions. A term T is an implicant for a given scalar Boolean function f if T = 1 implies f = 1. An implicant T is a prime implicant for the scalar Boolean function f if any term obtained by dropping a literal from it is not an implicant (Boros et al., 1997). Prime implicants are the fundamental blocks for generating an extension of a given partially scalar Boolean function. Indeed, the minimal DNF representation of a scalar Boolean function is obtained when its terms T are prime implicants (Crama & Hammer, 2002). The concepts of implicants and prime implicants can be generalized in the case of vector Boolean functions.

Definition 1. (McCluskey, 1986). Given a vector Boolean function f:{0,1}n → {0,1}m, of components f1, f2,..., fm, a term T is a multiple implicant (resp. multiple prime implicant) of f if: 1. It is either an implicant (resp. prime implicant) of one of the functions fi, i=1,...,m or; It is an implicant (resp. prime implicant) of one of the product (conjunction) function fi1 ⋅ f i2 ⋅ ⋅ ⋅ fik, 1 ≤ ik ≤ m.

2.

Here we introduce the concept of a vector disjunctive normal form VDNF as a vector of m components, each corresponding to a single DNF. A vector Boolean function may be represented by a VDNF. Notice that, by considering the class of positive scalar Boolean functions introduced above, it is well-known that a scalar Boolean function f is positive if and only if f can be represented by a DNF in which all the literals of each term are uncomplemented (Boros, et al., 1998). Hence, by referring to the definition of positive vector Boolean functions, a vector Boolean function f is positive if and only if it can be represented by a VDNF whose components are represented by DNFs in which all the literals of each term are uncomplemented. We want to find a short representation of a vector Boolean function f in the sense described by the following theorem. Theorem 2. (Existence of a short VDNF (McCluskey, 1986)). There exists a short representation of a vector Boolean function f, in which each component f i is the disjunction of multiple prime implicants of f , such that, all the terms that occur only in the expression for fi are prime implicants of fi; all the terms that occur in both the expressions for fi and fj with i ≠ j but in no other expressions are prime implicants for fi ⋅ fj; and so forth.

29

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

Multiple Logic Minimization Problem
It was already noticed that a scalar Boolean function f :{0,1}n → {0,1} may have numerous DNF representations (Schneeweiss, 1989). In many applications a short DNF representation of f is preferred over a longer one (Crama & Hammer, 2002). The problem of constructing a DNF representation as short as possible is referred to as the logic minimization problem. The same problem arises for a vector Boolean function f:{0,1}n → {0,1}m. That is, we search for a short VDNF representation of f with m=2 in our application. Perhaps, the most obvious technique is to find a short representation of every component (DNF) of the VDNF. Unfortunately, some simple examples show that this method does not necessarily lead to a short VDNF representation (McCluskey, 1986). The problem of finding a short VDNF is usually referred to as the multiple logic minimization problem. In our work, we refer to the Quine-McCluskey approach for solving it (McCluskey, 1986). This approach is divided into two phases. In the first phase, a prime implicant table is generated. For each (binarized) observation s in the Master pdVBf y(S1, S2, S3), along with its classification y(s)=(y1, y2), a minterm and a multiple prime implicant for y(s)=(y1, y2) are found. That is, for each observation s, a minterm for y1 and y2, and a prime implicant for y1, y2 and y1 ⋅ y2 are generated. Each row of the prime implicant table corresponds to a multiple prime implicant, while each column corresponds to a minterm. In the second phase, the Quine-McCluskey approach solves a set-covering problem by finding the minimum number of rows that covers all the columns. It is well-known that the set-covering problem is NP-complete, and therefore, this approach to multiple logic minimization does not provide a polynomial algorithm. In order to solve the set-covering problem, a reduction of the table is performed. For this, let us first recall the following definitions from McCluskey (1986):

Definition 2. A multiple prime implicant of a vector Boolean function f = (f1, f2,..., fm) is essential for a function fi if there is a minterm in the representation of fi that is included in only one multiple prime implicant. Definition 3. Let f = (f1, f2,..., fm) be a vector Boolean function and let P1, P2,..., Pt be the corresponding set of multiple prime implicants. Then, a minterm for a function fi is a distinguished minterm if and only if it is included in only one conjunction of literals that is a multiple prime implicant of fi, or of any of the product (conjunction) of functions involving fi. Definition 4. A multiple prime implicant of a vector Boolean function f = (f1, f2,..., fm) is essential for a given function fi if and only if it includes a distinguished minterm of fi. In the prime implicant table, rows and columns corresponding to essential multiple prime implicant and distinguished minterm are called essential rows and distinguished columns, respectively. The reduction of the table is first performed by deleting all the essential rows and dstinguished columns (McCluskey, 1986). The table so obtained can be further reduced by applying the essential reducing algorithm (ERA) described in Crama and Hammer (2002). The final reduced table A* is the matrix of the corresponding set-covering problem. Given a set-covering matrix A, in Crama and Hammer (2002), it is proved that the ERA algorithm applied to A preserves the solutions of a set-covering problem. For further details about the algorithm for finding a short VDNF representation of a vector Boolean function f, the interested reader can refer to McCluskey (1986). However, in the next section we provide a numerical example for finding an extension of a pdVBf.

30

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

A numerIcAl exAmple
In this section, we provide a numerical example for the computation of a VDNF. Let us consider the Master pdVBf given in Table 1, where si is a binarized observation, i=1,...,14 and xj, j=1,2,3,4 are the cut-points. Our aim is to find an extension of the above Master pdVBf as short as possible. By applying the first step of the Quine-McCluskey procedure, we can find an extension of the pdVBf of Table 1 by generating all the prime implicants both for y1 and y2 and for the function y1 × y2 (see Table 2). The resulting extension is:

y1 = P1 ∨ P2 ∨ P3 ∨ P5 ∨ P6 ∨ P7 = = (x1 ⋅ x’2) ∨ (x1 ⋅ x4) ∨ (x2 ⋅ x4) ∨ (x’1 ⋅ x2 ⋅ x4) ∨ (x3 ⋅ x4) ∨ (x’2 ⋅ x3) y2 =P4 ∨ P5 ∨ P6 ∨ P7 = x3 ∨ (x’1 ⋅ x2 ⋅ x4) ∨ (x3 ⋅ x4) ∨ (x’2 ⋅ x3). This extension may not be a short one, so then, we apply the second step of the Quine-McCluskey procedure in order to eliminate all the redundancies. The first step is to select all the essential prime implicants (see Definitions 2-4).

Table 1. Partially defined vector Boolean function
x1 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 0 0 0 0 0 0 0 1 1 1 1 0 1 1 x2 0 0 0 0 1 1 1 0 0 0 0 1 1 1 x3 0 1 1 0 0 1 1 0 0 1 1 0 0 1 x4 0 0 1 1 1 0 1 0 1 0 1 0 1 0 y1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 y2 0 1 1 0 1 1 1 0 0 1 1 0 0 1

Table 2. Prime implicants
Prime Implicants for y1 P1 = (x1 × x’2) P2 = (x2 × x4) P3 = (x1 × x4) Prime Implicants for y2 P4 = x3 Prime Implicants for y1 × y2 P5 = (x’1 × x2 × x4) P6 = (x’2 × x3 ) P7 = (x3 × x4)

31

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

Table 3. The covering table
s2 P1 P2 P3 P4 P5 P6 P7 x x x x y1 x x x x x x x x y2 x x x x x s3 s5 s7 s8 x s9 x s10 x s11 x x x x x x x x x x x x y 1 × y2 x x x y2 y1 s13 s2 s3 s5 s6 s7 s10 s11 s14

Table 4. The second covering table
s5 P2 P3 P5 P7 x x y1 × y2 x y1 x s7 x s13 x y1 x

In fact, y1 contains three terms less than before, while y2 contains two terms less than before. Alternatively, a short extension can be obtained by solving the following set-covering problem: min s1 + s2 + s3 + s4 s.t. s1 + s3 ≥ 1 s1 + s3 + s4 ≥ 1 s1 + s2 ≥ 1 si = {0,1}, i=1,2,3 whose optimal solution is (1, 0, 0, 0) which refers to the same dominating prime implicant P2.

For y1 they are P1 and P6, while for y2 they are P4 and P5. We delete from Table 2 all the minterms covered by the essential prime implicants. The remaining minterms to be covered are shown in Table 4. The prime implicant P2 in Table 4, dominates the other ones for y1, that is, it covers all the minterms covered by P3, P5, P7, and other minterms not covered by P3, P5, P7. It also completes, with the essential prime implicants P1 and P6, the covering of the component y1 of the table. The resulting extension is: y1 = P1 ∨ P2 ∨ P6 = (x1 ⋅ x’2) ∨ (x2 ⋅ x4) ∨ (x’2 ⋅ x3) y2 =P4 ∨ P5 = x3 ∨ (x’1 ⋅ x2 ⋅ x4) which is shorter than the first extension found.

the ApplIcAtIon
In our application, we refer to three time series of the Mibtel stock exchange market for the years 1999-2001. We considered these time series because they do not present a well-defined primary trend and, for each attribute, they show a considerable presence of lateral movements, or the so called sideways. Each observation is related to the open, close, high, and low daily prices of the Mibtel stock exchange market. We use both graphical and algorithmic tools of the financial technical analysis to generate numerical attributes or technical indicators. The interpretation of these indicators

32

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

allows us to classify our dataset into the three classes S1, S2, and S3. These indicators are obtained by computing the moving averages defined on some combinations of the open, close, high, and low prices. There are several indicators that can be considered by a financial expert to predict future movements in the market, and there is not a standard criterium for choosing a given subset of indicators. In the present work, we use a particular type of technical indicator called Oscillators. The oscillators are particularly useful when the market does not present a well-defined trend that can be used to identify situations of oversold and overbought. An overbought or oversold condition merely indicates that there is a high probability of a reaction in the market. These conditions suggest that there could be an opportunity to buy or sell securities. Just one oscillator is not able to provide information about the market situation. In the Appendix, we describe the characteristics as well as the meaning of the oscillators we used in our application.

experImentAl results
This section reports on numerical experiments with the use of the VDNF technique for classification. We consider the three time series (1999-2001) as the initial classified archive. It is composed of 747 observations, where | S1| = 53, | S2| = 66, and | S3| = 379 are the cardinalities of the three classes, respectively. In our experiments, we used as training sets both the 1999 time series only, representing 33% of the observations of the entire archive, and the 1999-2000 time series, representing 66% of the whole archive. Then, the accuracy of our method was tested on the complement of the two training sets. Since most of the observations in the archive are in the S3 class, we did not use larger training sets. Otherwise, we would have obtained a test set with very few observations in one of the two classes S1 and S2, and this would have resulted in a bad classifica-

tion accuracy. Moreover, in contrast with other classification applications, we did not estimate the effectiveness of our technique by randomly selecting a subset of the dataset as training set. This is due to our particular application. In fact, for financial applications, the aim is to predict whether the market will continue to go up or down, that is, whether it is in an oversold or in an overbought condition. Thus, the data included in a training set must represent a continuum of observations for a given time period. However, determining the appropriate time period for historical inputs is one of the biggest challenges in financial forecasting. We present a comparison of our VDNF approach with some standard classification methods. In particular, we compared our technique with support vector machines, decision trees, and naïve Bayes. For this, we use the implementation of these techniques contained in the Weka software (Witten & Frank, 2005). Moreover, classification results provided by neural networks and linear and quadratic discriminant analysis are reported as well. These last methods were implemented in the Matlab 7.0 environment. All the experiments were performed on a PC AMD Athlon 2500+ GHz. More details on the implementation of the classification methods compared are given as follows: Support vector machines (SVM): We obtained support vector classifiers by using both a radial basis function (RBF) and a polynomial function as kernels. Furthermore, parameter-selection strategies have also been considered. That is, we searched for the appropriate value of a parameter C, which controls the trade-off between the classifier capacity and the training errors, and of parameters γ and d in the two kernel functions. For instance, in the case of RBF, we find the best values of the parameters (C, γ) to be used in the classification algorithm by searching on the following two grids of possible points for (C, γ).

33

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

We first consider the grid [1⋅10-4, 1⋅10-3 ,..., 1⋅104] × [1⋅10-3, 1⋅10-2,..., 1⋅103]. Let (C0, γ0) be the pair associated with the best classification value. Then, we use the grid [0.2C0,...,8C0] × [0.2γ0,...,8γ0] to select the best pair. Similarly, for the polynomial kernel, the best classification values were obtained by searching the parameters (C, d) sequentially in the two grids [1⋅10-4, 1⋅10-3 ,..., 1⋅104] ‰ [1,10,50,100] and [0.2C0,...,8C0] × [0.2 d 0,0.4 d 0,0.6 d 0,0.8d 0,2d 0,3d 0,4d 0]. These grids were considered both when the training set consisted of 33% of the dataset, and when it consisted of 66% of the dataset. For the RBF, by referring to the two percentages of the training sets, the pairs that gave the best classification values were C’=600 and γ’=6, and C’=20 and γ’=60, respectively. For the polynomial kernel, the best pairs were C’=600 and d’=20, and C’=4000 and d’=8, respectively. In Table 5, we report the percentages of well-classified instances obtained by using the two kernel functions. Decision trees: We referred to the popular C4.5 algorithm implemented in the Weka software for constructing decision trees. We tested a set of parameters in the range [1,...,20], indicating the minimum number of instances to specify in a leaf of the tree. For both the percentages of the training sets, the best classification was obtained with the parameter set to the value 2. Moreover,

we observed that a pruning strategy did not yield better percentages of correctly classified instances than an unpruned strategy. In Table 5, we report the best values of well-classified instances found. Naïve Bayes: The popular Naïve Bayes method is another simple but yet effective classifier. This method learns the conditional probability of each attribute, given the class label from the training data. Classification is then done by applying Bayes rule to compute the probability of a class value, given the particular instance, and predicting the class value with the highest probability. A strong independence assumption is made, that is, all the attributes are assumed conditionally independent, given the value of the class attribute. Numerical values of the attributes are usually handled by assuming that they have a Gaussian probability distribution. Since this assumption may be incorrect, we also implemented the method referring to the kernel density estimation that does not assume any particular distribution for the attribute values. Neural networks: For training a feedforward neural network, we used the backpropagation algorithm implemented in Matlab 7.0 with the regularized mean square error (MSE regularized) as performance function. Although there are many variants of the backpropagation algorithm

Table 5. Percentage of well-classified instances with standard classification methods
Classifier Support Vector Machine with RBF Support Vector Machine with Polynomial Kernel Decision Trees Naïve Bayes Naïve Bayes with kernel estimation Neural Networks Linear Discriminant Analysis Quadratic Discriminant Analysis 33% Training 95.38 95.58 94.97 91.76 93.17 74.83 82.73 52.81 66% Training 96.78 96.38 95.79 86.34 86.54 79.52 70.60 75.10

34

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

in Matlab, we adopted the resilient backpropagation training algorithm that is able to eliminate the harmful effects of the magnitudes of the partial derivatives and is generally much faster than other procedures. A validation set was also introduced in order to check the progress of training. This set was formed by 3-months observations from December 2000 to February 2001. Both with a training set formed by 33% of the observations and with one formed by 66% of the observations, we trained six single-layer and four double-layer networks with [50,100,150,200,250,300] neurons and [10,15,25,50] neurons per layer, respectively. Each network was trained five times with randomly generated input weights. In Table 5, we report only the best classification percentages obtained. Discriminant analysis: We used the tools for the linear and quadratic discriminant analysis of Matlab 7.0 that implement Fisher’s Method. We now consider the VDNF approach. Given the archive of observations, we derive three different binarized tables, each referring to three

different binarization strategies. Namely, the three binarized archives are obtained by first applying only a single cut-point method, then only an interval cut-point method, and finally, by using both a single cut-point and an interval cut-point method (Boros, et al., 1997; Boros, et al., 2000). We report the classification accuracy of our technique by referring to each Master pdVBf obtained. In general, regardless of the method used, a binary encoding of a dataset of observations generates a great number of Boolean variables (cut-points). In fact, many of the Boolean variables introduced by a binarization procedure may not be needed to explain the phenomenon. Hence, a size reduction is actually necessary in order to prevent insurmountable computational difficulties at the VDNF generation stage. Following Boros et al. (2000), we reduce the dimension of the Master pdVBf by deleting the redundant variables. In order to generate the VDNF with the Quine-McCluskey procedure, we used two softwares: 1. ESPRESSO II: It generates the prime implicant table, as described before, and then applies a reduction process to this table.

Table 6. Classification accuracy with 33% training
Single Cut Point Dimension Espresso II Boom 2.3 249×155 249×155 Classified 94.98 95.18 Interval Cut-Point Dimension 249×146 249×146 Classified 88.55 94.77 Single-Interval Cut Point Dimension 249×288 249×288 Classified 84.54 95.18

Table 7. Classification accuracy with 66% training
Single Cut Point Dimension Espresso II Boom 2.3 498×191 498×191 Classified 98.99 98.99 Interval Cut-Point Dimension 498×181 498×181 Classified 98.99 98.99 Single-Interval Cut Point Dimension 498×346 498×346 Classified 95.58 97.98

35

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

2.

Finally, depending on the dimension of the resulting reduced table, either an exact algorithm or a greedy-like procedure is implemented for solving the set-covering problem. BOOM v2.3: It implements a stochastic greedy-like procedure for generating the prime implicant table. After applying a reduction process to this table, a stochastic greedy-like procedure is implemented for solving the resulting set-covering problem.

In Tables 6 and 7, we report the results when the training set consists of 33% of the dataset and of 66% of the dataset, respectively. For each software used, we present the Master pdVBf dimension (i.e., number of observations times number of cut-points used) and the percentage of the well-classified points in the test set. Recalling the definition of the three classes, S1, S2, and S3, we observed that the percentage of well-classified observations in the test set represents how many times we were able to correctly classify the observations in S1, S2, and S3, and therefore, how many times we were able to make a profitable operation (to buy, to sell or to wait) in the stock exchange market. At the beginning of this section we pointed out that, from an application point of view, in financial forecasting problems it make no sense to apply a classification method by randomly selecting a subset of the data set as training set. Nevertheless, we next report some experimental results on

randomly generated training sets with the aim of evaluating the efficiency of our technique. Let us consider the 1999 time series only as dataset. It has 249 observations where | S1| = 34, | S2| = 27, and | S3| = 188 are the cardinalities of the three classes in which the dataset is subdivided, respectively. We tested the classification accuracy of our method by extracting from the 1999 series two different samples that were used as training sets, each containing about 47% of the observations in the archive (119 observations). The observations in the samples were chosen according to this rule: (1) for each sample, the number r1, r2, and r3 of the observations that must be classified in each class were firstly decided; (2) from the 1999 time series we extract, at random, r1 units from the observations in S1, r2 units from the observations in S2, and r3 units from the observations in S3. We choose this rule since, by extracting at random 119 observations directly from the 1999 series, it may result in a sample in which one of the ri, i=1,2,3 could be zero. Also, a training set (a sample) with very few observations in one class may give a bad classification accuracy. In our application, for the first sample we considered r1=19, r2=12 and r3=88, while for the second sample we considered r1=19, r2=15 and r3=85. As test set, we used the rest of the 1999 time series composed of 130 observations. The results are reported in Table 8. For each sample, we provide the dimension of the Master pdVBf generated, and the percentages of the well-classified observations in the test set for both software. We used only the simple cut-point method for the binarization of the numerical dataset.

Table 8. Dataset composed by the 1999 time series
Master pdVBf dimension First sample Second sample 119×101 119×112 90.77 86.15 Well-Classified Espresso II Boom 2.3 91.54 92.31

36

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

conclusIon And further reseArch
In this chapter, we proposed a new classification technique that is based on combinatorics, optimization, and partially defined vector Boolean functions. Our technique is concerned with classification problems, where the goal is to extract salient features from an archive of observations in order to separate one set of observations from another. We developed a method that can be efficiently used when the observations in the archive are divided into three (or more) classes according to a ternary classification rule. In particular, in this chapter, we applied our classification method to a financial problem, the financial timing decision problem. The classification performance of our technique was tested on three financial time series. We compared our technique with some standard classification approaches. In particular, we compared it with support vector machines, decision trees, naïve Bayes, neural networks, and linear and quadratic discriminant analysis, obtaining encouraging results. Our technique seems to provide a high classification accuracy, as well as a wide applicability for classification problems where a classification in three or more classes is needed. In fact, it outperforms almost all the standard classification methods, and compares favorably with the SVM classifiers that have proved highly successful in a number of classification studies. Moreover, it reveals a good explanatory power of the phenomenon under study, since a VDNF, which makes use of multiple prime implicants, better captures the combination between the attributes considered. Of course we are conscious that further research is needed for a better understanding of the mathematical and computational aspect of this technique, and also further classification problems need to be considered in order to better understand the domain of applicability of our method.

references
Archer, N., & Wang, S. (1993). Application of the backpropagation neural network algorithm with monotonicity constraints for two-group classification problems. Decision Sciences, 24, 60-75. Bennett, K., & Mangasarian, O.L. (1992). Robust linear programming discrimination of two linearly inseparable sets. Optimization Methods and Software, 1, 23-34. Bishop, C.M. (Ed.). (2004). Neural network in pattern recognition. New York: Oxford University Press. Boom 2.3. Retrieved from http://vlsicad.eecs. umich.edu Boros, E., Hammer, P. L., Ibaraki, T., & Kogan, A. (1997). A logical analysis of numerical data. Mathematical Programming, 79, 163-190. Boros, E., Hammer, P. L., Ibaraki, T., Kogan, A., Mayoraz, E., & Muchnick, I. (2000). An implementation of Logical Analysis of Data. IEEE Transactions on Knowledge and Data Engineering, 12, 292-306. Boros, E., Ibaraki, T., & Mikino, K. (1998). Error-free and best-fit extensions of partially defined Boolean Functions. Information and Computation, 140, 254-283. Boulle, A., Chandramohan, D., & Weller, O. (2001). A case study of using artificial neural networks for classifying cause of death from autopsy. International Journal of Epidemiology, 30, 515-520. Bredensteiner, E.J., & Bennett, K. P. (1999). Multicategory classification by support vector machines. Computational Optimization and Application, 12, 53-79. Burges, C.J.C. (1998). A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2, 121-167.

37

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

Coakley, J., & Brown, C. (2000). Artificial neural networks in accounting and finance: Modeling issues. International Journal of Intelligent Systems in Accounting, Finance and Management, 9, 119-144. Cortes, C., & Vapnik, V. (1995). Support vector networks. Machine Learning, 20, 273-297. Crama, Y., & Hammer, P. L. (2002). Boolean functions—Theory, algorithms, and applications. Retrieved from http://www.rogp.hec.ulg. ac.be/Crama/ Crama, Y., Hammer, P.L., & Ibaraki, T. (1988). Cause-effect relationships and partially defined Boolean functions. Annals of Operations Research. 16, 299-326. Espresso II. Retrieved from http://www-cad.eecs. berkeley.edu Felici, G., & Truemper, K. (2000). A MINSAT approach for learning in logic domains. INFORMS Journal on Computing, 13, 1-17. Freed, N., & Glover, F. (1986). Evaluating alternative linear programming models to solve the two-group discriminant problem. Decision Sciences, 17, 151-162. Fu, Z., Golden, B., Lele, S., Raghavan, S., & Wasil, E. (2003). A genetic algorithm-based approach for building accurate decision trees. Informs Journal of Computing, 5, 3-22. Glover, F. (1990). Improved linear programming models for discriminant analysis. Decision Sciences, 21, 771-785. Hammer, P.L. (1986). Partially defined Boolean functions and cause-effect relationships. Paper presented at the International Conference on Multi-Attribute Decision Making Via OR-Based Expert Sytems, University of Passau, Passau Germany. Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks: The

combination of knowledge and statistical data. Machine Learnin,. 20, 197-243. Kamath, A.P., Karmarkar, N.K., Ramakrishnan, K.J., & Resende, M.G.C. (1992). A continuous approach to inductive inference. Mathematical Programming, 57, 215-238. Jang, G.S., Lai, F., & Parng, T.M. (1993). Intelligent stock trading decision support system using dual adaptive-structure neural networks. Journal of Information Science and Engineering, 9, 271-297. Jefferys, W.H., & Berger, J.O. (1992). Ockhams razor and Bayesian analysis. American Science, 80, 64-72. Lam W., & Bacchus, F. (1994). Learning bayesian belief networks. An approach based on the MDL principle. Computational Intelligence, 10, 269-293. Lawrence, S., & Giles, L. (2000). Overfitting and neural networks: Conjugate gradient and backpropagation. IEEE Computer Society, 114-119. Lin, Y. (2002). Support vector machines and the bayes rule in classification. Data Mining and Knowledge Discovery, 6, 259-275. Mangasarian, O.L. (1965). Linear and nonlinear separation of patterns by linear programming. Operations Research, 13, 444-452. Mangasarian, O.L. (1968). Multi-surface method of pattern separation. IEEE Transactions on Information Theory, 14, 801–807. Mangasarian, O.L., Street, W.N., & Wolberg, W.H. (1995). Breast cancer diagnosis and prognosis via linear programming. Operations Research, 43, 570-577. Mangasarian, O.L., Setiono, R., & Wolberg, W. H. (1990). Pattern recognition via linear programming: Theory and application to medical diagnosis. In T. H. Coleman & Y. Li (Eds.), Large-scale

38

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

numerical optimization (pp. 22-30). Philadelphia: SIAM Publication. McCluskey, E. J. (Ed.). (1986). Logic design principle. NJ: Prentice Hall, Inc. Murphy, J. J. (Ed.). (1997). Analisi Tecnica dei Mercati Finanziari. New York Institute of Finance. Niimi, A., & Tazaki, E. (2000). Genetic programming combined with association rule algorithm for decision tree construction. Paper presented at the fourth international conference on knowledge-based intelligent engeneering systems & allied technologies, Brighton, UK. Ooyen, A., & Nienhuis, B. (1992). Improving the convergence of the back-propagation algorithm. Neural Networks, 5, 465-471. Pernkopf, F. (2005). Bayesian network classifiers versus selective k-NN classifier. Pattern Recognition, 38, 1-10. Quinlan, J. R. (Ed.). (1993). C4.5: Programs for machine learning. San Mateo, CA: MorganKaufmann. Schneeweiss, W.G. (Ed.). (1989). Boolean funcions with engineering applications and computer programs. Berlin: Springer-Verlag.

Street, W.N. (in press). Multicategory decision trees using nonlinear programming. Informs Journal on Computing. Triantaphyllou, E., Allen, L., Soyster, L., & Kumara, S.R.T. (1994). Generating logical expressions from positive and negative examples via a branch-and-bound approach. Computer and Operations Research, 21, 185-197. Vapnik, V., & Lerner, A. (1963). Pattern recognition using generalized portrait method. Automation and Remote Control, 24, 774-780. Vapnik, V. (Ed.). (1995). The nature of statistical learning theory. Springer-Verlag. Witten, I.H., & Frank, E. (Ed.). (2005). Data mining: Practical machine learning tools and techniques (2nd ed.). San Francisco: Morgan Kaufmann. Wolberg, W.H., & Mangasarian, O.L. (1990). Multisurface method of pattern separation for medical diagnosis applied to breast cytology. In Proceedings of the National Academy of Sciences USA, 87, 9193-9196. Yeung, D.Y. (1993). Constructive neural networks as estimators of Bayesian discriminant functions. Pattern Recognition, 26, 189-204.

39

Vector DNF for Datasets Classifications: Application to the Financial Timing Decision Problem

AppendIx: the technIcAl oscIllAtors Relative Strength Index (RSI)
The relative strength index is a well-known oscillator developed by Welles Wilder Jr. RSI measures the relative changes between high and low closing prices, and provides an insight of overbought and oversold conditions. The term relative strength generally implies a comparison between two different markets or indices. RSI provides early “warning signals” (buy or sell signals) if it is used in conjunction with other indicators. The relative strength index values can be plotted on a vertical scale ranging from 0 to 100. The 70 and 30 values are refered to as warning signals. An RSI value above 70 is related to an overbought condition, indicating a (probably) selling period, while a value below 30 refers to an oversold condition, indicating a (probably) buying period. The values 80 and 20 are often preferred by some traders. The information provided by the RSI depends upon the time interval on which it is computed. The shorter the interval, the more sensitive is the information provided by the index. Time intervals of 9, 10, and 25 days are often considered. Extending the time period makes the oscillator smoother and narrower in amplitude. RSI signals should always be used in conjunction with trend-reversal indicator prices.

Rate of Change (ROC)
ROC measures the “speed” of the prices in the market. Indeed, ROC is sometimes referred to as the price rate of change (PROC). Growing values of ROC indicate a bullish prices increasing period, while falling values of ROC indicate a bearish prices decreasing period. The ROC index displays the amount of price changes over a given time period. ROC can be represented as a wave. When the wave is above an equilibrium line, usually the zero-line, we assume to be in a buying period. When the wave falls below the equilibrium line, we assume to be in a selling period. When the wave starts growing from below the equilibrium line, we have an indication of a coming bullish period. The symmetric configuration is considered a forthcoming bearish period. Like the RSI, the ROC can also be computed referring to different time periods. If ROC is computed referring to a 10- or 12-day interval, it is a good short-term price indicator.

Moving Average Convergence/Divergence Trading Method (MACD)
The MACD method, developed by Gerald Appel, is a trend indicator, telling us whether a stock is in an uptrend or in a downtrend (Murphy, 1997). The direction of a long-term trend is the first assessment one should consider in any market. An uptrend is preferred and indicates a buying period, while a downtrend indicates a selling period. The simplest representation of this indicator is composed of two lines: the MACD line, which is the difference between two exponential moving averages (EMAs), and a signal line, which is an EMA of the MACD line itself. The signal or trigger line is plotted on top of the MACD to show buy or sell opportunities. Gerald Appel’s MACD method uses a 26-day and 12-day EMA, based on the daily close prices, and a 9-day EMA for the signal line. The basic MACD trading rule is to buy when the MACD rises above its signal line. Similarly, a sell signal occurs when the MACD crosses below its signal line. If the MACD line is above the signal line, it denotes the beginning of a trend. An uptrend typically stops when the MACD line falls below the signal line.

40

41

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations
Xenia Naidenova Military Medical Academy, Russia

Chapter III

AbstrAct
The purpose of this chapter is to demonstrate the possibility of transforming a large class of machinelearning algorithms into commonsense reasoning processes based on using well-known deduction and induction logical rules. The concept of a good classification (diagnostic) test for a given set of positive examples lies in the basis of our approach to the machine-learning problems. The task of inferring all good diagnostic tests is formulated as searching the best approximations of a given classification (a partitioning) on a given set of examples. The lattice theory is used as a mathematical language for constructing good classification tests. The algorithms of good tests inference are decomposed into subtasks and operations that are in accordance with main human commonsense reasoning rules.

IntroductIon
The development of a full online computer model for integrating deductive and inductive reasoning is of great interest in machine learning. The main tendency of integration is to combine, into a

whole system, some already well-known models of learning (inductive reasoning) and deductive reasoning. For instance, the idea of combining inductive learning from examples with prior knowledge and default reasoning has been advanced in Giraud-Carrier and Martinez (1994).

Copyright © 2008, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

Obviously, this way leads to a lot of difficulties in knowledge representation because deductive reasoning tasks are often expressed in the classical first-order logic language (FOL), but machinelearning tasks use a variant of simbolic-valued attribute language (AVL). The principe of “aggregating” different models of human thinking for constructing intelligent computer systems leads to dividing the whole process into two separate modes: learning and execution or deductive reasoning. This division is used, for example, in Zakrevskij (1982, 1987; Zakrevskij & Vasylkova, 1997). This approach is based on using finite spaces of Boolean or multivalued attributes for modeling natural subject areas. It combines inductive inference used for extracting knowledge from data with deductive inference (the type of theorem proving) for solving pattern recognition problems. The inductive inference is reduced to looking for empty (forbidden) intervals of Boolean space of attributes describing a given set of positive examples. The deductive inference relates to the situation when an object is contemplated with known values of some attributes and unknown values of some others, including a goal attribute. The possible values of the latter ones are to be calculated on the base of implicative regularities in the Boolean space of attributes. In Zakrevskij (2001), the results of prolonged research conducted in that direction at the Institute of Engineering Cybernetics in Minsk are given. The fundamental unified model for combining inductive reasoning with deductive reasoning is developed in the framework of inductive logic programming (ILP). ILP is a discipline that investigates the inductive construction of first-order clausal theories from examples and background knowledge. ILP has the same goal as machine learning, namely, to develop tools and techniques to induce hypotheses from examples and to obtain new knowledge from experience; but, the traditional theoretical basis of ILP is in the framework of first-order predicate calculus.

Inductive inference in ILP is based on inverting deductive inference rules; for example, inverting resolution (rules of absorption, identification, intraconstruction, and interconstruction), inverting implication (inductive inference under θ-subsumption). There is a distinction between concept learning and program synthesis. Concept learning and classification problems, in general, are inherently object oriented. It is difficult to interpret concepts as subsets of domain examples in the frameworks of ILP. One of the ways to overcome this difficulty has been realized in a transformation approach: an ILP task is transformed into an equivalent learning task in different representation formalism. This approach is realized in LINUS (Lavraĉ & Džeroski, 1994; Lavraĉ, Gamberger, & Jovanoski, 1999), which is an ILP learnerinducing hypotheses in the form of constrained deductive hierarchical database (DHDB) clauses. The main idea of LINUS is to transform the problem of learning relational DHDB descriptions into the attribute-value learning task. This is achieved by the so-called DHDB interface. The interface transforms the training examples from the DHDB form into the form of attributevalue tuples. Some well-known attribute-value learners can then be used to induce “if-then” rules. Finally, the induced rules are transformed back into the form of DHDB clauses. The LINUS uses already-known algorithms, for example, the decision tree induction system ASSISTANT, and two rule induction systems: an ancestor of AQ15 named NEWGEM, and CN2. A simple form of predicate invention through first-order feature construction is proposed by Lavraĉ and Flash (2000). The constructed features are used then for propositional learning. Another way for combining ILP with an attribute-value learner has been developed in Lisi and Malerba (2004). In this work, a novel ILP setting is proposed. This setting adopts AL-log as a knowledge representation language. It allows a unified treatment of both the relational

42

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

and structural features of data. This setting has been implemented in SPADA, an ILP system developed for mining multilevel association rules in spatial databases and applied to geographic data mining. AL-log is a hybrid knowledge representation system that integrates the description logic ALC (Schmidt-Schauss & Smolka, 1991) and the deductive database language DATALOG (Ceri, Gotlob, & Tanca, 1990). Therefore, it embodies two subsystems, called structural and relational, respectively. The description logic ALC allows for the specification of structural knowledge in terms of concepts, roles, and individuals. Individuals represent objects in the domain of interest. Concepts represent classes of these objects, while roles represent binary relations between concepts. Complex concepts can be defined from primitive concepts and roles by applying constructors such as ∩ (conjunction), ∪ (disjunction), and ¬ (negation). ALC knowledge bases have an intensional part and an extensional part. In the intensional part, relations between concepts are syntactically expressed as inclusion statements of the form C ⊆ D where C and D are two arbitrary concepts. As for the extensional part, it is possible to specify instances of relations between individuals and concepts. Relations are expressed as membership assertions, for example, concept assertions of the form a : C (“a belongs to C”). The formal model of conceptual reasoning, based on an algebraic lattice, has been obtained in two independent ways. One way goes back to the works of the great psychologist J. Piaget, who introduced the concept of grouping (1959) to explain methods of object classification used mainly by 7- to 11-year-old children. The idea of concepts’ classification as a lattice arose from practical tasks of developing information retrieval and pattern recognition systems. In 1974, Shreider described the classification algebra as idempotent semigroup with the unit element.

In the same year, Boldyrev (1974) advanced the formalization of pattern recognition system as algebra with two binary operations of refinement and generalization defined by an axiom system, including lattice axioms. The ideas of Boldyrev have been used often for minimization of Boolean partial functions with a large number of “Don’t Care” conditions, but we have been interested, from the beginning of our investigation, in applying the lattice theory for feature extraction and classification of attribute-value’s tuples, and later, of concepts (symbols, names...). The formal concept analysis (FCA), based on the concept lattice, has been advanced by Wille (1992). The problems of the FCA have been extensively studied by Stumme et al. (Stumme, Taouil, Bastide, Pasquier, & Lakhal, 2000), Dowling (1993), Salzberg (1991). Some algorithms for building concept lattices are considered in Nourine and Raynaud (1999), Ganter (1984), Kuznetsov (1993), and Kuznetsov and Obiedkov (2001). A lot of experience has been obtained on the application of algebraic lattices in machine learning. From this point of view, the JSM-method of reasoning (Finn, 1984, 1988, 1991, 1999) is interesting. The JSM-method of hypotheses’ automatic generation formalizes a special class of plausible reasoning. The technique of this method is a synthesis of several cognitive procedures: empirical induction based on modeling John S. Mill’s joint rule of similarity-distinction (Mill, 1900), causal analogy, and Charles S. Peirce’s abduction. Similarity in the JSM-method is both a relation and an operation that is idempotent, commutative and associative (i.e., it induces a semilattice on objects’ descriptions and their generalizations). Being described in algebraic terms, the JSMmethod can be implemented in the procedural programming languages. In Galitsky et al. (Galitsky, Kuznetsov, & Vinogradov, 2005), the system JASMINE, based on the JSM-method, is presented. The system extends this methodology by implementing (1) a

43

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

combination of abductive, inductive, and analogical reasoning for hypotheses generation, and (2) multivalued logic-based deductive reasoning for verification of their consistency. Formally, all the above components can be represented as deductive inference via logic programming (Anshakov, Finn, & Skvortsov, 1989; Finn, 1999). In fact, JASMINE is based on the logic programming implementation (Vinogradov, 1999). The idea of using algebraic lattices for knowledge or data representation is realized by a lot of researchers. We can mention some of them: the works of the French group (Ganascia, 1989); the work on conceptual clustering (Carpineto & Romano, 1996); the works related to conceptual knowledge discovery (Mephu & Njiwoua, 1998; Stumme, Wille, & Wille, 1998). The following works are devoted to the application of algebraic lattices for extracting functional and implicative dependencies from data: Demetrovics and Vu (1993), Mannila and Räihä (1992), Mannila and Räihä (1994), Huntala et al. (Huntala, Karkkainen, Porkka, & Toivonen, 1999), Cosmadakis et al. (Cosmadakis, Kanellakis, & Spyratos, 1986), Naidenova and Polegaeva (1986), Megretskaya (1988), Naidenova et al. (Naidenova, Polegaeva, & Iserlis, 1995a), Naidenova et al. (Naidenova, Plaksin, & Shagalov, 1995b), Naidenova (1992, 2001. An advantage of the algebraic lattices approach is based on the fact that an algebraic lattice can be defined both as an algebraic structure that is declarative, and as a system of dual operations with the use of which the elements of this lattice and the links between them can be generated. Our approach to machine-learning problems is based on the concept of a good diagnostic (classification) test. We have chosen the lattice theory as a model for inferring good diagnostic tests from examples from the very beginning of our work in this direction. This concept has been advanced firstly in the framework of inferring functional and implicative dependencies from relations (Naidenova & Polegaeva, 1986). But

later, the fact has been revealed that the task of inferring all good diagnostic tests for a given set of positive and negative examples can be formulated as the search of the best approximation of a given classification on a given set of examples, and that it is this task that some well-known machine-learning problems can be reduced to (Naidenova, 1996): finding keys and functional dependencies in database relations, finding association rules, finding implicative dependencies, inferring logical rules (if-then rules, rough sets, “ripple down” rules), decision tree construction, learning by discovering concept hierarchies, eliminating irrelevant features from the set of exhaustively generated features. In this chapter, we would like to demonstrate the possibility of transforming a large class of machine-learning algorithms for inferring good classification tests into the commonsense reasoning processes based on using well-known logical reasoning rules. In this chapter, we describe the forms of an expert’s rules (rules of the first type). The rules of the first type can be represented with the use of only one class of logical rules based on implicative dependencies between concepts (names). Then we describe commonsense reasoning operations (deductive and inductive) or rules of the second type. The concept of a good diagnostic test is introduced, and the problem of inferring all good diagnostic tests for a given classification on a given set of examples is formulated. We give the description of the mathematical model underlying algorithms of inferring good tests from examples. This model allows one to demonstrate that the inferring good tests entails applying deductive and inductive commonsense reasoning rules of the second type. We propose a decomposition of learning algorithms into operations and subtasks with the use of which good diagnostic tests inferring is transformed into an incremental process. The concepts of an essential value and an essential example are also introduced. We describe an incremental learning algorithm DIAGaRa

44

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

and an approach to incremental inferring good diagnostic tests. The chapter ends with a brief summary section.

letters A, B, C, D, a, b, c, d …will be used as attributes’ values in logical rules. We consider the following rules of the first type: • Implication: a, b, c → d. This rule means that if the values standing on the left side of the rule are simultaneously true, then the value on the right side of the rule is always true. Interdiction or forbidden rule: (a special case of implication) a, b, c → false (never). This rule interdicts a combination of values enumerated on the left side of the rule. The rule of interdiction can be transformed into several implications such as a, b → not c; a, c → not b; b, c → not a. Compatibility: a, b, c → rarely; a, b, c → frequently. This rule says that the values enumerated on the left side of the rule can simultaneously occur rarely (frequently). The rule of compatibility presents the most frequently observed combination of values that is different from a law or regularity, with only one or two exceptions. Compatibility is equivalent to a collection of assertions as follows: a, b → c rarely ( frequently) a, c → b rarely ( frequently) b, c → b rarely ( frequently) • Diagnostic rule: x, d → a; x, b → not a; d, b → false. For example, d and b can be two values of the same attribute. This rule works when the truth of “x” has been proven and it is necessary to determine whether “a” is true or not. If “x & d” is true, then “a” is true, but if “x & b” is true, then “a” is false. Rule of alternatives: a or b → true (always); a, b → false. This rule says that a and b cannot be simultaneously true; either a or b can be true, but not both. This rule is a variant of interdiction.

the logIcAl reAsonIng rules
We need the following three types of rules in order to realize logical inference (deductive and inductive):

• • INSTANCES or relationships between objects or facts really observed. Instance can be considered as a logical rule with the least degree of generalization. On the other hand, instances can serve as a source of a training set of positive and negative examples for inductive inference of generalized rules. RULES OF THE FIRST TYPE, or logical rules. These rules describe regular relationships between objects and their properties and between properties of different objects. The rules of the first type can be given explicitly by an expert, or derived automatically from examples with the help of some learning process. These rules are represented in the form of “if-then” assertions. RULES OF THE SECOND TYPE or inference rules with the help of which rules of the first type are used, updated, and inferred from data (instances). The rules of the second type embrace both inductive and deductive reasoning rules.

•

•

•

the rules of the First type
The rules of the first type can be represented with the use of only one class of logical statements; namely, the statements based on implicative dependencies between names. Names are used for designating concepts, things, events, situations, or any evidences. They can be considered as attributes’ values in the formal representations of logical rules. In our further consideration, the

•

45

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

deductive reasoning rules of the second type
Deductive steps of commonsense reasoning consist of inferring consequences from some observed facts with the use of statements of the form “ifthen” (i.e., knowledge). For this goal, deductive rules of reasoning are applied, the main forms of which are modus ponens, modus tollens, modus ponendo tollens, and modus tollendo ponens. Let х be a collection of true values of some attributes (or evidences), observed simultaneously. • Using implication: Let r be an implication, left(r) be the left part of r and right(r) be the right part of r. If left(r) ⊆ x, then x can be extended by right(r): х ← х ∪ right(r). Using implication is based on modus ponens: if A, then B; A; hence B. Using interdiction: Let r be an implication y → not k. If left(r) ⊆ x, then k is a forbidden value for all the extensions of x. Using interdiction is based on modus ponendo tollens: either A or B (A, B – alternatives); A; hence not B; either A or B; B; hence not A. Using compatibility: Let r = “a, b, c → k, rarely ( frequently),” where rarely, frequently are the values of a special attribute (SA). If left(r) ⊆ x, then k can be used for an extension of x with the value of SA equal to “rarely” (“frequently”). The application of several rules of compatibility leads to the appearance of several values “rarely” and/or “frequently” in the extension of x.

result “less less frequently,” and hence the values “rarely” and “frequently” must have the ordering scale of measuring. Using compatibility is based on modus ponens. • Using diagnostic rules: Let r be a diagnostic rule such as “x, d → a; x, b → not a,” where “х” is true, and “a,” “not a” are hypotheses or possible values of some attribute. Using a diagnostic rule is based on modus ponens and modus ponendo tollens. There are several ways for refuting one of the hypotheses: 1. To infer either d or b with the use of one’s knowledge; 2. To involve new known facts and/or statements for inferring (with the use of inductive reasoning rules of the second type) new rules of the first type for distinguishing the hypotheses “a” and “not a”; to apply these new rules; 3. To get, from an observation, which of the values d or b is true? Using rule of alternatives: Let “a” and “b” be two alternative hypotheses about the value of some attribute. If one of these hypotheses is inferred with the help of reasoning operations, then the other one is rejected. Using a rule of alternatives is based on modus tollendo ponens: either A or B (A, B – alternatives); not A; hence B; either A or B; not B; hence A.

•

•

•

Computing the value of SA for the extension of x requires special consideration. In any case, the appearance of at least one value “rarely” means that the total result of the extension will have the value of SA equal to “rarely.” Two values equal to “frequently” lead to the result “less frequently,” three values equal to “frequently” lead to the

The operations enumerated can be named as “forward reasoning” rules. Experts also use implicative assertions in a different way. This way can be named as “backward reasoning.” • Generating hypothesis or abduction rule: Let r be an implication y → k. Then the following hypothesis is generated “if k is true, then it is possible that y is true.”

46

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

•

Using modus tollens: Let r be an implication y → k. If “not k” is inferred, then “not y” is also inferred.

•

• Natural diagnostic reasoning is not any method of proving the truth. It has another goal: to infer all possible hypotheses about the value of some target attribute. These hypotheses must not contradict with the expert’s knowledge and the situation under consideration. The process of inferring hypotheses is reduced to extending maximally a collection x of attribute values such that none of the forbidden pairs of values would belong to the extension of x.

•

The joint method of similarity-distinction: This method consists of applying two previous methods simultaneously. The method of concomitant changes: This rule means that if the change of a previous event (value) A is accompanied by the change of an event (value) a, and all the other previous events (values) do not change, then A is aplausible reason of a. The method of residuum: Let U be a complex phenomenon abcd, and we know that A is the reason of a, B is the reason of b, and C is the reason of c. Then it is possible to suppose that there is an event D that is a reason of d.

Inductive reasoning rules of the second type
Inductive steps of common sense reasoning consist of using already known facts and statements, observations, and experience for inferring new logical rules of the first type or correcting those that turn out to be false. For this goal, inductive reasoning rules are applied. The main forms of induction are the canons of induction that have been formulated by English logician Mill (1900). These canons are known as the five induction methods of reasoning: method of only similarity, method of only distinction, joint method of similarity-distinction, method of concomitant changes, and method of residuum. • The method of only similarity: This rule means that if the previous events (values) A, B, C lead to the events (values) a, b, c and the events (values) A, D, E lead to the events (values) a, d, e, then A is a plausible reason of a. The method of only distinction: This rule means that if the previous events (values) A, B, C lead to (or give rise to) the events (values) a, b, c and the events (values) B, C lead to the events (values) b, c, then A is a plausible reason of a.

the concept oF A good clAssIFIcAtIon test
Our approach to machine-learning problems is based on the concept of a good diagnostic (classification) test. A good classification test can be understood as an approximation of a given classification on a given set of examples (Naidenova, 1996; Naidenova & Polegaeva, 1986). A good diagnostic test is defined as follows. Let R be a set of examples and S = {1, 2, …i, …, n} be the set of indices of examples, where n is the number of examples of R. Let R(+) and S(+) be the set of positive examples and the set of indices of positive examples, respectively. Let R(-) = R/R(+) denote the set of negative examples. Let U be the set of attributes and Т be the set of attributes values (values, for short), each of which appears at least in one of the examples of R. Denote by s(A), A ∈ T the subset {i ∈ S: A appears in ti, ti ∈ R}, where S = {1, 2, .., n}. Following Cosmadakis et al. (1986), we call s(A) the interpretation of A ∈ T in R. The definition of s(A) can be extended to the definition of s(t) for any collection t, t ⊆ T of values as follows: if t = A1 A2 ... Am, then s(t) = s(A1) ∩ s(A2) ∩ ... ∩ s(Am).

•

47

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

Definition 1. A collection t ⊆ T (s(t) ≠ ∅) of values is a diagnostic test for the set R(+) of examples if and only if the following condition is satisfied: t ⊄ t*, ∀ t*, t*∈ R(-) (the equivalent condition is s(t) ⊆ S(+)). Let k be the name of a set R(k) of examples. To say that a collection t of values is a diagnostic test for R(k) is equivalent to say that it does not cover any example t*, t* ∉ R(k). At the same time, the condition s(t) ⊆ S(k) implies that the following implicative dependency is true: “if t, then k.” Thus a diagnostic test, as a collection of values, makes up the left side of a rule of the first type. It is clear that the set of all diagnostic tests for a given set R(+) of examples (call it “DT(+)”) is the set of all the collections t of values for which the condition s(t) ⊆ S(+) is true. For any pair of diagnostic tests ti, tj from DT(+), only one of the following relations is true: s(ti) ⊆ s(tj), s(ti) ⊇ s(tj), s(ti) ≈ s(tj), where the last one means that s(ti) and s(tj) are incomparable, that is, s(ti) ⊄ s(tj) and s(tj) ⊄ s(ti). This consideration leads to the concept of a good diagnostic test. Definition 2. A collection t ⊆ T (s(t) ≠ ∅) of values is a good test for the set R(+) of examples if and only if s(t) ⊆ S(+) and, simultaneously, the condition s(t) ⊂ s(t*) ⊆ S(+) is not satisfied for any t*, t*⊆ T, such that t* ≠ t. Now we shall give the following definitions. Definition 3. A collection t of values is irredundant if for any value v ∈ t the following condition is satisfied: s(t) ⊂ s(t/v). If a collection t of values is a good test for R(+) and, simultaneously, it is an irredundant collection of values, then any proper subset of t is not a test for R(+). Definition 4. A collection t ⊆ T of values is maximally redundant if for any implicative de-

pendency X → v, which is satisfied in R, the fact that t contains X implies that t also contains v. If t is a maximally redundant collection of values, then for any value v ∉ t, v ∈ T the following condition is satisfied: s(t) ⊃ s(t ∪ v). In other words, a maximally redundant collection t of values covers the number of examples greater than any collection (t ∪ v) of values, where v ∉ t. If a diagnostic test t for a given set R(+) of examples is a good one and it is a maximally redundant collection of values, then for any value v ∉ t, v ∈ T the following condition is satisfied: (t ∪ v) is not a good test for R(+). Any example t in R is a maximally redundant collection of values because for any value v ∉ t, v ∈ T s(t ∪ v) is equal to ∅. For example, in Table 1, the collection “Blond Bleu” is a good irredundant test for class 1 and, simultaneously, it is maximally redundant collection of values. The collection “Blond Embrown” is a test for class 2 but it is not good and, simultaneously, it is maximally redundant collection of values. The collection “Embrown” is a good irredundant test for class 2. The collection “Red” is a good irredundant test for class 1. The collection “Tall Red Bleu” is a good maximally redundant test for class 1. It is clear that the best tests for pattern recognition problems must be good irredundant tests. These tests allow constructing the shortest rules of the first type with the highest degree of generalization. One of the possible ways for searching for good irredundant tests for a given class of positive examples is the following: first, find all good maximally redundant tests; second, for each good maximally redundant test, find all good irredundant tests contained in it. This is a convenient strategy as each good irredundant test belongs to one and only one good maximally redundant test with the same interpretation (Naidenova, 1999).

48

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

Table 1. Example 1of Data Classification (This example is adopted from (Ganascia, 1989))
Index of example 1 2 3 4 5 6 7 8 Height Low Low Tall Tall Tall Low Tall Tall Color of hair Blond Brown Brown Blond Brown Blond Red Blond Color of eyes Bleu Bleu Embrown Embrown Bleu Embrown Bleu Bleu Class 1 2 2 2 2 2 1 1

.Note to Table 1 and all the following tables: the values of attributes must not be considered as the words of English language, they are the abstract symbols only

the duAlIty oF good dIAgnostIc tests
In our definition of good tests, we used, implicitly, correspondences of Galois G on S×T and two relations S → T, T → S (Ore, 1944; Riguet, 1948). Let s ⊆ S, t ⊆ T. We define the relations as follows: S → T: t(s) = {intersection of all ti: ti ⊆ T, i ∈ s} and T → S: s(t) = {i: i ∈ S, t ⊆ ti}. Extending s by an index j* of some new example leads to receiving a more general feature of examples: (t ∪ A) ⊇ t implies s(t ∪ A) ⊆ s(t). Extending t by a new value A leads to decreasing the number of examples possessing the general feature “tA” in comparison with the number of examples possessing the general feature “t”: (t ∪ A) ⊇ t implies s(t ∪ A) ⊆ s(t). Now we shall introduce the following generalization operations (functions): generalization_of(t) = t′ = t(s(t)); generalization_of(s) = s′ = s(t(s)).

that s(t(s)) ⊇ s. This generalization operation gives the maximal set of examples possessing the feature t(s). As a result of the generalization of t, the sequence of operations t → s(t) → t(s(t)) gives that t(s(t)) ⊇ t. This generalization operation gives the maximal general feature for examples the indices of which are in s(t). These generalization operations are not artificially constructed operations. One can perform, mentally, a lot of such operations during a short period of time. We give some examples of these operations. Suppose that somebody has seen two films (s) with the participation of Gerard Depardieu (t(s)). After that he tries to know all the films with his participation (s(t(s))). One can know that Gerard Depardieu acts with Pierre Richard (t) in several films (s(t)). After that he can discover that these films are the films of the same producer, Francis Veber t(s(t)). Namely, these generalization operations are used for searching for good diagnostic tests. Now we define a diagnostic test as a dual object, that is, as a pair (SL, TA), SL ⊆ S, TA ⊆ T, SL = s(TA) and TA = t(SL). Definition 5. Let PM = {s1, s2, …, sm} be a family of subsets of some set M. Then PM is a Sperner system (Sperner, 1928) if the following condition

As a result of the generalization of s, the
sequence of operations s → t(s) → s(t(s)) gives

49

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

is satisfied: si ⊄ sj and sj ⊄ si, ∀(i,j), i ≠ j, i, j = 1, …, m. Let R, S, R(+), R(-), S(+) be defined as before. Definition 6. To find all good maximally redundant tests (GMRTs) for a given class R(+) of examples means to construct a family PS of subsets s1, s2, …, sj, …, snp of the set S(+) such that: 1. 2. PS is a Sperner system. Each sj is a maximal set in the sense that adding to it the index i, such that i ∉ sj, i ∈ S(+), implies s(t(sj ∪ i)) ⊄ S(+). Putting it in another way, t(sj ∪ i) is not a test for the class R(+).

the relation ≤, where (s, t) ≤ (s*, t*) is satisfied if and only if s ⊆ s* and t ⊇ t*. The set Ψ = (MUT, ∪, ∩) is an algebraic lattice, where operations ∪, ∩ are defined for all pairs (s*, t*), (s, t) ∈ MUT in the following way (Wille, 1992): (s*, t*) ∪ (s, t) = ((s* ∪ s), (t* ∩ t)), (s*, t*) ∩ (s, t) = ((s* ∩ s), (t* ∪ t)). The unit element and the zero element are (S, ∅) and (∅, T), respectively. Inferring good tests is reduced to inferring, for any element (s*, t*) ∈ MUT, all the elements nearest to it in the lattice with respect to the ordering ≤, that is, inferring all (s, t), that (s*, t*) ≤ (s, t), and there does not exist any (s**, t**) such that (s*, t*) ≤ (s**, t**) ≤ (s, t), or inferring all (s, t), that (s*, t*) ≥ (s, t), and there does not exist any (s**, t**) such that (s*, t*) ≥ (s**, t**) ≥ (s, t). Inferring the chains of lattice elements ordered by the inclusion relation lies in the foundation of generating all types of diagnostic tests: s0 ⊆ … ⊆ si ⊆ si+1 ⊆ … ⊆ sm (t(s0) ⊇ t(s1) ⊇ … ⊇ t(si) ⊇ t(si+1) ⊇ … ⊇ t(sm)), (2) t0 ⊆ … ⊆ ti ⊆ ti+1 ⊆ … ⊆ tm (s(t0) ⊇ s(t1) ⊇ … ⊇ s(ti) ⊇ s(ti+1) ⊇ … ⊇ s(tm)). (1)

The set TGOOD of all GMRTs is determined as follows: {t: t(sj), sj ∈ PS, ∀j = 1, …, np}. Definition 7. To find all good irredundant tests (GIRTs) for a given class R(+) of examples means to construct a family PRT of subsets t1, t2, …, tj, …, tnq of the set T such that: 1. ∀tj, j = 1, …, nq, tj ⊄ t, ∀t, t ∈ R(-) and, simultaneously, ∀tj, j = 1, …, nq, s(tj) ≠ ∅ there does not exist a collection s* ≠ s(tj), s* ⊆ S of indices such that s(tj) ⊂ s* ⊆ S(+). PRT is a Sperner system. Each tj is a minimal set in the sense that removing from it any value A belonging to it implies s(tj without A) ⊄ S(+).

2. 3.

Inductive rules for constructing elements of a dual lattice
We use the following variants of inductive transition from one element of a chain to its nearest element in the lattice: 1. 2. 3. 4. from sq = (i1, i2, …, iq) to sq+1 = (i1, i2, …, iq+1); from tq = (A1, A2, …, Aq) to tq+1 = (A1, A2, …, Aq+1); from sq = (i1, i2, …, iq) to sq-1 = (i1, i2, …, iq-1); from tq = (A1, A2, …, Aq) to tq-1 = (A1, A2, …, Aq-1).

generAtIon oF duAl objects wIth the use oF lAttIce operAtIons
Let R be a table of examples and S, T are defined as before. Let MUT be the set of all dual objects, that is, the set of all pairs (s, t), s ⊆ S, t ⊆ T, s = s(t), and t = t(s). This set is partially ordered by

50

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

We need the special rules for realizing these inductive transitions.

the generalization rule
The generalization rule is used to get all the collections of indices sq+1 = {i1, i2, … iq, iq+1} from a collection sq = {i1, i2, … iq} such that t(sq) and t(sq+1) are tests for a given class of positive examples. The termination condition for constructing a chain of generalizations is: for all the extension sq+1 of sq, t(sq+1) is not a test for a given class of positive examples. Consider some of the possible realizations of this rule for inferring GMRTs. The first variant of generalization rule. Let S(test) be the partially ordered set of elements s = {i1, i2, … iq}, q = 1, 2, …, nt - 1 obtained as a result of generalizations and satisfying the following condition: t(s) is a test for R(+). Here nt denotes the number of positive examples. Let STGOOD be the partially ordered set of elements s satisfying the following condition: t(s) is a GMRT for R(+). Next we use an inductive rule for extending elements of S(test) and constructing {i1, i2, … iq+1} from {i1, i2, … iq}, q = 1, 2, .., nt - 1. This rule relies on the following consideration: if the set {i1, i2, … iq+1} corresponds to a test for R(+), then all its proper subsets must correspond to tests too and, consequently, they must be in S(test). Having constructed a set sq+1 = {i1, i2, … iq+1}, we determine whether it corresponds to the test or not. If t(sq+1) is not a test, then sq+1 is deleted, otherwise it is inserted in S(test). If all the extensions of s do not correspond to tests, then s corresponds to a GMRT and it is inserted in STGOOD. The function to_be_test(t) is defined as follows: if s(t) ∩ S(+) = s(t) then true else false. This variant of generalization rule is used in an algorithm of inferring GMRTs given in Naidenova and Polegaeva (1991). An analogous inductive extension of items’ collections is also used in two algorithms, Apriory and AprioryTid, proposed in Agrawal and Srikant (1994) for min-

ing association rules between items in a large database of sales transactions. The second variant of generalization rule. This rule allows for each element s the following: • • To avoid constructing the set of all its subsets. To avoid the repetitive generation of it.

Consider a way for choosing indices admissible for extending sq. Suppose that S(test) and STGOOD are not empty and s ∈ S(test). Construct the set V: V = {∪ s’, s ⊆ s’, s’ ∈ {S(test) ∪ STGOOD}}. The set V is the union of all the collections of indices in S(test) and STGOOD containing s; hence, s is in the intersection of these collections. If we want an extension of s not to be included in any element of {S(test) ∪ STGOOD}, we must use, for extending s, the indices not appearing simultaneously with s in the set V. The set of indices, candidates for extending s, is the set: CAND(s) = nts/V, where nts = {∪ s, s ∈ S(test)}. An index j* ∈ CAND(s) is not admissible for extending s if, at least for one index i ∈ s, the pair {i, j*} either does not correspond to a test or it corresponds to a good test (it belongs to STGOOD). Let Q be the set of forbidden pairs of indices for extending s: Q = {{i, j}⊆ S(+): t({i, j}) is not a test for R(+)}. Then the set of admissible indices is select(s) = {i, i ∈ CAND(s): (∀j) ( j ∈ s), {i, j} ∉ {STGOOD or Q}}. The set Q can be generated in the beginning of searching all GMRTs for R(+). The procedure EXTENSION(s) takes select(s) and returns the set ext(s) of all possible extensions of s in the form snew = (s ∪ j), j ∈ select(s) and snew corresponds to a test for R(+). This procedure executes the function generalization_of(snew)

51

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

for each element snew ∈ ext(s) (for this function, please see the introduction to Section 4). If ext(s) and the set V are empty, then s corresponds to a GMRT for R(+) and s is transferred from S(test) to STGOOD. If ext(s) contains one and only one element, then this element corresponds to a GMRT, it is inserted in STGOOD and s is deleted from S(test). In all other cases, the set ext(s) substitutes s in S(test). This variant of generalization rule is a complex process in which both deductive and inductive reasoning rules of the second type are performed (please, see Table 2). The knowledge acquired during the process of generalization (the sets Q, S(test), STGOOD) is used for pruning the search in the domain space. The generalization rule realizes the joint method of similarity-distinction. The extending of s results in obtaining the subsets of positive examples of more and more power with more and more generalized features (set of values). This operation is analogous to the generalization rule applied for star generation under conceptual clustering (Michalski, 1983). An algorithm, NIAGaRa, based on this variant of generalization rule is used in Naidenova (2001), for inferring GMRTs.

collection tq = {A1, A2, …, Aq} such that tq and tq+1 are irredundant collections of values, and they are not tests for a given set of positive examples. The termination condition for constructing a chain of specifications is: for all the extensions tq+1 of tq, tq+1 is either a redundant collection of values or a test for a given set of positive examples. This rule is used for inferring GIRTs. The first variant of specification rule. Let TGOOD be the partially ordered set of elements t satisfying the following condition: t is a good irredundant test for R(+). We denote by SAFE the set of elements t such that t is an irredundant collection of values but not a test for R(+). Next we use an inductive rule for extending elements of SAFE and constructing tq+1 = (A1, A2, …, Aq+1) from tq = (A1, A2, …, Aq) q = 1, 2, .., na – 1, where na is the number of values in the set T. This rule relies on the following consideration: if the collection of values {A1, A2, … Aq+1} is an irredundant one, then all its proper subsets must be irredundant collections of values too and, consequently, they must be in SAFE. Having constructed a set tq+1 = {A1, A2, … Aq+1}, we determine whether it is the irredundant collection of values or not. If the collection tq+1 is redundant, then it is deleted from SAFE. If it is the test for R(+), then it is transferred from SAFE to TGOOD. If tq+1 is irredundant but not a test for R(+), then it is a candidate for extension.

The Specification Rule
The specification rule is used to get all the collections of values tq+1 = {A1, A2, …, Aq+1} from a

Table 2. Using deductive and inductive rules of the second type
Inductive rules Generalization rule Forming Q Forming CAND(s) Forming select(s) Forming ext(s) Function_to_be test(t) Generalization_of(snew) Generating forbidden rules The joint method of similarity-distinction Using forbidden rules The method of only similarity Using implication Lattice operations Process Deductive and inductive rules of the second type

52

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

We use the function to_be_irredundant(t) = if (∀Ai) (Ai ∈ t) s(t) ≠ s(t/ Ai) then true else false It is easy to see that this variant of specification rule is algorithmically equivalent to the first variant of the generalization rule. This rule is used in an algorithm given in Megretskaya (1988) for inferring GIRTs. The second variant of specification rule. It is an inductive extension rule containing a method for choosing admissible values for extending t in case of t is not a test, but its extension is a test for R(+). We extend t by choosing admissible values as follows: these values appear simultaneously with t in the examples of R(+), and do not appear with t in any example of R(-). These values are to be said essential ones. To get them, we construct two sets V(-) and V(+) as follows: V(-) = {∪ t’: t ⊆ t’, t’ ∈ R(-)}; V(+) = {∪ t’: t ⊆ t’, t’ ∈ R(+)}. The set ess(t) of essential values for t is equal to V(+)/V(-). Thus searching essential values requires a special reasoning operation, a diagnostic induction reasoning rule.

tive examples. Then sbmin(t) = t/ sbmax(t) is the minimal set of essential values in t. This inductive rule generates diagnostic rules of the first type. It is based on the inductive method of only distinction. We see that the diagnostic rules of the first type obtained with the use of inductive diagnostic rules are used immediately in the process of good tests construction. An analogous rule is defined in Michalski (1983) and Michalski and Larson (1978). If a newly presented training example contradicts an already constructed concept description, the specialization rule is applied to generate a new consistent concept description.

the dual Inductive diagnostic rule
The dual inductive diagnostic rule is used to get a collection of indices sq-1 = (i1, i2, …, iq-1) from a collection sq = (i1, i2, …, iq) such that t(sq-1) is a test, but t(sq) is not a test for a given set of positive examples. This rule uses a method for choosing indices admissible for deleting from sq. By analogy with an essential value, we define an essential example. Definition 9. Let s be a subset of indices of positive examples; assume also that t(s) is not a test. The example tj, j ∈ s is to be said an essential one if t(s/j) proves to be a test for a given set of positive examples. Generally, we are interested in finding the maximal subset sbmax(s) ⊂ s such that t(s) is not a test, but t’ = t(sbmax(s)) is a test for a given set of positive examples. Then sbmin(s) = s/sbmax(s) is the minimal set of indices of essential examples in s. The dual inductive diagnostic rule is used for inferring compatibility rules of the first type. The number of indices in sbmax(s) can be understood as a measure of “carrying-out” for an acquired rule related to sbmax(s), namely, t(sbmax(s)) →

the Inductive diagnostic rule
The inductive diagnostic rule is used to get a collection of values tq+1 = {A1, A2, …, Aq+1} from a collection tq = {A1, A2, … Aq} such that tq is not a test, but tq+1 is a test for a given set of positive examples. We extend tq by choosing values that appear simultaneously with it in the examples of R(+), and do not appear in any example of R(-). These values are to be said essential ones. Definition 8. Let t be a collection of values that is a test for a given set of positive examples. We say that the value A in t is essential if (t/A) is not a test for a given set of positive examples. Generally, we are interested in finding the maximal subset sbmax(t) ⊂ t such that t is a test, but sbmax(t) is not a test for a given set of posi-

53

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

k(R(+)) frequently, where k(R(+)) is the name of the set R(+). Assume s* is a collection of indices of positive examples such that t(s*) is not a test. Next we describe the procedure with the use of which a quasi-maximal subset qsbmax(s*) ⊂ s* is obtained such that t(qsbmax(s*)) is a test for a given set of positive examples. We begin with the first index i1 of s*, then we take the next index i2 of s* and evaluate the function to_be_test (t({i1, i2})). If the value of this function is “true,” then we take the next index i3 of s* and evaluate the function to_be_test (t({i1, i2, i3})). If the value of the function to_be_test (t({i1, i2})) is “false,” then the index i2 of s* is skipped and the function to_be_test (t({i1, i3})) is evaluated. We continue this process until we achieve the last index of s*. The dual inductive diagnostic rule is based on the inductive method of only distinction. We see that the compatibility rules of the first type, obtained with the use of dual inductive diagnostic rule, are used immediately in the process of good tests construction. The rules for constructing diagnostic tests as elements of dual lattice generate logical rules of the first type, as shown in Table 3.

the decomposItIon oF InFerrIng good dIAgnostIc tests Into subtAsks
To transform good diagnostic tests inferring into an incremental process, we introduce two kinds of subtasks (Naidenova & Ermakov, 2001): For a given set of positive examples: 1. 2. Given a positive example t, find all GМRТs contained in t. Given a nonempty collection of values Х (maybe only one value) such that it is not a test, find all GMRTs containing Х.

The subtask of the first kind. We introduce the concept of an example’s projection proj(R)[t] of a given positive example t on a given set R(+) of positive examples. The proj(R)[t] is the set Z = {z: (z is nonempty intersection of t and t’) & (t’ ∈ R(+)) & (z is a test for a given class of positive examples)}. If the proj(R)[t] is not empty and contains more than one element, then it is a subtask for inferring all GMRTs that are in t. If the projection contains one and only one element equal to t, then t is a GMRT.

Table 3. Deductive rules of the first type obtained with the use of inductive rules for inferring diagnostic tests
Inductive rules Generalization rule Specification rule Inductive diagnostic rule Dual inductive diagnostic rule Action Extending s (narrowing t) Extending t (narrowing s) Searching for essential values Searching for essential examples Inferring deductive rules of the first type Implications Implications Diagnostic rules Compatibility rules (approximate implications)

54

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

The subtask of the second kind. We introduce the concept of an attributive projection proj(R)[A] of a given value A on a given set R(+) of positive examples. The projection proj(R)[A] = {t: (t ∈ R(+)) & (A appears in t)}. Another way to define this projection is: proj(R)[A] = {ti: i ∈ (s(A) ∩ S(+))}. If the attributive projection is not empty and contains more than one element, then it is a subtask of inferring all GМRТs containing a given value A. If A appears in one and only one example, then A does not belong to any GMRT different from this example. Forming the projection of A makes sense if A is not a test and the intersection of all positive examples in which A appears is not a test too, that is, s(A) ⊄ S(+) and t′ = t(s(A) ∩ s(+)) is not a test for a given set of positive examples. The decomposition of good classification tests inferring into subtasks of the first and second kinds implies introducing a set of special rules to realize the following operations: choosing an

example (value) for a subtask, forming a subtask, deleting values or examples from a subtask, and some other rules controlling the process of inferring good tests. The following theorem gives the foundation for reducing projections both of the first and the second kind. The proof of this theorem can be found in Naidenova et al. (1995b).

theorem 1
Let A be a value from T, X be a maximally redundant test for a given set R(+) of positive examples, and s(A) ⊆ s(X). Then A does not belong to any maximally redundant good test for R(+) different from X. It is convenient to choose essential values in an example and essential examples in a projection for the decomposition of inferring GMRTs into the subtasks of the first or second kind.

Table 4. Example 2 of data classification
Index of example 1 2 3 4 5 6 7 8 Height Low Low Tall Tall Tall Low Tall Tall Color of hair Blond Brown Brown Blond Brown Blond Red Blond Color of eyes Bleu Bleu Embrown Embrown Bleu Embrown Bleu Bleu 1 1 1 2 2 2 2 2 Class

Table 5. The subtask for the value “Low”
Index of example 1 2 6 Height Low Low Low Color of hair Blond Brown Blond Color of eyes Bleu Bleu Embrown 1 1 2 Class

55

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

We give a small example for inferring all the GMRTs for the instances of class 1 presented in Table 4. In Table 4, we have: S(+) = {1,2,3}, s(Low) → {1,2,6}, s(Brown) → {2,3,5}, s(Bleu) → {1,2,5,7,8}, s(Tall) → {3,4,5,7,8}, s(Embrown) → {3,4,6}, and s(Blond) →{1,4,6,8}. We discover that the value “Low” is essential in lines 1 and 2. Then it is convenient to form the subtask of the second kind for this value as shown in Table 5. In Table 5, we have: S(+) = {1,2}, s(Low) → {1,2,6}, s(Brown) → {2}, s(Bleu) → {1,2}, and s(Blond) →{1,6}. We have: s(Bleu) = {1,2} ⊆ S(+). It means that the collection of values “Low Bleu” is a test for class 1. Analogously, for the value “Brown,” we have: s(Brown) = {2} ⊆ S(+). It means that the collection of values “Low Brown” is a test for class 1 but not a good one because of s(Brown) = {2} ⊆ s(Bleu). It is clear that these values cannot belong to any test different from the tests already obtained. We delete “Brown” and “Bleu” from further consideration in this subtask. But after deleting these values, line 1 and 2 are not tests for class 1. Hence, the subtask is over. Return to the main problem. Now we can delete the value “Low” from further consideration because we have gotten all good tests containing this value for class 1. But we know that the value “Low” is essential in lines 1 and 2; this fact means that these lines are not tests for class 1 after deleting this value. The following step may be the inference of all irredundant tests contained in line 3 (covering only one line 3) for class 1. In our case, the collection of values “Brown Embrown” is a GIRT contained in line 3. A recursive procedure, based on using attributive subtasks for inferring GMRTs, has been described in Naidenova et al. (1995b). In the following part of this chapter, we give an algorithm based on the subtasks of the first kind combined

with searching essential examples. This algorithm is used only for inferring GMRTs.

An Algorithm for Inferring gmrts with the use of the subtask of the First kind
The algorithm DIAGaRa is the basic recursive algorithm for solving a subtask of the first kind. The initial information for the algorithm of finding all the GMRTs contained in a positive example is the projection of this example on the current set R(+). Essentially the projection is simply a subset of examples defined on a certain restricted subset t* of values. Let s* be the subset of indices of positive examples producing the projection. It is useful to introduce the characteristic W(t) of any collection t of values named by the weight of t in the projection: W(t) = ||s(t) ∩ s*|| is the number of positive examples of the projection containing t. Let WMIN be the minimal permissible value of the weight. Let STGOOD be the partially ordered set of elements s satisfying the condition that t(s) is a good test for R(+). The basic algorithm consists of applying the sequence of the following steps: • Step 1: Check whether the intersection of all the elements of projection is a test and if so, then s* is stored in STGOOD if s* corresponds to a good test at the current step; in this case, the subtask is over. Otherwise the next step is performed (we use the function to_be_test(t): if s(t) ∩ S(+) = s(t) (s(t) ⊆ S(+)) then true else false). Step 2: For each value A in the projection, the set splus(A) = {s* ∩ s(A)} and the weight W(A) = ||splus(A)|| are determined, and if the weight is less than the minimum permissible weight WMIN, then the value А is deleted from the projection. We can also delete the value A if W(A) is equal to WMIN and

•

56

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

•

•

•

•

t(splus(A)) is not a test; in this case A will not appear in a maximally redundant test t with W(t) equal to or greater than WMIN. Step 3: The generalization operation is performed: t′ = t(splus(А)), А ∈ t*; if t′ is a test, then the value A is deleted from the projection and splus(A) is stored in STGOOD if splus(A) corresponds to a good test at the current step. Step 4: The value A can be deleted from the projection if splus(A) ⊆ s’ for some s’ ∈ STGOOD. Step 5: If at least one value has been deleted from the projection, then the reduction of the projection is necessary. The reduction consists of deleting the elements of projection that are not tests (as a result of previous eliminating values). If, under reduction, at least one element has been deleted from the projection, then Step 2, Step 3, Step 4, and Step 5 are repeated. Step 6: Check whether the subtask is over or not. The subtask is over when either the projection is empty or the intersection of all elements of the projection corresponds to a test (see Step 1). If the subtask is not over, then the choice of an essential example in this projection is performed and the new subtask is formed with the use of this essential example. The new subsets s* and t* are constructed and the basic algorithm runs recursively. The important part of the basic algorithm is how to form the set STGOOD.

that is, s1 ⊆ s2. Under formation of STGOOD, a collection s of indices is stored in STGOOD if and only if it is not absorbed by any collection of this set. It is necessary also to delete from STGOOD all the collections of indices that are absorbed by s if s is stored in STGOOD. Thus, when the algorithm is over, the set STGOOD contains all the collections of indices that correspond to GMRTs and only such collections. Essentially, the process of forming STGOOD is an incremental procedure of finding all maximal elements of a partially ordered set. The set TGOOD of all the GMRTs is obtained as follows: TGOOD = {t: t = t(s), (∀s) (s ∈ STGOOD)}.

An Approach to Incremental Inferring good diagnostic tests
Incremental learning is necessary when a new portion of observations or examples becomes available over time. Suppose that each new example comes with the indication of its class membership. The following actions are necessary with the arrival of a new example: • Check whether it is possible to perform generalization of some existing GMRTs for the class to which the new example belongs (class of positive examples), that is, whether it is possible to extend the set of examples covered by some existing GMRTs or not. Infer all the GMRTs contained in the new example. Check the validity of the existing GMRTs for negative examples, and if it is necessary: Modify tests that are not valid (test for negative examples is not valid if it is included in a positive example, that is, in other words, it accepts an example of positive class).

• • •

We give in the Appendix an example of the work of the algorithm DIAGaRa. An approach for forming the set STGOOD. Let L(S) be the set of all subsets of the set S. L(S) is the set lattice (Rasiova, 1974). The ordering determined in the set lattice coincides with the set-theoretical inclusion. It will be said that subset s1 is absorbed by subset s2, that is, s1 ≤ s2, if and only if the inclusion relation is hold between them,

Thus the process of inferring all the GMRTs is divided into the subtasks that conform to three acts of reasoning:

57

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

•

•

•

Pattern recognition or using already known rules (tests) for determining the class membership of a new positive example and generalization of these rules that recognize it correctly (deductive reasoning and increasing the power of already existing inductive knowledge). Inferring new rules (tests) that are generated by a new positive example (inductive reasoning a new knowledge). Correcting rules (tests) of alternative (negative) classes that accept a new positive example (these rules do not permit to distinguish a new positive example from some negative examples) (deductive and inductive diagnostic reasoning to modify knowledge).

The first act reveals the known rules satisfied with a new example, the induction base of these rules can be enlarged. The second act can be reduced to the subtask of the first kind. The third act can be reduced either to the inductive diagnostic rule and the subtask of the first or to the subtask of the second kind.

conclusIon
This work is an attempt to transform a large class of machine-learning tasks into a commonsense reasoning process based on using well-known deduction and induction logical rules. For this goal, we have chosen the task of inferring good classification (diagnostic) tests for a given partitioning on a given training set of examples because a lot of well-known machinelearning problems, such as inferring functional, implicative, and associative dependencies from data, are reduced to this task. We proposed a unified model for combining inductive reasoning with deductive reasoning in the framework of inferring and using implicative

logical rules. The key concept of our approach is the concept of a good diagnostic test. We define a good diagnostic test as the best approximation of a given classification on a given set of examples. We have used the lattice theory as the mathematical model for constructing good classification tests. We define a diagnostic test as a dual object, that is, as an element of the concept lattice introduced in the formal concept analysis. The links between dual elements of concept lattice reflect both inclusion relations between concepts (structural knowledge) and implicative relations between concept descriptions (deductive knowledge). Inferring the chains of lattice elements ordered by the inclusion relation lies in the foundation of generating all types of diagnostic tests. We considered four variants of inductive transition from one element of a chain to its nearest element in the lattice. We have constructed the special rules for realizing these inductive transitions: the generalization rule, the specification rule, the inductive diagnostic rule, and the dual inductive diagnostic rule. We have divided commonsense reasoning rules in two classes: rules of the first type and rules of the second type. The rules of the first type are represented with the use of implicative logical statements. The rules of the second type or reasoning rules (deductive and inductive) are rules with the help of which rules of the first type used, updated, and inferred from data. The deductive reasoning rules of the second type are modus ponens, modus ponendo tollens, modus tollendo ponens, and modus tollens. The inductive reasoning rules of the second type are the following ones: the method of only similarity, the method of only distinction, the joint method of similarity-distinction, and some others. The analysis of the inference for lattice construction allows demonstrating that this inference engages both inductive and deductive reasoning rules of the second type. During the lattice construction, the rules of the first type (implications, interdic-

58

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

tions, rules of compatibility) are generated and used immediately. We have introduced the decomposition of inferring good tests for a given set of positive examples into operations and subtasks that are in accordance with human commonsense reasoning operations. This decomposition allows, in principle, to transform the process of inferring good tests into a “step by step” commonsense reasoning process. We have given also the algorithm DIAGaRa for inferring good maximally redundant tests, and an approach to incrementally inferring good diagnostic tests.

his invariable attention to the author. None of these people bear any responsibility for the content as presented, of course.

Appendix
An example of using algorithm DIAGaRa. The data to be processed are in Table 6 (the set of positive examples) and in Table 7 (the set of negative examples). We begin with s* = S(+) = {{1}, {2}, …, {14}}, t* = T = {A1, A2, ….., A26}, SPLUS = {splus(Ai): Ai ∈ t*} (see SPLUS in Table 8). In Tables 8, 9, A* denotes the collection of values {A8, A9} and A+ denotes the collection of values {A14, A15} because splus(A8) = splus(A9) and splus(A14) = splus(A15). We use the algorithm DIAGaRa for inferring all the GMRTs having a weight equal to or greater than WMIN = 4 for the training set of the positive examples represented in Table 6. Please observe that splus(A12) = {2,3,4,7} and t({2,3,4,7}) is a test; therefore, A12 is deleted from t* and splus(A12) is inserted into STGOOD. Then W(A*), W(A13), and W(A16) are less than WMIN; hence, we can delete A*, A13, and A16 from t*. Now t10 is not a test and can be deleted.

Acknowledgment
The author is very grateful to Professor Evangelos Triantaphyllou (Louisiana State University), who inspired and supported this work, to Dr. Giovanni Felici (IASI – Italian National Research Council), for his invaluable critical remarks, and to Prof. Carlo Vercellis (Milan Polytechnic Institute) for

Table 6. The set of positive Examples R(+)
Index of example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 R(+) A1 A2 A5 A6 A21 A23 A24 A26 A4 A7 A8 A9 A12 A14 A15 A22 A23 A24 A26 A3 A4 A7 A12 A13 A14 A15 A18 A19 A24 A26 A1 A4 A5 A6 A7 A12 A14 A15 A16 A20 A21 A24 A26 A2 A6 A23 A24 A7 A20 A21 A26 A3 A4 A5 A6 A12 A14 A15 A20 A22 A24 A26 A3 A6 A7 A8 A9 A13 A14 A15 A19 A20 A21 A22 A16 A18 A19 A20 A21 A22 A26 A2 A3 A4 A5 A6 A8 A9 A13 A18 A20 A21 A26 A1 A2 A3 A7 A19 A20 A21 A22 A26 A2 A3 A16 A20 A21 A23 A24 A26 A1 A4 A18 A19 A23 A26 A23 A24 A26

59

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

Table 7. The set of negative Examples R(-)
Index of example 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 R(-) A3 A8 A16 A23 A24 A7 A8 A9 A16 A18 A1 A21 A22 A24 A26 A1 A7 A8 A9 A13 A16 A2 A6 A7 A9 A21 A23 A10 A19 A20 A21 A22 A24 A1 A20 A21 A22 A23 A24 A1 A3 A6 A7 A9 A16 A2 A6 A8 A9 A14 A15 A16 A1 A4 A5 A6 A7 A8 A16 A7 A13 A19 A20 A22 A26 A1A2 A3 A6 A7 A16 A1 A2 A3 A5 A6 A13 A16 A1 A3 A7 A13 A19 A21 A1 A4 A5 A6 A7 A8 A13 A16 A1 A2 A3 A6 A12 A14 A15 A16 A1 A2 A5 A6 A14 A15 A16 A26 Index of example 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 A1 A2 A3 A7 A9 A13 A18 A1 A5 A6 A8 A9 A19 A20 A22 A2 A8 A9 A18 A20 A21 A22 A23 A26 A1 A2 A4 A5 A6 A7 A9 A13 A16 A1 A2 A6 A7 A8 A13 A16 A18 A1 A2 A3 A4 A5 A6 A7 A12 A14 A15 A16 A1 A2 A3 A4 A5 A6 A9 A12 A13 A16 A1 A2 A3 A4 A5 A6 A14 A15 A19 A20 A23 A26 A2 A3 A4 A5 A6 A7 A12 A13 A14 A15 A16 A2 A3 A4 A5 A6 A7 A9 A12 A13 A14 A15 A19 A1 A2 A3 A4 A5 A6 A12 A16 A18 A19 A20 A21 A26 A4 A5 A6 A7 A8 A9 A12 A13 A14 A15 A16 A3 A4 A5 A6 A8 A9 A12 A13 A14 A15 A18 A19 A1 A2 A3 A4 A5 A6 A7 A8 A9 A12A13 A14 A15 A1 A3 A4 A5 A6 A7 A12 A13 A14 A15 A16 A23 A24 A1 A2 A3 A4 A5 A6 A8 A9 A12 A14 A15 A18 A22 A2 A8 A9 A12 A14 A15 A16 R(-)

Table 8. The set SPLUS of the collection splus(A) for all A in Tables 6 and 7
SPLUS = {splus(Ai): s(Ai) ∩ s(+), Ai ∈ T}: splus(A*) → {2,8,10} splus(A13) → {3,8,10} splus(A16) → {4,9,12} splus(A1) → {1,4,11,13} splus(A5) → {1,4,7,10} splus(A12) → {2,3,4,7} splus(A18) → {3,9,10,13} splus(A2) → {1,5,10,11,12} splus(A+) → {2,3,4,7,8} splus(A19) → {3,8,9,11,13} splus(A22) → {2,7,8,9,11} splus(A23) → {1,2,5,12,13,14} splus(A3) → {3,7,8,10,11,12} splus(A4) → {2,3,4,7,10,13} splus(A6) → {1,4,5,7,8,10} splus(A7) → {2,3,4,6,8,11} splus(A24) → {1,2,3,4,5,7,12,14} splus(A20) → {4,6,7,8,9,10,11,12} splus(A21) → {1,4,6,8,9,10,11,12} splus(A26) → {1,2,3,4,6,7,9,10,11,12,13,14}

60

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

After modifying splus(A) for A5, A18, A2, A3, A4, A6, A20, A21, and A26 we find that W(A5) = 3, therefore, A5 is deleted from t* . Then W(A18) turns out to be less than WMIN and we delete A18; this implies deleting t13. Next we modify splus(A) for A1, A19, A23, A4, A26 and find that splus(A4) = {2,3,4,7}. A4 is deleted from t*. Finally, W(A1) turns out to be less than WMIN and we delete A1. We can delete also the values A2, A19 because W(A2), W(A19) = 4, t(splus(A2)), t(splus(A19)) are not tests and, therefore, these values will not appear in a maximally redundant test t with W(t) equal to or greater than 4. After deleting these values, we can delete the examples t9, t5 because A19 is essential in t9, and A2 is essential in t5. Next we can observe that splus(A23) = {1,2,12,14} and t({1,2,12,14}) is a test; thus, A23 is deleted from t* and splus(A23) is inserted into STGOOD. We can delete the values A22 and A6 because W(A22) and W(A6) are now equal to 4, t(splus(A22)) and t(splus(A6)) are not tests, and these values will not appear in a maximally redundant test with weight equal to or greater than 4. Now t14 and t1 are not tests and can be deleted. Choose t12 as a subtask because t(splus(A21)/{12}) and t(splus(A24)/{12}) will be tests. By resolving this subtask, we find that t12 does not produce a new test. We delete it. Then splus(A21) is equal to {4,6,8,11}, t({4,6,8,11}) is a test, thus A21 is deleted from t* and splus(A21) is inserted into STGOOD. We can also delete the value A24 because t(splus(A24)) is the GMRTs already obtained.

We can delete the value A3 because W(A3) is now equal to 4, t(splus(A3)) is not a test, and this value will not appear in a maximally redundant test with weight equal to or greater than 4. We can delete t6 because now this example is not a test. Then we can delete the value A20 because t(splus(A20)) is the GMRTs already obtained. These deletions imply that all of the remaining rows t2, t3, t4, t7, t8, and t11 are not tests. The list of the GMRTs with the weight equal to or greater than WMIN = 4 is given in Table 9.

reFerences
Agraval, R., & Srikant, R. (1994). Fast algorithms for mining association rules. In Proceedings of the 20th VLDB Conference. Santiago, Chile. Anshakov, O.M., Finn, V.K., & Skvortsov, D.P. (1989). On axiomatization of many-valued logics associated with formalization of plausible reasoning. Studia Logica, 42(4), 423-447. Boldyrev, N.G. (1974). Minimization of Boolean partial functions with a large number of “Don’t Care” conditions and the problem of feature extraction. Proceedings of International Symposium “Discrete Systems” (pp.101-109). Riga, Latvia. Carpineto, C., & Romano, G. (1996). A lattice conceptual clustering system and its application to browsing retrieval. Machine Learning, 24, 95-122. Ceri, C., Gotlob, G., & Tanca, L. (1990). Logic programming and databases. Springer. Cosmadakis, S., Kanellakis, P.C., & Spyratos, N. (1986). Partition semantics for relations. Journal of Computer and System Sciences, 33(2), 203-233. Demetrovics J., & Vu, D.T. (1993). Generating Armstrong relation schemes and inferring functional dependencies from relations. International Journal on Information Theory & Applications, 1(4), 3-12.

Table 9. The Sets STGOOD and TGOOD for the Examples of Tables 6 and 7.
№ 1 2 3 STGOOD {2,3,4,7} {1,2,12,14} {4,6,8,11} TGOOD A4 A12 A+ A24 A26 A23 A24 A26 A7 A20 A21

61

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

Dowling, C.E. (1993). On the irredundant generation of knowledge spaces. Journal of Math. Psych., 37(1), 49-62. Finn, V. K. (1984). Inductive models of knowledge representation in man-machine and robotics systems. Proceedings of VINITI, Vol. А, 58-76. Finn, V. K. (1988). Commonsense inference and commonsense reasoning. Review of Science and Technique (Itogi Nauki i Tekhniki), Series “The Theory of Probability. Mathematical Statistics. Technical Cybernetics,” 28, 3-84. Finn, V. K. (1991). Plausible reasoning in systems of JSM type. Review of Science and Technique (Itogi Nauki i Tekhniki), Series “Informatika,” 15, 54-101. Finn, V. K. (1999). The synthesis of cognitive procedures and the problem of induction. NTI, Series 2(1-2), 8-44. Moscow, Russia: VINITI. Galitsky, B. A., Kuznetsov, S. O., & Vinogradov, D. V. (2005). JASMINE: A hybrid reasoning tool for discovering causal links in biological data. Retrieved from http://www.dcs.bbk. ac.uk/~galitsky/Jasmine Ganascia, J. - Gabriel. (1989). EKAW - 89 tutorial notes: Machine learning. Third European Workshop on Knowledge Acquisition for KnowledgeBased Systems (pp. 287-296). Paris, France. Ganter, B. (1984). Two basic algorithms in concepts analysis (FB4-Preprint, No. 831). TH Darmstadt. Giraud-Carrier, C., & Martinez, T. (1994). An incremental learning model for commonsense reasoning. Proceedings of the Seventh International Symposium on Artificial Intelligence (ISAI’94), ITESM (pp. 134-141). Huntala, Y., Karkkainen, J., Porkka, P., & Toivonen, H. (1999). TANE: An efficient algorithm for discovering functional and approximate dependencies. The Computer Journal, 42(2), 100-111.

Kuznetsov, S. O. (1993). Fast algorithm of constructing all the intersections of finite semi-lattice objects. NTI, Series 2(1), 17-20. Moscow, Russia: VINITI. Kuznetsov, S. O., & Obiedkov, S. A. (2001). Comparing performance of algorithms for generating concept lattices. J. Exp. Theor. Artif. Intell. 14(2-3), 183-216. Lavraĉ, N., & Džeroski, S. (1994). Inductive logic programming: Techniques and applications. Chichester: Ellis Horwood. Lavraĉ, N., & Flash, P. (2000). An extended transformation approach to inductive logic programming. CSTR-00-002, March, 2000 (pp. 1-42). University of Bristol, Department of Computer Science. Lavraĉ, N., Gamberger, D., & Jovanoski, V. (1999). A study of relevance for learning in deductive databases. Journal of Logic Programming, 40(2/3), 215-249. Lisi, F., & Malerba, D. (2004). Inducing multilevel association rules from multiple relations. Machine Leaning, 55, 175-210. Mannila, H., & Räihä, K.-J. (1992). On the complexity of inferring functional dependencies. Discrete Applied Mathematics, 40, 237-243. Mannila, H., & Räihä, K.-J. (1994). Algorithm for inferring functional dependencies. Data & Knowledge Engineering, 12, 83-99. Megretskaya, I. A. (1988). Construction of natural classification tests for knowledge base generation. In The Problem of the Expert System Application in the National Economy (pp. 89-93). Kishinev, Moldavia. Mephu Nguifo, E., & Njiwoua, P. (1998). Using lattice based framework as a tool for feature extraction. In H. Lui & H. Motoda (Eds.), Feature extraction, construction, and selection: A data mining perspective. Kluwer.

62

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

Michalski, R. S. (1983). A theory and methodology of inductive learning. Artificial Intelligence, 20, 111-161. Michalski, R. S., & Larsen, I. B. (1978). Selection of most representative training examples and incremental generation of VL1 Hypotheses: The Underlying methodology and the description of programs ESEL and AQII. (Report No. 78-867). Dep. of Comp. Science, Univ. of Illinois at Urbana-Champaign, IL, USA. Michalski, R. S., & Ram, A. (1995). Learning as goal-driven inference. In A. Ram & D. B. Leake (Eds), Goal-driven learning. Cambridge, MA: MIT Press/Bradford Books. Mill, J. S. (1900). The system of logic. Moscow, Russia: Russian Publishing Company “Book Affair.” Naidenova, X. A. (1992). Machine learning as a diagnostic task. In I. Arefiev (Ed.), KnowledgeDialogue-Solution (pp. 26-36). Materials of the short-term scientific seminar. Saint-Petersburg, Russia. Naidenova, X. A. (1996). Reducing machine learning tasks to the approximation of a given classification on a given set of examples. In Proceedings of the 5th National Conference at Artificial Intelligence (Kazan, Tatarstan), 1, 275-279. Naidenova, X. A. (1999). The data-knowledge transformation. Text processing and cognitive technologies (Pushchino, Russia), 3, 130-151. Naidenova, X. A. (2001). Inferring good diagnostic tests as a model of common sense reasoning. Proceedings of the International Conference “Knowledge-Dialog-Solution”, 2, 501-506. SaintPetersburg, Russia: State North-West Technical University, Publishing House “Lan”. Naidenova, X. A., & Ermakov, A. E. (2001). The decomposition of algorithms of inferring good diagnostic tests. In A. Zakrevskij (Ed.), Proceedings of the 4th International Conference

“Computer-Aided Design of Discrete Devices (CAD DD’2001)” (Vol. 3, pp. 61-69), Institute of Engineering Cybernetics, National Academy of Sciences of Belarus. Minsk, Belarus. Naidenova, X. A., Plaksin, M. V., & Shagalov, V. L. (1995b). Inductive inferring all good classification tests. Proceedings of International Conference “Knowledge-Dialog-Solution” (Jalta, Ukraine), 1, 79-84. Naidenova, X. A., & Polegaeva, J. G. (1986). An algorithm of finding the best diagnostic tests. In G. E. Mintz & P. P. Lorents (Eds), The 4th All Union Conference “Application of mathematical logic methods” (pp. 63-67). Tallinn, Estonia: Institute of Cybernetics, National Acad. of Sciences of Estonia. Naidenova, X. A., & Polegaeva, J. G. (1991). SISIF—The system of knowledge acquisition from experimental facts. In J. L. Alty & L. I. Mikulich (Eds.), Proceedings of the IFIP TC5/ WG5.3 Conference “Industrial applications of artificial intelligence” (pp. 87-92). North-Holland, Amsterdam, the Netherlands. Naidenova, X. A., Polegaeva, J. G., & Iserlis, J. E. (1995a). The system of knowledge acquisition based on constructing the best diagnostic classification tests.In Proceedings of International Conference “Knowledge-Dialog-Solution” (Jalta, Ukraine), 1, 85-95. Nourine, L., & Raynaud, O. (1999). A fast algorithm for building lattices. Information Processing Letters, 71, 199-204. Ore, O. (1944). Galois connexions. Trans. Amer. Math. Society, 55(1), 493-513. Piaget, J. (1959). La genèse des structures logiques elémentaires. Neuchâtel. Rasiova, H. (1974). An algebraic approach to non-classical logic (Studies in Logic, Vol. 78). Amsterdam; London: North-Holland Publishing Company.

63

Reducing a Class of Machine Learning Algorithms to Logical Commonsense Reasoning Operations

Riguet, J. (1948). Relations binaires, fermetures, correspondences de Galois. Bull. Soc. Math., 76(3), 114-155. Salzberg, S. (1991). A nearest hyper rectangle learning method. Machine Learning, 6, 277309. Schmidt-Schauss, M., & Smolka, G. (1991). Attributive concept descriptions with complements. Artificial Intelligence, 48(1), 1-26. Shreider, J. (1974). Algebra of classification. Proceedings of VINITI, Series 2(9), 3-6. Sperner, E. (1928). Eine Satz uber untermengen einer endlichen menge. Mat. Z, 27(11), 544-548. Stumme, G., Taouil, R., Bastide, Y., Pasquier, N., & Lakhal, L. (2000). Fast computation of concept lattices using data mining techniques. In Proceeding the 7th International Workshop on Knowledge Representation Meets Databases (KRDB 2000) (pp. 129-139). Stumme, G., Wille, R., & Wille, U. (1998). Conceptual knowledge discovery in databases using formal concept analysis methods. Proceeding the 2nd European Symposium on Principles of Data Mining and Knowledge Discovery (PKDD’98).

Vinogradov, D. V. (1999). Logic programs for quasi-axiomatic theories, NTI, Series 2(1-2), 6164. Moscow, Russia: VINITI. Wille, R. (1992). Concept lattices and conceptual knowledge system. Computer Math. Appl., 23(69), 493-515. Zakrevskij, A. D. (1987). Implicative regularities in formal cognition models. LMPS’87 Abstracts, 1, 373-375. Zakrevskij, A. D. (1982). Revealing implicative regularities in the Boolean space of attributes and pattern recognition, Kibernetika, 1, 1-6. Zakrevskij, A. D. (2001). A logical approach to the pattern recognition problem. Proceedings of the International Conference “KnowledgeDialog-Solution” (KDS’2001), 2, 238-245. SaintPetersburg, Russia: State North-West Technical University, Publishing House “Lan”). Zakrevskij, A. D., & Vasylkova, I, V. (1997). Inductive inference systems in logical recognition in case of partial data. Proceedings of the Fourth International Conference on Pattern Recognition and Information Processing (Minsk-Szczecin), 1, 322-326.

64

65

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification
Giovanni Felici Consiglio Nazionale delle Ricerche, Italy Valerio Gatta Sapienza Università de Roma, Italy

Chapter IV

AbstrAct
The analysis of quality of services is an important issue for the planning and the management of many businesses. The ability to address the demands and the relevant needs of the customers of a given service is crucial to determine its success in a competitive environment. Many quantitative tools in the areas of statistics and mathematical modeling have been designed and applied to serve this purpose. Here we consider an application of a well-established statistical technique, the stated preference models (SP), to identify, from a sample of customers, significant weights to attribute to different aspects of the service provided; such aspects may additively compose an overall satisfaction index. In addition, such a weighting system is applied to a larger set of customers and a comparison is made between the overall satisfaction identified by the SP index and the overall satisfaction directly declared by the customers. Such a comparison is performed by two rule-based classification systems, decision trees, and the logic data miner Lsquare. The results of these two tools help in identifying the differences between the two measurements from the structural point of view, and provide an improved interpretation of the results. The application considered is related to the customers of a large Italian airport.

Copyright © 2008, IGI Global, distributing in print or electronic forms without written permission of IGI Global is prohibited.

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

IntroductIon
Although quality is recognized as a key tool in the management of services, its measurement still remains a fairly subjective concept. The range of definitions used is vast and spreads from “the conformity of the specific or requisites” through to “the suitability for use” arriving at the ample sphere of “client satisfaction” (Franceschini, 2001; Negro, 1995). Many statistical and data analysis techniques have been proposed to measure the effective and perceived quality of the customers of a given service. Despite such efforts, some aspects of the issue still remain unsettled and the decision maker is faced with a number of choices to make when he/she has to plan a quality measurement campaign. In this chapter, we try to extend the range of tools usually deployed in this setting, integrating the results of consolidated techniques for quality surveys, the Stated P reference models (SP), with the application of rule-based classification algorithms. Such algorithms are used to analyze the results obtained by SP and to compare them with the satisfaction level directly declared by the users of the service under study. Our intention is to show the appropriateness of such advanced data analysis tools, typical of the area of data mining, to perform a deeper analysis of the survey data and to better understand the structure of the different methods available to measure service quality and customer satisfaction. The data considered for this application is derived from a survey on airport customers conducted on a large Italian airport, where some of the variables have been appropriately coded, as part of the results obtained are to be considered confidential. The results presented are not to be considered for interpretation purposes. The chapter is organized as follows. The next section analyzes in more detail the issue of measuring the quality of a service through interviews to service users. The main techniques available are briefly introduced and described. Following, we explain, with a larger degree of detail, the main

concepts behind SP, how such types of surveys are built, and what statistical and inferential tools are typically used to put such models to work. Then, we describe some partial results obtained from the realization of an SP survey on airport users. Such results are used to infer a factor-weighting system for a larger customer satisfaction survey. The comparison between the quality index obtained by the SP model and the one detected directly in the survey is the topic of the last two sections of this chapter. In one we propose the use of decision trees to compare the classification models for both quality indices that are obtained from a set of explanatory variables; in the other, we use a logic-based data-mining system, Lsquare, to derive explanations that link, through logic formulas, the overall quality index, and the preference level attributed by the customers to five relevant factors. Finally, some conclusions are drawn.

MeAsurIng servIce quAlIty
In marketing literature, the study of service quality has focused on its evaluation by the customers. When a consumer is put in a central position as the final judge of the quality, the typical customer satisfaction survey (CSS) is based on the compilation of assessment by the clients regarding the diverse characteristics of the services through suitable scales, to which specific graduation techniques are applied (Edwards, 1957). Above all, customer satisfaction market research is used by means of questionnaires and verbal scales, which the people interviewed use to express judgement about the aspects that influence the quality of the said service. These scales are usually made up of five or seven levels pinpointed by adjectives, labels, or graduated segments. In such a way, the person interviewed is able to agree or disagree with each item. Each individual identifies an association between his own feelings and one of the categories in the scale that is offered to him/her. The most

66

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

common instrument for measuring service quality is the Servqual scale, a method that takes inspiration from the disconfirmation theory, based on the difference between the quality conceived and that expected by the client (Parasuraman Zeithaml, & Berry, 1988). Servqual is a two-part questionnaire containing several statements: one part to measure what the client would expect from a general firm in the sector to which the service under examination refers, the other to assess how the client has perceived the service offered. Despite Servqual has been applied across a broad variety of service contexts, it has been criticized on methodological and psychometric grounds by many researchers: the Servperf model (Cronin & Taylor, 1992), the Evaluated Performance (Teas, 1993), and the Retail Service Quality Scale (Dabholkar, Thorpe, & Rentz, 1996) are additional examples of how the subject has been treated in literature. According to these approaches, the analysis of the data is achieved through multivariate statistical techniques such as factorial analysis, hierarchical, and multidimensional model. Often, the global service-quality index is simply computed as the average of the clients’ responses on the overall service evaluation and then, through the relationship among the latter and the judgements on each service dimension (attribute), the importance weights of the single service characteristics are calculated. Sometimes, the importance of each dimension is obtained by directly asking respondents to allocate a certain amount of points across the dimensions. It is to stress that these procedures may lead to partial or biased measures. With the aim to overcome this problem, we consider an SP survey combined with CSS. By doing so, we are able to get the relative importance measures of the attributes, jointly evaluated, that is, based on an explicit trade-off between attributes, and we use the latter information to calculate the service quality indicator (SQI in the following).

stAted preference Models
SP methods refers to a family of techniques that foresee interviewing individuals concerning their preferences regarding a set of different options to estimate utility functions. The options are none other than descriptions of goods or services that differentiate for the characteristics they hold. They mainly deal with hypothetical situations made up ad hoc by the researcher. By their nature, SP methods require purpose-designed surveys for their collection of data. Such methods were originally developed in the marketing research field in the early 1970s, and have become widely used since 1978, with the objective of identifying the customers’ preferences structure for products available or not yet available on the market. The flexibility of these techniques and the rich information that can be extracted allow their application also in transport, environmental, and medical fields. A preference can be expressed in three different ways: respondent may give a rank between options (no metric valuation); they may rate a set of alternatives; or they may choose the best scenario in a given set. The latter is less informative but easier and faster for individuals than the other tasks, and it is the one that they make in reality, by comparing a set of situations and selecting one. Furthermore, this method does not require any assumptions to be made about order or cardinality measurement (Louviere, 1988). We therefore concentrate, in this chapter, on choicebased conjoint analysis, whose seminal precursory paper was written by Mc Fadden (1974). The formation of a preference and the decisional process are, however, two very delicate aspects of the theory of the behavior. The huge complexitywhich stems from their analysis, a series of simplified measures, as well as the knowledge of the theory of the process that leads the individual to give certain answers (Louviere, Hensher, & Swait, 2000).

67

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

The theoretical basis is represented by the microeconomic theory of choice and by the random utility theory (RUT). The first maintains that each decision maker possesses a preference relation, (),  amidst the range of possible choices that satisfy a rational axiom. Such rationality is guaranteed by the completeness and transitivity properties (Mas-Colell, Whinston, & Green, 1995), which guarantees the representability of the structure of the individual’s preferences through the mathematical function U, called utility function, which has an ordinal worth. Consider two alternatives, i and j (which can be goods or services), belonging to a set of choices C, meaning a collection of available alternatives from which the individual is asked to choose, we get:
i  j ⇔ Ui > U j 

analyst has incomplete information mainly caused by the impossibility to consider all the factors that influence the preference of the individual. That implies that utility is not an exact known factor and must be treated as a random variable, made up of a systematic component with a margin of error. Utility of alternative i perceived by individual q can be represented as the sum of both a systematic component and a random one:
U iq = Viq + εiq

(3)

The systematic component is a function, linear in its parameters, of the fundamental attributes:

Vi = X i

(4)

(1)

In this context, utility is defined as the capacity of the object in question (goods or service) to satisfy the needs and meet the preferences of the decision maker. The choices will be carried out in order to guarantee the highest level of utility possible. Utility maximization, as a decisional rule, implies that the alternative i would be chosen if:
Ui > U j , ∀j ≠ i ∈ C

(2)

A first extension of the microeconomic theory for individual choice is suggested by Lancaster (1966). Here utility is defined in terms of different attributes. The decision, then, would directly stem from the utility that springs from the attributes and, consequently, the preference towards any certain product or service would only be indirect. This hypothesis allows one to represent the choice between alternatives as between attributes. A coherent approach with the above-mentioned measures is RUT, originally proposed by Thurstone (1927), by which the decision maker has a perfect discrimination capacity, while the

where is the vector of the coefficients associated to the vector X of explanatory variables associated with alternative i. The random component is then included as it is envisaged that some factors that influence the choice of the decision maker are not measurable. Manski (1973) identified four fonts of randomness due to incomplete information: important attributes not taken into consideration, preferences not detected that differed between individuals, measurement errors, errors gone unnoticed. In synthesis it is assumed that the decision maker is fully informed, has rational preferences, can observe the alternatives with ease and without cost, and choose in a rewarding way that which offers the greatest utility. In the case of choice between two or more alternatives, equation (2) becomes:
∀j ≠ i ∈ C , U iq > U jq ⇔ (Viq − V jq ) > (
jq

−

iq

)

(5) According to RUT, the analyst, not being able to observe the difference to the right-hand member of the last equation, is not able to indicate, with a deterministic concept, when such an inequality is valid and, therefore, turns to a probabilistic approach. Then, the probability that the individual

68

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

q chooses the alternative i from the set of choices C is given by:

∀j ≠ i ,

Pq ( i |C ) = P ( 

jq

−

iq

) < (Viq − V jq ) 
(6)

In order to calculate such a probability, it is enough to define the statistical distribution of the random term, and equation (6) could be rewritten as follows:

∀j ≠ i , Pq ( i |C ) = ∫ I [(

a closed-form expression for the integral in (7), quite the opposite to that which happens for the Logit models that are much easier to use. The most popular model is the multinominal logit (ML), which is expressed as: (see equation (8)). Parameters of this model are estimated using maximun likelihood, thus determining the set of coefficients that, when inserted in the deterministic part of the utility function, maximize the joint probablity across all the observations of the choices actually made.

jq

−

iq

) < (Viq − V jq )] f (

q

)d (7)

q

where f ( q ) is the density function of the random vector q = ( 1q ,..., Jq ), while I(•) is the indicator function that assumes value 1 when the expression in brackets is true, and 0 otherwise. The probability that each random term ( jq − iq ) is below the observed quantity (Viq − V jq ) is none other than a cumulated distribution that can be rewritten in terms of multidimensional integral over the density of the unobserved portion of utility. Different specifications of density, meaning different assumptions about the distribution of the error term, generate various discrete choice models that can be used to analyze the gathered choice data with the purpose of estimating the b-parameters and calculating an SQI. The most popular models are the Logit and the Probit (see (Train, 2003) for reference). The first derives from the Gumbel distribution, the latter from the Normal distribution. The latter has the disadvantage of presenting yet another complex calculation; in fact, they do not have

An ApplIcAtIon to AIrport dAtA
In this section, we illustrate some results extracted from a survey conducted by the statistics department of the Sapienza Università de Rome, for one of the major Italian airports. The aim of the project was to identify the relative importance weights of the dimensions that still characterize their own customer satisfaction surveys, and use these measures to properly calculate an SQI; as stated, we want to link SP methods with CSS. The survey was conducted over a period of 9 months, and dealt with many aspects related to customer satisfaction and perceived service quality. While the main survey was taking place, we also conducted a parallel survey using the SP method. One of the first steps in designing a conjoint study is to fix a set of attributes and corresponding attribute levels that need to be evaluated by the respondents. The identification of relevant attributes is usually done through

Equation 8.
Pq ( i |C ) = ∫ [ ∏ exp( −e
j ≠i −(
iq +Viq −V jq

)

)]e

−

iq

exp( −e

−

iq

)d

iq

=

e
J

X iq X jq

∑ j =1 e

69

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

literature reviews, focus group discussions, or direct questioning. Given our objective, in the actual study, attribute levels were simply selected according to the items; for these were used verbal scales to evaluate five different elements linked with the airport service (e.g., airport enviroment, waiting, time, and others). As anticipated, part of the information is confidential to the client of this study and thus, from now on, we confine the description of the five factors to the coded names F1, F2, F3, F4, F5 with six qualitative levels from Excellent to Very poor, mapped into the values 6, 5, …1, respectively. At the same time the results presented are to be considered just an example of the methods adopted and by no means the true conclusions of the study. Statistical design theory is used to combine the levels of the attributes into a number of alternatives to be presented to respondents. The total number of options is a function both of the number of attributes and of the number of attribute levels. Here the total number of possible combinations was 7776; however, respondents can only evaluate a fairly limited number of options because of cognitive burden and fatigue. Through a for-

mal experimental design, we constructed three choice sets per interview. One of these choice sets had a control function; it was formed by two fixed-design alternatives. Dominance refers to a situation where one option is superior to another on every attribute, so that no trade-offs are involved in selecting the alternatives. In the final analysis, we ignored all the interviews in which the agents failed to correctly answer the control choice exercise. To allow for a rich variation in the combination of attribute levels we used a block design and we prepared 200 different versions of the survey form. The interview was composed of two sections: in the first one, respondents were asked to fill in the form in Table 1; in the second one, the questions were put in a behavioral choice context and the interviewee had to make repeated choices between two alternatives. An example of a choice set is shown in Table 2. Overall, 1,000 face-to-face interviews were obtained at the airport station, according to a random sampling strategy; such sample size guaranteed the desired level of accuracy on the estimated probabilities. Table 3 provides information about the first section of the interview. The frequencies distributions of the judgments are

Table 1. Items and verbal scales in CSS
What is your general judgement on the airport? Overall evaluation Excellent (6) Good(5) Fairly good (4) Barely Satisfactory (3) Poor (2) Very poor (1)

What is your opinion on: F1 F2 F3 F4 F5

Excellent (6)

Good(5)

Fairly good (4)

Barely Satisfactory (3)

Poor (2)

Very poor (1)

70

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

very similar between the airport dimensions. The most representative class is Good, about 60% of the sample for the overall evaluation and for the other attributes, except for F5 where it is 36%. If we just assign to the six categories the values from 1 to 6, we will see, on average, that F3 is the best evaluated attribute while F5 is the worst. Before getting into the econometric analysis, SP data need to be correctly organized. For all of the considered attributes there is no continuous scale; more than two levels are specified. We need to use an effect coding scheme. This creates (l-1) variables that can take the values 1, 0, -1, where l is the number of levels. We decided to exclude the lower category. For example, for F1 we have the situation in Table 4.

Now, we turn our attention to the issue of parameter estimation. We may obtain information about the relative importance of the attribute levels by using discrete choice models. In particular, since we have only two alternatives per choice set, binary logit is used. The estimation results are reported in Table 5. The preferred model is the one in which the F3 attribute is recoded as having two categories instead of six, named “High” (Fairly good, Good, or Excellent) and “Low” (Very poor, poor, Barely satisfactory). Therefore, a new single dummy variable is created (F3_h) that takes value 1 when the judgment on F3 is “High” and 0 otherwise. In this final model, we included all the variables that have significant parameter.

Table 2. Example of a Choice set used in the study
If you were to have these alternatives available to you, which one would you choose?. Factors Airport A Airport B judgement judgement F1 F2 F3 F4 F5 Good Barely satisfactory Very poor Poor Barely satisfactory Fairly good Very poor Good Excellent Fairly good

Table 3. Frequencies of the judgements on the airport dimensions
Barely Satisfactory

Very poor

Poor

Fairly good

Good

Excellent

Mean

Overall evaluation F1 F2 F3 F4 F5

0,3% 0,4% 0,3% 0,6% 0,4% 4,2%

1,2% 1,8% 1,8% 1,7% 2,1% 6,2%

3,0% 6,1% 4,8% 3,5% 4,3% 12,1%

21,3% 25,4% 24,6% 22,1% 22,4% 33,7%

66,7% 59,0% 60,2% 56,3% 60,1% 36,1%

7,5% 7,3% 8,3% 15,9% 10,7% 7,7%

4,75 4,63 4,67 4,80 4,72 4,15

71

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

Table 4. Effect coding for factor F1
VARIABLES F2_bs F1_fg F1_g -1 -1 -1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0

LEVELS Very poor Poor Barely Satisfactory Fairly good Good Excellent

F1_p -1 1 0 0 0 0

F1_e -1 0 0 0 0 1

Table 5. Estimation results of binary logit model
Discrete choice (binary logit) model Maximum Likelihood Estimates Log likelihood function = -964.3959
Pseudo-R2=0.48923 Variable F1_p F1_bs F1_fg F1_g F1_e F2_p F2_g F2_e F3_h F4_p F4_bs F4_fg F4_g F4_e F5_p F5_fg F5_g F5_e Coefficient -0.5214 -0.2341 0.2332 0.5543 0.7119 -0.3031 0.2994 0.5167 0.2200 -0.5079 -0.1945 0.1885 0.3607 0.6781 -0.5678 0.2202 0.5194 0.7129 Std. Err. 0.0784 0.0718 0.0777 0.0762 0.0804 0.0722 0.0712 0.0696 0.0676 0.0740 0.0771 0.0731 0.0792 0.0837 0.0780 0.0689 0.0748 0.0740 |b/St.Er -6.6530 -3.2600 3.0010 7.2750 8.8530 -4.1960 4.2040 7.4190 3.2540 -6.8650 -2.5220 2.5780 4.5540 8.1000 -7.2800 3.1940 6.9480 9.6310 P[|Z|>z] 0.0000 0.0011 0.0027 0.0000 0.0000 0.0000 0.0000 0.0000 0.0011 0.0000 0.0117 0.0099 0.0000 0.0000 0.0000 0.0014 0.0000 0.0000

The overall explanatory power of this nonlinear model is very good; in fact, a pseudo-R 2 of 0.5 is equivalent to about 0.8-0.9 for a linear model (Domencich & Mc Fadden, 1975). In the last two columns, the Wald test is reported. The relative importance weights of the attribute levels are summarized in Table 6. The magnitude

and the signs are consistent with our a-priori. In fact, for each attribute, the effect on utility increases, moving from the lowest category Very poor to the highest category Excellent. However, it should be noticed that this growth is not linear. Based on the information presented in Table 6 and those gathered from the first section of the

72

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

Table 6. Relative importance weights of the attribute levels
ATTRIBUTES F1 F2 F3 F4 F5 Very poor Poor -0.7438 -0.5130 0.0000 -0.5249 -0.8847 -0.5214 -0.3031 0.0000 -0.5079 -0.5678 IMPORTANCE WEIGHTS Barely Fairly Good -0.2341 0.0000 0.0000 -0.1945 0.0000 0.2332 0.0000 0.2200 0.1885 0.2202 0.5543 0.2994 0.2200 0.3607 0.5194 Excellent 0.7119 0.5167 0.2200 0.6781 0.7129

interview, SQI can be computed through the following formula:

explAnAtory Models wIth decIsIon trees
In this section, we consider the construction of explanatory models for the satisfaction indices based on decision trees. Decision trees are a widely used technique to extract knowledge from data. They are based on an iterative and hierarchic partition of the training set in subsets of decreasing entropy, where the entropy is computed on the frequency distribution of the nominal variable that is to be classified. Given the tree-shaped hierarchic nature of the subset identification, the final subsets are called leaves. The variables used to split each subset into its child are then used to build, in a leaf-to-root path, the rule that identifies the subset of the training set that represents that leaf. The decision trees thus represent a particular type of rule-based classification system that partitions the training data, and associates with each element of the partition a single class of the variable that is to be classified (such variable is often referred, in the data-mining jargon, as target variable). Extensive variants of such techniques have been proposed and refined since the seminal work

SQI q = ∑∑
k =1 l =1

5

6

kl

X klq

(10)

where bkl is the parameter of the SP model corresponding to the l-th value of the k-th factor, and Xklq has value 1 if the judgement expressed by user q for factor k is at level l. Therefore, the SQI for user q is obtained by simply adding up the importance weights of the attribute levels relevant to the judgment expressed by user q. Then the overall SQI is measured by taking the individual SQI average for the sampled users. Table 7 shows the overall SQI and the contributions of each attribute. The greatest contribution to the actual SQI is given by F1 attribute, while the smaller one is given by F3 attribute. In order to obtain a relative measure of SQI, we normalized the index in this way:
0 < SQI ′ = SQI − SQI min <1 SQI max − SQI min

(11)

Table 7. Service quality index and its attributes contributions
SQI Mean Minimum Maximum 1.390 -2.667 2.840 F1 0.411 -0.744 0.712 F2 0.213 -0.513 0.517 Contributions F3 F4 0.207 0.312 0 -0.525 0.22 0.678 F5 0.247 -0.885 0.713

73

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

of Breiman et al. (Breiman, Friedman, Olshen, & Stone, 1984) on classification and regression trees. For a detailed description of this method, we address the interested reader to the large body of available literature. Here we adopt a very flexible and user-friendly data mining tool that implements several variants of decision trees, the open source software WEKA (see Witten & Frank, 2005). WEKA is a large software project developed at the University of Waikito, New Zealand, that puts together a large collection of classification and regression tools (among others, neural network support vector machines, logistic regressions, associative rules) in a common experimental environment where the user can edit and preprocess the data files. The choice of rule-based models expressed as a decision tree is driven by the objective of understanding and interpreting the relations between the satisfaction and other characteristics expressed by the customers in the survey. In particular, we adopt the J48 algorithm, a recent implementation of the classical Quinlan’s C4.5 (Quinlan, 1993). J48 allows one to control the dimension of the tree, thus avoiding potential overfitting from data, by two alternative parameters: a pruning process, controlled by a confidence factor, and a

lower bound on the minimum number of elements that can be associated with a leaf. While the latter parameter is very straightforward and has no relation with the characteristics of the data analyzed, the former is based on probabilistic models associated with each leaf’s data to reduce the size of the tree without losing predictive power, and may exhibit a more consistent behavior. In the following experiments, we tested different levels of confidence, maintaining a very small value of the lower bound on the minimum number of elements per leaf. In the previous sections, we have examined the process to build a consistent SQI using SP, and how this can provide additional information about the customer’s preference structure. When computing the SP-based SQI to a larger amount of interviews (in this work we present partial results obtained on a subset of approx. 5,400 interviews), we can then compare, for each interview, the service quality provided directly by the customer in the interview (referred to as SAT index, for overall satisfaction) and the SQI computed from the judgment expressed by the same customer on the five factors that have been considered (namely, F1, F2, F3, F4, and F5 introduced in the previous section). The two indices (the

Table 8. Description of Variables for Decision Trees
Variable Name MON TIME SEX AGE OCC NAT FLYER USER TERM FLIGHT REAS SAT SQI Description month of year time slot of flight sex of passenger age of passenger (classes) occupation of passenger nationality of passenger flying frequency (Heavy, Light) using airport frequency (Heavy, Light) Terminal type of flight (national, international) reason for travel overall satisfaction declared by passenger satisfaction computed by Stated Preference Models

74

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

SQI satisfaction index and the SAT satisfaction index) have a low degree of linear correlation; not surprisingly, also the correlation among the SAT index and the satisfaction indices associated with the service components used to compute the SQI index appears to be low. Here we construct decision trees, where the target variables are, in turn, SAT and SQI, while the explicatory variables are chosen among the set of variables measured by the survey. In Table 8, we report the complete list of the variables that was submitted to the classification algorithm. As anticipated, the SAT index is determined on a scale from 1 to 6 (1= minimum, 6=maximum). In order to compare with SQI, we rescaled both measures from 0 to 1, and then created for both

indices a dichotomous variable with a threshold value of 0.8, obtaining a binary version of the two variables (referred to as SATb and SQIb, respectively). The values of the two binary variables are indicated with GOOD (passengers with satisfaction of at least 80%) and BAD (passengers with satisfaction below 80%). The main interest of the study is towards the analysis of the extreme values, as one wants to understand what are the elements that separate the highly satisfied passengers from the rest. In particular, here we want to check whether the classification model is able to explain why the two indices are different and for what type of customers. The first experiments reported are related with the J48 decision tree, where the target variable is

Table 9. Performance of Decision Trees J48 for target variable SATb
Confidence Level 0.5 0.4 0.3 0.25 0.2 0.1 Lower Bound on Leaves 2 2 2 2 2 2 Percent Correct 95.37% 95.51% 95.61% 95.70% 95.90% 95.90% True Positive True Negative Rate Rate 94.30% 96.10% 94.20% 96.40% 93.80% 96.80% 93.80% 97.00% 93.50% 97.50% 93.50% 97.50% Number of Leaves 71 42 42 15 15 15 Training Time (secs) 0.09 0.09 0.08 0.08 0.09 0.09

Figure 1. Decision Tree for SATb
feb good 209 MAr good 188 Apr good 576 Jun good 1643/2 MAy good 214 Int.ntl Jul nAt nAtIonAl bAd 645/68 lIght Month user Int.ntl heAvy Aug nAt terM A b nAtIonAl bAd 463/10 sep oct bAd 896 bAd 63 bAd 17/1 c flyer lIght good 4 good 171/45 heAvy bAd 3 good 317/92

good 11/3

75

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

the binarized SATb index. In Table 9, we see the performance of the algorithm with different values of confidence factor. The table reports overall correct recognition rate, false positive and false negative rate, number of leaves in the corresponding tree, and time spent in the training phase. The data set was composed of 5,422 valid records, and straightforward 10-fold cross validation was used to produce the results described. The performance of the model is of very good quality, and we see a high correct recognition rate (above 95%) without relevant difference between the true positive and the true negative rates. We then analyze the structure of the tree that obtains the best recognition rate, which, in this case, corresponds to that with the lowest number of leaves. The tree is obtained with a confidence value of 0.20 or lower, and is depicted in Figure 1; attached to each leaf is the number of elements of the complete training set that are associated to that leaf (the first figure refers to the elements in the class that characterized the leaf, the second figure to those of the other class; some leaves still exhibit a nonnegligible degree of entropy, due to the effect of pruning). At first glance we note that the relevant variables for the classification are the month of the interview, the nationality of the passenger (national or international), the flying frequency and the terminal. No significant information appears to be carried by the age, the sex, the occupation, and the travel reason of the airport’s customers. It is interesting to note that

there is a strong relation between the target variable and the month of the interview; months from February to May present only GOOD level of satisfaction, while September and October are composed only of BAD records. On the other hand, we note that for the months of July and August, the national passengers are dissatisfied (even if with some degree of imprecision, as expressed by the frequency attached to the related leaves). For international passengers during the month of August, a more articulated behavior is brought to evidence: those who do not fly very often from the analyzed airport declare satisfaction (i.e., GOOD value of the target variable), while those who use the airport often are significantly dissatisfied; in particular, those leaving and arriving at one of the three terminals (A) and those that use terminal C and fly frequently (although the small amount of records in these leaves does not provide a strong significance to these leaves). The same analysis is repeated, substituting the target variable SATb with SQIb, maintaining the same set of explanatory variables. In Table 10, we report the results of the J48 algorithm for different values of the confidence factor. In this case, the separation problem appears to be slightly more difficult, as for the same level of confidence, the algorithm requires a larger number of leaves for convergence. Training time and recognition percentage confirm this evidence; moreover, the correct recognition rates are lower than those obtained in the previous model, although still

Table 10. Performance of Decision Trees J48 for target variable SQIb
Confidence Level 0.5 0.4 0.3 0.25 0.2 0.1 Lower Bound on Leaves 2 2 2 2 2 2 Percent Correct 81.92% 82.12% 80.80% 82.57% 82.55% 82.64% True Positive True Negative Rate Rate 80.70% 80.80% 81.20% 81.60% 82.40% 83.70% 83.10% 83.40% 83.40% 83.50% 82.70% 81.60% Number of Leaves 325 218 140 100 26 19 Training Time (secs) 0.44 0.2 0.24 0.22 0.2 0.55

76

The Analysis of Service Quality Through Stated Preference Models and Rule-Based Classification

balanced between true positive and true negative. Nevertheless, the best performance, obtained for a small confidence level (0.1), is above 82%, and thus the information provided by the model can definitely be considered interesting. The analysis of the tree (Figure 2) highlights several aspects of interest for the comparison of the two indices SAT and SQI in their binarized versions. At first glance we see that the important role of month is maintained in this second model; such variable is still selected by the algorithm to perform the first, and most relevant split in the training data. But here we see that the month of April turns out to be strongly characterized by dissatisfied clients (BAD value of the SQI b target variable) differently from what happened with SATb; at the same time, the month of May gets a split on the time of the day variable, where one branch, associated with the morning time slot, sees dissatisfied customers in terminals A and C and satisfied customers in terminal B. The other branch, associated with afternoon and nighttime slots, shows an unusual pattern of dissatisfied international customers and satisfied national ones. As in the SATb model, in the month of July the national passengers are dissatisfied. The

rest of the tree is substantially equivalent to that derived for the SATb model; the month of August is split in the same nodes, as well as September and October. The comparison of the two trees highlights, with a certain precision, the few structural differences between the two indices. The main conclusion is that the differences are restricted to the months of April and May. In these 2 months it appears that the direct evaluation of the satisfaction given by the customers is somewhat optimistic with respect to the more refined index obtained by the SP method. Such results could be properly interpreted by the users of the survey; one possible interpretation may be related with the way the questionnaires were submitted in those periods, or with the particular type of traffic there present. Of more interest is the coherent structure of both trees for the months of July and August, where the airport traffic is typically characterized by a larger percentage of leisure travelers and international traffic. In both months, the dissatisfaction expressed by national travelers emerges with strong evidence. Analogously, we record the negative characterization of September and October; although, here we see that while the

Figure 2. Decision Tree for SATb
feb good 209 MAr good 188 Apr good 576 Jun good 1643/2 MAy good 214 Int.ntl Jul nAt nAtIonAl bAd 645/68 lIght Month user Int.ntl heAvy Aug nAt terM A b nAtIonAl bAd 463/10 sep oct bAd 896 bAd 63 bAd 17/1 c flyer lIght good 4 good 171/45 heAvy bAd 3 good 317/92

good 11/3

77

