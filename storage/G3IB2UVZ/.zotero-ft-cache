Wesley Chu, Tsau Young Lin (Eds.) Foundations and Advances in Data Mining

Studies in Fuzziness and Soft Computing, Volume 180
Editor-in-chief Prof. Janusz Kacprzyk Systems Research Institute Polish Academy of Sciences ul. Newelska 6 01-447 Warsaw Poland E-mail: kacprzyk@ibspan.waw.pl Further volumes of this series can be found on our homepage: springeronline.com
Vol. 165. A.F. Rocha, E. Massad, A. Pereira Jr. The Brain: From Fuzzy Arithmetic to Quantum Computing, 2005 ISBN 3-540-21858-0 Vol. 166. W.E. Hart, N. Krasnogor, J.E. Smith (Eds.) Recent Advances in Memetic Algorithms, 2005 ISBN 3-540-22904-3 Vol. 167. Y. Jin (Ed.) Knowledge Incorporation in Evolutionary Computation, 2005 ISBN 3-540-22902-7 Vol. 168. Yap P. Tan, Kim H. Yap, Lipo Wang (Eds.) Intelligent Multimedia Processing with Soft Computing, 2005 ISBN 3-540-22902-7 Vol. 169. C.R. Bector, Suresh Chandra Fuzzy Mathematical Programming and Fuzzy Matrix Games, 2005 ISBN 3-540-23729-1 Vol. 170. Martin Pelikan Hierarchical Bayesian Optimization Algorithm, 2005 ISBN 3-540-23774-7 Vol. 171. James J. Buckley Simulating Fuzzy Systems, 2005 ISBN 3-540-24116-7 Vol. 173. Bogdan Gabrys, Kauko Leiviskä, Jens Strackeljan (Eds.) Do Smart Adaptive Systems Exist?, 2005 ISBN 3-540-24077-2 Vol. 174. Mircea Negoita, Daniel Neagu, Vasile Palade Computational Intelligence: Engineering of Hybrid Systems, 2005 ISBN 3-540-23219-2 Vol. 175. Anna Maria Gil-Lafuente Fuzzy Logic in Financial Analysis, 2005 ISBN 3-540-23213-3 Vol. 176. Udo Seiffert, Lakhmi C. Jain, Patric Schweizer (Eds.) Bioinformatics Using Computational Intelligence Paradigms, 2005 ISBN 3-540-22901-9 Vol. 177. Lipo Wang (Ed.) Support Vector Machines: Theory and Applications, 2005 ISBN 3-540-24388-7 Vol. 178. Claude Ghaoui, Mitu Jain, Vivek Bannore, Lakhmi C. Jain (Eds.) Knowledge-Based Virtual Education, 2005 ISBN 3-540-25045-X Vol. 179. Mircea Negoita, Bernd Reusch (Eds.) Real World Applications of Computational Intelligence, 2005 ISBN 3-540-25006-9 Vol. 180. Wesley Chu, Tsau Young Lin (Eds.) Foundations and Advances in Data Mining, 2005 ISBN 3-540-25057-3

Wesley Chu Tsau Young Lin (Eds.)

Foundations and Advances in Data Mining

ABC

Professor Wesley Chu
University of California at Los Angeles Department of Computer Science 3731H Boelter Hall Los Angeles, CA 90095-1596 USA E-mail: wwc@cs.ucla.edu

Professor Tsau Young Lin
San Jose State University Dept. Mathematics and Computer Science San Jose, CA 95192-0103 USA E-mail: tylin@mathcs.sjsu.edu

Library of Congress Control Number: 2005921893

ISSN print edition: 1434-9922 ISSN electronic edition: 1860-0808 ISBN-10 3-540-25057-3 Springer Berlin Heidelberg New York ISBN-13 978-3-540-25057-9 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlm or in any other way, and storage in data banks. Duplication of this publication or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and permission for use must always be obtained from Springer. Violations are liable for prosecution under the German Copyright Law. Springer is a part of Springer Science+Business Media springeronline.com c Springer-Verlag Berlin Heidelberg 2005 Printed in will follow The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.
A Typesetting: by the authors and TechBooks using a Springer L TEX macro package Cover design: E. Kirchner, Springer Heidelberg

Printed on acid-free paper

SPIN: 11362197

89/TechBooks

543210

Preface

With the growing use of information technology and the recent advances in web systems, the amount of data available to users has increased exponentially. Thus, there is a critical need to understand the content of the data. As a result, data-mining has become a popular research topic in recent years for the treatment of the “data rich and information poor” syndrome. Currently, application oriented engineers are only concerned with their immediate problems, which results in an ad hoc method of problem solving. Researchers, on the other hand, lack an understanding of the practical issues of data-mining for real-world problems and often concentrate on issues (e.g. incremental performance improvements) that are of no signiﬁcance to the practitioners. In this volume, we hope to remedy these problems by (1) presenting a theoretical foundation of data-mining, and (2) providing important new directions for data-mining research. We have invited a set of well respected data mining theoreticians to present their views on the fundamental science of data mining. We have also called on researchers with practical data mining experiences to present new important data-mining topics. This book is organized into two parts. The ﬁrst part consists of four chapters presenting the foundations of data mining, which describe the theoretical point of view and the capabilities and limits of current available mining techniques. The second part consists of seven chapters which discuss the new data mining topics. The ﬁrst part of the book includes four chapters. The ﬁrst chapter, authored by T. Poggio and S. Smale, is entitled “The Mathematics of Learning: Dealing with Data.” The authors present the mathematical formula of learning theory. In particular, they present an algorithm for supervised learning by training sets and show that the algorithm performs well in a number of applications involving regression as well as binary classiﬁcation. The second chapter, by H. Tsukimoto, is entitled “Logical Regression Analysis: From Mathematical Formulas to Linguistic Rules.” He presents a solution for solving the accurate prediction and comprehensive rules in supervised learning. The author has developed a data mining technique called Logical Regression Analysis

VI

Preface

which consists of regression analysis, and the Approximation Method, that can provide comprehensive rules and also accurate prediction. The paper also shows how to apply the techniques for mining images. The third chapter, by T.Y. Lin, is entitled “A Feature/Attribute Theory for Association Mining and Constructing the Complete Feature Set” The author points out the importance of selecting correct attributes in data mining and develops a theory of features for association mining (AM). Based on the isomorphism theorem in AM, he concludes that it is suﬃcient to perform AM in canonical models, and constructs the complete feature set for every canonical model. Using the isomorphism theorem, the complete feature set can be derived for each relation. Though the number of possible features is enormous, it can be shown that the high frequency patterns features can be derived within polynomial time. The fourth chapter is entitled “A new theoretical framework for K-means-type clustering,” and is authored by J. Peng and Y. Xia. The authors present generalized K-means type clustering method as the 0–1 semi-deﬁnite programming (SDP). The classical K-means algorithm, minimal sum of squares (MSSC), can be interpreted as a special heuristic. Moreover, the 0–1 SDP model can be further approximated by the relaxed and polynomially solvable linear and semi-deﬁnite programming. The 0–1 SDP model can be applied to MSCC and to other scenarios of clustering as well. The second part of the book, from Chaps. 5 to 11, present seven topics covering recent advances in data mining. Chapter 5, entitled “Clustering via Decision Tree Construction,” is authored by B. Liu, Y. Xia, and P. Yu. They propose a novel clustering technique based on supervised learning called decision tree construction. The key idea is to use a decision tree to partition the data space into cluster (or dense) regions and empty (or sparse) regions (which produce outliers and anomalies). This technique is able to ﬁnd “natural” clusters in large high dimensional spaces eﬃciently. Experimental data shows that this technique is eﬀective and scales well for large high dimensional datasets. Chapter 6, “Incremental Mining on Association Rules,” is written by Wei-Guang Teng and Ming-Syan Chen. Due to the increasing use of the record-based databases where data is being constantly added, incremental mining is needed to keep the knowledge current. The authors propose incremental mining techniques to update the data mining on association rules. Chapter 7, is entitled “Mining Association Rules from Tabular Data Guided by Maximal Frequent Itemsets” and authored by Q. Zou, Y. Chen, W. Chu, and X. Lu. Since many scientiﬁc applications are in tabular format, the authors propose to use the maximum frequency itemset (MFI) as a road map to guide us towards generating association rules from tabular dataset. They propose to use information from previous searches to generate MFI and the experimental results show that such an approach to generating MFI yields signiﬁcant improvements over conventional methods. Further, using tabular format rather than transaction data set to derive MFI can reduce the search space and the time needed for support-counting. The authors use spreadsheet to present rules and use spreadsheet operations to sort and select rules, which

Preface

VII

is a very convenient way to query and organize rules in a hierarchical fashion. An example was also given to illustrate the process of generating association rules from the tabular dataset using past medical surgery data to aid surgeons in their decision making. Chapter 8, entitled “Sequential Pattern Mining by Pattern-Growth: Principles and Extensions,” presents the sequential pattern growth method and studies the principles and extensions of the method such as (1) mining constraint-based sequential patterns, (2) mining multi-level, multi dimensional sequential patters, and (3) mining top-k closed sequential patterns. They also discuss the applications in bio-sequence pattern analysis and clustering sequences. Chapter 9, entitled “Web Page Classiﬁcation,” is written by B. Choi and Z. Yao. It describes systems that automatically classify web pages into meaningful subject-based and genre-based categories. The authors describe tools for building automatic web page classiﬁcation systems, which are essential for web mining and constructing semantic web. Chapter 10 is entitled “Web Mining – Concepts, Applications, and Research Directions,” and was written by Jaideep Srivastava, Prasanna Desikan, and Vipin Kumar. The authors present the application of data mining techniques to extract knowledge from web content, structure, and usage. An overview of accomplishments in technology and applications in web mining is also included. Chapter 11, by Chris Clifton, Murat Kantarcioglu, and Jaideep Vaidya is entitled, “Privacy-Preserving Data Mining.” The goal of privacy-preserving data mining is to develop data mining models that do not increase the risk of misuse of the data used to generate those models. The author presents two classes of privacy-preserving data-mining. The ﬁrst is based on adding noise to the data before providing it to the data miner. Since real data values are not revealed, individual privacy is preserved. The second class is derived from the cryptographic community. The data sources collaborate to obtain data mining results without revealing anything except those results. Finally, we would like to thank the authors for contributing their work in the volume and the reviewers for commenting on the readability and accuracy of the work. We hope the theories presented in this volume will give data mining practitioners a scientiﬁc perspective in data mining and thus provide more insight into their problems. We also hope that the new data mining topics will stimulate further research in these important directions. California March 2005 Wesley W. Chu Tsau Young Lin

Contents

Part I Foundations of Data Mining The Mathematics of Learning: Dealing with Data T. Poggio, S. Smale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

Logical Regression Analysis: From Mathematical Formulas to Linguistic Rules H. Tsukimoto . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 A Feature/Attribute Theory for Association Mining and Constructing the Complete Feature Set Tsau Young Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 A New Theoretical Framework for K-Means-Type Clustering J. Peng, Y. Xia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

Part II Recent Advances in Data Mining Clustering Via Decision Tree Construction B. Liu, Y. Xia, P.S. Yu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 Incremental Mining on Association Rules W.-G. Teng, M.-S. Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 Mining Association Rules from Tabular Data Guided by Maximal Frequent Itemsets Q. Zou, Y. Chen, W.W. Chu, X. Lu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163

X

Contents

Sequential Pattern Mining by Pattern-Growth: Principles and Extensions J. Han, J. Pei, X. Yan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 Web Page Classiﬁcation B. Choi, Z. Yao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 Web Mining – Concepts, Applications and Research Directions J. Srivastava, P. Desikan, V. Kumar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 Privacy-Preserving Data Mining C. Clifton, M. Kantarcıoˇlu, J. Vaidya . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309 g

Part I

Foundations of Data Mining

The Mathematics of Learning: Dealing with Data
T. Poggio1 and S. Smale2
1

2

CBCL, McGovern Institute, Artiﬁcial Intelligence Lab, BCS, MIT tp@ai.mit.edu Toyota Technological Institute at Chicago and Professor in the Graduate School, University of California, Berkeley smale@math.berkeley.edu

Summary. Learning is key to developing systems tailored to a broad range of data analysis and information extraction tasks. We outline the mathematical foundations of learning theory and describe a key algorithm of it.

1 Introduction
The problem of understanding intelligence is said to be the greatest problem in science today and “the” problem for this century – as deciphering the genetic code was for the second half of the last one. Arguably, the problem of learning represents a gateway to understanding intelligence in brains and machines, to discovering how the human brain works and to making intelligent machines that learn from experience and improve their competences as children do. In engineering, learning techniques would make it possible to develop software that can be quickly customized to deal with the increasing amount of information and the ﬂood of data around us. Examples abound. During the last decades, experiments in particle physics have produced a very large amount of data. Genome sequencing is doing the same in biology. The Internet is a vast repository of disparate information which changes rapidly and grows at an exponential rate: it is now signiﬁcantly more than 100 Terabytes, while the Library of Congress is about 20 Terabytes. We believe that a set of techniques, based on a new area of science and engineering becoming known as “supervised learning” – will become a key technology to extract information from the ocean of bits around us and make sense of it. Supervised learning, or learning-from-examples, refers to systems that are trained, instead of programmed, with a set of examples, that is a set of input-output pairs. Systems that could learn from example to perform a speciﬁc task would have many
This paper is reprinted from Notices of the AMS, 50(5), 2003, pp. 537–544.
T. Poggio and S. Smale: The Mathematics of Learning: Dealing with Data, StudFuzz 180, 3–19 (2005) c Springer-Verlag Berlin Heidelberg 2005 www.springerlink.com

4

T. Poggio and S. Smale

applications. A bank may use a program to screen loan applications and approve the “good” ones. Such a system would be trained with a set of data from previous loan applications and the experience with their defaults. In this example, a loan application is a point in a multidimensional space of variables characterizing its properties; its associated output is a binary “good” or “bad” label. In another example, a car manufacturer may want to have in its models, a system to detect pedestrians that may be about to cross the road to alert the driver of a possible danger while driving in downtown traﬃc. Such a system could be trained with positive and negative examples: images of pedestrians and images without people. In fact, software trained in this way with thousands of images has been recently tested in an experimental car of Daimler. It runs on a PC in the trunk and looks at the road in front of the car through a digital camera [26, 36, 43]. Algorithms have been developed that can produce a diagnosis of the type of cancer from a set of measurements of the expression level of many thousands human genes in a biopsy of the tumor measured with a cDNA microarray containing probes for a number of genes [46]. Again, the software learns the classiﬁcation rule from a set of examples, that is from examples of expression patterns in a number of patients with known diagnoses. The challenge, in this case, is the high dimensionality of the input space – in the order of 20,000 genes – and the small number of examples available for training – around 50. In the future, similar learning techniques may be capable of some learning of a language and, in particular, to translate information from one language to another. What we assume in the above examples is a machine that is trained, instead of programmed, to perform a task, given data of the form (xi , yi )m . Training means synthesizing a function i=1 that best represents the relation between the inputs xi and the corresponding outputs yi . The central question of learning theory is how well this function generalizes, that is how well it estimates the outputs for previously unseen inputs. As we will see later more formally, learning techniques are similar to ﬁtting a multivariate function to a certain number of measurement data. The key point, as we just mentioned, is that the ﬁtting should be predictive, in the same way that ﬁtting experimental data (see Fig. 1) from an experiment in physics can in principle uncover the underlying physical law, which is then used in a predictive way. In this sense, learning is also a principled method for distilling predictive and therefore scientiﬁc “theories” from the data. We begin by presenting a simple “regularization” algorithm which is important in learning theory and its applications. We then outline brieﬂy some of its applications and its performance. Next we provide a compact derivation of it. We then provide general theoretical foundations of learning theory. In particular, we outline the key ideas of decomposing the generalization error of a solution of the learning problem into a sample and an approximation error component. Thus both probability theory and approximation theory play key roles in learning theory. We apply the two theoretical bounds to the algorithm and describe for it the tradeoﬀ – which is key in learning theory and its

The Mathematics of Learning: Dealing with Data

5

Fig. 1. How can we learn a function which is capable of generalization – among the many functions which ﬁt the examples equally well (here m = 7)?

applications – between number of examples and complexity of the hypothesis space. We conclude with several remarks, both with an eye to history and to open problems for the future.

2 A Key Algorithm
2.1 The Algorithm How can we ﬁt the “training” set of data Sm = (xi , yi )m with a function i=1 f : X → Y – with X a closed subset of IRn and Y ⊂ IR – that generalizes, eg is predictive? Here is an algorithm which does just that and which is almost magical for its simplicity and eﬀectiveness: 1. Start with data (xi , yi )m . i=1 2. Choose a symmetric, positive deﬁnite function Kx (x ) = K(x, x ), continun ous on X × X. A kernel K(t, s) is positive deﬁnite if i,j=1 ci cj K(ti , tj ) ≥ 0 for any n ∈ IN and choice of t1 , . . . , tn ∈ X and c1 , . . . , cn ∈ IR. An example of such a Mercer kernel is the Gaussian K(x, x ) = e− restricted to X × X.
x−x 2 2σ 2

.

(1)

6

T. Poggio and S. Smale
m

3. Set f : X → Y to f (x) =

ci Kxi (x) .
i=1

(2)

where c = (c1 , . . . , cm ) and (mγI + K)c = y (3)

where I is the identity matrix, K is the square positive deﬁnite matrix with elements Ki,j = K(xi , xj ) and y is the vector with coordinates yi . The parameter γ is a positive, real number. The linear system of (3) in m variables is well-posed since K is positive and (mγI + K) is strictly positive. The condition number is good if mγ is large. This type of equations has been studied since Gauss and the algorithms for solving it eﬃciently represent one the most developed areas in numerical and computational analysis. What does (2) say? In the case of Gaussian kernel, the equation approximates the unknown function by a weighted superposition of Gaussian “blobs”, each centered at the location xi of one of the m examples. The weight ci of each Gaussian is such to minimize a regularized empirical error, that is the error on the training set. The σ of the Gaussian (together with γ, see later) controls the degree of smoothing, of noise tolerance and of generalization. Notice that for Gaussians with σ → 0 the representation of (2) eﬀectively becomes a “look-up” table that cannot generalize (it provides the correct y = yi only when x = xi and otherwise outputs 0). 2.2 Performance and Examples The algorithm performs well in a number of applications involving regression as well as binary classiﬁcation. In the latter case the yi of the training set (xi , yi )m take the values {−1, +1}; the predicted label is then {−1, +1}, dei=1 pending on the sign of the function f of (2). Regression applications are the oldest. Typically they involved ﬁtting data in a small number of dimensions [44, 45, 53]. More recently, they also included typical learning applications, sometimes with a very high dimensionality. One example is the use of algorithms in computer graphics for synthesizing new images and videos [5, 20, 38]. The inverse problem of estimating facial expression and object pose from an image is another successful application [25]. Still another case is the control of mechanical arms. There are also applications in ﬁnance, as, for instance, the estimation of the price of derivative securities, such as stock options. In this case, the algorithm replaces the classical Black-Scholes equation (derived from ﬁrst principles) by learning the map from an input space (volatility, underlying stock price, time to expiration of the option etc.) to the output space (the price of the option) from historical data [27]. Binary classiﬁcation applications abound. The algorithm was used to perform binary classiﬁcation on a

The Mathematics of Learning: Dealing with Data

7

number of problems [7, 34]. It was also used to perform visual object recognition in a view-independent way and in particular face recognition and sex categorization from face images [8, 39]. Other applications span bioinformatics for classiﬁcation of human cancer from microarray data, text summarization, sound classiﬁcation1 Surprisingly, it has been realized quite recently that the same linear algorithm not only works well but is fully comparable in binary classiﬁcation problems to the most popular classiﬁers of today (that turn out to be of the same family, see later). 2.3 Derivation The algorithm described can be derived from Tikhonov regularization. To ﬁnd the minimizer of the the error we may try to solve the problem – called Empirical Risk Minimization (ERM) – of ﬁnding the function in H which minimizes 1 m
m

(f (xi ) − yi )2
i=1

which is in general ill-posed, depending on the choice of the hypothesis space H. Following Tikhonov (see for instance [19]) we minimize, instead, over the hypothesis space HK , for a ﬁxed positive parameter γ, the regularized functional m 1 (yi − f (xi ))2 + γ f 2 , (4) K m i=1 where f 2 is the norm in HK – the Reproducing Kernel Hilbert Space K (RKHS), deﬁned by the kernel K. The last term in (4) – called regularizer – forces, as we will see, smoothness and uniqueness of the solution. Let us ﬁrst deﬁne the norm f 2 . Consider the space of the linear span of Kxj . We use K xj to emphasize that the elements of X used in this construction do not have anything to do in general with the training set (xi )m . Deﬁne an inner i=1 product in this space by setting Kx , Kxj = K(x, xj ) and extend linearly r to j=1 aj Kxj . The completion of the space in the associated norm is the RKHS, that is a Hilbert space HK with the norm f 2 (see [2, 10]). Note K that f, Kx = f (x) for f ∈ HK (just let f = Kxj and extend linearly). To minimize the functional in (4) we take the functional derivative with respect to f , apply it to an element f of the RKHS and set it equal to 0. We obtain 1 m
1

m

(yi − f (xi ))f (xi ) − γ f, f = 0 .
i=1

(5)

The very closely related Support Vector Machine (SVM) classiﬁer was used for the same family of applications, and in particular for bioinformatics and for face recognition and car and pedestrian detection [25, 46].

8

T. Poggio and S. Smale

Equation (5) must be valid for any f . In particular, setting f = Kx gives
m

f (x) =
i=1

ci Kxi (x)

(6)

where ci =

yi − f (xi ) mγ

(7)

since f, Kx = f (x). Equation (3) then follows, by substituting (6) into (7). Notice also that essentially the same derivation for a generic loss function V (y, f (x)), instead of (f (x) − y)2 , yields the same (6), but (3) is now diﬀerent and, in general, nonlinear, depending on the form of V . In particular, the popular Support Vector Machine (SVM) regression and SVM classiﬁcation algorithms correspond to special choices of non-quadratic V , one to provide a “robust” measure of error and the other to approximate the ideal loss function corresponding to binary (miss)classiﬁcation. In both cases, the solution is still of the same form of (6) for any choice of the kernel K. The coeﬃcients ci are not given anymore by (7) but must be found solving a quadratic programming problem.

3 Theory
We give some further justiﬁcation of the algorithm by sketching very brieﬂy its foundations in some basic ideas of learning theory. Here the data (xi , yi )m i=1 is supposed random, so that there is an unknown probability measure ρ on the product space X × Y from which the data is drawn. This measure ρ deﬁnes a function (8) fρ : X → Y satisfying fρ (x) = ydρx , where ρx is the conditional measure on x×Y . From this construction fρ can be said to be the true input-output function reﬂecting the environment which produces the data. Thus a measurement of the error of f is (f − fρ )2 dρX
X

(9)

where ρX is the measure on X induced by ρ (sometimes called the marginal measure). The goal of learning theory might be said to “ﬁnd” f minimizing this error. Now to search for such an f , it is important to have a space H – the hypothesis space – in which to work (“learning does not take place in a vacuum”). Thus consider a convex space of continuous functions f : X → Y , (remember Y ⊂ IR) which as a subset of C(X) is compact, and where C(X) is the Banach space of continuous functions with ||f || = maxX |f (x)|. A basic example is (10) H = IK (BR )

The Mathematics of Learning: Dealing with Data

9

where IK : HK → C(X) is the inclusion and BR is the ball of radius R in HK . m 1 Starting from the data (xi , yi )m = z one may minimize m i=1 (f (xi ) − yi )2 i=1 over f ∈ H to obtain a unique hypothesis fz : X → Y . This fz is called the empirical optimum and we may focus on the problem of estimating (fz − fρ )2 dρX
X

(11)

It is useful towards this end to break the problem into steps by deﬁning a “true optimum” fH relative to H, by taking the minimum over H of X (f − fρ )2 . Thus we may exhibit (fz − fρ )2 = S(z, H) +
X X

(fH − fρ )2 = S(z, H) + A(H)

(12)

where S(z, H) =
X

(fz − fρ )2 −
X

(fH − fρ )2

(13)

The ﬁrst term, (S) on the right in (12) must be estimated in probability over z and the estimate is called the sample errror (sometime also the estimation error). It is naturally studied in the theory of probability and of empirical processes [16, 30, 31]. The second term (A) is dealt with via approximation theory (see [15] and [12, 13, 14, 32, 33]) and is called the approximation error. The decomposition of (12) is related, but not equivalent, to the well known bias (A) and variance (S) decomposition in statistics. 3.1 Sample Error First consider an estimate for the sample error, which will have the form: S(z, H) ≤ (14)

with high conﬁdence, this conﬁdence depending on and on the sample size m. Let us be more precise. Recall that the covering number or Cov#(H, η) is the number of balls in H of radius η needed to cover H. Theorem 3.1 Suppose |f (x) − y| ≤ M for all f ∈ H for almost all (x, y) ∈ X × Y . Then P robz∈(X×Y )m {S(z, H) ≤ } ≤ 1 − δ where δ = Cov#(H, 24M )e− 288M 2 .
m

The result is Theorem C ∗ of [10], but earlier versions (usually without a topology on H) have been proved by others, especially Vapnik, who formulated the notion of VC dimension to measure the complexity of the hypothesis space for the case of {0, 1} functions. In a typical situation of Theorem 3.1 the hypothesis space H is taken to be as in (10), where BR is the ball of radius R in a Reproducing Kernel Hilbert Space (RKHS) with a smooth K (or in a Sobolev

10

T. Poggio and S. Smale

space). In this context, R plays an analogous role to VC dimension[50]. Estimates for the covering numbers in these cases were provided by Cucker, Smale and Zhou [10, 54, 55]. The proof of Theorem 3.1 starts from Hoeﬀding inequality (which can be regarded as an exponential version of Chebyshev’s inequality of probability theory). One applies this estimate to the function X × Y → IR which takes (x, y) to (f (x) − y)2 . Then extending the estimate to the set of f ∈ H introduces the covering number into the picture. With a little more work, Theorem 3.1 is obtained. 3.2 Approximation Error The approximation error X (fH − fρ )2 may be studied as follows. Suppose B : L2 → L2 is a compact, strictly positive (selfadjoint) operator. Then let E be the Hilbert space {g ∈ L2 , B −s g < ∞} with inner product g, h E = B −s g, B −s h L2 . Suppose moreover that E → L2 factors as E → C(X) → L2 with the inclusion JE : E → C(X) well deﬁned and compact. Let H be JE (BR ) when BR is the ball of radius R in E. A theorem on the approximation error is Theorem 3.2 Let 0 < r < s and H be as above. Then fρ − fH
2

≤

1 R

2r s−r

B −r fρ

2s s−r

We now use · for the norm in the space of square integrable functions on 1/2 X, with measure ρX . For our main example of RKHS, take B = LK , where K is a Mercer kernel and LK f (x) =
X

f (x )K(x, x )

(15)

and we have taken the square root of the operator LK . In this case E is HK as above. Details and proofs may be found in [10] and in [48]. 3.3 Sample and Approximation Error for the Regularization Algorithm The previous discussion depends upon a compact hypothesis space H from which the experimental optimum fz and the true optimum fH are taken. In the key algorithm of Sect. 2, the optimization is done over all f ∈ HK with a regularized error function. The error analysis of Sects. 3.1 and 3.2 must therefore be extended. Thus let fγ,z be the empirical optimum for the regularized problem as exhibited in (4)

The Mathematics of Learning: Dealing with Data

11

1 m Then

m

(yi − f (xi ))2 + γ f
i=1

2 K

.

(16)

(fγ,z − fρ )2 ≤ S(γ) + A(γ) where A(γ) (the approximation error in this context) is A(γ) = γ 1/2 LK 4 fρ and the sample error is S(γ) = 32M 2 (γ + C)2 ∗ v (m, δ) γ2
−1 2

(17)

(18)

(19)

where v ∗ (m, δ) is the unique solution of m 3 v − ln 4 4m δ v − c1 = 0 . (20)

Here C, c1 > 0 depend only on X and K. For the proof one reduces to the case of compact H and applies Theorems 3.1 and 3.2. Thus ﬁnding the optimal solution is equivalent to ﬁnding the best tradeoﬀ between A and S for a given m. In our case, this bias-variance problem is to minimize S(γ) + A(γ) over γ > 0. There is a unique solution – a best γ – for the choice in (4). For this result and its consequences see [11].

4 Remarks
The Tradeoﬀ between Sample Complexity and Hypothesis Space Complexity For a given, ﬁxed hypothesis space H only the sample error component of the error of fz can be be controlled (in (12) only S(z, H) depends on the data). In this view, convergence of S to zero as the number of data increases (Theorem 3.1) is then the central problem in learning. Vapnik called consistency of ERM (eg convergence of the empirical error to the true error) the key problem in learning theory and in fact much modern work has focused on reﬁning the necessary and suﬃcient conditions for consistency of ERM (the uniform Glivenko-Cantelli property of H, ﬁnite Vγ dimension for γ > 0 etc., see [19]). More generally, however, there is a tradeoﬀ between minimizing the sample error and minimizing the approximation error – what we referred to as the bias-variance problem. Increasing the number of data points m decreases the sample error. The eﬀect of increasing the complexity of the hypothesis space is trickier. Usually the approximation error decreases but the sample error increases. This means that there is an optimal complexity of the hypothesis

12

T. Poggio and S. Smale

space for a given number of training data. In the case of the regularization algorithm described in this paper this tradeoﬀ corresponds to an optimum value for γ as studied by [3, 11, 35]. In empirical work, the optimum value is often found through cross-validation techniques [53]. This tradeoﬀ between approximation error and sample error is probably the most critical issue in determining good performance on a given problem. The class of regularization algorithms, such as (4), shows clearly that it is also a tradeoﬀ – quoting Girosi – between the curse of dimensionality (not enough examples) and the blessing of smoothness (which decreases the eﬀective “dimensionality” eg the complexity of the hypothesis space) through the parameter γ. The Regularization Algorithm and Support Vector Machines. There is nothing to stop us from using the algorithm we described in this paper – that is square loss regularization – for binary classiﬁcation. Whereas SVM classiﬁcation arose from using – with binary y – the loss function V (f (x, y)) = (1 − yf (x))+ , we can perform least-squares regularized classiﬁcation via the loss function V (f (x, y)) = (f (x) − y)2 . This classiﬁcation scheme was used at least as early as 1989 (for reviews see [7, 40] and then rediscovered again by many others (see [21, 49]), including Mangasarian (who refers to square loss regularization as “proximal vector machines”) and Suykens (who uses the name “least square SVMs”). Rifkin [47] has conﬁrmed the interesting empirical results by Mangasarian and Suykens: “classical” square loss regularization works well also for binary classiﬁcation (examples are in Tables 1 and 2). In references to supervised learning the Support Vector Machine method is often described (see for instance a recent issue of the Notices of the AMS [28]) according to the “traditional” approach, introduced by Vapnik and followed by almost everybody else. In this approach, one starts with the concepts of separating hyperplanes and margin. Given the data, one searches for the linear hyperplane that separates the positive and the negative examples, assumed to be linearly separable, with the largest margin (the margin is deﬁned as
Table 1. A comparison of SVM and RLSC (Regularized Least Squares Classiﬁcation) accuracy on a multiclass classiﬁcation task (the 20newsgroups dataset with 20 classes and high dimensionality, around 50,000), performed using the standard “one vs. all” scheme based on the use of binary classiﬁers. The top row indicates the number of documents/class used for training. Entries in the table are the fraction of misclassiﬁed documents. From [47] 800 250 100 30 SVM RLSC SVM RLSC SVM RLSC SVM RLSC 0.131 0.129 0.167 0.165 0.214 0.211 0.311 0.309

The Mathematics of Learning: Dealing with Data

13

Table 2. A comparison of SVM and RLSC accuracy on another multiclass classiﬁcation task (the sector105 dataset, consisting of 105 classes with dimensionality about 50, 000). The top row indicates the number of documents/class used for training. Entries in the table are the fraction of misclassiﬁed documents. From [47] 52 20 10 3 SVM RLSC SVM RLSC SVM RLSC SVM RLSC 0.072 0.066 0.176 0.169 0.341 0.335 0.650 0.648

the distance from the hyperplane to the nearest example). Most articles and books follow this approach, go from the separable to the non-separable case and use a so-called “kernel trick” (!) to extend it to the nonlinear case. SVM for classiﬁcation was introduced by Vapnik in the linear, separable case in terms of maximizing the margin. In the non-separable case, the margin motivation loses most of its meaning. A more general and simpler framework for deriving SVM algorithms for classiﬁcation and regression is to regard them as special cases of regularization and follow the treatment of Sect. 2. In the case of linear functions f (x) = w · x and separable data, maximizing the margin is exactly 1 equivalent to maximizing ||w|| , which is in turn equivalent to minimizing ||w||2 , which corresponds to minimizing the RKHS norm. The Regularization Algorithm and Learning Theory. The Mercer theorem was introduced in learning theory by Vapnik and RKHS by Girosi [22] and later by Vapnik [9, 50]. Poggio and Girosi [23, 40, 41] had introduced Tikhonov regularization in learning theory (the reformulation of Support Vector Machines as a special case of regularization can be found in [19]). Earlier, Gaussian Radial Basis Functions were proposed as an alternative to neural networks by Broomhead and Loewe. Of course, RKHS had been pioneered by Parzen and Wahba [37, 53] for applications closely related to learning, including data smoothing (for image processing and computer vision, see [4, 42]). A Bayesian Interpretation. The learning algorithm (4) has an interesting Bayesian interpretation [52, 53]: the data term – that is the ﬁrst term with the quadratic loss function – is a model of (Gaussian, additive) noise and the RKHS norm (the stabilizer) corresponds to a prior probability on the hypothesis space H. Let us deﬁne P [f |Sm ] as the conditional probability of the function f given the training examples Sm = (xi , yi )m , P [Sm |f ] as the i=1 conditional probability of Sm given f , i.e. a model of the noise, and P [f ] as the a priori probability of the random ﬁeld f . Then Bayes theorem provides the posterior distribution as P [f |Sm ] = P [Sm |f ] P [f ] . P (Sm )

If the noise is normally distributed with variance σ, then the probability P [Sm |f ] is 1 − 12 m (yi −f (x ))2 i=1 i P [Sm |f ] = e 2σ ZL

14

T. Poggio and S. Smale
1 − f Zr e
2 K

where ZL is a normalization constant. If P [f ] = normalization constant, then P [f |Sm ] =
1 1 e−( 2σ2 ZD ZL Zr

where Zr is another
2 K

m 2 i=1 (yi −f (xi )) +

f

).

One of the several possible estimates of f from P [f |Sm ] is the so called Maximum A Posteriori (MAP) estimate, that is
m

max P [f |Sm ] = min
i=1

(yi − f (xi ))2 + 2σ 2 f

2 K

.

which is the same as the regularization functional, if λ = 2σ 2 /m (for details and extensions to models of non Gaussian noise and diﬀerent loss functions see [19]). Necessary and Suﬃcient Conditions for Learnability. Compactness of the hypothesis space H is suﬃcient for consistency of ERM, that is for bounds of the type of Theorem 3.1 on the sample error. The necessary and suﬃcient condition is that H is a uniform Glivenko-Cantelli class of functions, in which case no speciﬁc topology is assumed for H2 . There are several equivalent conditions on H such as ﬁniteness of the Vγ dimension for all positive γ (which reduces to ﬁniteness of the VC dimension for {0, 1} functions)3 . We saw earlier that the regularization algorithm (4) ensures (through the resulting compactness of the “eﬀective” hypothesis space) well-posedness of the problem. It also yields convergence of the empirical error to the true error (eg bounds such as Theorem 3.1). An open question is whether there is a connection between well-posedness and consistency. For well-posedness the critical condition is usually stability of the solution. In the learning problem, this condition refers to stability of the solution of ERM with respect to small changes of the training set Sm . In a similar way, the condition number (see [6] and especially [29]) characterizes the stability of the solution of (3). Is it possible that some
Deﬁnition: Let F be a class of functions f . F is a uniform Glivenko-Cantelli class if for every ε > 0
2

m→∞

lim sup IP
ρ

f ∈F

sup |Eρm f − Eρ f | > ε

=0.

(21)

where ρn is the empirical measure supported on a set x1 , . . . , xn . 3 In [1] – following [17, 51] – a necessary and suﬃcient condition is proved for uniform convergence of |Iemp [f ] − Iexp [f ]|, in terms of the ﬁniteness for all γ > 0 of a combinatorial quantity called Vγ dimension of F (which is the set V (x), f (x), f ∈ H), under some assumptions on V . The result is based on a necessary and suﬃcient (distribution independent) condition proved by [51] which uses the metric entropy of F deﬁned as Hm ( , F ) = supxm ∈X m log N ( , F , xm ), where N ( , F , xm ) is the ∞ ∞ covering of F wrt lxm ( lxm is the l∞ distance on the points xm ): Theorem (Dudley, ( see [18]). F is a uniform Glivenko-Cantelli class iﬀ limm→∞ Hmm ,F ) = 0 for all > 0.

The Mathematics of Learning: Dealing with Data

15

speciﬁc form of stability may be necessary and suﬃcient for consistency of ERM? Such a result would be surprising because, a priori, there is no reason why there should be a connection between well-posedness and consistency: they are both important requirements for ERM but they seem quite diﬀerent and independent of each other. Learning Theory, Sample Complexity and Brains. The theory of supervised learning outlined in this paper and in the references has achieved a remarkable degree of completeness and of practical success in many applications. Within it, many interesting problems remain open and are a fertile ground for interesting and useful mathematics. One may also take a broader view and ask: what next? One could argue that the most important aspect of intelligence and of the amazing performance of real brains is the ability to learn. How then do the learning machines we have described in the theory compare with brains? There are of course many aspects of biological learning that are not captured by the theory and several diﬃculties in making any comparison. One of the most obvious diﬀerences, however, is the ability of people and animals to learn from very few examples. The algorithms we have described can learn an object recognition task from a few thousand labeled images. This is a small number compared with the apparent dimensionality of the problem (thousands of pixels) but a child, or even a monkey, can learn the same task from just a few examples. Of course, evolution has probably done a part of the learning but so have we, when we choose for any given task an appropriate input representation for our learning machine. From this point of view, as Donald Geman has argued, the interesting limit is not “m goes to inﬁnity,” but rather “m goes to zero”. Thus an important area for future theoretical and experimental work is learning from partially labeled examples (and the related area of active learning). In the ﬁrst case there are only a small number of labeled pairs (xi , yi )i=1 – for instance with yi binary – and many . Though interesting work has begun in this diunlabeled data (xi )m , m +1 rection, a satisfactory theory that provides conditions under which unlabeled data can be used is still lacking. A comparison with real brains oﬀers another, and probably related, challenge to learning theory. The “learning algorithms” we have described in this paper correspond to one-layer architectures. Are hierarchical architectures with more layers justiﬁable in terms of learning theory? It seems that the learning theory of the type we have outlined does not oﬀer any general argument in favor of hierarchical learning machines for regression or classiﬁcation. This is somewhat of a puzzle since the organization of cortex – for instance visual cortex – is strongly hierarchical. At the same time, hierarchical learning systems show superior performance in several engineering applications. For instance, a face categorization system in which a single SVM classiﬁer combines the real-valued output of a few classiﬁers, each trained to a diﬀerent component of faces – such as eye and nose –, outperforms a single classiﬁer trained on full images of faces [25]. The theoretical issues surrounding hierarchical systems of this type are wide open, and likely to be of paramount importance for the next major development of eﬃcient

16

T. Poggio and S. Smale

classiﬁers in several application domains. Why hierarchies? There may be reasons of eﬃciency – computational speed and use of computational resources. For instance, the lowest levels of the hierarchy may represent a dictionary of features that can be shared across multiple classiﬁcation tasks (see [24]). Hierarchical system usually decompose a task in a series of simple computations at each level – often an advantage for fast implementations. There may also be the more fundamental issue of sample complexity. We mentioned that an obvious diﬀerence between our best classiﬁers and human learning is the number of examples required in tasks such as object detection. The theory described in this paper shows that the diﬃculty of a learning task depends on the size of the required hypothesis space. This complexity determines in turn how many training examples are needed to achieve a given level of generalization error. Thus the complexity of the hypothesis space sets the speed limit and the sample complexity for learning. If a task – like a visual recognition task – can be decomposed into low-complexity learning tasks, for each layer of a hierarchical learning machine, then each layer may require only a small number of training examples. Of course, not all classiﬁcation tasks have a hierarchical representation. Roughly speaking, the issue is under which conditions a function of many variables can be approximated by a function of a small number of functions of subsets of the original variables. Neuroscience suggests that what humans can learn can be represented by hierarchies that are locally simple. Thus our ability of learning from just a few examples, and its limitations, may be related to the hierarchical architecture of cortex. This is just one of several possible connections, still to be characterized, between learning theory and the ultimate problem in natural science – the organization and the principles of higher brain functions.

Acknowledgments
Thanks to Felipe Cucker, Federico Girosi, Don Glaser, Sayan Mukherjee, Massimiliano Pontil, Martino Poggio and Ryan Rifkin.

References
1. N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive dimensions, uniform convergence, and learnability. J. of the ACM, 44(4):615–631, 1997. 14 2. N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 686:337– 404, 1950. 7 3. A.R. Barron. Approximation and estimation bounds for artiﬁcial neural networks. Machine Learning, 14:115–133, 1994. 12 4. M. Bertero, T. Poggio, and V. Torre. Ill-posed problems in early vision. Proceedings of the IEEE, 76:869–889, 1988. 13

The Mathematics of Learning: Dealing with Data

17

5. D. Beymer and T. Poggio. Image representations for visual learning. Science, 272(5270):1905–1909, June 1996. 6 6. O. Bousquet and A. Elisseeﬀ. Stability and generalization. Journal of Machine Learning Research, (2):499–526, 2002. 14 7. D.S. Broomhead and D. Lowe. Multivariable functional interpolation and adaptive networks. Complex Systems, 2:321–355, 1988. 7, 12 8. R. Brunelli and T. Poggio. Hyberbf networks for real object recognition. In Proceedings IJCAI, Sydney, Australia, 1991. 7 9. C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:1–25, 1995. 13 10. F. Cucker and S. Smale. On the mathematical foundations of learning. Bulletin of AMS, 39:1–49, 2001. 7, 9, 10 11. F. Cucker and S. Smale. Best choices for regularization parameters in learning theory: on the bias-variance problem. Foundations of Computational Mathematics, 2(4):413–428, 2002. 11, 12 12. I. Daubechies. Ten lectures on wavelets. CBMS-NSF Regional Conferences Series in Applied Mathematics. SIAM, Philadelphia, PA, 1992. 9 13. R. DeVore, R. Howard, and C. Micchelli. Optimal nonlinear approximation. Manuskripta Mathematika, 1989. 9 14. R.A. DeVore, D. Donoho, M. Vetterli, and I. Daubechies. Data compression and harmonic analysis. IEEE Transactions on Information Theory Numerica, 44:2435–2476, 1998. 9 15. R.A. DeVore. Nonlinear approximation. Acta Numerica, 7:51–150, 1998. 9 16. L. Devroye, L. Gy¨rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recogo nition. Number 31 in Applications of mathematics. Springer, New York, 1996. 9 17. R.M. Dudley. Universal Donsker classes and metric entropy. Ann. Prob., 14(4):1306–1326, 1987. 14 18. R.M. Dudley, E. Gine, and J. Zinn. Uniform and universal glivenko-cantelli classes. Journal of Theoretical Probability, 4:485–510, 1991. 14 19. T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Advances in Computational Mathematics, 13:1–50, 2000. 7, 11, 13, 14 20. T. Ezzat, G. Geiger, and T. Poggio. Trainable videorealistic speech animation. In Proceedings of ACM SIGGRAPH 2002, San Antonio, TX, pp. 388–398, 2002. 6 21. G. Fung and O.L. Mangasarian. Proximal support vector machine classiﬁers. In KDD 2001: Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, 2001. 12 22. F. Girosi. An equivalence between sparse approximation and Support Vector Machines. Neural Computation, 10(6):1455–1480. 13 23. F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks architectures. Neural Computation, 7:219–269, 1995. 13 24. T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer Series in Statistics. Springer Verlag, Basel, 2001. 16 25. B. Heisele, T. Serre, M. Pontil, T. Vetter, and T. Poggio. Categorization by learning and combining object parts. In Advances in Neural Information Processing Systems 14 (NIPS’01), volume 14, pp. 1239–1245. MIT Press, 2002. 6, 7, 15 26. B. Heisele, A. Verri, and T. Poggio. Learning and vision machines. Proceedings of the IEEE, 90:1164–1177, 2002. 4 27. J. Hutchinson, A. Lo, and T. Poggio. A nonparametric approach to pricing and hedging derivative securities via learning networks. The Journal of Finance, XLIX(3), July 1994. 6

18

T. Poggio and S. Smale

28. R. Karp. Mathematical challenges from genomics and molecular biology. Notices of the AMS, 49:544–553, 2002. 12 29. S. Kutin and P. Niyogi. Almost-everywhere algorithmic stability and generalization error. Technical report TR-2002-03, University of Chicago, 2002. 14 30. S. Mendelson. Improving the sample complexity using global data. IEEE Transactions on Information Theory, 48(7):1977–1991, 2002. 9 31. S. Mendelson. Geometric parameters in learning theory. Submitted for publication, 2003. 9 32. C.A. Micchelli. Interpolation of scattered data: distance matrices and conditionally positive deﬁnite functions. Constructive Approximation, 2:11–22, 1986. 9 33. C.A. Micchelli and T.J. Rivlin. A survey of optimal recovery. In C.A. Micchelli and T.J. Rivlin, editors, Optimal Estimation in Approximation Theory, pp. 1– 54. Plenum Press, New York, 1976. 9 34. J. Moody and C. Darken. Fast learning in networks of locally-tuned processing units. Neural Computation, 1(2):281–294, 1989. 7 35. P. Niyogi and F. Girosi. Generalization bounds for function approximation from scattered noisy data. Advances in Computational Mathematics, 10:51–80, 1999. 12 36. C. Papageorgiou, M. Oren, and T. Poggio. A general framework for object detection. In Proceedings of the International Conference on Computer Vision, Bombay, India, January 1998. 4 37. E. Parzen. An approach to time series analysis. Ann. Math. Statist., 32:951–989, 1961. 13 38. T. Poggio and R. Brunelli. A novel approach to graphics. A.I. Memo No. 1354, Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, 1992. 6 39. T. Poggio and S. Edelman. A network that learns to recognize 3D objects. Nature, 343:263–266, 1990. 7 40. T. Poggio and F. Girosi. Networks for approximation and learning. Proceedings of the IEEE, 78(9), September 1990. 12, 13 41. T. Poggio and F. Girosi. Regularization algorithms for learning that are equivalent to multilayer networks. Science, 247:978–982, 1990. 13 42. T. Poggio, V. Torre, and C. Koch. Computational vision and regularization theory. Nature, 317:314–319, 1985b. 13 43. T. Poggio and A. Verri. Introduction: Learning and vision at cbcl. International Journal of Computer Vision, 38–1, 2000. 4 44. M.J.D. Powell. Radial basis functions for multivariable interpolation: a review. In J.C. Mason and M.G. Cox, editors, Algorithms for Approximation. Clarendon Press, Oxford, 1987. 6 45. M.J.D. Powell. The theory of radial basis functions approximation in 1990. In W.A. Light, editor, Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions, pp. 105–210. Oxford University Press, 1992. 6 46. Ramaswamy, Tamayo, Rifkin, Mukherjee, Yeang, Angelo, Ladd, Reich, Latulippe, Mesirov, Poggio, Gerlad, Loda, Lander, and Golub. Multiclass cancer diagnosis using tumor gene expression signatures. Proceedings of the National Academy of Science, December 2001. 4, 7 47. R.M. Rifkin. Everything Old Is New Again: A Fresh Look at Historical Approaches to Machine Learning. PhD thesis, Massachusetts Institute of Technology, 2002. 12, 13 48. S. Smale and D. Zhou. Estimating the approximation error in learning theory. Analysis and Applications, 1:1–25, 2003. 10

The Mathematics of Learning: Dealing with Data

19

49. J. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and J. Vandewalle. Least Squares Support Vector Machines. 12 50. V.N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998. 10, 13 51. V.N. Vapnik and A.Y. Chervonenkis. On the uniform convergence of relative frequences of events to their probabilities. Th. Prob. and its Applications, 17(2):264–280, 1971. 14 52. G. Wahba. Smoothing and ill-posed problems. In M. Golberg, editor, Solutions methods for integral equations and applications, pp. 183–194. Plenum Press, New York, 1979. 13 53. G. Wahba. Splines Models for Observational Data. Series in Applied Mathematics, Vol. 59, SIAM, Philadelphia, 1990. 6, 12, 13 54. D. Zhou. The regularity of reproducing kernel hilbert spaces in learning theory. 2001. preprint. 10 55. D. Zhou. The covering number in learning theory. J. Complexity, 18:739–767, 2002. 10

Logical Regression Analysis: From Mathematical Formulas to Linguistic Rules
H. Tsukimoto
Tokyo Denki University tsukimoto@c.dendai.ac.jp

1 Introduction
Data mining means the discovery of knowledge from (a large amount of) data, and so data mining should provide not only predictions but also knowledge such as rules that are comprehensible to humans. Data mining techniques should satisfy the two requirements, that is, accurate predictions and comprehensible rules. Data mining consists of several processes such as preprocessing and learning. This paper deals with learning. The learning in data mining can be divided into supervised learning and unsupervised learning. This paper deals with supervised learning, that is, classiﬁcation and regression. The major data mining techniques are neural networks, statistics, decision trees, and association rules. When these techniques are applied to real data, which usually consist of discrete data and continuous data, they each have their own problems. In other words, there is no perfect technique, that is, there is no technique which can satisfy the two requirements of accurate predictions and comprehensible rules. Neural networks are black boxes, that is, neural networks are incomprehensible. Multiple regression formulas, which are the typical statistical models, are black boxes too. Decision trees do not work well when classes are continuous [19], that is, if accurate predictions are desired, comprehensibility has to be sacriﬁced, and if comprehensibility is desired, accurate predictions have to be sacriﬁced. Association rules, which are unsupervised learning techniques, do not work well when the right-hand sides of rules, which can be regarded as classes, are continuous [21]. The reason is almost the same as that for decision trees. Neural networks and multiple regression formulas are mathematical formulas. Decision trees and association rules are linguistic rules. Mathematical formulas can provide predictions but cannot provide comprehensible rules. On

H. Tsukimoto: Logical Regression Analysis: From Mathematical Formulas to Linguistic Rules, StudFuzz 180, 21–61 (2005) c Springer-Verlag Berlin Heidelberg 2005 www.springerlink.com

22

H. Tsukimoto

the other hand, linguistic rules can provide comprehensible rules but cannot provide accurate predictions in continuous classes. How can we solve the above problem? The solution is rule extraction from mathematical formulas. Rule extraction from mathematical formulas is needed for developing the perfect data mining techniques satisfying the two data mining requirements, accurate predictions and comprehensible rules. Several researchers have been developing rule extraction techniques from neural networks, and therefore, neural networks can be used in data mining [14], whereas few researchers have studied rule extraction from linear formulas. We have developed a rule extraction technique from mathematical formulas such as neural networks and linear formulas. The technique is called the Approximation Method. We have implemented the Approximation Method for neural networks in a data mining tool KINOsuite-PR[41]. Statistics have been developing many techniques. The outputs of several techniques are linear formulas. The Approximation Method can extract rules from the linear formulas, and so the combination of statistical techniques and the Approximation Method works well for data mining. We have developed a data mining technique called Logical Regression Analysis(LRA). LRA consists of regression analysis1 and the Approximation Method. Since there are several regression analyses, LRA has several versions. In other words, LRA can be applied to a variety of data, because a variety of regression analyses have been developed. So far, in data mining, many researchers have been dealing with symbolic data or numerical data and few researchers have been dealing with image data. One of LRA’s merits is that LRA can deal with image data, that is, LRA can discover rules from image data. Therefore, this paper explains the data mining from images by LRA. There are strong correlations between the pixels of images, and so the numbers of samples are small compared with the numbers of the pixels, that is, the numbers of attributes(variables). A regression analysis for images must work well when the correlations among attributes are strong and the number of data is small. Nonparametric regression analysis works well when the correlations among attributes are strong and the number of data is small. There are many kinds of images such as remote-sensing images, industrial images and medical images. Within medical images, there are many types such as brain images, lung images, and stomach images. Brain functions are the most complicated and there are a lot of unknown matters, and consequently the discovery of rules between brain areas and brain functions is a signiﬁcant subject. Therefore, we have been dealing with functional brain images.
1 The regression analysis includes the nonlinear regression analysis using neural networks.

Logical Regression Analysis

23

We have been applying LRA using nonparametric regression analysis to fMRI images to discover the rules of brain functions. In this paper, for simpliﬁcation, “LRA using nonparametric regression analysis” is abbreviated to “LRA”. LRA was applied to several experimental tasks. This paper reports the experimental results of ﬁnger tapping and calculation. The results of LRA include discoveries that have never been experimentally conﬁrmed. Therefore, LRA has the possibility of providing new evidence in brain science. Section 2 explains the problems in the major data mining techniques and how the problems can be solved by rule extraction from mathematical formulas. Section 3 surveys the rule extraction from neural networks. Section 4 explains the Approximation Method, which has been developed by the author. The Approximation Method is mathematically based on the multilinear function space, which is also explained. Section 5 explains the Approximation Method in the continuous domain. Section 6 discusses a few matters. Section 7 brieﬂy explains the data mining from fMRI images. Section 8 describes the data mining from fMRI images by Logical Regression Analysis (LRA). Section 9 shows the experiments of calculations. This paper is mainly based on [32, 33, 35], and [36].

2 The Problems of Major Data Mining Techniques
The major data mining techniques, that is, decision trees, neural networks, statistics and association rules, are reviewed in terms of the two requirements of accurate predictions and comprehensible rules. 2.1 Neural Networks Neural networks can provide accurate predictions in the discrete domain and the continuous domain. The problem is that the training results of neural
Data

Regression Analysis

The Approximation Method

Rules predictions (Predictions)

Fig. 1. Logical Regression Analysis

24

H. Tsukimoto

x

w1 S1 w2 w3 w4 S2 w5 w6 z

y Input layer Hidden layer
Fig. 2. Neural network

Output layer

networks are sets of mathematical formulas, that is, neural networks are incomprehensible black boxes. For example, Fig. 2 shows a trained neural network. In Fig. 2, x and y are inputs, z is an output, and ti ’s are the outputs of the two hidden units. Each output is represented as follows: t1 = S(w1 x + w2 y + h1 ) , t2 = S(w3 x + w4 y + h2 ) , z = S(w5 t1 + w6 t2 + h3 ) , where wi ’s stand for weight parameters, S(x) stands for sigmoid function and hi ’s stand for biases. The weight parameters are as follows: w1 = 2.51 , w2 = −4.80 , w3 = −4.90 , w4 = 2.83 , w5 = 2.52 , w6 = −4.81 , h1 = −0.83 , h2 = −1.12 , h3 = −0.82 . From the above weight parameters and the above formulas, we cannot understand what the neural network has learned. Therefore, the trained neural network is a black box. What the neural network has learned will be explained later using a rule extraction technique. 2.2 Multiple Regression Analysis There are a lot of statistical methods. The most typical method is multiple regression analysis, and therefore, only multiple regression analysis is discussed here. Multiple regression analysis usually uses linear formulas, and so only linear regression analysis is possible and nonlinear regression analysis is impossible, while neural networks can perform nonlinear regression analysis. However, linear regression analysis has the following advantages.

Logical Regression Analysis

25

1. The optimal solution can be calculated. 2. The linear regression analysis is the most widely used statistical method in the world. 3. Several regression analysis techniques such as nonparametric regression analysis and multivariate autoregression analysis have been developed based on the linear regression analysis. Linear regression analysis can provide appropriate predictions in the continuous domain and discrete domain. The problem is that multiple regression formulas are mathematical formulas as showed below.
n

y=
i=1

ai xi + b ,

where y is a dependent variable, xi s are independent variables, b is a constant, ai s are coeﬃcients, and n is the number of the independent variables. The mathematical formulas are incomprehensible black boxes. A mathematical formula consisting of a few independent variables may be understandable. However, a mathematical formula consisting of a lot of independent variables cannot be understood. Moreover, in multivariate autoregression analysis, there are several mathematical formulas, and so the set of the mathematical formulas cannot be understood at all. Therefore, rule extraction from linear regression formulas is important. 2.3 Decision Trees When a class is continuous, the class is discretized into several intervals. When the number of the intervals, that is, the number of the discretized classes, is small, comprehensible trees can be obtained, but the tree cannot provide accurate predictions. For example, Fig. 3 shows a tree where there are two

x <5.0

5.0<x

y<3.0

3.0<y

y<4.0

4.0<y [5.2-10.0]

[0.0-1.5] [3.4-5.1] [1.6-3.3]

Fig. 3. Decision tree with a continuous class

26

H. Tsukimoto

x <5.0

5.0<x

y<3.0 z=ax+by+c

3.0<y y<4.0 Z=dx+ey+f z=gx+hy+i
Fig. 4. Regression tree

4.0<y z=jx+ky+l

continuous attributes and a continuous class. To improve the prediction accuracy, let the number of intervals be large. When the number of intervals is large, the trees obtained are too complicated to be comprehensible. From the above simple discussion, we can conclude that it is impossible to obtain trees which can satisfy the two requirements for data mining techniques, accurate predictions and comprehensible rules at the same time. Therefore, decision trees cannot work well when classes are continuous [19]. As a solution for continuous classes, for example, Quinlan presented Cubist[40]. Cubist generates piecewise-linear models, which are a kind of regression trees. Figure 4 shows an example. As seen from this ﬁgure, the tree is an extension of linear formulas, and so the tree is a prediction model, that is, it is incomprehensible. As a result, this solution has solved the inaccurate prediction problem but has generated the incomprehensibility problem. 2.4 Association Rules Association rules are described as a∧b→c, where c can be regarded as a class. Association rules do not work well when “classes” like c in the above rule are continuous. When there are many intervals, the rules are too complicated to be comprehensible, whereas when there are few intervals, the rule cannot provide accurate predictions, and therefore some concessions are needed [21]. Some techniques such as fuzzy techniques can be applied to the above problem [11], but while fuzzy techniques can improve predictions, they degrade the comprehensibility. Table 1 shows the summary. in neural network means that training results are incomprehensible. in linear regression means that training results are incomprehensible.

Logical Regression Analysis Table 1. Data mining techniques Type 1 2 3 4 Class Discrete Discrete Continuous Continuous Attribute Discrete Continuous Discrete Continuous neural network linear regression regression tree decision tree ◦ ◦

27

in regression tree means that training results are incomprehensible. means that decision trees cannot work well in continuous classes. Association rules are omitted in Table 1, because association rules do not have classes. However, the evaluations for association rules are the same as those for decision trees. Thus, we conclude that there is no technique which can satisfy the two requirements for data mining techniques, that is, the technique which can provide accurate predictions and comprehensible rules in the discrete domain and the continuous domain. 2.5 The Solution for the Problem The solution for the above problem is extracting comprehensible rules from mathematical formulas such as neural networks and multiple regression formulas[38]. When rules are extracted from mathematical formulas, the rules are not used for predictions, but used only for human comprehension. The mathematical formulas are used to make the predictions. A set of a prediction model and a rule (or rules) extracted from the prediction model is the perfect data mining technique. How a rule extracted from a prediction model is used is brieﬂy explained. When a prediction model predicts, it only outputs a class or a ﬁgure. Humans cannot understand how or why the prediction model outputs the class or the ﬁgure. A rule extracted from the prediction model explains how or why the prediction model outputs the class or the ﬁgure. For example, let a neural network be trained using process data consisting of three attributes (temperature (t), pressure (p), humidity (h)), and a class, the quality of a material (q). Let the rule extracted be as follows: (200 ≤ t ≤ 300) ∨ (p ≤ 2.5) ∨ (70 ≤ h ≤ 90) → q ≤ 0.2 . Assume that the network is used for a prediction, and the inputs for the network are t = 310, p = 2.0, and h = 60 and the output from the network is 0.1, which means low quality. The above rule shows that the network outputs 0.1, because the pressure is 2.0, which is below 2.5, that is, p ≤ 2.5 holds.

28

H. Tsukimoto

Without the rule, we cannot know how or why the network outputs 0.1 indicating the low quality. Note that the rule extracted from the neural network is not used for the predictions, which are made by the neural network. An operator can understand that the neural network predicts low quality, because the pressure is low. Therefore, the operator can raise the pressure (for example, by manipulating a valve) to raise the quality. If the operator does not know why the neural network predicts the low quality, the operator cannot take an appropriate measure to raise the quality. Because the operator understands how the neural network predicts, the operator can take an appropriate measure. That is, the extracted rule enables the operator to take the appropriate measure. If the neural network only predicts the low quality (without the extracted rule), the operator does not know what to do to raise the quality. The beneﬁt of the extracted rule is very large. 2.6 KINOsuite-PR The rule extraction technique for neural networks has been implemented in the data mining tool KINO- PR[41]. KINO stands for Knowledge INference by Observation and PR stands for Predictions and Rules. KINOsuite-PR is the ﬁrst commercial data mining tool that has the rule extraction. The rule extraction technique is called the Approximation Method. Next, the Approximation Method is explained. Before the explanation, the rule extraction from neural networks is surveyed in Sect. 3.

3 The Survey of Rule Extraction from Neural Networks
This section brieﬂy surveys algorithms for rule extraction from neural networks. Some rule extraction algorithms are based on the structurization of neural networks, but the rule extraction algorithms cannot be applied to neural networks trained by other training methods such as the backpropagation method. Some rule extraction algorithms are dedicated to hybrid models that include symbolic and subsymbolic knowledge. The rule extraction algorithms mentioned above are outside the scope of this paper, because, in data mining, it is desired that rules can be extracted from any neural network trained by any training method. There are several algorithms for rule extraction from neural networks [1, 2]. The algorithms can be divided into decompositional algorithms and pedagogical algorithms. Decompositional algorithms extract rules from each unit in a neural network and aggregate them into a rule. For example, [5] is a decompositional algorithm. Pedagogical algorithms generate samples from a neural network and induce a rule from the samples. For example, [6] is a pedagogical algorithm. Decompositional algorithms can present training results of each unit in neural networks, and so we can understand the training results by the

Logical Regression Analysis

29

unit, while pedagogical algorithms can present only the results of neural networks, and so we cannot understand the training results by the unit. Therefore, decompositional algorithms are better than pedagogical algorithms in terms of understandability of the inner structures of neural networks. Rule extraction algorithms are compared in several items, namely network structures, training methods, computational complexity, and values. Network structure: This means the types of network structures the algorithm can be applied to. Several algorithms can be applied only to particular network structures. Most algorithms are applied to three-layer feedforward networks, while a few algorithms can be applied only to recurrent neural networks, where Deterministic Finite-state Automata(DFAs) are extracted [17]. Training method: This means the training methods the rule extraction algorithm can be applied to. Several rule extraction algorithms depend on training methods, that is, the rule extraction algorithms can extract rules only from the neural networks trained by a particular training method [5, 20]. The pedagogical algorithms basically do not depend on training methods. Computational complexity: Most algorithms are exponential in computational complexity. For example, in pedagogical algorithms, the total number of samples generated from a neural network is 2n , where n is the number of inputs to the neural network. It is very diﬃcult to generate many samples and induce a rule from many samples. Therefore, it is necessary to reduce the computational complexity to a polynomial. Most decompositional algorithms are also exponential in computational complexity. Values: Most algorithms can be applied only to discrete values and cannot be applied to continuous values. The ideal algorithm can be applied to any neural network, can be applied to any training method, is polynomial in computational complexity, and can be applied to continuous values. There has been no algorithm which satisﬁes all of the items above. More detailed review can be found in [7].

4 The Approximation Method
4.1 The Outline We have developed a rule extraction technique for mathematical formulas, which is called the Approximation Method. We presented an algorithm for extracting rules from linear formulas and extended it to the continuous domain[24]. Subsequently, we presented polynomial algorithms for extracting rules from linear formulas [26] and applied them to discover rules from numerical data [16]. We also developed an inductive learning algorithm based on rule extraction from linear formulas[29]. The data mining technique through

30

H. Tsukimoto

regression analysis and the Approximation Method is called Logical regression Analysis(LRA). We have been applying LRA using nonparametric regression analysis to discover rules from functional brain images[9, 30, 34, 36, 37]. We extended the algorithms for linear formulas to the algorithms for neural networks [27], and modiﬁed them to improve the accuracies and simplicities[28, 33]. The Approximation Method for a linear formula is almost the same as the Approximation Method for a unit in a neural network. The Approximation Method for neural networks basically satisﬁes the four items listed in the preceding section, that is, the method can be applied to any neural network trained by any training method, is polynomial in computational complexity, and can be applied to continuous values. However, the method has a constraint, that is, the method can be applied only to neural networks whose units’ output functions are monotone increasing. The Approximation Method is a decompositional method. There are two kinds of domains, discrete domains and continuous domains. The continuous domain will be discussed later. The discrete domains can be reduced to {0, 1} domains by dummy variables, so only these domains have to be discussed. In the {0, 1} domain, the units of neural networks can be approximated to the nearest Boolean functions, which is the basic idea for the Approximation Method. The Approximation Method is based on the multilinear function space. The space, which is an extension of Boolean algebra of Boolean functions, can be made into a Euclidean space and includes linear functions and neural networks. The details can be found in [31] and [33]. 4.2 The Basic Algorithm of the Approximation Method A unit of a neural network or a linear function is a function. The basic algorithm approximates a function by a Boolean function. Note that “a function” in the following sentences means a unit of a neural network or a linear function. Let f (x1 , . . . , xn ) stand for a function, and (fi )(i = 1, . . . , 2n ) be the values of the function. Let the values of the function be the interval [0,1]. The values of a unit of a neural network are [0,1]. The values of a linear function can be normalized to [0,1] by some normalization method. Let g(x1 , . . . , xn ) stand for a Boolean function, and (gi )(gi = 0 or 1, i = 1, . . . , 2n ) be the values of the Boolean function. A Boolean function is represented by the following formula[3]:
2n

g(x1 , . . . , xn ) =
i=1

gi ai ,

where is disjunction, ai is an atom, and gi is the value at the domain corresponding to the atom. An atom is as follows:
n

ai =
j=1

e(xj ) (i = 1, . . . , 2n ) ,

Logical Regression Analysis

31

where stands for conjunction, and e(xj ) = xj or xj , where x stands for the negation of x. The domain (x1 , . . . , xn ) corresponding to an atom is as follows: When e(xj ) = xj , xj = 1, and when e(xj ) = xj , xj = 0. The above formula can be easily veriﬁed. The basic algorithm is as follows: gi = 1 (fi ≥ 0.5) , 0 (fi < 0.5) .

This algorithm minimizes Euclidean distance. The Boolean function is represented as follows: n
2

g(x1 , . . . , xn ) =
i=1

gi ai ,

where gi is calculated by the above algorithm. Example 1. Figure 5 shows a case of two variables. Crosses stand for the values of a function and dots stand for the values of the Boolean function. 00, 01, 10 and 11 stand for the domains, for example, 00 stands for x = 0, y = 0. In this case, there are four domains as follows: (0, 0), (0, 1), (1, 0), (1, 1) The atoms corresponding to the domains are as follows: (0, 0) ⇔ xy , (0, 1) ⇔ xy, (1, 0) ⇔ x¯, (1, 1) ⇔ xy . ¯¯ ¯ y The values of the Boolean function g(x, y) are as follows: g(0, 0) = 0, g(0, 1) = 1, g(1, 0) = 0, g(1, 1) = 1 . Therefore, in the case of Fig. 5, the Boolean function is represented as follows: g(x, y) = g(0, 0)¯y ∨ g(0, 1)¯y ∨ g(1, 0)x¯ ∨ g(1, 1)xy . x¯ x y The Boolean function is reduced as follows:
1 . x . x

0

x . 00 01

x . 10 11

Fig. 5. Approximation

32

H. Tsukimoto

g(x, y) = g(0, 0)¯y ∨ g(0, 1)¯y ∨ g(1, 0)x¯ ∨ g(1, 1)xy x¯ x y g(x, y) = 0¯y ∨ 1¯y ∨ 0x¯ ∨ 1xy x¯ x y g(x, y) = xy ∨ xy ¯ g(x, y) = y . Example 2. An example of a linear function Let a linear function be as follows: z = 0.3x − 0.5y + 0.6 There are four domains as follows: (0, 0), (0, 1), (1, 0), (1, 1) The values of the function at the domains are as follows: z(0, 0) = 0.6, z(0, 1) = 0.1, z(1, 0) = 0.9, z(1, 1) = 0.4 . By approximating the function to a Boolean function g(x, y), g(0, 0) = 1, g(0, 1) = 0, g(1, 0) = 1, g(1, 1) = 0 . are obtained. The Boolean function g(x, y) is as follows: g(x, y) = g(0, 0)¯y ∨ g(0, 1)¯y ∨ g(1, 0)x¯ ∨ g(1, 1)xy x¯ x y g(x, y) = 1¯y ∨ 0¯y ∨ 1x¯ ∨ 0xy x¯ x y g(x, y) = xy ∨ x¯ ¯¯ y g(x, y) = y . ¯ Example 3. An example of a neural network We show what the neural network in Fig. 2 has learned by rule extraction. Two hidden units and the output unit are as follows: t1 = S(w1 x + w2 y + h1 ) , t2 = S(w3 x + w4 y + h2 ) , z = S(w5 t1 + w6 t2 + h3 ) , where wi ’s stand for weight parameters, S(x) stands for sigmoid function and hi ’s stand for biases. The training results by the back-propagation method with 1000 time repetitions are as follows: w1 = 2.51, w2 = −4.80, w3 = −4.90 , w4 = 2.83, w5 = 2.52, w6 = −4.81 , h1 = −0.83, h2 = −1.12, h3 = −0.82 .

Logical Regression Analysis

33

For example, t1 = S(2.51x − 4.80y − 0.83) , and the values of t1 (1, 1), t1 (1, 0), t1 (0, 1), and t1 (0, 0) are as follows: t1 (1, 1) = S(2.51 · 1 − 4.80 · 1 − 0.83) = S(−3.12) , t1 (1, 0) = S(2.51 · 1 − 4.80 · 0 − 0.83) = S(1.68) , t1 (0, 1) = S(2.51 · 0 − 4.80 · 1 − 0.83) = S(−5.63) , t1 (0, 0) = S(2.51 · 0 − 4.80 · 0 − 0.83) = S(−0.83) . S(−3.12) 0, S(1.68) 1, S(−5.63) t1 0, and S(−0.83) xy . 0; therefore

In a similar manner, t2 is approximated by the following Boolean function: t 2 = xy ¯ z is approximated as follows: ¯ z = t1 t 2 . ¯ By substituting t1 = xy and t2 = xy in the above formula, z = xy¯y = xy(x ∨ y ) = x¯ ∨ x¯ = x¯ x ¯ y y y has been obtained. Thus, we understand that the neural network in Fig. 2 has learned x¯. y 4.3 Multilinear Functions The computational complexity of the basic algorithm is exponential, which is obvious from the discussion above. Therefore, a polynomial algorithm is needed. The multilinear function space is necessary for the polynomial algorithm[28, 33]. This subsection explains that the multilinear function space of the domain {0, 1} is a Euclidean space. First, the multilinear functions are explained. Second, it is shown that the multilinear function space is the linear space spanned by the atoms of Boolean algebra of Boolean functions. Third, it is explained that the space can be made into a Euclidean space. Finally, it is explained that neural networks are multilinear functions. What Multilinear Functions Are Deﬁnition 1. Multilinear functions of n variables are as follows:
2n

ai xei1 · ·xein , n 1
i=1

where ai is real, xi is a variable, and ei is 0 or 1.

34

H. Tsukimoto

Example. Multilinear functions of 2 variables are as follows: axy + bx + cy + d . Multilinear functions do not contain any terms such as xk1 xk2 · · · xkn , n 1 2 where ki ≥ 2. A function f : {0, 1}n → R (1)

is a multilinear function, because xki = xi holds in {0, 1} and so there is i no term like (1) in the functions. In other words, multilinear functions are functions which are linear when only one variable is considered and the other variables are regarded as parameters. The Multilinear Function Space of the Domain {0, 1} is the Linear Space Spanned by the Atoms of Boolean Algebra of Boolean Functions Deﬁnition 2. The atoms of Boolean algebra of Boolean functions of n variables are as follows:
n

φi =
j=1

e(xj ) (i = 1, . . . , 2n ) ,

where e(xj ) = xj or xj . Example. The atoms of Boolean algebra of Boolean functions of 2 variables are as follows: x ∧ y, x ∧ y, x ∧ y, x ∧ y . Theorem 1. The space of multilinear functions ({0, 1}n → R) is the linear space spanned by the atoms of Boolean algebra of Boolean functions. The proof can be found in [31] and [33]. Example. A linear function of the atoms of 2 variables is axy + bxy + cxy + dxy . This function is transformed to the following: pxy + qx + ry + s , where p = a − b − c + d, A multilinear function q = b − d, r = c − d, s=d.

Logical Regression Analysis

35

pxy + qx + ry + s can be transformed into axy + bxy + cxy + dxy , where a = p + q + r + s, b = q + s, c = r + s, d=s. Now, it has been shown that the multilinear function space of the domain {0, 1} is the linear space spanned by the atoms of Boolean algebra of Boolean functions. The dimension of the space is 2n . Next, it is shown that the multilinear function space is made into a Euclidean space. The Multilinear Function Space of the Domain {0, 1} is a Euclidean Space Deﬁnition 3. The inner product is deﬁned as follows: < f, g >=
{0,1}n

fg .

The sum in the above formula is done over the whole domain. Example. In the case of two variables, < f, g > =
{0,1}2

f g = f (0, 0)g(0, 0) + f (0, 1)g(0, 1) + f (1, 0)g(1, 0) + f (1, 1)g(1, 1) .

Theorem 2. Atoms φi s have unitarity and orthogonality. < φi , φi > = 1 (unitarity) < φi , φj > = 0(i = j) (orthogonality) The proof can be found in [31] and [33]. Example. An example of unitarity and orthogonality of two variables is as follows: < x¯, x¯ > = y y
{0,1}2

x¯x¯ = y y
{0,1}2

x2 y 2 ¯

=
{0,1}2

x¯ = 1 · ¯ + 1 · ¯ + 0 · ¯ + 0 · ¯ y 1 0 1 0

= 1·0+1·1+0·0+0·1=1.

36

H. Tsukimoto

(The domain is {0,1}, and so x2 = x, y 2 = y .) ¯ ¯ < xy, x¯ >= y
{0,1}2

xyx¯ = y
{0,1}2

x2 y y = 0 . ¯

Deﬁnition 4. Norm is deﬁned as follows: |f | = Example |x¯| = y < f, f > = √
{0,1}n f 2

.

< x¯, x¯ > = 1 y y

From the above discussion, the space becomes a ﬁnite-dimensional inner product space, namely a Euclidean space. Neural Networks are Multilinear Functions Theorem 3. When the domain is {0, 1}, neural networks are multilinear functions. Proof. As described in this subsection, a function whose domain is {0, 1} is a multilinear function. Therefore, when the domain is {0, 1}, neural networks, that is, the functions which neural networks learn are multilinear functions. From the above discussions, in the domain of {0,1}, a neural network is a multilinear function, a Boolean function is a multilinear function, and the multilinear function space is a Euclidean space. Therefore, a neural network can be approximated to a Boolean function by Euclidean distance. 4.4 A Polynomial-Time Algorithm for Linear Formulas[29] The Condition that xi1 · ·xik xik+1 · ·xil Exists in the Boolean Function After Approximation The following theorem holds. Theorem 4. Let p1 x1 + . . . + pn xn + pn+1 stand for a linear function. Let a1 x1 · ·xn + a2 x1 · ·xn + . . . + a2n x1 · ·xn stand for the expansion by the atoms of Boolean algebra of Boolean functions. The ai ’s are as follows: a1 = p1 + . . . + pn + pn+1 ,

Logical Regression Analysis

37

a2 = p1 + . . . + pn−1 + pn+1 , ... a2n−1 = p1 + pn+1 , a2n−1 +1 = p2 + p3 + . . . + pn−1 + pn + pn+1 , ... a2n −1 = pn + pn+1 , a2n = pn+1 . Proof. The following formula holds. p1 x1 + . . . + pn xn + pn+1 = a1 x1 · ·xn + a2 x1 · ·xn + . . . + a2n x1 · ·xn . Each ai can be easily calculated as follows. For example, let x1 = x2 = ·· = xn = 1 . Then the right-hand side of the above formula is a1 and the left-hand side is p1 + . . . + pn + pn+1 . Thus, a1 = p1 + . . . + pn + pn+1 is obtained. Similarly, let x1 = x2 = ·· = xn−1 = 1, xn = 0 , then a2 = p1 + . . . + pn−1 + pn+1 is obtained. The others can be calculated in the same manner. Thus, the theorem has been proved. Theorem 5. Let p1 x1 + . . . + pn xn + pn+1 stand for a linear function. The condition that xi1 · ·xik xik+1 · ·xil exists in the Boolean function after approximation is as follows:
ik

pj + pn+1 +
i1 1≤j≤n,j=i1 ,...,il ,pj <0

pj ≥ 0.5 .

38

H. Tsukimoto

Proof. Consider the existence condition of x1 in the Boolean function after approximation. For simpliﬁcation, this condition is called the existence condition. Because x1 = x1 x2 · ·xn ∨ x1 x2 · ·xn ∨ . . . ∨ x1 x2 · ·xn , the existence of x1 equals the existence of the following terms: x1 x2 · ·xn , x1 x2 · ·xn , ... x1 x2 · ·xn . The existence of the above terms means that all coeﬃcients of these terms a1 , a2 , . . . , a2n−1 are greater than or equal to 0.5 (See 4.2). That is, M IN {ai } ≥ 0.5(1 ≤ i ≤ 2n−1 ) . M IN {ai } will be denoted by M IN ai for simpliﬁcation. Because ai ’s (1 ≤ i ≤ 2n−1 ) are a1 = p1 + . . . + pn + pn+1 , a2 = p1 + . . . + pn−1 + pn+1 , ... a2n−1 = p1 + pn+1 , each ai (1 ≤ i ≤ 2n−1 ) contains p1 . If each pj is non-negative, a2n−1 (= p1 + pn+1 ) is the minimum because the other ai ’s contain other pj ’s, and therefore the other ai ’s are greater than or equal to a2n−1 (= p1 +pn+1 ). Generally, since each pj is not necessarily non-negative, the M IN ai is ai which contains all negative pj . That is, M IN ai = p1 + pn+1 +
1≤j≤n,j=1,pj <0

pj ,

which necessarily exists in ai (1 ≤ i ≤ 2n−1 ), because ai (1 ≤ i ≤ 2n−1 ) is p1 + pn+1 + arbitrary sum of pj (2 ≤ j ≤ n)) . From the above arguments, the existence condition of x1 , M IN ai ≥ 0.5, is as follows: pj ≥ 0.5 . p1 + pn+1 +
1≤j≤n,j=1,pj <0

Since p1 x1 + . . . + pn xn + pn+1

Logical Regression Analysis

39

is symmetric for xi , the above formula holds for other variables; that is, the existence condition of xi is pi + pn+1 +
1≤j≤n,j=i,pj <0

pj ≥ 0.5 .

Similar discussions hold for xi , and so we have the following formula: pn+1 +
1≤j≤n,j=i,pj <0

pj ≥ 0.5 .

Similar discussions hold for higher order terms xi1 · ·xik xik+1 · ·xil , and so we have the following formula:
ik

pj + pn+1 +
i1 1≤j≤n,j=i1 ,...,il ,pj <0

pj ≥ 0.5 .

Generation of DNF Formulas The algorithm generates terms using the above formula from the lowest order up to a certain order. A DNF formula can be generated by taking the disjunction of the terms generated by the above formula. A term whose existence has been conﬁrmed does not need to be rechecked in higher order terms. For example, if the existence of x is conﬁrmed, then it also implies the existence of xy, xz, . . . , because x = x ∨ xy ∨ xz; hence, it is unnecessary to check the existence of xy, xz, . . .. As can be seen from the above discussion, the generation method of DNF formulas includes reductions such as xy ∨ xz = x. Let f = 0.65x1 + 0.23x2 + 0.15x3 + 0.20x4 + 0.02x5 be the linear function. The existence condition of xi1 · ·xik xik+1 · ·xil is
ik

pj + pn+1 +
i1 1≤j≤n,j=i1 ,...,il ,pj <0

pj ≥ 0.5 .

In this case, each pi is positive and pn+1 = 0; therefore the above formula can be simpliﬁed to
ik

pj ≥ 0.5 .
i1

For xi , the existence condition is pi ≥ 0.5 . For i = 1, 2, 3, 4, 5, p1 ≥ 0.5 ,

40

H. Tsukimoto

therefore x1 exists. For xi xj , the existence condition is pi + pj ≥ 0.5 . For i, j = 2, 3, 4, 5, pi + pj < 0.5 , therefore no xi xj exists. For xi xj xk , the existence condition is pi + pj + pk ≥ 0.5 . For i, j, k = 2, 3, 4, 5, p2 + p3 + p4 ≥ 0.5 , therefore x2 x3 x4 exists. Because higher order terms cannot be generated from x5 , the algorithm stops. Therefore, x1 and x2 x3 x4 exist and the DNF formula is the disjunction of these terms, that is, x1 ∨ x2 x3 x4 . 4.5 Polynomial-Time Algorithms for Neural Networks The basic algorithm is exponential in computational complexity, and therefore, polynomial algorithms are needed. The authors have presented polynomial algorithms. The details can be found in [27, 28, 33]. The outline of the polynomial algorithms follows. Let a unit of a neural network be as follows: S(p1 x1 + . . . + pn xn + pn+1 ) , where S(·) is a sigmoid function. The Boolean function is obtained by the following algorithm. 1. Check if xi1 · ·xik xik+1 · ·xil exists in the Boolean function after the approximation by the following formula:   S pn+1 + .
ik i1

pj +
1≤j≤n,j=i1 ,...,il ,pj ≤0

pj  ≥ 0.5

Logical Regression Analysis

41

2. Connect the terms existing after the approximation by logical disjunction to make a DNF formula. 3. Execute the above procedures up to a certain (usually two or three) order. After Boolean functions are extracted from all units, the Boolean functions are aggregated into a Boolean function (rule) for the network. An example follows. Let S(0.65x1 + 0.23x2 + 0.15x3 + 0.20x4 + 0.02x5 − 0.5) be a unit of a neural network. For xi (i = 1, 2, 3, 4, 5), S(p1 + p6 ) ≥ 0.5 , and therefore x1 exists. For xi xj (i, j = 2, 3, 4, 5), S(pi + pj + p6 ) < 0.5 , and therefore no xi xj exists. For xi xj xk (i, j, k = 2, 3, 4, 5), S(p2 + p3 + p4 + p6 ) ≥ 0.5 , and therefore x2 x3 x4 exists. Because higher order terms cannot be generated from x5 , the algorithm stops. Therefore, x1 and x2 x3 x4 exist and the DNF formula is the disjunction of these terms, that is, x1 ∨ x2 x3 x4 . If accurate rules are obtained, the rules are complicated, and if simple rules are obtained, the rules are not accurate. It is diﬃcult to obtain rules which are simple and accurate at the same time. We have presented a few techniques to obtain simple and accurate rules. For example, attribute selection works well for obtaining simple rules[28, 33].

42

H. Tsukimoto

4.6 Computational Complexity of the Algorithm and Error Analysis The computational complexity of generating the mth order terms is a polynon mial of m , that is, a polynomial of n. Thus, the computational complexity m=k n of generating up to the kth order terms is m=1 m , which is a polynomial of n. Therefore, the computational complexity of generating DNF formulas from trained neural networks is a polynomial of n. However, the sum of the m=n n number of up to the highest (=nth) order terms is m=1 m = 2n ; therefore, the computational complexity of generating up to the highest order terms is exponential. If it is desired that the computational complexity be reduced to a polynomial, the terms are generated only up to a certain order. If it is desired that understandable rules are obtained from trained neural networks, higher order terms are unnecessary. Therefore, actual computational complexity is a low order polynomial. In the case of the domain {0, 1}, the following theorem holds[13].:
1/2 ˆ f (S)2 ≤ 2M 2−k /20 ,

|S|>k

where f is a Boolean function, S is a term, |S| is the order of S, k is any ˆ integer, f (S) denotes the Fourier Transform of f at S and M is the circuit’s size of the function. The above formula shows that the high order terms have very little power; that is, low order terms are informative. Therefore, a good approximation can be obtained by generating up to a certain order; that is, the computational complexity can be reduced to a polynomial by adding small errors.

5 The Approximation Method in the Continuous Domain
When classes are continuous, extracting rules from neural networks is important. However, in the continuous domain, few algorithms have been proposed. For example, algorithms for extracting fuzzy rules have been presented[4], but fuzzy rules are described by linear functions, and so fuzzy rules are incomprehensible. 5.1 The Basic Idea In this section, the algorithm is extended to continuous domains. Continuous domains can be normalized to [0,1] domains by some normalization method, so only these domains have to be discussed. First, we have to present a system

Logical Regression Analysis

43

y 1 1-x x

0 0 1

x

Fig. 6. Direct proportion and inverse proportion

of qualitative expressions corresponding to Boolean functions, in the [0,1] domain. The author presents the expression system generated by direct proportion, inverse proportion, conjunction and disjunction. Figure 6 shows the direct proportion and the inverse proportion. The inverse proportion (y = 1−x) is a little diﬀerent from the conventional one (y = −x), because y = 1 − x is the natural extension of the negation in Boolean functions. The conjunction and disjunction will also be obtained by a natural extension. The functions generated by direct proportion, inverse proportion, conjunction and disjunction are called continuous Boolean functions, because they satisfy the axioms of Boolean algebra. Since it is desired that a qualitative expression be obtained, some quantitative values should be ignored. For example, function “A” in Fig. 7 is diﬀerent from direct proportion x but the function is a proportion. So the two functions should be identiﬁed as the same in the qualitative expression. That is, in [0,1], xk (k ≥ 2) should be identiﬁed with x in the qualitative expression. Mathematically, a norm is necessary, by which the distance among the two functions is 0. The qualitative norm can be introduced. In {0,1}, a unit of a neural network is a multilinear function. The multilinear function space is a Euclidean space. See Sect. 4. So the unit can be approximated to a Boolean function by Euclidean norm. In [0,1], similar facts hold, that is, a unit of a neural network is a multilinear function in the qualitative expression, that is, the qualitative norm, and the space of multilinear

(1,1) 1 x A 0 0 1
Fig. 7. Qualitative norm

44

H. Tsukimoto

functions is a Euclidean space in the qualitative norm. Thus the unit can be approximated to a continuous Boolean function by Euclidean norm. 5.2 Continuous Boolean Functions This subsection brieﬂy describes continuous Boolean functions [25, 31]. First, τ is deﬁned, which is necessary for the deﬁnition of the qualitative norm. Deﬁnition 5. τx is deﬁned as follows: Let f (x) be a real polynomial function. Consider the following formula: f (x) = p(x)(x − x2 ) + q(x) , where q(x) = ax + b, where a and b are real, that is, q(x) is the remainder. τx is deﬁned as follows: τx : f (x) → q(x) . The above deﬁnition implies the following property: τx (xk ) = x , where k ≥ 2. Deﬁnition 6. In the case of n variables, τ is deﬁned as follows:
n

τ=
i=1

τxi .

For example, τ (x2 y 3 + y + 1) = xy + y + 1 . Theorem 6. The functions obtained from Boolean functions by extending the domain from {0, 1} to [0, 1] can satisfy all axioms of Boolean algebra with the logical operations deﬁned below. Proof can be found in [25] and [31]. AND: τ (f g), OR: τ (f + g − f g), NOT: τ (1 − f ). Therefore, the functions obtained from Boolean functions by extending the domain from {0, 1} to [0,1] are called continuous Boolean functions. For example, xy and 1 − y(= y) are Boolean functions, where x, y ∈ [0, 1]. We show a simple example for logical operation. (X ∨ Y ) ∧ (X ∨ Y ) = X ∨ Y is calculated as follows:

Logical Regression Analysis

45

τ ((x + y − xy)(x + y − xy)) = τ (x2 + y 2 + x2 y 2 + 2xy − 2x2 y − 2xy 2 ) = x + y + xy + 2xy − 2xy − 2xy = x + y − xy . In the continuous domain, fuzzy rules can be obtained from trained neural networks by some algorithms [1] . The expression by continuous Boolean functions is more understandable than fuzzy rules, whereas continuous Boolean functions are worse than fuzzy rules in accuracy. 5.3 The Multilinear Function Space of the Domain [0,1] Multilinear functions of the domain [0,1] are considered. In the domain [0,1], a qualitative norm has to be introduced. Deﬁnition 7. An inner product in the case of n variables is deﬁned as follows:
1

< f, g >= 2n
0

τ (f g)dx ,

where f and g are multilinear functions. The above deﬁnition can satisfy the properties of inner product[23, 24, 31]. Deﬁnition 8. Norm |f | is deﬁned as follows: |f | = < f, f >.

The distance between functions is roughly measured by the norm. For example, function A in Fig. 7, which stands for xk , is diﬀerent from x. However, by the norm, the distance between the two functions is 0, because τ in the norm
1

< f, g > =

2n
0

τ (f g)dx

identiﬁes xk (k ≥ 2) with x. Therefore, the two functions are identiﬁed as being the same one in the norm. The norm can be regarded as a qualitative norm, because, roughly speaking, the norm identiﬁes increasing functions as direct proportions, identiﬁes decreasing functions as inverse proportions, and ignores the function values in the intermediate domain between 0 and 1. Theorem 7. The multilinear function space in the domain [0,1] is a Euclidean space with the above inner product: Proof can be found in [23, 24] and [31]. The orthonormal system is as follows:
n

φi =
j=1

e(xj ) (i = 1, . . . , 2n ) ,

46

H. Tsukimoto

where e(xj ) = 1 − xj or xj . It is easily understood that these orthonormal functions are the expansion of atoms in Boolean algebra of Boolean functions. In addition, it can easily be veriﬁed that the orthonormal system satisﬁes the following properties: < φi , φj > = 0 (i = j) , 1 (i = j) ,
2n

f =
i=1

< f, φi > φi .

Example. In the case of 2 variables, the orthonormal functions are as follows: xy, x(1 − y), (1 − x)y, (1 − x)(1 − y) . and the representation by orthonormal functions of x + y − xy of two variables (dimension 4) is as follows: f = 1 · xy + 1 · x(1 − y) + 1 · (1 − x)y + 0 · (1 − x)(1 − y) . When the domain is [0,1], neural networks are approximated to multilinear functions with the following: xn = x (n ≤ a) 0 (n > a),

where a is a natural number. When a = 1, the above approximation is the linear approximation. 5.4 The Polynomial Algorithm in the Continuous Domain The polynomial algorithm for the continuous domain is the same as that for the discrete domain, because the multilinear function space of [0,1] is the same as the multilinear function space of {0, 1}. The rules obtained are continuous Boolean functions. A theoretical analysis of the error in the case of the [0,1] domain will be included in future work. However, experimental results show that the algorithm works well in the continuous domains, as explained in [28] and [33].

6 Discussions
6.1 On the Continuous Domain When classes are continuous, other techniques such as decision tree do not work well as explained in Sect. 2. Therefore, rule extraction from mathematical

Logical Regression Analysis

47

formulas with continuous classes is important. Continuous Boolean functions work well for linear formulas, but cannot express the detailed information on neural networks, because the neural networks are nonlinear. The continuous Boolean functions are insuﬃcient, and therefore, extracting rules described by intervals such as (200 ≤ t ≤ 300) ∨ (p ≤ 2.5) ∨ (70 ≤ h ≤ 90) → q ≤ 0.2 from neural networks is needed. Usual real data consist of discrete data and continuous data. Therefore, rule extraction from a mixture of discrete attributes and continuous attributes is needed. 6.2 On the Prediction Domain Training domains are much smaller than prediction domains. For example, in the case of voting-records, which consist of 16 binary attributes, the number of possible training data is 216 (= 65536), while the number of training data is about 500. The outputs of a neural network are almost 100% accurate for about 500 training data, and are predicted values for the other approximately 65000 data. These predicted values are probabilistic, because the parameters for the neural network are initialized probabilistically. 65000 is much greater than 400, that is, the probabilistic part is much larger than the non-probabilistic part. Therefore, when a rule is extracted from a neural network, the predicted values of the neural network have to be dealt with probabilistically. We have developed two types of algorithms. The ﬁrst one deals with the whole domain equally [27]. The second one deals with only the training domain and basically ignores the prediction domain [28, 33]. Both algorithms can be regarded as opposite extremes. We also have developed an algorithm which deals with the prediction domains probabilistically[16]. Future work includes the development of algorithms dealing with the prediction domains appropriately. 6.3 Logical Regression Analysis There are several mathematical formulas obtained by regression analyses. It is desired that rule extraction techniques from mathematical formulas be applied to nonparametric regression analysis, (multivariate) autoregression analysis, regression trees, diﬀerential equations (diﬀerence equations), and so on. See Fig. 8. The data mining technique consisting of regression analysis and the Approximation Method is called Logical Regression Analysis (LRA). We have applied the Approximation Method to nonparametric regression analysis, that is, we have developed LRA using nonparametric regression analysis. We have been applying LRA using nonparametric regression analysis to fMRI images to discover the rules of brain functions.

48

H. Tsukimoto

neural networks multivariate autoregression linear formulas nonparametric regression regression trees differential equations (diffence equations)
Fig. 8. The development of LRA

7 On the Data Mining from fMRI Images
The rest of the paper describes the data mining from fMRI images by LRA. This section explains the data mining from fMRI images. The analysis of brain functions using functional magnetic resonance imaging (fMRI), positron emission tomography (PET), magnetoencephalography(MEG) and so on is called non-invasive analysis of brain functions[18]. As a result of the ongoing development of non-invasive analysis of brain functions, detailed functional brain images can be obtained, from which the relations between brain areas and brain functions can be understood, for example, a relation between a few areas in the brain and an auditory function[10]. Several brain areas are responsible for a brain function. Some of them are connected in series, and others are connected in parallel. Brain areas connected in series are described by “AND” and brain areas connected in parallel are described by “OR”. Therefore, the relations between brain areas and brain functions are described by rules. Researchers are trying to heuristically discover the rules from functional brain images. Several statistical methods such as principal component analysis, have been developed. However, the statistical methods can only present some principal areas for a brain function. They cannot discover rules. fMRI images can be dealt with by supervised inductive learning. However, the conventional inductive learning algorithms[19] do not work well for fMRI images, because there are strong correlations between attributes (pixels) and a small number of samples. There are two solutions for the above two problems. The ﬁrst one is the modiﬁcation of the conventional inductive learning algorithms. The other one is nonparametric regression analysis. The modiﬁcation of the conventional inductive learning algorithms would require a lot of eﬀort. On the other hand, nonparametric regression analysis has been developed for the above two problems. Therefore, we have been using nonparametric regression analysis for the data mining from fMRI images. The outputs of nonparametric regression analysis are linear formulas, which are not rules. However, we have already developed a rule extraction algorithm from regression formulas [16, 26, 29], that is, we have developed Logical Regression Analysis (LRA) as described above.

Logical Regression Analysis

49

Since brains are three dimensional, three dimensional LRA is appropriate. However, the three dimensional LRA needs a huge computation time, for example, many years. Therefore, we applied two dimensional LRA to fMRI images as the ﬁrst step. We applied LRA to artiﬁcial data, and we conﬁrmed that LRA works well for artiﬁcial data[30]. We applied LRA to several experimental tasks such as ﬁnger tappings and calculations. In the experiments of ﬁnger tapping, we compared the results of LRA with z-score[39], which is the typical conventional method. In the experiments, LRA could rediscover a little complicated relation, but z-score could not rediscover the relation. As the result, we conﬁrmed that LRA works better than z-score. The details can be found in [37]. In the experiments of calculations, we conﬁrmed that LRA worked well, that is, LRA rediscovered well-known facts regarding calculations, and discovered new facts regarding calculations. This paper reports the experiments of calculations.

8 The Data Mining from fMRI Images by Logical Regression Analysis
First, the data mining from fMRI images by Logical Regression Analysis is outlined. Second, nonparametric regression analysis is brieﬂy explained. Finally, related techniques are described[34, 36, 37]. 8.1 The Outline of Data Mining from fMRI Images by Logical Regression Analysis The brain is 3-dimensional. In fMRI images, a set of 2-dimensional images(slices) represents a brain. See Fig. 9. 5 slices are obtained in Fig. 9. Figure 10 shows a real fMRI image. When an image consists of 64×64(= 4096) pixels, Fig. 10 can be represented as Fig. 11. In Fig. 11, white pixels mean activations and dot pixels mean inactivations. Each pixel has the value of the activation. An experiment consists of several measurements. Figure 12 means that a subject repeats a task (for example, ﬁnger tapping) three times. “ON” in the upper part of the ﬁgure means that a subject executes the task and “OFF”

Fig. 9. fMRI images(3 dimension)

50

H. Tsukimoto

Fig. 10. A fMRI image

1

2

3

...

64 128

65 66 67 129 130 131

4033

4096

Fig. 11. An example of fMRI image

ON

OFF

Fig. 12. Measurement

means that the subject does not execute the task, which is called rest. Bars in the lower part of the ﬁgure mean measurements. The ﬁgure means 24 measurements. When 24 images(samples) have been obtained, the data of a slice can be represented as Table 2. Y(N) in the class stand for on (oﬀ) of an experimental task. From Table 2, machine learning algorithms can be applied to fMRI images. In the case of Table 2, the attributes are continuous and the class is discrete.

Logical Regression Analysis Table 2. fMRI Data 1 S1 S2 ··· S24 10 21 ··· 16 2 · · · 4096 Class 20 16 ··· 39 · · · 11 · · · 49 ··· ··· · · · 98 Y N ··· N

51

Attributes (pixels) in image data have strong correlations between adjacent pixels. Moreover it is very diﬃcult or impossible to obtain suﬃcient fMRI brain samples, and so there are few samples compared with the number of attributes (pixels). Therefore, the conventional supervised inductive learning algorithms such as C4.5[19] do not work well, which was conﬁrmed in [30]. Nonparametric regression analysis works well for strong correlations between attributes and a small number of samples. LRA using nonparametric regression analysis works well for the data mining from fMRI images. Figure 13 shows the processing ﬂow. First, the data is mapped to the standard brain[22]. Second, the brain parts of fMRI images are extracted using Standard Parametric Mapping:SPM (a software for brain images analysis[39]) Finally, LRA is applied to each slice.

Data Preprocessing
Mapping to standard brain

Extracting brain part

Logical regression analysis Nonparametric regression

Rule extraction Comparison Rule
Fig. 13. Processing ﬂow

z-score

52

H. Tsukimoto

8.2 Nonparametric Regression Analysis First, for simpliﬁcation, the 1-dimensional case is explained[8]. 1-Dimensional Nonparametric Regression Analysis Nonparametric regression analysis is as follows: Let y stand for a dependent variable and t stand for an independent variable and let tj (j = 1, . . . , m) stand for measured values of t. Then, the regression formula is as follows: y= aj tj + e(j = 1, . . . , m) ,

where aj are real numbers and e is a zero-mean random variable. When there are n measured values of y, yi = aj tij + ei (i = 1, . . . , n) .

For example, in the case of Table 2, m = 4096, n = 24, t11 = 10, t12 = 20, t1 4096 = 11, and y1 = Y. In usual linear regression, error is minimized, while, in nonparametric regression analysis, error plus continuity or smoothness is minimized. When continuity is added to error, the evaluation value is as follows:
n n

1/n
i=1

(yi − yi )2 + λ ˆ
i=1

(ˆi+1 − yi )2 , y ˆ

where y is an estimated value. The second term in the above formula is the ˆ diﬀerence of ﬁrst order between the adjacent dependent variables, that is, the continuity of the dependent variable. λ is the coeﬃcient of continuity. When λ is 0, the evaluation value consists of only the ﬁrst term, that is, error, which means the usual regression. When λ is very big, the evaluation value consists of only the second term, that is, continuity, which means that the error is ignored and the solution y is a constant. ˆ The above evaluation value is eﬀective when the dependent variable has continuity, for example, when the measured values of the dependent variable are adjacent in space or in time. Otherwise, the above evaluation value is not eﬀective. When the dependent variable does not have continuity, the continuity of coeﬃcients aj s is eﬀective, which means that adjacent measured values of the independent variable have continuity in the inﬂuence over the dependent variables. The evaluation value is as follows:
n m

1/n
i=1

(yi − yi )2 + λ ˆ
j=1

(aj+1 − aj )2

When λ is ﬁxed, the above formula is the function of ai (yi is the function ˆ of ai ). Therefore, ai s are determined by minimizing the evaluation value, and the optimal value of λ is determined by cross validation.

Logical Regression Analysis

53

Calculation Let X stand for n × m matrix. Let tij be an element of X. Let y stand for a vector consisting of yi . m × m matrix C is as follows:   1 −1  −1 2 −1   C=   −1 2 −1 ... Cross validation CV is as follows: CV = n˜t y y ˜ y = Diag(I − A)−1 (I − A)y ˜ A = X(Xt X + (n − 1)λC)−1 Xt , where Diag(A) is a diagonal matrix whose diagonal components are A’s diagonal components. The coeﬃcients ˆ are as follows: a ˆ = (Xt X + nλo C)−1 Xt y , a where λo is the optimal λ determined by cross validation. 2-Dimensional Nonparametric Regression Analysis In 2-dimensional nonparametric regression analysis, the evaluation value for the continuity of coeﬃcients aij is modiﬁed. In one dimension, there are two adjacent measured values, while, in two dimensions, there are four adjacent measured values. The evaluation value for the continuity of coeﬃcients is not (ai+1 − ai )2 , but the diﬀerences of ﬁrst order between a pixel and the four adjacent pixels in the image. For example, in the case of pixel 66 in Fig. 11, the adjacent pixels are pixel 2, pixel 65, pixel 67, and pixel 130, and the evaluation value is as follows: (a2 − a66 )2 + (a65 − a66 )2 + (a67 − a66 )2 + (a130 − a66 )2 . Consequently, in the case of two dimensions, C is modiﬁed in the way described above. When continuity is evaluated, four adjacent pixels are considered, while smoothness is evaluated, eight adjacent pixels are considered, for example, in the case of pixel 66 in Fig. 11, the adjacent pixels are pixel 1, pixel 2, pixel 3, pixel 65, pixel 67, pixel 129, pixel 130 and pixel 131.

54

H. Tsukimoto

8.3 Related Techniques This subsection brieﬂy explains two popular techniques, z-score and independent component analysis, and compares them with LRA. z-Score z-score is widely used in fMRI images. z-score is calculated pixel-wise as follows: |M t − M c| , z-score = √ σt2 + σc2 where M t: M c: σt: σc: Average of task images Average of rest images Standard deviation of task images Standard deviation of rest images

Task images mean the images in which a subject performs an experimental task. Rest images mean the images in which a subject does not perform an experimental task. When z-score is 0, the average of task images equals the average of rest images. When z-score is 1 or 2, the diﬀerence between the average of task images and the average of rest images is big. The areas whose z-scores are big are related to the experimental task. However, z-score does not tell which slices are related to the experimental task and does not tell the connections among the areas such as serial connection or parallel connection. Independent Component Analysis Independent component analysis (ICA)[12] can be roughly deﬁned as follows. Let n independent source signals at a time t be denoted by S(t) = (S1 (t), . . . , Sn (t)) . Let m (mixed) observed signals at a time t be denoted by X(t) = (X1 (t), . . . , Xm (t)) . X (t) is assumed to be the linear mixture of S(t), that is, X(t) = AS(t) , where A is a matrix. ICA obtains source signals S(t) from observed signals X under the assumption of the independence of source signals. Notice that the matrix A is unknown. Independent Component Analysis (ICA) is applied to fMRI images. LRA is advantageous compared with ICA respecting the following points:

Logical Regression Analysis

55

1. LRA uses classes. That is, LRA uses task/rest information, while ICA does not use task/rest information. 2. LRA conserves the spatial topologies in the images, while ICA cannot conserve the spatial topologies in the images. 3. LRA works well in the case of small samples, while it is not sure if ICA works well in the case of small samples. 4. LRA does not fall into a local minimum, while ICA falls into a local minimum. 5. LRA’s outputs can represent the connections among areas, while ICA’s outputs cannot represent the connections among the areas.

9 The Experiments of Calculations
In the experiments of calculations, we conﬁrmed that LRA worked well, that is, we rediscovered well-known facts regarding calculations, and discovered new facts regarding calculations. In the experiment, a subject adds a number repeatedly in the brain. The experimental conditions follow: Magnetic ﬁeld: 1.5 tesla Pixel number: 64 × 64 Subject number: 8 Task sample number: 34 Rest sample number: 36 Table 3 shows the errors of nonparametric regression analysis. Slice 0 is the image of the bottom of brain and slice 31 is the image of the top of brain. We focus on the slices whose errors are small, that is, the slices related to calculation. 133, . . . , 336 in the table are the ID numbers of subjects. Table 4 summarizes the results of LRA. Numbers in parenthesis mean slice numbers. Figure 14,. . . , Fig. 19 show the main results. White indicates high activity, dark gray indicates low activity, and black indicates non-brain parts. White and dark gray ares are connected by conjunction. For example, let A stand for the white area in Fig. 14 and B stand ¯ for the dark gray area in Fig. 14. Then, Fig. 14 is interpreted as A ∧ B, which means area A is activated and area B is inactivated. The extracted rules are represented by conjunctions, disjunctions and negations of areas. The conjunction of areas means the co-occurrent activation of the areas. The disjunction of areas means the parallel activation of the areas. The negation of an area means a negative correlation. LRA can generate rules including disjunctions. However, the rules including disjunctions are too complicated to be interpreted by human experts in brain science, because they have paid little attention to the phenomena. Therefore, the rules including disjunctions are not generated in the experiments. Researchers in brain science have paid attention to positive correlations, and have not paid attention to negative correlations. LRA can detect negative correlations, and so is expected to discover new facts.

56

H. Tsukimoto Table 3. Results of nonparametric regression analysis Slice 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 133 0.924 0.418 0.375 0.016 0.456 0.120 0.965 1.001 1.001 0.968 1.001 1.001 1.001 0.828 0.550 0.528 0.571 0.009 0.089 0.642 0.887 0.282 0.281 0.521 0.814 0.336 0.603 0.535 0.719 0.942 0.898 0.746 135 0.882 0.030 0.538 0.510 0.437 0.469 0.434 0.230 0.388 0.473 0.008 0.066 0.736 0.793 0.822 0.805 0.778 0.007 0.060 0.238 0.514 0.532 0.415 0.422 0.270 0.394 0.008 0.062 0.010 0.310 0.360 0.026 312 0.444 0.546 0.337 0.585 0.519 0.473 0.602 0.430 0.434 0.362 0.447 0.383 0.302 0.525 0.349 0.298 0.494 0.159 0.663 0.573 0.383 0.256 0.613 0.229 0.401 0.411 0.390 0.324 0.371 0.400 0.547 0.023 317 0.547 0.587 0.435 0.430 0.446 0.376 0.138 0.309 0.222 0.281 0.357 0.380 0.312 0.222 0.523 0.569 0.509 0.615 0.010 0.405 0.376 0.028 0.167 0.227 0.439 0.195 0.180 0.191 0.271 0.257 0.283 0.445 321 0.0039 0.298 0.278 0.282 0.157 0.265 0.380 0.119 0.478 0.390 0.341 0.167 0.397 0.455 0.023 0.107 0.008 0.238 0.011 0.185 0.149 0.018 0.045 0.048 0.013 0.469 0.477 0.308 0.167 0.169 0.209 0.187 331 0.870 0.814 0.723 0.743 0.636 0.698 0.475 0.175 0.246 0.409 0.358 0.275 0.021 0.845 0.229 0.439 0.354 0.159 0.033 0.426 0.177 0.219 0.213 0.306 0.212 0.148 0.107 0.279 0.436 0.353 0.467 0.197 332 0.455 0.028 0.381 0.402 0.419 0.385 0.420 0.482 0.387 0.193 0.227 0.115 0.181 0.204 0.130 0.338 0.377 0.561 0.519 0.470 0.214 0.303 0.352 0.050 0.350 0.414 0.358 0.455 0.237 0.023 0.464 0.084 336 0.306 0.946 0.798 0.798 0.058 0.366 0.541 0.547 0.704 0.913 0.908 0.914 0.909 0.733 0.474 0.374 0.493 0.774 0.711 0.689 0.430 0.548 0.528 0.450 0.570 0.689 0.541 0.413 0.649 0.775 0.157 0.195

Figures are taken from feet, and so the left side in the ﬁgures represents the right side of the brain, and the right side in the ﬁgures represents the left side of the brain. The upper side in the ﬁgures represents the front of the brain, and the lower side in the ﬁgures represents the rear of the brain. Activation in the left angular gyrus and supramarginal gyrus was observed in 4 and 3 cases, respectively, and activation in the right angular gyrus and supramarginal gyrus was observed in 3 cases and 1 case, respectively. Clinical observations show that damage to the left angular and supramarginal gyrii causes acalculia which is deﬁned as an impairment of the ability to calculate. Despite the strong association of acalculia and left posterior parietal lesions, there are certain characteristics of acalculia that have led to the suggestion

Logical Regression Analysis Table 4. Results of LRA NO. 133 cingulate gyrus (17) Left Cerebellum(3) superior frontal gyrus(17) inferior frontal gyrus(17) superior temporal plane(17,18) middle frontal gyrus(18,21) angular gyrus(21,22) inferior frontal gyrus(17,18) superior temporal plane(17,18) precuneus(26,28) gyrus (21) inferior frontal gyrus(15) angular gyrus(21) supramarginal gyrus(18,21) middle frontal gyrus(23) gyrus (27) inferior frontal gyrus(18,21) cuneus(21,22,26,27) gyrus (16,22,24) inferior frontal gyrus(14) postcentral gyrus(16,18) cuneus(21) parieto-occipital sulcus(22) supramarginal gyrus(24) gyrus (25,26) inferior frontal gyrus(12,17,18) angular gyrus(17,18,26) supramarginal gyrus(17,18) inferior temporal gyrus(1) Cerebellum(1) postcentral gyrus(33) middle temporal gyrus(12) pre-,post-central gyrus(14) angular gyrus(23) middle frontal gyrus(29) Cerebellum(0,5) middle temporal gyrus(4,5) middle frontal gyrus(30,31) precentral gyrus(31) superior frontal gyrus(32) Right

57

Cerebellum(0,5) middle frontal gyrus(25)

135

312 cingulate

superior frontal gyrus(26) superior parietal gyrus(26,28) angular gyrus(17)

317 cingulate 321 cingulate

angular gyrus(26) cuneus(16,18) parieto-occipital sulcus(21) supramarginal gyrus(22,24) angular gyrus(17,18,26) middle temporal gyrus(7,12,17) inferior temporal gyrus(1) Cerebellum(1) middle frontal gyrus(29) superior parietal gyrus(29,43) Cerebellum(0,5) superior parietal gyrus(30) occipital gyrus(11)

331 cingulate

332

336

of a right-hemispheric contribution. Clinical observations also suggest that acalculia is caused by lesions not only in the left parietal region and frontal cortex but also in the right parietal region. Figure 14 shows slice 18 of subject 331 and Fig. 15 shows slice 26 of subject 331.

58

H. Tsukimoto

Fig. 14. Slice 18 331

Fig. 15. Slice 26 331

Fig. 16. Slice 17 135

Signiﬁcant activation was observed in the left inferior frontal gyrus in 6 out of 8 cases. On the other hand, none was observed in the right inferior frontal gyrus. The result suggests that the left inferior frontal gyrus including Broca’s area is activated in most subjects in connection with implicit verbal processes required for the present calculation task. Furthermore, signiﬁcant activation in frontal region including middle and superior frontal regions was found in 8 cases (100%) in the left hemisphere and in 3 cases in the right hemisphere. The left dorsolateral prefrontal cortex may play an important role as a working memory for calculation. Figure 16 shows slice 17 of 135 and Fig. 17 shows slice 18 of subject 317. In addition to these activated regions, activation in cingulate gyrus, cerebellum, central regions and occipital regions was found. The activated regions depended on individuals, suggesting diﬀerent individual strategies. Occipital regions are related to spatial processing, and the cingulate gyrus is related to intensive attention. Central regions and the cerebellum are related to motor imagery. 5 out of 8 subjects used their cingulate gyrus, which means that they were intensively attentive. Figure 18 shows slice 17 of 133. 3 out of 8 subjects use cerebellum, which is thought not to be related to calculation. Figure 19 shows slice 1 of 332. The above two results are very interesting discoveries that have never been experimentally conﬁrmed so far. The problem

Fig. 17. Slice 18 317

Fig. 18. Slice 17 133

Fig. 19. Slice 1 332

Logical Regression Analysis

59

of whether these regions are speciﬁcally related to mental calculation or not is to be investigated in further research with many subjects. LRA has generated rules consisting of regions by conjunction and negation. As for conjunctions and negations, the results showed the inactivated regions simultaneously occurred with the activated regions. In the present experiment, inactivation in the brain region contralateral to the activated region was observed, suggesting inhibitory processes through corpus callosum. LRA has the possibility of providing new evidence in brain hemodynamics.

10 Conclusions
This paper has explained that rule extraction from mathematical formulas is needed for perfect data mining techniques. This paper has brieﬂy reviewed the rule extraction techniques for neural networks and has brieﬂy explained the Approximation Method developed by the author. The author has developed a data mining technique called Logical Regression Analysis (LRA), which consists of regression analysis and the Approximation Method. One of LRA’s merits is that LRA can deal with images. The author has been applying LRA using nonparametric regression analysis to fMRI images to discover the rules of brain functions. The LRA works better than z-score and has discovered new “relations” respecting brain functions. In the experiments, LRA was applied to slices, that is, 2-dimensional fMRI images. However, complicated tasks such as calculation are related to at least a few areas, and so the application of LRA to a set of a few slices is necessary for fruitful data mining from fMRI images. It is desired that LRA be applied to 3-dimensional fMRI images. However, the nonparametric regression analysis of 3-dimensional fMRI images needs a huge computational time. Therefore, the computational time should be reduced, which is included in future work. There are a lot of open problems in the ﬁeld of LRA, therefore the authors hope that researchers will tackle this ﬁeld.

References
1. Andrews R, Diederich J, Tickle AB (1995) Survey and critique of techniques for extracting rules from trained artiﬁcial neural networks. Knowledge-Based Systems 8 (6):373–189. 28, 45 2. Andrews R, Diederich J eds (1996) Rules and Networks, Proceedings of the Rule Extraction from Trained Artiﬁcial Neural Networks Workshop, AISB’96. Queensland University of Technology. 28 3. Birkhoﬀ G, Bartee TC (1970) Modern Applied Algebra. McGraw−Hill. 30 4. Castro JL, Mantas CJ, Benitez JM (2002) Interpretation of Artiﬁcial Neural Networks by Means of Fuzzy Rules. IEEE Transactions on Neural Networks 13 (1):101–116. 42

60

H. Tsukimoto

5. Craven MW, Shavlik JW (1993) Learning symbolic rules using artiﬁcial neural networks. Proceedings of the Tenth International Machine Learning Conference: 73–80. 28, 29 6. Craven MW, Shavlik JW (1994) Using sampling queries to extract rules from trained neural networks. Proceedings of the Eleventh International Machine Learning Conference: 37–45. 28 7. Etchells, TA (2003) Rule Extraction from Neural Networks: A Practical and Eﬃcient Approach, Doctoral Dissertation, Liverpool John Moores University. 29 8. Eubank RL (1998) Spline Smoothing and Nonparametric Regression. Marcel Dekker, Newyork. 52 9. Kakimoto M, Morita C, Kikuchi Y, Tsukimoto H (2000) Data Mining from Functional Brain Images. Proceedings of the First International Workshop on Multimedia Data Mining, MDM/KDD2000: 91–97. 30 10. Kikuchi Y, Endo H, Yoshizawa S, Kita M, Nishimura C, Tanaka M, Kumagai T, Takeda T (1997) Human cortico-hippocampal activity related to auditory discrimination revealed by neuromagnetic ﬁeld. Neuro Report 8:1657–1661. 48 11. Kuok CM, Fu A, Wong MH (1998) Mining fuzzy association rules in databases. ACM SIGMOD record 27 (1):1–12. 26 12. Lee TW (1998) Independent Component Analysis. Kluwer Academic Publishers. 54 13. Linial N, Mansour Y, Nisan N (1993) Constant Depth Circuits, Fourier Transform, and Learnability. Journal of the ACM 40 (3):607–620. 42 14. Lu HJ, Setiono R, Liu H (1998) Eﬀective data mining using neural networks. IEEE Trans. on Knowledge and Data Engineering 8:957–961. 22 15. Miwa T et al. (1998) PLS regression and nonparametric regression in spectrum data analysis. Proc. 17th Japan SAS User Convention and Research Presentation, pp. 137–148. 16. Morita C, Tsukimoto H (1998) Knowledge discovery from numerical data. Knowledge-based Systems 10 (7):413–419. 29, 47, 48 17. Omlin CW, Giles CL (1996) Extraction of Rules from Discrete-time Recurrent Neural Networks. Neural Networks 9 (1):41–52. 29 18. Posner M.I, Raichle M.E (1997) Images of Mind. W H Freeman & Co. 48 19. Quinlan JR (1993) C4.5: Programs for machine learning. Morgan Kaufmann Pub. 21, 26, 48, 51 20. Setiono R, Liu H (1995) Understanding Neural Networks via Rule Extraction. Proceedings of The 14th International Joint Conference on Artiﬁcial Intelligence: 480–485. 29 21. Srikant S, Agrawal R (1996) Mining quantitative association rules in large relational tables. ACM SIGMOD conf. on management of Data: 41–46. 21, 26 22. Talairach J, Tournoux P (1988) Coplanar Streoaxic atlas of the human brain. Thieme Medica. 51 23. Tsukimoto H, Morita C (1994) The Discovery of Propositions in Noisy Data. Machine Intelligence 13:143–167. Oxford University Press. 45 24. Tsukimoto H (1994) The discovery of logical propositions in numerical data. AAAI’94 Workshop on Knowledge Discovery in Databases: 205–216. 29, 45 25. Tsukimoto H (1994) On continuously valued logical functions satisfying all axioms of classical logic. Systems and Computers in Japan 25 (12):33–41. SCRIPTA TECHNICA, INC. 44 26. Tsukimoto H, Morita C (1995) Eﬃcient algorithms for inductive learning-An application of multi-linear functions to inductive learning. Machine Intelligence 14:427–449. Oxford University Press. 29, 48

Logical Regression Analysis

61

27. Tsukimoto H (1996) An Algorithm for Extracting Propositions from Trained Neural Networks Using Multilinear Functions. Rules and Networks, Proceedings of the Rule Extraction from Trained Artiﬁcial Neural Networks Workshop, AISB’96: 103–114. Queensland University of Technology. 30, 40, 47 28. Tsukimoto H (1997) Extracting Propositions from Trained Neural Networks. Proceedings of The 15th International Joint Conference on Artiﬁcial Intelligence: 1098–1105. 30, 33, 40, 41, 46, 47 29. Tsukimoto H, Morita C, Shimogori N (1997) An Inductive Learning Algorithm Based on Regression Analysis. Systems and Computers in Japan 28(3):62–70. SCRIPTA TECHNICA, INC. 29, 36, 48 30. Tsukimoto H, Morita C (1998) The Discovery of Rules from Brain Images. Discovery Science, Proceedings of the First International Conference DS’98: 198– 209. 30, 49, 51 31. Tsukimoto H (1999) Symbol pattern integration using multilinear functions. In: Furuhashi T, Tano S, Jacobsen HA (eds) Deep Fusion of Computational and Symbolic Processing. Springer Verlag. 30, 34, 35, 44, 45 32. Tsukimoto H (1999) Rule extraction from prediction models. The Third PaciﬁcAsia International Conference on Knowledge Discovery and Data Mining (PAKDD’99): 34–43. 23 33. Tsukimoto H (2000) Extracting Rules from Trained Neural Networks. IEEE Transactions on Neural Networks 11(2):377–389. 23, 30, 33, 34, 35, 40, 41, 46, 47 34. Tsukimoto H, Kakimoto M, Morita C, Kikuchi Y, Hatakeyama E, Miyazaki Y (2000) Knowledge Discovery from fMRI Brain Images by Logical Regression Analysis. LNAI 1967 The Proceedings of The Third International Conference on Discovery Science: 212–224. 30, 49 35. Tsukimoto H, Morita C (2001) Connectionism as Symbolicism:Artiﬁcial Neural Networks as Symbols. Sanshusha. 23 36. Tsukimoto H, Kakimoto M, Morita C, Kikuchi Y (2002) Rule Discovery from fMRI Brain Images by Logical Regression Analysis. Progress in Discovery Science: 232–245. Springer-Verlag. 23, 30, 49 37. Tsukimoto H, Kakimoto M, Morita C, Kikuchi Y (2004) Nonparametric regression analysis of functional brain images. Systems and Computers in Japan. 35(1):67–78. John Wiley and Sons. 30, 49 38. Zhou ZH, Jiang Y, Chen SF (2000) A general neural framework for classiﬁcation rule mining. International Journal of Computers, Systems and Signals 1(2):154– 168. 27 39. http://www.ﬁl.ion.ucl.ac.uk/spm. 49, 51 40. http://www.rulequest.com/cubist-info.html. 26 41. http://www2.toshiba.co.jp/datamining/index.htm. 22, 28

A Feature/Attribute Theory for Association Mining and Constructing the Complete Feature Set
Tsau Young Lin
Department of Computer Science San Jose State University, San Jose, California 95192-0103 tylin@cs.sjsu.edu Summary. A correct selection of features (attributes) is vital in data mining. For this aim, the complete set of features is constructed. Here are some important results: (1) Isomorphic relational tables have isomorphic patterns. Such an isomorphism classiﬁes relational tables into isomorphic classes. (2) A unique canonical model for each isomorphic class is constructed; the canonical model is the bitmap indexes or its variants. (3) All possible features (attributes) is generated in the canonical model. (4) Through isomorphism theorem, all un-interpreted features of any table can be obtained.

Keywords: attributes, feature, data mining, granular, data model

1 Introduction
Traditional data mining algorithms search for patterns only in the given set of attributes. Unfortunately, in a typical database environment, the attributes are selected primarily for record-keeping, not for understanding of real world. Hence, it is highly possible that there are no visible patterns in the given set of attributes; see Sect. 2.2. The fundamental question is: Is there a suitable transformation of features/attributes so that • The “invisible” patterns become visible in this new set? Fortunately, the answer is yes. To answer this question, we critically analyze the essence of association mining. Based on it, we are able • To construct the complete set of features for a given relational table. Many applications will be in the forth coming volumes [10]. Here are some important results:(Continue the count from the abstract) (5) all high frequency
T.Y. Lin: A Feature/Attribute Theory for Association Mining and Constructing the Complete Feature Set, StudFuzz 180, 63–78 (2005) c Springer-Verlag Berlin Heidelberg 2005 www.springerlink.com

64

T.Y. Lin

patterns (generalized association rules) of the canonical model can be generated by a ﬁnite set of linear inequalities within polynomial time. (6) Through isomorphism theorem, all high frequency patterns of any relational table can be obtained. 1.1 Basics Terms in Association Mining (AM) First, we recall (in fact, formalize) some basic terms. In traditional association rule mining, two measures, called the support and conﬁdence, are the main criteria. Among the two, support is the essential measure. In this paper, we will consider the support only. In other words, we will be interested in the high frequency patterns that are not necessary in the form of rules. They could be viewed as undirected association rules, or just associations. Association mining is originated from the market basket data [1]. However, in many software systems, the data mining tools are added to general DBMS. So we will be interested in data mining on relational tables. To be deﬁnitive, we have the following translations: 1. 2. 3. 4. a relational table is a bag relation (i.e., repeated tuples are permitted [8]) an item is an attribute value, a q-itemset is a subtuple of length q, or simply q-subtuple, A q-subtuple is a q-association or (high frequency) q-pattern, if its occurrences are greater than or equal to a given threshold.

2 Background and Scope
2.1 Scope – A Feature Theory Based on the Finite Data A feature is also called an attribute; the two terms have been used interchangeably. In the classical data model, an attribute is a representation of property, characteristic, and so forth [17]. It represents a human view of the universe (a slice of real world) – an intension view [5]. On the other hand, in modern data mining (DM), we are extracting information from the data. So in principle, the real world, including features (attributes), is encoded by and only by a ﬁnite set of data. This is an extension view or data view of the universe. However, we should caution that each techniques of data mining often use some information (background knowledge) other than data [6]. So the encoding of the universe is diﬀerent for diﬀerent techniques. For examples association mining (AM) (Sect. 3) uses only the relational table, while clustering techniques utilize not only the table (of points), but also the geometry of the ambient space. So the respective feature theories will be diﬀerent. In this paper, we will focus on Association Mining. Next, we will show some peculiar phenomena of the ﬁnite encoding. Let Table 1θ and 2θ be the new tables derived from the tables in Sect. 2.2 by

Features and Foundations Table 1. Ten point in (X,Y)-coordinate and Rotated coordinate Segment# Y S1 2 0 √ S2 3 = 1.73 √ 1 √ S3 2 = 1.41 √2 S4 1 3 S5 0 2 X-Y coordinate Table 1A X’ Y’ 1.99 0.17 1.64 1.15 1.29 1.53 0.85 1.81 −0.17 1.99 Rotates -5 degree Table 1B

65

rotating the coordinate systems θ degree. It should be easy to verify that (see Sect. 4.1 for the notion of isomorphism).

Proposition 2.1.1. Table 1A, 1B and 1θ are isomorphic, so are the Table 2A, 2B and 2θ. This proposition says even though the rotations of the coordinate system generate inﬁnitely many distinct features/attributes, they reduce to the same feature/attribute if the universe is encoded by a relational table. The main result of this paper is to determine all possible features of the encoded world. 2.2 Background – Mining Invisible Patterns Let us consider a table of 5 points in X-Y-plane, as shown in Table 1A. The ﬁrst column is the universe of the geometric objects. It has two attributes, which are the “X-Y coordinates.” This table has no association rule of length 2. By transforming, the “X-Y coordinates” to “Polar coordinate system” (Table 2A), interestingly Associations of length 2 appear . The key question is how can we ﬁnd such appropriate new features (polar coordinates).
Table 2. Ten points in polar coordinate and rotated coordinate Segment# Length Direction Length Direction S1 2.0 0 2.0 5 S2 2.0 30 2.0 35 S3 2.0 45 2.0 45 S4 2.0 60 2.0 65 S5 2.0 90 2.0 95 X-Y coordinate Rotates -5 degree Table 2A Table 2B

66

T.Y. Lin

3 Formalizing Association Mining
In this section, we will critically analyze the association (rule) mining. Let us start with a general question: What is data mining? There is no universally accepted formal deﬁnition of data mining, however the following informal description (paraphrase from [7]) is rather universal: • Deriving useful patterns from data. This “deﬁnition” points out key ingredients: data, patterns, methodology of derivations and the real world meaning of patterns (useful-ness). We will analyze each of them. 3.1 Key Terms “Word” and “Symbol” First we need to precisely deﬁne some key terms. A symbol is a string of “bit and bytes.” It has no formal real world meaning, more precisely, any real world interpretation (if there is one) does not participate in formal processing or computing. Mathematicians (in group theory, more speciﬁcally) use the term “word” for such purpose. However, in this paper, a “word” will be more than a symbol. A symbol is termed a word, if the intended real world meaning does participate in the formal processing or computing. In AI, there is a similar term, semantic primitive [2]; it is a symbol whose real world interpretation is not implemented. So in automated computing, a semantic primitive is a symbol. 3.2 What are Data? – A Table of Symbols To understand the nature of the data, we will examine how the data is created: In traditional data processing, (1) we select a set of attributes, called relational schema. Then (2) we (knowledge) represent a set of real world entities by a table of words. Kmap : V → Kword ; v −→ k where Kword is a table of words (this is actually the usual relational table). Each word, called an attribute value, represents a real world fact (to human); however the real world semantic is not implemented. Since Kword is a bag relation [8], it is more convenient to use the graph Kgraph = {(v, K(v) | v ∈ V }. If the context is clear, we may drop the subscript, so K is a map, an image or a graph. Next, how is the data processed? In traditional data processing environment, for example, the attribute name COLOR means exactly what a human thinks. Therefore its possible values are yellow, blue, and etc. More importantly, • DBMS processes these data under human commands, and carries out the human perceived-semantics. Such processing is called Computing with Words.

Features and Foundations

67

However, in the system, COLOR, yellow, blue, and etc are “bits and bytes” without any meaning, they are pure symbols. The same relational table is used by Association Mining (AM). But, the data are processed without human interventions, so the table of words Kword is processed as a table Ksymbol of symbols. DM : Kword ⇒ Ksymbol In summary, • The data (relational table) in AM is a table of symbols. 3.3 What are Patterns? and Computing with Symbols What are the possible patterns? The notion depends on the methodology. So we will examine the algorithms ﬁrst. A typical AM algorithm treats words as symbols. It just counts and does not consult human for any possible real world meaning of any symbol. As we have observed in previous section no real world meaning of any symbols is stored in the system. So an AM algorithm is merely a computing of pure symbols. AM transforms a table Ksymbol of symbols into a set Asymbol of association (rules)s of symbols. These associations are “expressions” of symbols. Therefore, • All possible patterns of AM are expressions of the symbols of the relational table. 3.4 Interpretation and Realization of Patterns The output of an AM algorithm is examined by human. So each symbol is alive again. Its interpretation (to human only) is assigned at the data creation time. So the patterns are interpreted by these interpretations of symbols. 1. Interpretation: A pattern, an expression of symbols, is an expression of words (to human). So a pattern is a mathematical expression of real world facts. 2. Realization: A mathematical expression of real world facts may or may not correspond to a real world phenomenon.

4 Understanding the Data – A Table of Symbols
In the previous section, we have concluded that the input data to AM is a table of symbols. In this section, we will explore the nature of such a table.

68

T.Y. Lin

4.1 Isomorphism – Syntactic Nature of AM We have explained how data is processed in (automated) data mining: The algorithms “forget” the real world meaning of each word, and regard the input data as pure symbols. Since no real world meaning of each symbol participates in the computing process if we replace the given set of symbols by a new set, then we can derive new patterns by simply replacing the symbols in “old” patterns. Formally, we have (Theorem 4.1. of [12]) Theorem 4.1.1. Isomorphic relational tables have isomorphic patterns. Though this is a very important theorem, its proof does not increase the understanding. Its proof is in the appendix. Isomorphism is an equivalence relation deﬁned on the family of all relational tables, so it classiﬁes the tables into isomorphic classes. Corollary 4.1.2. A pattern is a property of an isomorphic class. The impacts of this simple theorem are rather far reaching. It essentially declares that patterns are syntactic in nature. They are patterns of the whole isomorphic class, even though many somorphic relations may have very different semantics. Corollary 4.1.3. The probability theory based on the item counting is a property of isomorphic class. We will illustrate the idea by an example. The following example is adopted from ([8], pp 702): Example 4.1.4. In this example, we will illustrate the notion of isomorphism of tables and patterns. In Table 3, we present two “copies” of relational tables; they are obviously isomorphic (by adding prime’ to one table you will get the other one). For patterns (support = 2), we have the following: Isomorphic tables K and K have isomorphic q-associations: 1. 2. 3. 4. 1-association 1-association 2-association 2-association in in in in K: 30, 40, bar, baz, K : 30 , 40 , bar , baz , K: (30, bar) and (40, baz), K’: (30 , bar ) and (40 , baz ).

Two sets of q-association (q = 1,2) are obviously isomorphic in the sense that adding prime to associations in K become associations in K .

Features and Foundations Table 3. A Relational Table K and its Isomorphic Copy K V e1 e2 e3 e4 e5 e6 e7 e8 → → → → → → → → → F 30 30 40 50 40 40 30 40 G V f oo e1 bar e2 baz e3 f oo e4 bar e5 bar e6 bar e7 baz e8 → → → → → → → → → F 30 30 40 50 40 40 30 40 G f oo bar baz f oo bar bar bar baz

69

4.2 Bitmaps and Granules – Intrinsic Representations Due to the syntactic nature, as we have observed in last section, we can have a more intrinsic representation, that is a representation in which only the internal structure of the table is important, the real world meaning of each attribute value can be ignored. We will continue to use the same example. The following discussions essential excerpt from ( [8], pp 702). Let us consider the bitmap indexes for K (see Table 3) the ﬁrst attributes, F , would have three bit-vectors. The ﬁrst, for value 30, is 11000110, because the ﬁrst, second, sixth, and seventh tuple have F = 30. The other two, for 40 and 50, respectively, are 00101001 and 00010000. A bitmap index for G would also have three bit-vectors: 10010000, 01001010, and 00100101. It should be obvious that we will have the exact same bitmap table for K . Next, we note that a bit vector can be interpreted as a subset of V , called an elementary granule. For example, the bit vector, 11000110, of F = 30 represents the subset {e1 , e2 , e6 , e7 }. Similarly, 00101001, of F = 40 represents the subset {e3 , e5 , e8 }, and etc. Let us summarize the discussions in the following proposition: Proposition 4.2.1. Using Table 4 as a translation table, we transform a table of symbols (Table 3) into its respective 1. a bitmap table, and Table 5. 2. a granular table, Table 6. Conversely, Proposition 4.2.2. Using Table 4 as an interpretation table that interpret 1. Table 5 and Table 6 into Table 3, where (to human) each symbol corresponds to a real world fact. 2. Note that F -granules (and G-granules too) are mutually disjoints and form a covering of V . So the granules of each attribute induces a partition on V (an equivalence relation).

70

T.Y. Lin Table 4. Translation Table F -Value 30 40 50 G-Value F oo Bar Baz Bit-Vectors Granules = 11000110 ({e1, e2, e6, e7}) = 00101001 ({e3, e5, e8}) = 00010000 ({e4}) = Bit-Vectors Granules = 10010000 ({e1, e4}) = 01001010 ({e2, e5, e7}) = 00100101 ({e3, e6, e8})

Table 5. Contrasting Tables of Symbols and Bitmaps Table K V e1 e2 e3 e4 e5 e6 e7 e8 → → → → → → → → → F 30 30 40 50 40 30 30 40 G f oo bar baz f oo bar baz bar baz Bitmap Table BK F -bit 11000110 11000110 00101001 00010000 00101001 11000110 11000110 00101001 G-bit 10010000 01001010 00100101 10010000 01001010 00100101 01001010 00100101

Table 6. Contrasting Tables of Symbols and Granules Table K U v1 v2 v3 v4 v5 v6 e7 e8 → → → → → → → → → F 30 30 40 50 40 30 30 40 G f oo bar baz f oo bar baz bar baz Granular Table GK EF {e1, e2, e6, e7} {e1, e2, e6, e7} {e3, e5, e8} {e4} {e3, e5, e8} {e1, e2, e6, e7} {e1, e2, e6, e7} {e3, e5, e8} EG {e1, e4} {e2, e5, e7} {e3, e6, e8} {e1, e4} {e2, e5, e7} {e3, e6, e8} {e2, e5, e7} {e3, e6, e8}

3. Each elementary granule, for example, the elementary granule {e1 , e2 , e6 , e7 } of F = 30, consists of all entities that have (are mapped to) the same attribute value, in this case, F -value 30. In other words, F -granule {e1 , e2 , e6 , e7 } is the inverse of the value F = 30. It should be obvious that these discussions can be generalized: They are summarize in Proposition 5.1.1.

Features and Foundations

71

5 The Model and Language of High Frequency Patterns
As we have observed in Sect. 3.3, informally patterns are expressions (subtuples) of the symbols of the relational table. Traditional association mining considers only the “conjunction of symbols.” Are there other possible expressions or formulas? A big Yes, if we look at a relational table as a logic system. There are many such logic views, for example, deductive database systems, Datalog [21], and Decision Logic [19] among others. For our purpose, such views are too “heavy”, instead, we will take an algebraic approach. The idea is stated in [13] informally. There, the notion of “logic language” was introduced informally by considering the “logical formulas” of the names of elementary granules. Each “logical formula” (of names) corresponds to a set theoretical formula of elementary granules. In this section, we shall re-visit the idea more formally. 5.1 Granular Data Model (GDM) – Extending the Expressive Power Based on example, we have discussed granular data model in Sect. 4.2. Now we will discuss the general case. Let V be set of real world entities, A = {A1 , A2 , . . . , An } be a set of attributes. Let their (active) attribute domains be C = {C 1 , C 2 , . . . , C n }, where active is a database term to emphasize the fact that C j is the set of distinct values that occur in the current representation. Each C j , often denoted by Dom(Aj ), is a Cantor set. A relational table K can be regarded as a map (knowledge representation) Kmap : V −→ Dom(A) = Dom(A1 ) × . . . Dom(An ) Similarly, an attribute is also a map Aj : V −→ Dom(Aj ) ; v −→ c . The inverse of such an attribute map deﬁnes a partition on V (hence an equivalence relation); we will denote it by Qj and list some of its properties in: Proposition 5.1.1 1. The inverse image S = (Aj )(−1) (c) is an equivalence class of Qj . We say S is elementary granule, and c is the name of it. 2. For a ﬁxed order of V , S can be represented by a bit-vector. We also say c is the name of the bit vector. 3. By replacing each attribute value of the table Ksymbol by its bit-vector or elementary granule (equivalence class), we have the bitmap table BK or granular table GK respectively. 4. The equivalence relations, Q = {Q1 , Q2 , . . . , Qn }, play the role of attributes in Table GK and BK .

72

T.Y. Lin

5. For uniformity, we write V /Qj = Dom(Qj ), namely, we regard the quotient set as the attribute domain. 6. Theoretically, GK and BK conceptually represent the same granular data model; the diﬀerence is only in representations and is an internal matter. 7. We will regard the table K as an interpretation of GK and BK . The interpretation is an isomorphism (via a table similar to Table 4) By Theorem 4.1.1., the patterns in K, GK , BK are isomorphic and hence is the same (identiﬁed via interpretation). • It is adequate to do the AM in GK . The canonical model GK is uniquely determined by its universe V , and the family Q of equivalence relations. In other words, the pair (V, Q) determines and is determined by GK . Deﬁnition 5.1.1. The pair (V, Q) is called granular data model (GDM). (V, Q) is a model of some rather simple kind of logic, where the only predicates are equivalence predicates (predicates that satisfy the reﬂexive, symmetric and transitive properties). It was considered by both Pawlak and Tony Lee and has been called knowledge base, relation lattice, granular strucutre [9, 13, 19]. Note that the set of all elementary granule in (V, Q) generate a sub-Boolean algebra of the power set of V . By abuse of notation, we will use (V, Q) to denote this algebra. Since GK is a table format of (V, Q), we need to describe how GK is “embedded” into the Boolean algebra. We will extend Proposition 5.1.1, Item 7 into Proposition 5.1.2. An attribute value of GK , which is an elementary granule, is mapped to the same granule in (V, Q). A subtuple of GK , consisting of a set of elementary granules is mapped into the granule that is the intersection of those elementary granules; note two subtuples may be mapped to the same granule. 5.2 Algebraic Language and Granular Boolean Algebra The attribute values in K are pure symbols. Now we will introduce a new Boolean algebra LK as follows: We will use ∪ and ∩ as the join and meet of this Boolean algebra. LK is a free Boolean algebra subject to the following conditions: The ∩ between symbols in the same columns are
j j Bi ∩ Bk = ∀i = k ∀j

This condition reﬂects the fact that the elementary granules of the same column are mutually disjoint. We can give a more algebraic description [3]. Let F be the free Boolean Algebra generated by the symbols in K. Let I be the ideal generated by

Features and Foundations
j j Bi ∩ Bk ∀ i, k, j

73

Then the quotient algebra F/I = LK . We will regard this Boolean algebra as a language and call it Granular algebraic language . An attribute value in K can be regarded as the name of the corresponding elementary granule in GK and the elementary granule is the meaning set of the name. Recall that GDM (V, Q) can be regarded as Boolean algebra of elementary granules, and GK is “embedded” in (V, Q) (Proposition 5.1.2.) So the name-to-meaning set assignment, K → GK , can be extended to a homomorphism of Boolean algebras: name-to-meaning: LK −→ (V, Q); formula → meaning set .

• High frequency patterns of AM are formulas with large meaning set (the cardinality is large).

6 The Formal Theory of Features in AM
The theory developed here is heavily depended on the nature of association mining (AM) that are formalized in Sect. 3. 6.1 Feature Extractions and Constructions Let us examine some informal assertions, e.g., [18]: “All new constructed features are deﬁned in terms of original features,. . . .” and “Feature extraction is a process that extracts a set of new features from the original features through some functional mapping.” In summary the new feature is derived (by construction or extraction) from the given set of attributes. We will formalize the idea of features in association mining (AM). Perhaps, we should re-iterate that we are not formalizing the general notion of features that involves human view. Let K be the given relational table that has attributes A = {A1 , . . . An }. Next, let An+1 . . . An+m be the new attributes that are constructed or extracted. As we remark in Sect. 5.1, an attribute is a mapping from the universe to a domain, so we have the following new mappings. An+k : V −→ Dom(An+k ) . Now, let us consider the extended table, Ka , that includes both old and additional new attributes {A1 , . . . An . . . An+m }. In this extended table, by the meaning of feature construction, An+k , should be (extension) functionally dependent (EFD) on A. This fact implies, by deﬁnition of EFD, there is a mapping

74

T.Y. Lin

f n+k : Dom(A1 ) × . . . × Dom(An ) −→ Dom(An+k ) . such that An+k = f n+k ◦ (A1 × . . . × An . Those new extracted or constructed features, such as f n+k is called derived feature. 6.2 Derived Features in GDM Now we will consider the situation in GK , the granular table of K. In this section, we will express EFD f n+k in granular format, in other words, the granular form of f n+k is: V /(Q1 ∩ . . . ∩ Qn ) = V /Q1 × . . . × V /Qn −→ V /Qn+k The ﬁrst equality is a simple property of quotient sets. The second map is f n+k in its granular form. The granular form of f n+k implies that Qn+k is a coarsening of (Q1 ∩ . . . ∩ Qk ). So we have the following Proposition 6.2. Qn+k is a derived feature of GK if and only if Qn+k is a coarsening of (Q1 ∩ . . . ∩ Qn ). Let the original Table K have attributes A = {A1 , . . . An }. Let B ⊆ A and Y ∈ A (e.g., Y = An+k and YE = Qn+k ). Proposition 6.3. Y is a feature constructed from B if and only if the induced equivalence relation YE is a coarsening of the induced equivalence relation BE = (Qj1 ∩ . . . ∩ Qjm ), where Y ∈ A and B ⊆ A The proposition says all the new constructed features are coarsening of the intersection of the original features.

7 Universal Model – Capture the Invisibles
Let ∆(V ) be the set of all partitions on V (equivalence relations); ∆(V ) forms a lattice, where meet is the intersection of equivalence relations and join is the “union,” where the “union,” denoted by ∪j Qj , is the smallest coarsening of all Qj , j = 1, 2, . . . ∆(V ) is called the partition lattice. Let (V, Q = {Q1 , . . . , Qn }) be a GDM. Let L(Q) be the smallest sublattice of ∆(V ) that contains Q, and L∗ (Q) be the set of all possible coarsenings of (Q1 ∩ . . . ∩ Qn ). L∗ (Q) obviously forms a sublattice of ∆(V ); the intersection and “union” of two coarsenings is a coarsening. From Proposition 6.2., we can easily establish Theorem 7.1. Let GK be a granular table; its GDM is (V, Q). Then (V, L∗ (Q)) is a GDM that consists of all possible features for GK .

Features and Foundations

75

The set of all possible features of GK is the set D of all those derived features. By Proposition 6.2., D is the set of all those coarsenings of (Q1 ∩ . . . ∩ Qn ). SO (V, L∗ (Q)) is the desirable one. Deﬁnition 7.2. The (V, L∗ (Q)) is the completion of (V, Q) and is called the universal model of K. We should point out that the cardinal number of L∗ (Q)) is enormous; it is bounded by the Bell number Bn , where n is the cardinality of the smallest partition in L∗ (Q) [4].

8 Conclusions
1. A feature/attribute, from human view, is a characteristic or property of the universe (a set of entities). Traditional data processing takes such a view and use them to represent the universe (knowledge representation). 2. A feature/attribute, in data mining, is deﬁned and encoded by data. So a feature in association mining is a partition of the universe. Under such a view, we have shown that a set of inﬁnite many distinct human-viewfeatures (rotations of coordinate systems) is reduced to a single dataencoded-feature (Sect. 2.1). 3. Such views are shared by those techniques, such as classiﬁcation, that utilize only the relational table of symbols in their algorithms. The other techniques, such as clustering and neural network, that utilize additional background knowledge, do not share the same view. 4. In association mining, we have the following applications [10, 11]: All generalized associations can be generated by a ﬁnite set of integral linear inequalities within polynomial time. 5. Finally, we would like to note that by the isomorphism theorem, two isomorphic relations may have totally distinct semantics. So relations with additional structures that capture some semantics may be worthwhile to be explored; see [13, 15].

9 Appendix
9.1 General Isomorphism Attributes Ai and Aj are isomorphic if and only if there is a one-to-one and onto map, s : Dom(Ai ) −→ Dom(Aj ) such that Aj (v) = s(Ai (v)) ∀ v ∈ V . The map s is called an isomorphism. Intuitively, two attributes (columns) are isomorphic if and only if one column turns into another one by properly renaming its attribute values. Let K = (V, A) and H = (V, B) be two information tables, where A = {A1 , A2 , . . . An } and B = {B 1 , B 2 , . . . B m }. Then, K and H are said

76

T.Y. Lin

to be isomorphic if every Ai is isomorphic to some B j , and vice versa. The isomorphism of relations is reﬂexive, symmetric, and transitive, so it classiﬁes all relations into equivalence classes; we call them isomorphic classes. Deﬁnition 9.1.1. H is a simpliﬁed relational table of K, if H is isomorphic to K and only has non-isomorphic attributes. Theorem 9.1.2. Let H be the simpliﬁed relational table of K. Then the patterns (large itemsets) of K can be obtained from those of H by elementary operations that will be deﬁned below. To prove the Theorem, we will set up a lemma, in which we assume there are two isomorphic attributes B and B in K, that is, degree K – degree H = 1. Let s : Dom(B) −→ Dom(B ) be the isomorphism and b = s(b). Let H be the new table in which B has been removed. Lemma 9.1.3. The patterns of K can be generated from those of H by elementary operations, namely, 1. If b is a large itemset in H, then b’ and (b, b’) are large in K. 2. If (a. ., b, c. . . ) is a large itemset in H, then (a. ., b’, c. . . ) and (a. ., b, b’, c,. . . ) are large in K. 3. These are the only large itemsets in K. The validity of this lemma is rather straightforward; and it provides the critical inductive step for Theorem; we ill skip the proof. 9.2 Semantics Issues The two relations, Tables 7 and 8, are isomorphic, but their semantics are completely diﬀerent. One table is about part, the other is about suppliers. These two relations have Isomorphic association rules; 1. Length one: TEN, TWENTY, March, SJ, LA in Table 7 and
Table 7. An Relational Table K V v1 v2 v3 v4 v5 v6 v7 v8 v9 K (S# −→ −→ −→ −→ −→ −→ −→ −→ −→ (S1 (S2 (S3 (S4 (S5 (S6 (S7 (S8 (S9 Business Amount (in m.) TWENTY TEN TEN TEN TWENTY TWENTY TWENTY THIRTY THIRTY Birth CITY) Day MAR NY MAR SJ FEB NY FEB LA MAR SJ MAR SJ APR SJ JAN LA JAN LA

Features and Foundations Table 8. An Relational Table K V v1 v2 v3 v4 v5 v6 v7 v8 v9 K (S# Weight −→ −→ −→ −→ −→ −→ −→ −→ −→ (P1 (P2 (P3 (P4 (P5 (P6 (P7 (P8 (P9 20 10 10 10 20 20 20 30 30 Part Name SCREW SCREW NAIL NAIL SCREW SCREW PIN HAMMER HAMMER Material STEEL BRASS STEEL ALLOY BRASS BRASS BRASS ALLOY ALLOY

77

2. Length one: 10, 20, Screw, Brass, Alloy in Table 8 3. Length two: (TWENTY, MAR), (Mar, SJ), (TWENTY, SJ)in one Table 7, 4. Length two: (20, Screw), (screw, Brass), (20, Brass), Table 8 However, they have very non-isomorphic semantics: 1. Table 7: (TWENTY, SJ), that is, the business amount at San Jose is likely 20 millions; it is isomorphic to (20, Brass), which is not interesting. 2. Table 8: (SCREW, BRASS), that is, the screw is most likely made from Brass; it is isomorphic to (Mar, SJ), which is not interesting.

References
1. R. Agrawal, T. Imielinski, and A. Swami, “Mining Association Rules Between Sets of Items in Large Databases,” in Proceeding of ACM-SIGMOD international Conference on Management of Data, pp. 207–216, Washington, DC, June, 1993 64 2. A. Barr and E. A. Feigenbaum, The handbook of Artiﬁcial Intelligence, Willam Kaufmann 1981. 66 3. G. Birkhoﬀ and S. MacLane, A Survey of Modern Algebra, Macmillan, 1977 72 4. Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992. 75 5. C. J. Date, C. Date, An Introduction to Database Systems, 7th ed., AddisonWesley, 2000. 64 6. Margaret H. Dunham, Data Mining Introduction and Advanced Topics Prentice Hall, 2003, ISBN 0-13-088892-3 64 7. U. M. Fayad, G. Piatetsky-Sjapiro, and P. Smyth, “From Data Mining to Knowledge Discovery: An overview.” In Fayard, Piatetsky-Sjapiro, Smyth, and Uthurusamy eds., Knowledge Discovery in Databases, AAAI/MIT Press, 1996. 66 8. H Gracia-Molina, J. Ullman, and J. Windin, J, Database Systems The Complete Book, Prentice Hall, 2002. 64, 66, 68, 69 9. T. T. Lee, “Algebraic Theory of Relational Databases,” The Bell System Technical Journal Vol 62, No 10, December, 1983, pp. 3159–3204. 72

78

T.Y. Lin

10. T. Y. Lin, “A mathematical Theory of Association Mining” In: Foundation and Novel Approach in Data Mining (Lin & et al), Spriner-Verlag, 2005, to appear. 63, 75 11. T. Y. Lin, “Mining Associations by Solving Integral Linear Inequalities,” in: the Proceedings of International Conference on Data Mining, Breighton, England, Nov 1–4, 2004. 75 12. T. Y. Lin “Attribute (Feature) Completion – The Theory of Attributes from Data Mining Prospect,” in: the Proceedings of International Conference on Data Mining, Maebashi, Japan, Dec 9–12, 2002, pp. 282–289. 68 13. T. Y. Lin, “Data Mining and Machine Oriented Modeling: A Granular Computing Approach,” Journal of Applied Intelligence, Kluwer, Vol. 13, No 2, September/October, 2000, pp. 113–124. 71, 72, 75 14. T. Y. Lin and M. Hadjimichael, “Non-Classiﬁcatory Generalization in Data Mining,” in Proceedings of the 4th Workshop on Rough Sets, Fuzzy Sets, and Machine Discovery, November 6–8, Tokyo, Japan, 1996, 404–411. 15. E. Louie, T. Y. Lin, “Semantics Oriented Association Rules,” In: 2002 World Congress of Computational Intelligence, Honolulu, Hawaii, May 12–17, 2002, 956–961 (paper # 5702) 75 16. E. Louie and T. Y. Lin, “Finding Association Rules using Fast Bit Computation: Machine-Oriented Modeling,” in: Foundations of Intelligent Systems, Z. Ras and S. Ohsuga (eds), Lecture Notes in Artiﬁcial Intelligence 1932, Springer-Verlag, 2000, pp. 486–494. (ISMIS’00, Charlotte, NC, Oct 11–14, 2000) 17. H. Liu and H. Motoda, “Feature Transformation and Subset Selection,” IEEE Intelligent Systems, Vol. 13, No. 2, March/April, pp. 26–28 (1998) 64 18. Hiroshi Motoda and Huan Liu “Feature Selection, Extraction and Construction,” Communication of IICM (Institute of Information and Computing Machinery, Taiwan) Vol 5, No. 2, May 2002, pp. 67–72. (proceeding for the workshop “Toward the Foundation on Data Mining” in PAKDD2002, May 6, 2002. 73 19. Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 71, 72 20. Z. Pawlak, Rough sets. International Journal of Information and Computer Science 11, 1982, pp. 341–356. 21. J. Ullman, Principles of Database and Knowledge-Base Systes, Vol 1, II, 1988, 1989, Computer Science Press. 71

A New Theoretical Framework for K-Means-Type Clustering
J. Peng and Y. Xia
Advanced optimization Lab, Department of Computing and Software McMaster University, Hamilton, Ontario L8S 4K1, Canada. pengj@mcmaster.ca, xiay@optlab.mcmaster.ca Summary. One of the fundamental clustering problems is to assign n points into k clusters based on the minimal sum-of-squares(MSSC), which is known to be NPhard. In this paper, by using matrix arguments, we ﬁrst model MSSC as a socalled 0–1 semideﬁnite programming (SDP). The classical K-means algorithm can be interpreted as a special heuristics for the underlying 0–1 SDP. Moreover, the 0–1 SDP model can be further approximated by the relaxed and polynomially solvable linear and semideﬁnite programming. This opens new avenues for solving MSSC. The 0–1 SDP model can be applied not only to MSSC, but also to other scenarios of clustering as well. In particular, we show that the recently proposed normalized kcut and spectral clustering can also be embedded into the 0–1 SDP model in various kernel spaces.

1 Introduction
Clustering is one of major issues in data mining and machine learning with many applications arising from diﬀerent disciplines including text retrieval, pattern recognition and web mining[12, 15]. Roughly speaking, clustering involves partition a given data set into subsets based on the closeness or similarity among the data. Typically, the similarities among entities in a data set are measured by a speciﬁc proximity function, which can be make precise in many ways. This results in many clustering problems and algorithms as well. Most clustering algorithms belong to two classes: hierarchical clustering and partitioning. The hierarchical approach produces a nested series of partitions consisting of clusters either disjoint or included one into the other.Those

The research of the ﬁrst author was partially supported by the grant # RPG 249635-02 of the National Sciences and Engineering Research Council of Canada (NSERC) and a PREA award. This research was also Supported by the MITACS project “New Interior Point Methods and Software for Convex Conic-Linear Optimization and Their Application to Solve VLSI Circuit Layout Problems”.
J. Peng and Y. Xia: A New Theoretical Framework for K-Means-Type Clustering, StudFuzz 180, 79–96 (2005) c Springer-Verlag Berlin Heidelberg 2005 www.springerlink.com

80

J. Peng and Y. Xia

clustering algorithms are either agglomerative or divisive. An agglomerative clustering algorithm starts with every singleton entity as a cluster, and then proceeds by successively merging clusters until a stopping criterion is reached. A divisive approach starts with an initial cluster with all the entities in it, and then performs splitting until a stopping criterion is reached. In hierarchical clustering, an objective function is used locally as the merging or splitting criterion. In general, hierarchical algorithms can not provide optimal partitions for their criterion. In contrast, partitional methods assume given the number of clusters to be found and then look for the optimal partition based on the object function. Partitional methods produce only one partition. Most partitional methods can be further classiﬁed as deterministic or stochastic, depending on whether the traditional optimization technique or a random search of the state space is used in the process. There are several diﬀerent ways to separate various clustering algorithms, for a comprehensive introduction to the topic, we refer to the book [12, 15], and for more recent results, see survey papers [4] and [13]. Among various criterion in clustering, the minimum sum of squared Euclidean distance from each entity to its assigned cluster center is the most intuitive and broadly used. Both hierarchical and partitional procedures for MSSC have been investigated. For example, Ward’s [27] agglomerative approach for MSSC has a complexity of O(n2 log n) where n is the number of entities. The divisive hierarchical approach is more diﬃcult. In [9], the authors provided an algorithm running in O(nd+1 log n) time, where d is the dimension of the space to which the entities belong. However, in many applications, assuming a hierarchical structure in partitioning based on MSSC is unpractical. In such a circumstance, the partitional approach directly minimizing the sum of squares distance is more applaudable. The traditional way to deal with this problem is to use some heuristics such as the well-known K-means [18]. To describe the algorithm, let us go into a bit more details. Given a set S of n points in a d-dimensional Euclidean space, denoted by S = {si = (si1 , . . . , sid ) ∈ Rd
T

i = 1, . . . , n}

the task of a partitional MSSC is to ﬁnd an assignment of the n points into k disjoint clusters S = (S1 , . . . , Sk ) centered at cluster centers cj (j = 1, . . . , k) based on the total sum-of-squared Euclidean distances from each point si to its assigned cluster centroid ci , i.e.,
k |Sj |

f (S, S) =
j=1 i=1

si − cj
(j)

(j)

2

,

where |Sj | is the number of points in Sj , and si is the ith point in Sj . Note that if the cluster centers are known, then the function f (S, S) achieves its minimum when each point is assigned to its closest cluster center. Therefore,

A New Theoretical Framework for K-Means-Type Clustering

81

MSSC can be described by the following bilevel programming problem (see for instance [2, 19]).
n c1 ,...,ck

min

min{ si − c1
i=1

2

, . . . , si − ck

2

}.

(1)

Geometrically speaking, assigning each point to the nearest center ﬁts into a framework called Voronoi Program, and the resulting partition is named Voronoi Partition. On the other hand, if the points in cluster Sj are ﬁxed, then the function
|Sj |

f (Sj , Sj ) =
i=1

si − cj
|Sj |

(j)

2

is minimal when cj = 1 |Sj |

si
i=1

(j)

.

The classical K-means algorithm [18], based on the above two observations, is described as follows: K-means clustering algorithm (1) Choose k cluster centers randomly generated in a domain containing all the points, (2) Assign each point to the closest cluster center, (3) Recompute the cluster centers using the current cluster memberships, (4) If a convergence criterion is met, stop; Otherwise go to step 2.
n×k

Another way to model MSSC is based on the assignment. Let X = [xij ] ∈ be the assignment matrix deﬁned by xij = 1 0 If si is assigned to Sj ; Otherwise .

As a consequence, the cluster center of the cluster Sj , as the mean of all the points in the cluster, is deﬁned by cj =
n l=1 xlj sl n l=1 xlj

.

Using this fact, we can represent (1) as

82

J. Peng and Y. Xia
k n

min
xij j=1 i=1 k

xij si −

n l=1 xlj sl n l=1 xlj

2

(2)

S.T.
j=1 n

xij = 1 (i = 1, . . . , n) xij ≥ 1 (j = 1, . . . , k)
i=1

(3)

(4) (5)

xij ∈ {0, 1} (i = 1, . . . , n; j = 1, . . . , k)

The constraint (3) ensures that each point si is assigned to one and only one cluster, and (4) ensures that there are exactly k clusters. This is a mixed integer programming with nonlinear objective [8], which is NP-hard. The difﬁculty of the problem consists of two parts. First, the constraints are discrete. Secondly the objective is nonlinear and nonconvex. Both the diﬃculties in the objective as well as in the constraints make MSSC extremely hard to solve. Many diﬀerent approaches have been proposed for attacking (2) both in the communities of machine learning and optimization [1, 3, 8]. Most methods for (2) are heuristics that can locate only a good local solution, not the exact global solution for (2). Only a few works are dedicated to the exact algorithm for (2) as listed in the references of [3]. Approximation methods provide a useful approach for (2). There are several diﬀerent ways to approximate (2). For example, by solving the so-called K-medians problem we can obtain a 2-approximately optimal solution for (2) in O(nd+1 ) time [10]. In [22], Mutousek proposed a geometric approximation method that can ﬁnd an (1 + ) approximately optimal solution for (2) in O(n logk n) time, where the constant hidden in the big-O notation depends polynomially on . Another eﬃcient way of approximation is to attack the original problem (typically NP-hard) by solving a relaxed polynomially solvable problem. This has been well studied in the ﬁeld of optimization, in particular, in the areas of combinatorial optimization and semideﬁnite programming [5]. We noted that recently, Xing and Jordan [29] considered the SDP relaxation for the so-called normalized k-cut spectral clustering. In the present paper, we focus on developing approximation methods for (2) based on linear and semideﬁnite programming (LP/SDP) relaxation. A crucial step in relaxing (2) is to rewrite the objective in (2) as a simple convex function of matrix argument that can be tackled easily, while the constraint set still enjoy certain geometric properties. This was possibly ﬁrst suggested in [6] where the authors owed the idea to an anonymous referee. However, the authors of [6] did not explore the idea in depth to design any usable algorithm. A similar eﬀort was made in [30] where the authors rewrote the objective in (2) as a convex quadratic function in which the argument is a n × k orthonormal matrix. Our model follows the same stream as in [6, 30]. However, diﬀerent from the approach [30] where the authors used only a quadratic objective and simple

A New Theoretical Framework for K-Means-Type Clustering

83

spectral relaxation, we elaborate more on how to characterize (2) exactly by means of matrix arguments. In particular, we show that MSSC can be modelled as the so-called 0–1 semideﬁnite programming (SDP), which can be further relaxed to polynomially solvable linear programming (LP) and SDP. Several diﬀerent relaxation forms are discussed. We also show that variants of K-means can be viewed as heuristics for the underlying 0–1 SDP. Our model provides novel avenues not only for solving MSSC, but also for solving clustering problems based on some other criterions. For example, the clustering based on normalized cuts can also be embedded into our model. Moreover, our investigation reveals some interesting links between the well-known K-means and some recently proposed algorithms like spectral clustering. The paper is organized as follows. In Sect. 2, we show that MSSC can be modelled as 0–1 SDP, which allows convex relaxation such as SDP and LP. In Sect. 3, we discuss algorithms and challenges for solving our 0–1 SDP model. Section 4 devotes to the discussion on the links between our model and some other recent models for clustering. Finally we close the paper by few concluding remarks.

2 Equivalence of MSSC to 0–1 SDP
In this section, we establish the equivalence between MSSC and 0–1 SDP. We start with a brief introduction to SDP and 0–1 SDP. In general, SDP refers to the problem of minimizing (or maximizing) a linear function over the intersection of a polyhedron and the cone of symmetric and positive semideﬁnite matrices. The canonical SDP takes the following form   min Tr(WZ) (SDP) S.T. Tr(Bi Z) = bi for i = 1, . . . , m  Z 0 Here Tr(.) denotes the trace of the matrix, and Z 0 means that Z is positive semideﬁnite. If we replace the constraint Z 0 by the requirement that Z 2 = Z, then we end up with the following problem   min Tr(WZ) (0–1 SDP) S.T. Tr(Bi Z) = bi for i = 1, . . . , m  Z 2 = Z, Z = Z T We call it 0–1 SDP owing to the similarity of the constraint Z 2 = Z to the classical 0–1 requirement in integer programming. We next show that MSSC can be modelled as 0–1 SDP. By rearranging the items in the objective of (2), we have

84

J. Peng and Y. Xia
n

 si
2

f (S, S) =
i=1



k

 xij  −
k

k

j=1

j=1

2 n i=1 xij si n i=1 xij

(6)

T = Tr WS WS − j=1

2 n i=1 xij si , n i=1 xij

where WS ∈ n×d denotes the matrix whose ith row is the vector si . Since X is an assignment matrix, we have
n n n n

X T X = diag
i=1

x2 , . . . , i1
i=1

x2 ik

= diag
i=1

xi1 , . . . ,
i=1

xik

.

Let

Z := [zij ] = X(X T X)−1 X T ,

T T T we can write (6) as Tr WS WS (I − Z) = Tr WS WS − Tr WS WS Z . Obvi2 ously Z is a projection matrix satisfying Z = Z with nonnegative elements. For any integer m, let em be the all one vector in m . We can write the constraint (3) as Xek = en .

It follows immediately Zen = ZXek = Xek = en . Moreover, the trace of Z should equal to k, the number of clusters, i.e., Tr(Z ) = k . Therefore, we have the following 0–1 SDP model for MSSC
T min Tr WS WS (I − Z) Ze = e, Tr(Z ) = k , Z ≥ 0, Z = Z T , Z 2 = Z.

(7)

We ﬁrst give a technical result about positive semideﬁnite matrix that will be used in our later analysis. Lemma 1. For any symmetric positive semideﬁnite matrix Z ∈ exists an index i0 ∈ {1, . . . , n} such that Zi0 i0 = max Zij .
i,j n×n

, there

Proof. For any positive semideﬁnite matrix Z, it is easy to see that Zii ≥ 0, i = 1, . . . , n .

A New Theoretical Framework for K-Means-Type Clustering

85

Suppose the statement of the lemma does not hold, i.e., there exists i0 = j0 such that Zi0 j0 = max Zij > 0 .
i,j

Then the submatrix Zi0 i0 Zj0 i0 Zi0 j0 Zj0 j0

is not positive semideﬁnite. This contradicts to the assumptuion in the lemma. Now we are ready to establish the equivalence between the models (7) and (2). Theorem 2.1. Solving the 0–1 SDP problem (7) is equivalent to ﬁnding a global solution of the integer programming problem (2). Proof. From the construction of the 0–1 SDP model (7), we know that one can easily construct a feasible solution for (7) from a feasible solution of (2). Therefore, it remains to show that from a global solution of (7), we can obtain a feasible solution of (2). Suppose that Z is a global minimum of (7). Obviously Z is positive semideﬁnite. From Lemma 1 we conclude that there exists an index i1 such that Zi1 i1 = max{Zij : 1 ≤ i, j ≤ n} > 0 . Let us deﬁne the index set I1 = {j : Zi1 j > 0} . Since Z 2 = Z, we have (Zi1 j )2 = Zi1 i1 ,
j∈I1

which implies
j∈I1

Zi1 j Zi j = 1 . Zi1 i1 1

From the choice of i1 and the constraint
n

Zi1 j =
j=1 j∈I1

Zi1 j = 1 ,

we can conclude that Zi1 j = Zi1 i1 , ∀j ∈ I1 . This further implies that the submatrix ZI1 I1 is a matrix whose elements are all equivalent, and we can decompose the matrix Z into a bock matrix with the following structure

86

J. Peng and Y. Xia

Z=

ZJ1 J1 0

0 ZI1 I1 ¯ ¯

,
2

(8)

¯ where I1 = {i : i ∈ I1 }. Since i∈I1 Zii = 1 and (ZI1 I1 ) = ZI1 I1 , we can consider the reduced 0–1 SDP as follows
T min Tr WS WS ¯ ¯ (I I1 I1

− Z)I1 I1 ¯ ¯

(9)

ZI1 I1 e = e, Tr ZI1 I1 = k − 1 , ¯ ¯ ¯ ¯
2 ZI1 I1 ≥ 0, ZI1 I1 = ZI1 I1 . ¯ ¯ ¯ ¯ ¯ ¯

Repeating the above process, we can show that if Z is a global minimum of the 0–1 SDP, then it can be decomposed into a diagonal block matrix as Z = diag (ZI1 I1 , . . . , ZIk Ik ) , where each block matrix ZIl Il is a nonnegative projection matrix whose elements are equal, and the sum of each column or each row equals to 1. Now let us deﬁne the assignment matrix X ∈ n×k Xij = 1 0 if i ∈ Ij otherwise

One can easily verify that Z = X(X T X)−1 X T . Our above discussion illustrates that from a feasible solution of (7), we can obtain an assignment matrix that satisﬁes the condition in (2). This ﬁnishes the proof of the theorem. By comparing (7) with (2), we ﬁnd that the objective in (7) is linear, while the constraint in (7) is still nonlinear, even more complex than the 0–1 constraint in (2). The most diﬃcult part in the constraint of (7) is the requirement that Z 2 = Z. Several diﬀerent ways for solving (7) will be discussed in the next section.

3 Algorithms for Solving 0–1 SDP
In this section, we focus on various algorithms for solving the 0–1 SDP model (7). From a viewpoint of the algorithm design, we can separate these algorithms into two groups. The ﬁrst group consists of the so-called feasible iterative algorithms, while the second group contains approximation algorithms (might be infeasible at some stage in the process) based on relaxation. It is worthwhile pointing out that our discussion will focus on the design of the algorithm as well as the links among various techniques, not on the implementation details of the algorithm and numerical testing. 3.1 Feasible Iterative Algorithms We ﬁrst discuss the so-called feasible iterative algorithms in which all the iterates are feasible regarding the constraints in (7), while the objective is reduced step by step until some termination criterion is reached. A general procedure for feasible iterative algorithms can be described as follows:

A New Theoretical Framework for K-Means-Type Clustering

87

Feasible Iterative Algorithm Step 1: Choose a starting matrix Z 0 satisfying all the constraints in (7), Step 2: Use a heuristics to update the matrix Z k such that the value of the objective function in (7) is decreased, Step 3: Check the termination criterion. If the criterion is reached, then stop; Otherwise go to Step 2. We point out that the classical K-means algorithm described in the introduction can be interpreted as a special feasible iterative scheme for attacking (7). To see this, let us recall our discussion on the equivalence between MSSC and (7), one can verify that, at each iterate, all the constraints in (7) are satisﬁed by the matrix transformed from the K-means algorithm. It is also easy to see that, many variants of the K-means algorithm such as the variants proposed in [11, 14], can also be interpreted as speciﬁc iterative schemes for (7). 3.2 Approximation Algorithms Based on LP/SDP Relaxations In the section we discuss the algorithms in the second group that are based on LP/SDP relaxation. We starts with a general procedure for those algorithm. Approximation Algorithm Based on Relaxation Step 1: Choose a relaxation model for (7), Step 2: Solve the relaxed problem for an approximate solution, Step 3: Use a rounding procedure to extract a feasible solution to (7) from the approximate solution. The relaxation step has an important role in the whole algorithm. For example, if the approximation solution obtained from Step 2 is feasible for (7), then it is exactly an optimal solution of (7). On the other hand, when the approximation solution is not feasible regarding (7), we have to use a rounding procedure to extract a feasible solution. In what follows we discuss how to design a rounding procedure. First, we note that when Z ∗ is a solution of (7), it can be shown that the matrix Z ∗ WS contains k diﬀerent rows, and each of these k diﬀerent rows represents one center in the ﬁnal clusters. A good approximate solution, although it might not be feasible for (7), should give us some indications on how to locate a feasible solution. Motivated by the above-mentioned observation, we can cast the rows of the matrix ZWS as a candidate set for the potential approximate centers in the ﬁnal clustering. This leads to the following rounding procedure.

88

J. Peng and Y. Xia

A Rounding Procedure Step 0: Input: an approximate solution Z and the matrix WS , Step 1: Select k rows from the rows of the matrix ZWS 2 as the initial centers, Step 2: Apply the classical K-means to the original MSSC using the selected initial centers. We mention that in [29], Xing and Jordan proposed a rounding procedure based on the singular value decomposition Z = U T U of Z. In their approach, Xing and Jordan ﬁrst cast the rows of U T as points in the space, and then they employed the classical K-means to cluster those points. Another way for extracting a feasible solution is to utilize branch and cut. In order to use branch and cut, we recall the fact that any feasible solution Z of (7) satisﬁes the following condition Zij (Zij − Zii ) = 0, i, j = 1, . . . , n .

If an approximate solution meets the above requirement, then it is a feasible solution and thus an optimal solution to (7). Otherwise, suppose that there exist indices i, j such that Zij (Zii − Zij ) = 0 , then we can add cut Zii = Zij or Zij = 0 to get two subproblems. By combining such a branch-cut procedure with our linear relaxation model, we can ﬁnd the exact solution to (7) in ﬁnite time, as the number of diﬀerent branches is 2 at most 2n . To summarize, as shown in our above discussion, ﬁnding a good approximation (or a nice relaxation) is essential for the success of approximation algorithms. This will be the main focus in the following subsections. Relaxations Based on SDP In this subsection, we describe few SDP-based relaxations for (7). First we recall that in (7), the argument Z is stipulated to be a projection matrix, i.e., Z 2 = Z, which implies that the matrix Z is a positive semideﬁnite matrix whose eigenvalues are either 0 or 1. A straightforward relaxation to (7) is replacing the requirement Z 2 = Z by the relaxed condition I Z 0.

Note that in (7), we further stipulate that all the entries of Z are nonnegative, and the sum of each row(or each column) of Z equals to 1. This means the eigenvalues of Z is always less than 1. In this circumstance, the constraint
2 For example, we can select k rows from ZWS based on the frequency of the row in the matrix, or arbitrarily select k centers.

A New Theoretical Framework for K-Means-Type Clustering

89

Z I becomes superﬂuous and can be waived. Therefore, we obtain the following SDP relaxation for MSSC
T min Tr WS WS (I − Z) Ze = e, Tr(Z ) = k , Z ≥ 0, Z 0.

(10)

The above problem is feasible and bounded below. We can apply many existing optimization solvers such as interior-point methods to solve (10). It is known that an approximate solution to (10) can be found in polynomial time. We noted that in [29], the model (10) with a slightly diﬀerent linear constraint3 was used as a relaxation to the so-called normalized k-cut clustering. As we shall show in Sect. 4, the model (10) can always provides better approximation to (7) than the spectral clustering. This was also observed and pointed out by Xing and Jordan [29]. However, we would like to point out here that although there exist theoretically polynomial algorithm for solving (10), most of the present optimization solvers are unable to handle the problem in large size eﬃciently. Another interesting relaxation to (7) is to further relax (10) by dropping some constraints. For example, if we remove the nonnegative requirement on the elements of Z, then we obtain the following simple SDP problem
T min Tr WS WS (I − Z) Ze = e, Tr(Z ) = k ,

(11)

I

Z

0.

The above problem can be equivalently stated as
T max Tr WS WS Z Ze = e, Tr(Z ) = k ,

(12)

I

Z

0.

In the sequel we discuss how to solve (12). Note that if Z is a feasible solution for (12), then we have 1 1 √ Ze = √ e , n n
1 which implies √n e is an eigenvector of Z with eigenvalue 1. Therefore, we can write any feasible solution of (12) Z as

Z = QΓ QT + where Q ∈
3

1 T ee , n

n×(n−1)

is a matrix satisfying the condition:

In [29], the constraint Ze = e in (7) is replaced by Zd = d where d is a positive scaling vector associated with the aﬃnity matrix, and the constraint Z I can not be waived.

90

J. Peng and Y. Xia
1 √ e] n

C.1 The matrix [Q :

is orthogonal,

and Γ = diag (γ1 , . . . , γn−1 ) is a nonnegative diagonal matrix. It follows k − 1 = Tr(Z) − 1 = Tr QΓQT = Tr QT QΓ = Tr(Γ) . Therefore, we can reduce (12) to
T T max Tr WS WS QΓQT = Tr QT WS WS QΓ

(13)

Tr(Γ) = k − 1 , In−1 Γ 0 .
T T Let λ1 (QT WS WS Q), . . . , λn−1 (QT WS WS Q) be the eigenvalues of the matrix T T Q WS WS Q listed in the order of decreasing values. The optimal solution of (13) can be achieved if and only if k−1

Tr Q

T

T WS WS QΓ

=
i=1

T λi QT WS WS Q .

Note that for any matrix Q satisfying Condition C.1, the summation of the T ﬁrst k − 1 largest eigenvalues of the matrix QT WS WS Q are independent of the choice of Q. This gives us an easy way to solve (13) and correspondingly (12). The algorithmic scheme for solving (12) can be described as follows: Algorithm Step 1: Choose a matrix Q satisfying C.1, Step 2: Use singular value decomposition method to compute the ﬁrst k − 1 T largest eigenvalues of the matrix QT WS WS Q and their corresponding eigenvectors v1 , . . . , vk−1 , Step 3: Set k−1 1 (Qvi )T Qvi . Z = eT e + n i=1 It should be mentioned that if k = 2, then Step 2 in the above algorithm T uses the eigenvector corresponding to the largest eigenvalue of QT WS WS Q. This eigenvector has an important role in Shi and Malik’ work [25] (See also [28]) for image segmentation where the clustering problem with k = 2 was discussed. LP Relaxation In this subsection, we propose an LP relaxation for (7). First we observe that if si and sj , sj and sk belong to the same clusters, then si and sk belong to

A New Theoretical Framework for K-Means-Type Clustering

91

the same cluster. In such a circumstance, from the deﬁnition of the matrix Z we can conclude that Zij = Zjk = Zik = Zii = Zjj = Zkk . Such a relationship can be partially characterized by the following inequality Zij + Zik ≤ Zii + Zjk . Correspondingly, we can deﬁne a metric polyhedron MET4 by MET = {Z = [zij ] : zij ≤ zii , zij + zik ≤ zii + zjk } .

Therefore, we have the following new model
T min Tr WS WS (I − Z)

(14)

Ze = e, Tr(Z ) = k , Z≥0, Z ∈ MET . If the optimal solution of (14) is not a feasible solution of (7), then we need to refer to the rounding procedure that we described earlier to extract a feasible solution for (7). Solving (14) directly for large-size data set is clearly unpractical due to the huge amount (O(n3 )) of constraints. In what follows we report some preliminary numerical results for small-size data set. Our implementation is done on an IBM RS-6000 workstation and the package CPLEX 7.1 with AMPL interface is used to solve the LP model (14). The ﬁrst data set we use to test our algorithm is the Soybean data (small) from the UCI Machine Learning Repository5 , see also [21]. This data set has 47 instances and each instance has 35 normalized attributes. It is known this data set has 4 clusters. As shown by the following table, for k from 2 to 4, we found the exact clusters by solving (14). K 2 3 4 The Soybean data Objective CPU Time(s) 404.4593 4.26 215.2593 1.51 205.9637 1.68

The second test set is the Ruspini data set from [24]. This data set, consisting of 75 points in 2 with four groups, is popular for illustrating clustering techniques [15]. The numerical result is listed as follows:
A similar polyhedron MET had been used by Karisch and Rendl, Leisser and Rendl in their works [16, 17] on graph partitioning. We changed slightly the deﬁnition of MET in [17] to adapt to our problem. 5 http://www.ics.uci.edu/ mlearn/MLRepository.html
4

92

J. Peng and Y. Xia

The Ruspini’s data K Objective CPU Time(s) 2 893380 27.81 3 510630 66.58 4 12881 7.22 5 10127 9.47 We observed that in our experiments, for all cases k = 2, . . . , 5, the solution of (14) is not feasible for (7). However, the resulting matrix is quite close to a feasible solution of (7). Therefore, we use the classical K-means to get the ﬁnal clusters. After a few iterations, the algorithm terminated and reported the numerical results that match the best known results in the literature for the same problem. The third test set is the Sp¨th’s postal zones data [26]. This data set cona tains 89 entities and each entity has 3 features. Correspondingly, we transform all the entities into points in 3 . It is known that the data set has 7 groups. In all cases that k runs from 2 to 9, we were able to ﬁnd the exact solution of (7) via solving (14). The Spath’s Postal Zone data K Objective CPU Time(s) 2 6.0255 ∗ 1011 283.26 3 2.9451 ∗ 1011 418.07 4 1.0447 ∗ 1011 99.54 5 5.9761 ∗ 1010 60.67 6 3.5908 ∗ 1010 52.55 7 2.1983 ∗ 1010 61.78 8 1.3385 ∗ 1010 26.91 9 7.8044 ∗ 109 18.04 It is worthwhile mentioning that, as shown in the tables, the running time of the algorithm does not increase as the cluster number k increases. Actually, from a theoretical viewpoint, the complexity of the algorithm for solving (14) is independent of k. This indicates our algorithm is scalable to large data set, while how to solve (14) eﬃciently still remains a challenge. In contrast, the complexity of the approximation algorithms in [22] increases with respect to k.

4 Relations to Other Clustering Methods
In the previous sections, we proposed and analyzed the 0–1 SDP model for MSSC. In this section, we consider the more general 0–1 SDP model for clustering

